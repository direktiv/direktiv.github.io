{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Direktiv is a cloud-agnostic, serverless flow engine that capitalizes on microservices, containers and custom code to create efficient processes. Using Kubernetes and Knative under the hood, this platform empowers you with the scalability of modern cloud computing. Direktiv provides a set of YAML definitions to define how the data should be processed, allowing developers to quickly and efficiently write their own business logic. Integrate Direktiv's event-driven system and container approach make it simple to link different systems. As a broker for multiple backends, Direktiv provides error handling, retries, logging and tracing capabilities to ensure seamless integration as well as greater visibility into the processes. Orchestrate Direktiv simplifies the orchestration of APIs to create higher-level services that can be used by any organization, either externally or internally. This is made possible through a YAML-based flow description system which makes it easy and fast to customize flows as well as expand capabilities. Automate Streamline repetitive duties within your team or organization by e.g. moving scripts and playbooks into an easily accessible platform. Whether you need to automate Continuous Integration, Infrastructure Management, Onboarding tasks or something else that is currently done manually - automating these processes can be a major advantage. Use a single platform which multiple users in your team have access to. See Also The Getting Started Guide . The direktiv.io website. The direktiv.io repository. The Godoc library documentation.","title":"Quickstart"},{"location":"#_1","text":"Direktiv is a cloud-agnostic, serverless flow engine that capitalizes on microservices, containers and custom code to create efficient processes. Using Kubernetes and Knative under the hood, this platform empowers you with the scalability of modern cloud computing. Direktiv provides a set of YAML definitions to define how the data should be processed, allowing developers to quickly and efficiently write their own business logic. Integrate Direktiv's event-driven system and container approach make it simple to link different systems. As a broker for multiple backends, Direktiv provides error handling, retries, logging and tracing capabilities to ensure seamless integration as well as greater visibility into the processes. Orchestrate Direktiv simplifies the orchestration of APIs to create higher-level services that can be used by any organization, either externally or internally. This is made possible through a YAML-based flow description system which makes it easy and fast to customize flows as well as expand capabilities. Automate Streamline repetitive duties within your team or organization by e.g. moving scripts and playbooks into an easily accessible platform. Whether you need to automate Continuous Integration, Infrastructure Management, Onboarding tasks or something else that is currently done manually - automating these processes can be a major advantage. Use a single platform which multiple users in your team have access to.","title":""},{"location":"#see-also","text":"The Getting Started Guide . The direktiv.io website. The direktiv.io repository. The Godoc library documentation.","title":"See Also"},{"location":"api/","text":"Direktiv API Direktiv Open API Specification Direktiv Documentation can be found at https://docs.direktiv.io/ Informations Version 1.0.0 Contact info@direktiv.io Content negotiation URI Schemes http https Consumes application/json text/plain Produces application/json text/event-stream Access control Security Schemes direktiv-token (header: KEY) Type : apikey Security Requirements direktiv-token All endpoints cloud_event_filter Method URI Name Summary POST /api/namespaces/{namespace}/eventfilter/{filtername} create cloudevent filter Creates new cloudEventFilter DELETE /api/namespaces/{namespace}/eventfilter/{filtername} delete cloudevent filter Delete existing cloudEventFilter GET /api/namespaces/{namespace}/eventfilter/{filtername} get cloud event filter Get specific cloudEventFilter GET /api/namespaces/{namespace}/eventfilter list cloudevent filter List existing cloudEventFilters PATCH /api/namespaces/{namespace}/eventfilter/{filtername} update cloudevent filter Update existing cloudEventFilter directory Method URI Name Summary PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory create directory Create a Directory events Method URI Name Summary GET /api/namespaces/{namespace}/events get event history Get events history. GET /api/namespaces/{namespace}/event-listeners get event listeners Get current event listeners. instances Method URI Name Summary POST /api/namespaces/{namespace}/instances/{instance}/cancel cancel instance Cancel a Pending Instance GET /api/namespaces/{namespace}/instances/{instance} get instance Get a Instance GET /api/namespaces/{namespace}/instances/{instance}/input get instance input Get a Instance Input GET /api/namespaces/{namespace}/instances get instance list Get List Instances GET /api/namespaces/{namespace}/instances/{instance}/metadata get instance metadata Get a Instance Metadata GET /api/namespaces/{namespace}/instances/{instance}/output get instance output Get a Instance Output logs Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=logs get workflow logs Get Workflow Level Logs GET /api/namespaces/{namespace}/instances/{instance}/logs instance logs Gets Instance Logs GET /api/namespaces/{namespace}/logs namespace logs Gets Namespace Level Logs GET /api/logs server logs Get Direktiv Server Logs metrics Method URI Name Summary GET /api/namespaces/{namespace}/metrics/failed namespace metrics failed Gets Namespace Failed Workflow Instances Metrics GET /api/namespaces/{namespace}/metrics/invoked namespace metrics invoked Gets Namespace Invoked Workflow Metrics GET /api/namespaces/{namespace}/metrics/milliseconds namespace metrics milliseconds Gets Namespace Workflow Timing Metrics GET /api/namespaces/{namespace}/metrics/successful namespace metrics successful Gets Namespace Successful Workflow Instances Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked workflow metrics invoked Gets Invoked Workflow Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed workflow metrics milliseconds Gets Workflow Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-sankey workflow metrics sankey Get Sankey metrics of a workflow revision. GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds workflow metrics state milliseconds Gets a Workflow State Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful workflow metrics successful Gets Successful Workflow Metrics namespace_services Method URI Name Summary POST /api/functions/namespaces/{namespace} create namespace service Create Namespace Service DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} delete namespace revision Delete Namespace Service Revision DELETE /api/functions/namespaces/{namespace}/function/{serviceName} delete namespace service Delete Namespace Service GET /api/functions/namespaces/{namespace}/function/{serviceName} get namespace service Get Namespace Service Details GET /api/functions/namespaces/{namespace} get namespace service list Get Namespace Services List GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods list namespace service revision pods Get Namespace Service Revision Pods List POST /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service Create Namespace Service Revision GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} watch namespace service revision Watch Namespace Service Revision GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions watch namespace service revision list Watch Namespace Service Revision List namespaces Method URI Name Summary PUT /api/namespaces/{namespace} create namespace Creates a namespace DELETE /api/namespaces/{namespace} delete namespace Delete a namespace GET /api/namespaces/{namespace}/config get namespace config Gets a namespace config GET /api/namespaces get namespaces Gets the list of namespaces PATCH /api/namespaces/{namespace}/config set namespace config Sets a namespace config node Method URI Name Summary DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node delete node Delete a node GET /api/namespaces/{namespace}/tree/{nodePath} get nodes Get List of Namespace Nodes operations Method URI Name Summary POST /api/functions/registries/test test registry Test a registry to make sure the connection is okay other Method URI Name Summary POST /api/namespaces/{namespace}/broadcast broadcast cloudevent Broadcast Cloud Event POST /api/namespaces/{namespace}/broadcast/{filtername} broadcast cloudevent filter Filter given cloud event and broadcast it POST /api/jq jq playground JQ Playground api to test jq queries POST /api/namespaces/{namespace}/events/{event}/replay replay cloudevent Replay Cloud Event GET /api/version version Returns version information for servers in the cluster. pod Method URI Name Summary GET /api/logs/{pod} pod logs Watch Pod Logs registries Method URI Name Summary POST /api/functions/registries/namespaces/{namespace} create registry Create a Namespace Container Registry DELETE /api/functions/registries/namespaces/{namespace} delete registry Delete a Namespace Container Registry GET /api/functions/registries/namespaces/{namespace} get registries Get List of Namespace Registries secrets Method URI Name Summary PUT /api/namespaces/{namespace}/secrets/{folder} create folder Create a Namespace Folder PUT /api/namespaces/{namespace}/secrets/{secret} create secret Create a Namespace Secret DELETE /api/namespaces/{namespace}/secrets/{folder} delete folder Delete a Namespace Folder DELETE /api/namespaces/{namespace}/secrets/{secret} delete secret Delete a Namespace Secret GET /api/namespaces/{namespace}/secrets get secrets Get List of Namespace Secrets or Search for Namespace Secrets by given name GET /api/namespaces/{namespace}/secrets/{folder} get secrets inside folder Get List of Namespace nodes inside Folder PATCH /api/namespaces/{namespace}/secrets/{secret} overwrite and search secret Overwrite a Namespace Secret variables Method URI Name Summary DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} delete instance variable Delete a Instance Variable DELETE /api/namespaces/{namespace}/vars/{variable} delete namespace variable Delete a Namespace Variable DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var delete workflow variable Delete a Workflow Variable GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} get instance variable Get a Instance Variable GET /api/namespaces/{namespace}/instances/{instance}/vars get instance variables Get List of Instance Variable GET /api/namespaces/{namespace}/vars/{variable} get namespace variable Get a Namespace Variable GET /api/namespaces/{namespace}/vars get namespace variables Get Namespace Variable List GET /api/namespaces/{namespace}/tree/{workflow}?op=var get workflow variable Get a Workflow Variable GET /api/namespaces/{namespace}/tree/{workflow}?op=vars get workflow variables Get List of Workflow Variables PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} set instance variable Set a Instance Variable PUT /api/namespaces/{namespace}/vars/{variable} set namespace variable Set a Namespace Variable PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var set workflow variable Set a Workflow Variable workflow_services Method URI Name Summary DELETE /api/functions/namespaces/{namespace}/tree/{workflow}?op=delete-service delete workflow service Delete Namespace Service GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function get workflow service Get Workflow Service Details GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision get workflow service revision Get Workflow Service Revision GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions get workflow service revision list Get Workflow Service Revision List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods list workflow service revision pods Get Workflow Service Revision Pods List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services list workflow services Get Workflow Services List workflows Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow Await Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow body Await Execute a Workflow With Body PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow create workflow Create a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=execute execute workflow Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging set workflow cloud event logs Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle toggle workflow Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow update workflow Update a Workflow Paths Await Execute a Workflow ( awaitExecuteWorkflow ) GET /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string. All responses Code Status Description Has headers Schema 200 OK successfully executed workflow schema Responses 200 - successfully executed workflow Status: OK Schema Await Execute a Workflow With Body ( awaitExecuteWorkflowBody ) POST /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow with optionally some input provided in the request body as json. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters. Input query parameters are only read if the request has no body. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string. Workflow Input body interface{} interface{} \u2713 The input of this workflow instance All responses Code Status Description Has headers Schema 200 OK successfully executed workflow schema Responses 200 - successfully executed workflow Status: OK Schema Broadcast Cloud Event ( broadcastCloudevent ) POST /api/namespaces/{namespace}/broadcast Broadcast a cloud event to a namespace. Cloud events posted to this api will be picked up by any workflows listening to the same event type on the namescape. The body of this request should follow the cloud event core specification defined at https://github.com/cloudevents/spec . Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace cloudevent body interface{} interface{} \u2713 Cloud Event request to be sent. All responses Code Status Description Has headers Schema 200 OK successfully sent cloud event schema Responses 200 - successfully sent cloud event Status: OK Schema Filter given cloud event and broadcast it ( broadcastCloudeventFilter ) POST /api/namespaces/{namespace}/broadcast/{filtername} Filter cloud event by given filtername and broadcast to a namespace. Cloud events posted to this api will filter cloud event by given filtername and be picked up by any workflows listening to the same event type on the namescape. The body of this request should follow the cloud event core specification defined at https://github.com/cloudevents/spec . Parameters Name Source Type Go type Separator Required Default Description filtername path string string \u2713 target filtername namespace path string string \u2713 target namespace cloudevent body interface{} interface{} \u2713 Cloud Event request to be sent. All responses Code Status Description Has headers Schema 200 OK schema Responses 200 Status: OK Schema Cancel a Pending Instance ( cancelInstance ) POST /api/namespaces/{namespace}/instances/{instance}/cancel Cancel a currently pending instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully cancelled instance schema Responses 200 - successfully cancelled instance Status: OK Schema Creates new cloudEventFilter ( createCloudeventFilter ) POST /api/namespaces/{namespace}/eventfilter/{filtername} Creates new cloud event filter in target namespace The body of this request should be a compilable javascript code without function header. Parameters Name Source Type Go type Separator Required Default Description filtername path string string \u2713 new filtername namespace path string string \u2713 target namespace script body interface{} interface{} \u2713 compilable javascript code. All responses Code Status Description Has headers Schema 200 OK schema Responses 200 Status: OK Schema Create a Directory ( createDirectory ) PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory Creates a directory at the target path. Parameters Name Source Type Go type Separator Required Default Description directory path string string \u2713 path to target directory namespace path string string \u2713 target namespace op query string string \u2713 \"create-directory\" the operation for the api All responses Code Status Description Has headers Schema 200 OK directory has been created schema default an error has occurred schema Responses 200 - directory has been created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create a Namespace Folder ( createFolder ) PUT /api/namespaces/{namespace}/secrets/{folder} Create a namespace folder. Parameters Name Source Type Go type Separator Required Default Description folder path string string \u2713 target secret namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK namespace folder has been successfully created schema default an error has occurred schema Responses 200 - namespace folder has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Creates a namespace ( createNamespace ) PUT /api/namespaces/{namespace} Creates a new namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to create All responses Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema Responses 200 - namespace has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create Namespace Service ( createNamespaceService ) POST /api/functions/namespaces/{namespace} Creates namespace scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/getting_started/functions-intro/ Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Service body CreateNamespaceServiceBody CreateNamespaceServiceBody \u2713 Payload that contains information on new service All responses Code Status Description Has headers Schema 200 OK successfully created service schema Responses 200 - successfully created service Status: OK Schema Inlined models CreateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 envs map of string map[string]string image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size integer int64 \u2713 Size of created service pods, 0 = small, 1 = medium, 2 = large Create a Namespace Container Registry ( createRegistry ) POST /api/functions/registries/namespaces/{namespace} Create a namespace container registry. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body CreateRegistryBody CreateRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully created namespace registry schema Responses 200 - successfully created namespace registry Status: OK Schema Inlined models CreateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL Create a Namespace Secret ( createSecret ) PUT /api/namespaces/{namespace}/secrets/{secret} Create a namespace secret. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret Secret Payload body string string \u2713 Payload that contains secret data. All responses Code Status Description Has headers Schema 200 OK namespace secret has been successfully created schema default an error has occurred schema Responses 200 - namespace secret has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create a Workflow ( createWorkflow ) PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow Creates a workflow at the target path. The body of this request should contain the workflow yaml. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"create-workflow\" the operation for the api workflow data body string string Payload that contains the direktiv workflow yaml to create. All responses Code Status Description Has headers Schema 200 OK successfully created workflow schema default an error has occurred schema Responses 200 - successfully created workflow Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete existing cloudEventFilter ( deleteCloudeventFilter ) DELETE /api/namespaces/{namespace}/eventfilter/{filtername} Delete existing cloud event filter in target namespace Parameters Name Source Type Go type Separator Required Default Description filtername path string string \u2713 target filtername namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK schema Responses 200 Status: OK Schema Delete a Namespace Folder ( deleteFolder ) DELETE /api/namespaces/{namespace}/secrets/{folder} Delete a namespace folder. Parameters Name Source Type Go type Separator Required Default Description folder path string string \u2713 target folder namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK namespace folder has been successfully deleted schema default folder not found schema Responses 200 - namespace folder has been successfully deleted Status: OK Schema OkBody Default Response folder not found Schema ErrorResponse Delete a Instance Variable ( deleteInstanceVariable ) DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Delete a instance variable. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted instance variable schema Responses 200 - successfully deleted instance variable Status: OK Schema Delete a namespace ( deleteNamespace ) DELETE /api/namespaces/{namespace} Delete a namespace. A namespace will not delete by default if it has any child resources (workflows, etc...). Deleting the namespace with all its children can be done using the recursive query parameter. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to delete recursive query boolean bool recursively deletes all child resources All responses Code Status Description Has headers Schema 200 OK namespace has been successfully deleted schema default an error has occurred schema Responses 200 - namespace has been successfully deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete Namespace Service Revision ( deleteNamespaceRevision ) DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Delete a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service revision schema Responses 200 - successfully deleted service revision Status: OK Schema Delete Namespace Service ( deleteNamespaceService ) DELETE /api/functions/namespaces/{namespace}/function/{serviceName} Deletes namespace scoped knative service and all its revisions. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service schema Responses 200 - successfully deleted service Status: OK Schema Delete a Namespace Variable ( deleteNamespaceVariable ) DELETE /api/namespaces/{namespace}/vars/{variable} Delete a namespace variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted namespace variable schema Responses 200 - successfully deleted namespace variable Status: OK Schema Delete a node ( deleteNode ) DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node Creates a directory at the target path. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace node path string string \u2713 path to target node op query string string \u2713 \"delete-node\" the operation for the api recursive query boolean bool whether to recursively delete child nodes All responses Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema Responses 200 - node has been deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete a Namespace Container Registry ( deleteRegistry ) DELETE /api/functions/registries/namespaces/{namespace} Delete a namespace container registry Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body DeleteRegistryBody DeleteRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully delete namespace registry schema Responses 200 - successfully delete namespace registry Status: OK Schema Inlined models DeleteRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL Delete a Namespace Secret ( deleteSecret ) DELETE /api/namespaces/{namespace}/secrets/{secret} Delete a namespace secret. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret All responses Code Status Description Has headers Schema 200 OK namespace secret has been successfully deleted schema default secret not found schema Responses 200 - namespace secret has been successfully deleted Status: OK Schema OkBody Default Response secret not found Schema ErrorResponse Delete Namespace Service ( deleteWorkflowService ) DELETE /api/functions/namespaces/{namespace}/tree/{workflow}?op=delete-service Deletes workflow scoped knative service. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version All responses Code Status Description Has headers Schema 200 OK successfully deleted service schema Responses 200 - successfully deleted service Status: OK Schema Delete a Workflow Variable ( deleteWorkflowVariable ) DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var Delete a workflow variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted workflow variable schema Responses 200 - successfully deleted workflow variable Status: OK Schema Execute a Workflow ( executeWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=execute Executes a workflow with optionally some input provided in the request body as json. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"execute\" the operation for the api Workflow Input body interface{} interface{} \u2713 The input of this workflow instance All responses Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema Responses 200 - node has been deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get specific cloudEventFilter ( getCloudEventFilter ) GET /api/namespaces/{namespace}/eventfilter/{filtername} Get specific cloud event filter by given name in target namespace Parameters Name Source Type Go type Separator Required Default Description filtername path string string \u2713 target filtername namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK schema Responses 200 Status: OK Schema Get events history. ( getEventHistory ) GET /api/namespaces/{namespace}/events Get recent events history. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got events history schema Responses 200 - successfully got events history Status: OK Schema Get current event listeners. ( getEventListeners ) GET /api/namespaces/{namespace}/event-listeners Get current event listeners. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got event listeners schema Responses 200 - successfully got event listeners Status: OK Schema Get a Instance ( getInstance ) GET /api/namespaces/{namespace}/instances/{instance} Gets the details of a executed workflow instance in this namespace. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance schema default an error has occurred schema Responses 200 - successfully got instance Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get a Instance Input ( getInstanceInput ) GET /api/namespaces/{namespace}/instances/{instance}/input Gets the input an instance was provided when executed. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance input schema Responses 200 - successfully got instance input Status: OK Schema Get List Instances ( getInstanceList ) GET /api/namespaces/{namespace}/instances Gets a list of instances in a namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace filter.field query string string field to filter filter.type query string string filter behaviour order.direction query string string order direction order.field query string string field to order by All responses Code Status Description Has headers Schema 200 OK successfully got namespace instances schema Responses 200 - successfully got namespace instances Status: OK Schema Get a Instance Metadata ( getInstanceMetadata ) GET /api/namespaces/{namespace}/instances/{instance}/metadata Gets the metadata of an instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance metadata schema Responses 200 - successfully got instance metadata Status: OK Schema Get a Instance Output ( getInstanceOutput ) GET /api/namespaces/{namespace}/instances/{instance}/output Gets the output an instance was provided when executed. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance output schema Responses 200 - successfully got instance output Status: OK Schema Get a Instance Variable ( getInstanceVariable ) GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Get the value sorted in a instance variable. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got instance variable schema Responses 200 - successfully got instance variable Status: OK Schema Get List of Instance Variable ( getInstanceVariables ) GET /api/namespaces/{namespace}/instances/{instance}/vars Gets a list of variables in a instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance variables schema Responses 200 - successfully got instance variables Status: OK Schema Gets a namespace config ( getNamespaceConfig ) GET /api/namespaces/{namespace}/config Gets a namespace config. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update All responses Code Status Description Has headers Schema 200 OK successfully got namespace config schema Responses 200 - successfully got namespace config Status: OK Schema Get Namespace Service Details ( getNamespaceService ) GET /api/functions/namespaces/{namespace}/function/{serviceName} Get details of a namespace scoped knative service. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got service details schema Responses 200 - successfully got service details Status: OK Schema Get Namespace Services List ( getNamespaceServiceList ) GET /api/functions/namespaces/{namespace} Gets a list of namespace knative services. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got services list schema Responses 200 - successfully got services list Status: OK Schema Get a Namespace Variable ( getNamespaceVariable ) GET /api/namespaces/{namespace}/vars/{variable} Get the value sorted in a namespace variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got namespace variable schema Responses 200 - successfully got namespace variable Status: OK Schema Get Namespace Variable List ( getNamespaceVariables ) GET /api/namespaces/{namespace}/vars Gets a list of variables in a namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace variables schema Responses 200 - successfully got namespace variables Status: OK Schema Gets the list of namespaces ( getNamespaces ) GET /api/namespaces Gets the list of namespaces. Parameters Name Source Type Go type Separator Required Default Description filter.field query string string field to filter filter.type query string string filter behaviour order.direction query string string order direction order.field query string string field to order by All responses Code Status Description Has headers Schema 200 OK successfully got list of namespaces schema Responses 200 - successfully got list of namespaces Status: OK Schema Get List of Namespace Nodes ( getNodes ) GET /api/namespaces/{namespace}/tree/{nodePath} Gets Workflow and Directory Nodes at nodePath. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace nodePath path string string \u2713 target path in tree All responses Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema Responses 200 - successfully got namespace nodes Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get List of Namespace Registries ( getRegistries ) GET /api/functions/registries/namespaces/{namespace} Gets the list of namespace registries. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace registries schema Responses 200 - successfully got namespace registries Status: OK Schema Get List of Namespace Secrets or Search for Namespace Secrets by given name ( getSecrets ) GET /api/namespaces/{namespace}/secrets Gets the list of namespace secrets. Also can use for search by setting query param op=search and term= Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema Responses 200 - successfully got namespace nodes Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get List of Namespace nodes inside Folder ( getSecretsInsideFolder ) GET /api/namespaces/{namespace}/secrets/{folder} Gets the list of namespace secrets and folders inside specific folder. Parameters Name Source Type Go type Separator Required Default Description folder path string string \u2713 target folder path namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace nodes inside sepcific folder schema default an error has occurred schema Responses 200 - successfully got namespace nodes inside sepcific folder Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get Workflow Level Logs ( getWorkflowLogs ) GET /api/namespaces/{namespace}/tree/{workflow}?op=logs Get workflow level logs. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow filter.field query string string field to filter filter.type query string string filter behaviour order.direction query string string order direction order.field query string string field to order by All responses Code Status Description Has headers Schema 200 OK successfully got workflow logs schema Responses 200 - successfully got workflow logs Status: OK Schema Get Workflow Service Details ( getWorkflowService ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function Get a workflow scoped knative service details. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version All responses Code Status Description Has headers Schema 200 OK successfully got service details schema Responses 200 - successfully got service details Status: OK Schema Get Workflow Service Revision ( getWorkflowServiceRevision ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision Get a workflow scoped knative service revision. This will return details on a single revision. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got service revision details schema Responses 200 - successfully got service revision details Status: OK Schema Get Workflow Service Revision List ( getWorkflowServiceRevisionList ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions Get the revision list of a workflow scoped knative service. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version All responses Code Status Description Has headers Schema 200 OK successfully got service revisions schema Responses 200 - successfully got service revisions Status: OK Schema Get a Workflow Variable ( getWorkflowVariable ) GET /api/namespaces/{namespace}/tree/{workflow}?op=var Get the value sorted in a workflow variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got workflow variable schema Responses 200 - successfully got workflow variable Status: OK Schema Get List of Workflow Variables ( getWorkflowVariables ) GET /api/namespaces/{namespace}/tree/{workflow}?op=vars Gets a list of variables in a workflow. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow variables schema Responses 200 - successfully got workflow variables Status: OK Schema Gets Instance Logs ( instanceLogs ) GET /api/namespaces/{namespace}/instances/{instance}/logs Gets the logs of an executed instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance id namespace path string string \u2713 target namespace filter.field query string string field to filter filter.type query string string filter behaviour order.direction query string string order direction order.field query string string field to order by All responses Code Status Description Has headers Schema 200 OK successfully got instance logs schema default an error has occurred schema Responses 200 - successfully got instance logs Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse JQ Playground api to test jq queries ( jqPlayground ) POST /api/jq JQ Playground is a sandbox where you can test jq queries with custom data. Parameters Name Source Type Go type Separator Required Default Description JQ payload body JqPlaygroundBody JqPlaygroundBody \u2713 Payload that contains both the JSON data to manipulate and jq query. All responses Code Status Description Has headers Schema 200 OK jq query was successful schema 400 Bad Request the request was invalid schema 500 Internal Server Error an unexpected internal error occurred schema Responses 200 - jq query was successful Status: OK Schema 400 - the request was invalid Status: Bad Request Schema 500 - an unexpected internal error occurred Status: Internal Server Error Schema Inlined models JqPlaygroundBody Properties Name Type Go type Required Default Description Example data string string \u2713 JSON data encoded in base64 query string string \u2713 jq query to manipulate JSON data List existing cloudEventFilters ( listCloudeventFilter ) GET /api/namespaces/{namespace}/eventfilter list all existing cloud event filter in target namespace Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK schema Responses 200 Status: OK Schema Get Namespace Service Revision Pods List ( listNamespaceServiceRevisionPods ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a namespace scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema Responses 200 - successfully got list of a service revision pods Status: OK Schema Get Workflow Service Revision Pods List ( listWorkflowServiceRevisionPods ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods List a revisions pods of a workflow scoped knative service. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema Responses 200 - successfully got list of a service revision pods Status: OK Schema Get Workflow Services List ( listWorkflowServices ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services Gets a list of workflow knative services. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got services list schema Responses 200 - successfully got services list Status: OK Schema Gets Namespace Level Logs ( namespaceLogs ) GET /api/namespaces/{namespace}/logs Gets Namespace Level Logs. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace filter.field query string string field to filter filter.type query string string filter behaviour order.direction query string string order direction order.field query string string field to order by All responses Code Status Description Has headers Schema 200 OK successfully got namespace logs schema Responses 200 - successfully got namespace logs Status: OK Schema Gets Namespace Failed Workflow Instances Metrics ( namespaceMetricsFailed ) GET /api/namespaces/{namespace}/metrics/failed Get metrics of failed workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Invoked Workflow Metrics ( namespaceMetricsInvoked ) GET /api/namespaces/{namespace}/metrics/invoked Get metrics of invoked workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Workflow Timing Metrics ( namespaceMetricsMilliseconds ) GET /api/namespaces/{namespace}/metrics/milliseconds Get timing metrics of workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Successful Workflow Instances Metrics ( namespaceMetricsSuccessful ) GET /api/namespaces/{namespace}/metrics/successful Get metrics of successful workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Overwrite a Namespace Secret ( overwriteAndSearchSecret ) PATCH /api/namespaces/{namespace}/secrets/{secret} Overwrite a namespace secret Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret Secret Payload body string string \u2713 Payload that contains secret data All responses Code Status Description Has headers Schema 200 OK namespace has been successfully overwritten schema default secret not found schema Responses 200 - namespace has been successfully overwritten Status: OK Schema OkBody Default Response secret not found Schema ErrorResponse Watch Pod Logs ( podLogs ) GET /api/logs/{pod} Watches logs of the pods for a service. This can be a namespace service or a workflow service. Parameters Name Source Type Go type Separator Required Default Description pod path string string \u2713 pod name All responses Code Status Description Has headers Schema 200 OK successfully watching pod logs schema Responses 200 - successfully watching pod logs Status: OK Schema Replay Cloud Event ( replayCloudevent ) POST /api/namespaces/{namespace}/events/{event}/replay Replay a cloud event to a namespace. Parameters Name Source Type Go type Separator Required Default Description event path string string \u2713 target cloudevent namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully replayed cloud event schema Responses 200 - successfully replayed cloud event Status: OK Schema Get Direktiv Server Logs ( serverLogs ) GET /api/logs Gets Direktiv Server Logs. Parameters Name Source Type Go type Separator Required Default Description filter.field query string string field to filter filter.type query string string filter behaviour order.direction query string string order direction order.field query string string field to order by All responses Code Status Description Has headers Schema 200 OK successfully got server logs schema default an error has occurred schema Responses 200 - successfully got server logs Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Set a Instance Variable ( setInstanceVariable ) PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Set the value sorted in a instance variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set instance variable schema Responses 200 - successfully set instance variable Status: OK Schema Sets a namespace config ( setNamespaceConfig ) PATCH /api/namespaces/{namespace}/config Sets a namespace config. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update Config Payload body SetNamespaceConfigBody SetNamespaceConfigBody Payload that contains the config information to set. Note: This payload only need to contain the properities you wish to set. All responses Code Status Description Has headers Schema 200 OK namespace config has been successfully been updated schema Responses 200 - namespace config has been successfully been updated Status: OK Schema Inlined models SetNamespaceConfigBody Properties Name Type Go type Required Default Description Example broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace Set a Namespace Variable ( setNamespaceVariable ) PUT /api/namespaces/{namespace}/vars/{variable} Set the value sorted in a namespace variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set namespace variable schema Responses 200 - successfully set namespace variable Status: OK Schema Set Cloud Event for Workflow to Log to ( setWorkflowCloudEventLogs ) POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging Set Cloud Event for Workflow to Log to. When configured type direktiv.instanceLog cloud events will be generated with the logger parameter set to the configured value. Workflows can be configured to generate cloud events on their namespace anything the log parameter produces data. Please find more information on this topic here: https://docs.direktiv.io/docs/examples/logging.html Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Cloud Event Logger body SetWorkflowCloudEventLogsBody SetWorkflowCloudEventLogsBody \u2713 Cloud event logger to target All responses Code Status Description Has headers Schema 200 OK successfully update workflow schema Responses 200 - successfully update workflow Status: OK Schema Inlined models SetWorkflowCloudEventLogsBody Properties Name Type Go type Required Default Description Example logger string string \u2713 Target Cloud Event Set a Workflow Variable ( setWorkflowVariable ) PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var Set the value sorted in a workflow variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set workflow variable schema Responses 200 - successfully set workflow variable Status: OK Schema Test a registry to make sure the connection is okay ( testRegistry ) POST /api/functions/registries/test Test a registry with provided url, username and token Parameters Name Source Type Go type Separator Required Default Description Registry Payload body TestRegistryBody TestRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK registry is valid schema 401 Unauthorized unauthorized to access the registry schema Responses 200 - registry is valid Status: OK Schema 401 - unauthorized to access the registry Status: Unauthorized Schema Inlined models TestRegistryBody Properties Name Type Go type Required Default Description Example password string string \u2713 token to authenticate with the registry url string string \u2713 The url to test if the registry is valid username string string \u2713 username to authenticate with the registry Set Cloud Event for Workflow to Log to ( toggleWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle Toggle's whether or not a workflow is active. Disabled workflows cannot be invoked. This includes start event and scheduled workflows. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Workflow Live Status body ToggleWorkflowBody ToggleWorkflowBody \u2713 Whether or not the workflow is alive or disabled All responses Code Status Description Has headers Schema 200 OK successfully updated workflow live status schema Responses 200 - successfully updated workflow live status Status: OK Schema Inlined models ToggleWorkflowBody Properties Name Type Go type Required Default Description Example live boolean bool \u2713 Workflow live status Update existing cloudEventFilter ( updateCloudeventFilter ) PATCH /api/namespaces/{namespace}/eventfilter/{filtername} Update existing cloud event filter in target namespace Parameters Name Source Type Go type Separator Required Default Description filtername path string string \u2713 target filtername namespace path string string \u2713 target namespace script body interface{} interface{} \u2713 compilable javascript code. All responses Code Status Description Has headers Schema 200 OK schema Responses 200 Status: OK Schema Create Namespace Service Revision ( updateNamespaceService ) POST /api/functions/namespaces/{namespace}/function/{serviceName} Creates a new namespace scoped knative service revision. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service body UpdateNamespaceServiceBody UpdateNamespaceServiceBody \u2713 Payload that contains information on service revision All responses Code Status Description Has headers Schema 200 OK successfully created service revision schema Responses 200 - successfully created service revision Status: OK Schema Inlined models UpdateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods Update a Workflow ( updateWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow Updates a workflow at the target path. The body of this request should contain the workflow yaml you want to update to. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow workflow data body string string Payload that contains the updated direktiv workflow yaml. All responses Code Status Description Has headers Schema 200 OK successfully updated workflow schema Responses 200 - successfully updated workflow Status: OK Schema Returns version information for servers in the cluster. ( version ) GET /api/version Returns version information for servers in the cluster. All responses Code Status Description Has headers Schema 200 OK version query was successful schema Responses 200 - version query was successful Status: OK Schema Watch Namespace Service Revision ( watchNamespaceServiceRevision ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Watch a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revision schema Responses 200 - successfully watching service revision Status: OK Schema Watch Namespace Service Revision List ( watchNamespaceServiceRevisionList ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions Watch the revision list of a namespace scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revisions schema Responses 200 - successfully watching service revisions Status: OK Schema Gets Invoked Workflow Metrics ( workflowMetricsInvoked ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked Get metrics of invoked workflow instances. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets Workflow Time Metrics ( workflowMetricsMilliseconds ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed Get the timing metrics of a workflow's instance. This returns a total sum of the milliseconds a workflow has been executed for. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Get Sankey metrics of a workflow revision. ( workflowMetricsSankey ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-sankey Get Sankey metrics of a workflow revision. If ref query is not provided, metrics for the latest revision will be retrieved. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ref query string string target workflow revision reference All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets a Workflow State Time Metrics ( workflowMetricsStateMilliseconds ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds Get the state timing metrics of a workflow's instance. This returns the timing of individual states in a workflow. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets Successful Workflow Metrics ( workflowMetricsSuccessful ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful Get metrics of a workflow, where the instance was successful. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Models ErrorResponse interface{} OkBody OkBody is an arbitrary placeholder response that represents an ok response body interface{} updateServiceRequest UpdateServiceRequest update service request interface{}","title":"API"},{"location":"api/#direktiv-api","text":"Direktiv Open API Specification Direktiv Documentation can be found at https://docs.direktiv.io/","title":"Direktiv API"},{"location":"api/#informations","text":"","title":"Informations"},{"location":"api/#content-negotiation","text":"","title":"Content negotiation"},{"location":"api/#access-control","text":"","title":"Access control"},{"location":"api/#all-endpoints","text":"","title":"All endpoints"},{"location":"api/#paths","text":"","title":"Paths"},{"location":"api/#models","text":"","title":"Models"},{"location":"environment/cli/","text":"Although developing flows with the web UI is easy, a command line tool can be used to make local flow development faster and more convenient. Direktiv's cli direktivctl is used for pushing and executing flows remotely. This enables the developer to stay in his development environment, e.g. Visual Studio Code. Installing The direktivctl is available for Linux, Windows, and Mac platforms and is distributed as a tar.gz file with every new release of Direktiv. The asset can be downloaded and unpacked to get the direktiv-sync binary. Linux Mac Mac ARM Windows Linux Installation Example curl -L https://github.com/direktiv/direktiv/releases/latest/download/direktivctl_amd64.tar.gz | tar -xz && \\ sudo mv direktivctl /usr/local/bin Setting up a Namespace Working with the CLI assumes that you create a directory which is mirroring a namespace in Direktiv. This directory can be empty or can be a populated from a github clone command. The only requirement is that the namespace already exists. The connection information (address, token and namespace) can be provided with arguments but it is easier to use a .direktiv.yaml with that information. Providing a token is optional but addr and namespace are required. Example .direktiv.yaml auth : \"my-api-key-token\" addr : \"https://my-direktiv.server\" namespace : \"direktiv\" This file has to be in the root folder of that project and after creating this, that directory is mirroring the file structure in Direktiv. Pushing and Executing After setup there are two commands available. The push command pushes a flow to Direktiv but does not execute it. This command works recursively e.g. direktivctl workflows push . . The exec command uploads and executes the flow. During execution the logs are printed to stdout . CLI Examples direktivctl workflows push myworkflow.yaml direktivctl workflows push myfolder/ direktivctl workflows exec mywf.yaml Workflow Attributes Based on naming convetion workflow attributes can be set as well. If the file starts with the characters as the flow direktivctl will assume it is a flow attribute and create it. mywf.yaml mywf.yaml.script.sh The above example will create a flow variable script.sh for the flow mywf.yaml . Profiles If multiple configurations are needed, e.g. for local and remote, direktivctl supports \"profiles\". A profile is a configuration in a list of configurations in the config file. A valid configuration file might look like this: profiles : - id : dev auth : 123 addr : http://localhost:8080 namespace : test - id : prod auth : 123 addr : http://10.100.91.17 namespace : test The tool supports both types of configuration files, but you cannot mix and match. Either it uses profiles or basic configuration. When using profiles, the default behaviour is to select the first profile defined in the list. To override this behaviour the -P / --profile flag can be used to select one of the other profiles according to its id . For the example above, to push to prod can be done with the flag --profile=prod . Other Ways to Configure For most configuration settings, direktivctl will check for values in three places in the following order: Commandline flags. Environment variables. A configuration file. As long as direktictl finds all of the values required, it doesn't care where it got them from. This means it's not strictly necessary to have a configuration file at all, so long as the settings are defined elsewhere. The flags are self explanatory, and otherwise available via help information ( -h / --help ). For environment variables, all settings are named the same way they appear in a configuration file, except for the following adjustments: All characters are UPPERCASE All dashes are replaced with underscores. All named are prefixed with DIREKTIV_ . For example the auth token can be defined with DIREKTIV_AUTH_TOKEN=my-api-key-token .","title":"direktivctl"},{"location":"environment/cli/#installing","text":"The direktivctl is available for Linux, Windows, and Mac platforms and is distributed as a tar.gz file with every new release of Direktiv. The asset can be downloaded and unpacked to get the direktiv-sync binary. Linux Mac Mac ARM Windows Linux Installation Example curl -L https://github.com/direktiv/direktiv/releases/latest/download/direktivctl_amd64.tar.gz | tar -xz && \\ sudo mv direktivctl /usr/local/bin","title":"Installing"},{"location":"environment/cli/#setting-up-a-namespace","text":"Working with the CLI assumes that you create a directory which is mirroring a namespace in Direktiv. This directory can be empty or can be a populated from a github clone command. The only requirement is that the namespace already exists. The connection information (address, token and namespace) can be provided with arguments but it is easier to use a .direktiv.yaml with that information. Providing a token is optional but addr and namespace are required. Example .direktiv.yaml auth : \"my-api-key-token\" addr : \"https://my-direktiv.server\" namespace : \"direktiv\" This file has to be in the root folder of that project and after creating this, that directory is mirroring the file structure in Direktiv.","title":"Setting up a Namespace"},{"location":"environment/cli/#pushing-and-executing","text":"After setup there are two commands available. The push command pushes a flow to Direktiv but does not execute it. This command works recursively e.g. direktivctl workflows push . . The exec command uploads and executes the flow. During execution the logs are printed to stdout . CLI Examples direktivctl workflows push myworkflow.yaml direktivctl workflows push myfolder/ direktivctl workflows exec mywf.yaml","title":"Pushing and Executing"},{"location":"environment/cli/#workflow-attributes","text":"Based on naming convetion workflow attributes can be set as well. If the file starts with the characters as the flow direktivctl will assume it is a flow attribute and create it. mywf.yaml mywf.yaml.script.sh The above example will create a flow variable script.sh for the flow mywf.yaml .","title":"Workflow Attributes"},{"location":"environment/cli/#profiles","text":"If multiple configurations are needed, e.g. for local and remote, direktivctl supports \"profiles\". A profile is a configuration in a list of configurations in the config file. A valid configuration file might look like this: profiles : - id : dev auth : 123 addr : http://localhost:8080 namespace : test - id : prod auth : 123 addr : http://10.100.91.17 namespace : test The tool supports both types of configuration files, but you cannot mix and match. Either it uses profiles or basic configuration. When using profiles, the default behaviour is to select the first profile defined in the list. To override this behaviour the -P / --profile flag can be used to select one of the other profiles according to its id . For the example above, to push to prod can be done with the flag --profile=prod .","title":"Profiles"},{"location":"environment/cli/#other-ways-to-configure","text":"For most configuration settings, direktivctl will check for values in three places in the following order: Commandline flags. Environment variables. A configuration file. As long as direktictl finds all of the values required, it doesn't care where it got them from. This means it's not strictly necessary to have a configuration file at all, so long as the settings are defined elsewhere. The flags are self explanatory, and otherwise available via help information ( -h / --help ). For environment variables, all settings are named the same way they appear in a configuration file, except for the following adjustments: All characters are UPPERCASE All dashes are replaced with underscores. All named are prefixed with DIREKTIV_ . For example the auth token can be defined with DIREKTIV_AUTH_TOKEN=my-api-key-token .","title":"Other Ways to Configure"},{"location":"environment/direktiv-development-environment/","text":"To improve function and flow development it is recommended to setup a local development environment. This section explains how to setup the development environment. Details about developing custom functions is described in this section . Running Direktiv As mentioned in the \" Getting Started \" guide there are two ways to set up a local development environment besides setting up a full Kubernetes with Direktiv. There is a Docker image and a multipass configuration. This section describes how they can be configured and used. Docker Setting up a development Direktiv instance on a local machine is very simple. Assuming docker is installed, run the following command: Starting Direktiv docker run --privileged -p 8080 :80 -p 31212 :31212 -d --name direktiv direktiv/direktiv-kube This command starts direktiv as container 'direktiv'. The initial boot-time will take a few minutes. The progress can be followed with: Direktiv Docker Logs docker logs direktiv -f Once all pods reach 'running' status, direktiv is ready and the URL http://localhost:8080/api/namespaces is accessible. The database uses a persistent volume so the data stored should survive restarts with 'docker stop/start' . The port-forward of 31212 is the included docker registry . If there is a requirement to execute kubectl commands the container can be accessed via docker exec . For convenience there is a kubectl shortcut kc and command completion is installed as well. Accessing Shell docker exec -it direktiv /bin/bash kc get pods -A Enabling Proxy The following settings can be passed as environmental variables to use this image in environments with a proxy. This has to be done on the first startup. Proxy Settings docker run --privileged -p 8080 :80 -p 31212 :31212 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \"127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,172.16.0.0/12,.svc,.default,.local,.cluster.local,localhost,.direktiv-services-direktiv\" -d --name direktiv -ti direktiv/direktiv-kube API Key If the instance requires an API key it can be added with an environment variable as well. Enable API Key docker run --privileged -p 8080 :80 -p 31212 :31212 -e APIKEY = 123 -d --name direktiv -ti direktiv/direktiv-kube Enable Eventing Knative Eventing is disabled by default in the Docker image but can be easily enabled during startup with EVENTING=true as environemtn variable. Enable Eventing docker run --privileged -p 8080 :80 -p 31212 :31212 -e EVENTING = true -d --name direktiv -ti direktiv/direktiv-kube Debug If there are issues starting nested Kubernetes it is possible to see the K3S debug logs on startup with the variable DEBUG . Enable Eventing docker run --privileged -p 8080 :80 -p 31212 :31212 -e DEBUG = true direktiv/direktiv-kube There is always the option to use the multipass configuration if the Docker image does not work. Multipass Multipass creates a virtual machine with Direktiv pre-configured. The configuration is different from the Docker image but all features are available to that approach as well. The cloud-init script will do the configuration during first boot and takes a few minutes to complete. Eventing is anebled by default. Start Multipass Instance multipass launch --cpus 4 --disk 20G --memory 6G --name direktiv --cloud-init https://raw.githubusercontent.com/direktiv/direktiv/main/build/docker/all/multipass/init.yaml After startup the machine can be access with a simple command. For convenience there is a kubectl shortcut and code completion installed. Accessing Shell multipass exec direktiv -- /bin/bash Warning multipass does not work in a VPN. The VPN needs to be turned off for this example installation. If the installation is not successful there is a cloud-init log available on the virtual machine /var/log/cloud-init-output.log to check the logs. The instance has an accessible network configured and the IP is accessible from the host. After startup the UI can be accessed with first IP listed under IPv4 . Display IP multipass info direktiv Name: direktiv State: Running IPv4: 10 .100.91.90 10 .42.0.0 10 .42.0.1 Release: Ubuntu 22 .04.2 LTS Image hash: 345fbbb6ec82 ( Ubuntu 22 .04 LTS ) CPU ( s ) : 4 Load: 0 .53 0 .51 0 .25 Disk usage: 5 .0GiB out of 9 .5GiB Memory usage: 2 .1GiB out of 3 .8GiB Enabling Proxy Enabling a proxy has to be done by changing the cloud-init file manually. The first step is to download the file from Github with e.g. curl. Download Cloud-Init curl https://raw.githubusercontent.com/direktiv/direktiv/main/build/docker/all/multipass/init.yaml > myinit.yaml The proxy configuration values need to be added as a file under /env . The following snippet is an example for such a configuration. Proxy YAML ... write_files : - encoding : b64 content : SCRIPT path : /home/install.sh permissions : '0755' - path : /env content : | HTTP_PROXY=http://10.100.6.16:3128 HTTPS_PROXY=http://10.100.6.16:3128 NO_PROXY=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,172.16.0.0/12,.svc,.default,.local,.cluster.local,localhost,.direktiv-services-direktiv append : true After changing the file multipass requires this file instead of the default one. Custom Cloud-Init multipass launch --cpus 4 --disk 20G --memory 6G --name direktiv --cloud-init myinit.yaml Warning Multipass can not read and use files in the /tmp directory. Do not place the custom init file in /tmp . Enabling API Key To add an API key it is also required to create a custom cloud-init configuration like the proxy does. The required variables is APIKEY . API Key YAML ... write_files : - encoding : b64 content : SCRIPT path : /home/install.sh permissions : '0755' - path : /env content : | APIKEY=123 append : true Deleting Multipass Intance To remove the instance the delete and purge command is required. Delete Multipass Instance multipass delete direktiv multipass purge Docker registry Direktiv pulls containers from a registry and use them as functions in flows. For development purposes the direktiv docker container as well as the multipass instances come with a registry installed. It is accessible via :31212. The value for is either the localhost for Docker or the IP of the multipass instance. To test the local repository the golang example from direktiv-apps can be used: git clone https://github.com/direktiv-apps/bash.git docker build bash/ -t <IP>:31212/bash docker push <IP>:31212/bash # confirm upload curl http://<IP>:31212/v2/_catalog Multipass Instances Docker doesn not support pushing to http registries. Therefore it has to be added as an insecure registry to the docker service . Add something like the following to /etc/docker/daemon.json and restart the Docker service. { \"insecure-registries\" : [ \"10.100.91.188:31212\" ] } Testing Configuration To test if everything is working this example creates a namespace and a flow and executes it. The value for <ADDRESS> has to be replaced with either localhost:8080 for Docker or the IP of the multipass instance. Testing Installation # create namespace 'test' curl -X PUT http://<ADDRESS>/api/namespaces/test # create the workflow file cat > helloworld.yml <<- EOF direktiv_api: workflow/v1 functions: - id: get type: reusable image: gcr.io/direktiv/functions/bash:1.0 states: - id: getter type: action action: function: get input: commands: - command: ehoc Hello EOF # upload flow curl -X PUT --data-binary @helloworld.yml \"http://<ADDRESS>/api/namespaces/test/tree/test?op=create-workflow\" # execute flow (initial call will be slightly slower than subsequent calls) curl \"http://<ADDRESS>/api/namespaces/test/tree/test?op=wait\"","title":"Standalone Environment"},{"location":"environment/direktiv-development-environment/#running-direktiv","text":"As mentioned in the \" Getting Started \" guide there are two ways to set up a local development environment besides setting up a full Kubernetes with Direktiv. There is a Docker image and a multipass configuration. This section describes how they can be configured and used.","title":"Running Direktiv"},{"location":"environment/direktiv-development-environment/#docker","text":"Setting up a development Direktiv instance on a local machine is very simple. Assuming docker is installed, run the following command: Starting Direktiv docker run --privileged -p 8080 :80 -p 31212 :31212 -d --name direktiv direktiv/direktiv-kube This command starts direktiv as container 'direktiv'. The initial boot-time will take a few minutes. The progress can be followed with: Direktiv Docker Logs docker logs direktiv -f Once all pods reach 'running' status, direktiv is ready and the URL http://localhost:8080/api/namespaces is accessible. The database uses a persistent volume so the data stored should survive restarts with 'docker stop/start' . The port-forward of 31212 is the included docker registry . If there is a requirement to execute kubectl commands the container can be accessed via docker exec . For convenience there is a kubectl shortcut kc and command completion is installed as well. Accessing Shell docker exec -it direktiv /bin/bash kc get pods -A","title":"Docker"},{"location":"environment/direktiv-development-environment/#multipass","text":"Multipass creates a virtual machine with Direktiv pre-configured. The configuration is different from the Docker image but all features are available to that approach as well. The cloud-init script will do the configuration during first boot and takes a few minutes to complete. Eventing is anebled by default. Start Multipass Instance multipass launch --cpus 4 --disk 20G --memory 6G --name direktiv --cloud-init https://raw.githubusercontent.com/direktiv/direktiv/main/build/docker/all/multipass/init.yaml After startup the machine can be access with a simple command. For convenience there is a kubectl shortcut and code completion installed. Accessing Shell multipass exec direktiv -- /bin/bash Warning multipass does not work in a VPN. The VPN needs to be turned off for this example installation. If the installation is not successful there is a cloud-init log available on the virtual machine /var/log/cloud-init-output.log to check the logs. The instance has an accessible network configured and the IP is accessible from the host. After startup the UI can be accessed with first IP listed under IPv4 . Display IP multipass info direktiv Name: direktiv State: Running IPv4: 10 .100.91.90 10 .42.0.0 10 .42.0.1 Release: Ubuntu 22 .04.2 LTS Image hash: 345fbbb6ec82 ( Ubuntu 22 .04 LTS ) CPU ( s ) : 4 Load: 0 .53 0 .51 0 .25 Disk usage: 5 .0GiB out of 9 .5GiB Memory usage: 2 .1GiB out of 3 .8GiB","title":"Multipass"},{"location":"environment/direktiv-development-environment/#docker-registry","text":"Direktiv pulls containers from a registry and use them as functions in flows. For development purposes the direktiv docker container as well as the multipass instances come with a registry installed. It is accessible via :31212. The value for is either the localhost for Docker or the IP of the multipass instance. To test the local repository the golang example from direktiv-apps can be used: git clone https://github.com/direktiv-apps/bash.git docker build bash/ -t <IP>:31212/bash docker push <IP>:31212/bash # confirm upload curl http://<IP>:31212/v2/_catalog Multipass Instances Docker doesn not support pushing to http registries. Therefore it has to be added as an insecure registry to the docker service . Add something like the following to /etc/docker/daemon.json and restart the Docker service. { \"insecure-registries\" : [ \"10.100.91.188:31212\" ] }","title":"Docker registry"},{"location":"environment/direktiv-development-environment/#testing-configuration","text":"To test if everything is working this example creates a namespace and a flow and executes it. The value for <ADDRESS> has to be replaced with either localhost:8080 for Docker or the IP of the multipass instance. Testing Installation # create namespace 'test' curl -X PUT http://<ADDRESS>/api/namespaces/test # create the workflow file cat > helloworld.yml <<- EOF direktiv_api: workflow/v1 functions: - id: get type: reusable image: gcr.io/direktiv/functions/bash:1.0 states: - id: getter type: action action: function: get input: commands: - command: ehoc Hello EOF # upload flow curl -X PUT --data-binary @helloworld.yml \"http://<ADDRESS>/api/namespaces/test/tree/test?op=create-workflow\" # execute flow (initial call will be slightly slower than subsequent calls) curl \"http://<ADDRESS>/api/namespaces/test/tree/test?op=wait\"","title":"Testing Configuration"},{"location":"environment/git/","text":"Git Mirrors Direktiv supports configuring a namespace to use a git repository as the source of its contents and configuration. Setting Up A Git Mirror The following arguments can be supplied when creating a mirror as a namespace: namespace url ref Auth: None Git access token: access_token SSH private_key public_key passphrase The following arguments are considered sensitive, and will never be returned via the API, except in a redacted form: access_token , private_key , passphrase . Authentication & Authorization None If a repository is public some providers will allow clients to access it without any form of authentication. SSH requires authentication, that's why the only way to use a zero-auth configuration is with git over HTTP. When using a zero-auth configuration very few fields are needed to set up a mirror. The following is an example: namespace : apps-svc url : https://github.com/direktiv/apps-svc.git ref : main Access Token Github allows users to create personal access tokens ( link ). These can be used as a form of authentication. These can only be used with git over HTTP. namespace : apps-svc url : https://github.com/direktiv/apps-svc.git ref : main access_token : my-access-token... SSH If the two previous approaches aren't sufficient for your needs, the reliable way of reaching a repository under any and all circumstances is with git over SSH. For this to work, you will need to provide both a public and a private key, and optionally a passphrase used to decrypt the private key, if it is password protected. You will also need to configure the remote server to recognize your SSH key. namespace : apps-svc url : https://github.com/direktiv/apps-svc.git ref : main public_key : my-public-key... private_key : my-private-key... passphrase : my-passphrase... Activities All Direktiv mirror operations are encapsulated within an \"activity\". This serves as a way of organizing the logic and logs of an operation in a convenient way. Cloning a remote repository can take time, and it's possible that the operation will fail or produce unexpected results. Check the list of recent activities and their logs to learn more about any issues you encounter. Modifying the Configuration of a Local Mirror For various reasons you may need to update the settings of a mirror. Whether it's to change which branch or commit it's referencing, or to update your credentials. All of this is supported. For convenience, Direktiv will only apply changes to settings you ask it to, anything else will remain unchanged. This means for example that you can swap from branch v1.0.x to v1.1.x without resupplying your SSH keys if you want to. Direktiv Projects When a filetree is intended for mirroring by Direktiv, we call it a Direktiv Project. What follows is an explanation of the rules for Direktiv Projects. File-System When performing a sync, Direktiv will produce a copy of every directory and file from the git project into your Direktiv file-system, with some exceptions and caveats. Other kinds of file-system objects such as symlinks are skipped. Files and directories will only be copied if their names are valid within Direktiv. At this time, valid names must conform to the following regex pattern: (([a-zA-Z][a-zA-Z0-9_\\-\\.]*[a-zA-Z0-9])|([a-zA-Z])) . Also, names beginning with . (hidden files) are also excluded. This helps to reduce pollution in your file-tree by avoiding common unhelpful git project contents such as .gitignore and .git/ . If a /.direktivignore file exists, Direktiv will use this file in exactly the same way .gitignore files are used by git to more precisely control what is and isn't copied. If a file ending with the .yaml or .yml extension is unambiguously identified as a Direktiv resource definition (by including a top level field direktiv_api ), it will not be copied. Instead, such a file will be processed and its definitions added to the namespace. More on this later. If a file ending with the .yaml or .yml extension is ambiguous, Direktiv will attempt to parse it as a workflow. If it succeeds without error, it will be added to the file-system as a Direktiv Workflow. Otherwise, it will be added to the file-system as a generic yaml file. Deprecated: the correct way to add a workflow is to unambiguously define one. This ambiguousness step is only included short-term to maintain backwards compatibility. The above steps only determine whether a file will be evaluated as a potential Direktiv resource definition or copied into the file-tree. Exclusion for any of the above reasons won't break direct references to such files in Direktiv resource definitions. This means, for example, that you can define a filter (more on this later) that references a Javascript file, while still excluding the javascript file from appearing in the file-tree using .direktivignore . Workflows As mentioned in the File-System section above, workflows will be loaded from any .yaml or .yml file that parses without error. This processing of ambiguous files may in rare circumstances lead to Direktiv creating workflows from yaml files that had nothing to do with Direktiv. This behaviour is therefore deprecated. The correct way to define a workflow is to include the direktiv_api field, set to workflow/v1 . For example, a file /hello.yaml containing the following contents defines a simple helloworld workflow of the same name and location in the file-tree: direktiv_api : workflow/v1 states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' When workflows are unambiguously defined this way, they do not need to parse successfully as valid workflows to be copied into the file-system. Namespace Services Namespace services can be defined within a Direktiv Project so they are automatically created during mirroring. This is done using appropriate Direktiv resource definition files. For example, a /services.yaml file containing the following contents defines a simple http requester service named requester . The services field is an array, meaning that these files may define one or more services each. The user may divide or combine their service definitions amongst as many files as they like. direktiv_api : services/v1 services : - name : requester image : direktiv/request:v4 NamespaceServicesDefinition Parameter Description Type Required direktiv_api Set it to 'services/v1'. string yes services []NamespaceServiceDefinition yes NamespaceServiceDefinition Parameter Description Type Required name The name for this service, which should be unique amonst all services within the namespace / project. string yes image The image to use for this service. string yes Filters CloudEvent filters can be defined within a Direktiv Project so they are automatically created during mirroring. This is done using appropriate Direktiv resource definition files. For example, a /filters.yaml file containing the following contents defines a simple event filter named dropper . The filters field is an array, meaning that these files may define one or more filters each. The user may divide or combine their filter definitions amongst as many files as they like. direktiv_api : filters/v1 filters : - name : dropper inline-js : | if (event[\"source\"] == \"mysource\") { nslog(\"rename source\") event[\"source\"] = \"newsource\" } if (event[\"source\"] == \"hello\") { nslog(\"drop me\") return null } return event When writing JavaScript it is often preferable not to directly embed it within YAML. This is supported as well. An alternative way of defining the same filter as above could be to have the following /filters.yaml file, along with the following /Dropper.js file. Notice that there is an illegal character (capital D) in that that file name, which automatically prevents it from appearing in the file-tree, but does not preclude it from being part of this filter definition. direktiv_api : filters/v1 filters : - name : dropper source : ./Dropper.js if ( event [ \"source\" ] == \"mysource\" ) { nslog ( \"rename source\" ) event [ \"source\" ] = \"newsource\" } if ( event [ \"source\" ] == \"hello\" ) { nslog ( \"drop me\" ) return null } return event EventFiltersDefinition Parameter Description Type Required direktiv_api Set it to 'services/v1'. string yes filters []EventFilterDefinition yes EventFilterDefinition Parameter Description Type Required name The name for this filter, which should be unique amonst all filters within the namespace / project. string yes inline_javascript JavaScript filter logic. string no source A filepath to a file containing JavaScript filter logic. string no Variables There are too many questions raised by syncing namespace & workflow variables that have no obvious or clearly best answer. This means they are bound to cause confusion, and since all problems they solved can now be solved in other ways, this feature is being removed. A deprecated way of syncing these still exists, but going forward users should think of alternative ways to solve these problems. Ways such as file-system files, which can now be read as variables by the engine. And jq initialization statements such as .x // 5 . What follows is a description of the deprecated behaviour. Note: variables are not subject to the same naming concerns as workflows, and so all files will be checked for adhering to these naming conventions, even if they would otherwise be excluded. Namespace Variables If a file is named like var.* then it is treated as a namespace variable. For example, var.style.css will become a namespace variable called style.css . Workflow Variables If a file is named such that it is prefixed with the full name of another file that was evaluated to be a workflow (followed by a . ), then the file is treated as a workflow variable attached to that workflow. For example, if you have a workflow /hello.yaml , and another file called /hello.yaml.page.html , then the /hello.yaml workflow will gain a workflow variable called page.html . Unsyncables Container Registries Configuring container registries requires a password, which is sensitive information that should not be committed to a git project. We therefore do not support any way to sync these at the moment. Secrets Sensitive information should not be committed to git projects. We therefore do not support any way to sync these at the moment. As a small help to users, when workflows are created referencing secrets that haven't been defined, uninitialized secrets are created. These don't contain any helpful information, but they populate the list of secrets within the namespace for easy adjustment. Errors The syncing process adopts a fault-tolerant approach to errors. This means that as often as possible, detected errors should be logged as such, but not prevent the greater sync operation from succeeding. Users are strongly encouraged to check the logs of their sync operation to confirm that there were no unexpected problems with their git repository.","title":"Git Sync"},{"location":"environment/git/#git-mirrors","text":"Direktiv supports configuring a namespace to use a git repository as the source of its contents and configuration.","title":"Git Mirrors"},{"location":"environment/git/#setting-up-a-git-mirror","text":"The following arguments can be supplied when creating a mirror as a namespace: namespace url ref Auth: None Git access token: access_token SSH private_key public_key passphrase The following arguments are considered sensitive, and will never be returned via the API, except in a redacted form: access_token , private_key , passphrase .","title":"Setting Up A Git Mirror"},{"location":"environment/git/#modifying-the-configuration-of-a-local-mirror","text":"For various reasons you may need to update the settings of a mirror. Whether it's to change which branch or commit it's referencing, or to update your credentials. All of this is supported. For convenience, Direktiv will only apply changes to settings you ask it to, anything else will remain unchanged. This means for example that you can swap from branch v1.0.x to v1.1.x without resupplying your SSH keys if you want to.","title":"Modifying the Configuration of a Local Mirror"},{"location":"environment/git/#direktiv-projects","text":"When a filetree is intended for mirroring by Direktiv, we call it a Direktiv Project. What follows is an explanation of the rules for Direktiv Projects.","title":"Direktiv Projects"},{"location":"environment/vscode/","text":"Setting Up Direktiv Workflow Validation in Visual Studio Code For efficient management of Direktiv workflows outside our UI, we offer a schema validation using Direktiv's official schema and the Red Hat YAML extension in Visual Studio Code. This provides automatic validation and intellisense. Follow the guide below to set it up. Prerequisites Visual Studio Code : Ensure that you have Visual Studio Code installed on your system. Internet Connection : This is essential for fetching the Direktiv schema. Step-by-step Guide 1. Install the Red Hat YAML Extension Begin by opening Visual Studio Code. Once it's up, navigate to the Extensions view by clicking on the Extensions icon on the Activity Bar situated on the side of the window. In the search bar, type YAML and then proceed to install the extension offered by Red Hat. 2. Modify the Settings After the extension is installed, you'll need to add the Direktiv schema configuration. Here's how: Click on the gear icon found in the lower left corner of the Visual Studio Code window. From the dropdown that appears, select Settings (JSON) . Within the settings JSON file that opens up, merge or add the following configuration: \"yaml.schemas\" : { \"https://raw.githubusercontent.com/direktiv/direktiv/stable/resources/direktiv.schema.json\" : \".wf.yaml\" } 3. Using the Schema Validation From now on, each time you open a .wf.yaml file in Visual Studio Code, the program will apply the schema validations grounded on the Direktiv schema automatically. If the schema is not automatically applied or to make use of the schema validation when using simply .yaml as extension: Direct your attention to the lower right corner of the status bar, where you should find an option labeled \"Select Language (YAML)\" or something similar. Click on this option and from the dropdown list that appears, choose direktiv-workflow . This action will apply the schema.","title":"VSCode"},{"location":"environment/vscode/#setting-up-direktiv-workflow-validation-in-visual-studio-code","text":"For efficient management of Direktiv workflows outside our UI, we offer a schema validation using Direktiv's official schema and the Red Hat YAML extension in Visual Studio Code. This provides automatic validation and intellisense. Follow the guide below to set it up.","title":"Setting Up Direktiv Workflow Validation in Visual Studio Code"},{"location":"environment/vscode/#prerequisites","text":"Visual Studio Code : Ensure that you have Visual Studio Code installed on your system. Internet Connection : This is essential for fetching the Direktiv schema.","title":"Prerequisites"},{"location":"environment/vscode/#step-by-step-guide","text":"","title":"Step-by-step Guide"},{"location":"events/","text":"Events Direktiv utilizes the HTTP Protocol Binding for CloudEvents , and offers two distinct ways to produce and consume events. The easiest approach is by using Direktiv's API directly in order to route your desired events. However, if you require more flexibility, Knative can assist with a more powerful and dynamic approach when it comes to eventing. What type of integration is ideal depends on which use cases are meant to be addressed by Direktiv. Independent from this integration approach system internal events within Direktiv are always supported. Event API The event API provides direct access to Direktiv's eventing system. The general API path is /api/namespaces/{namespace}/broadcast . Following the cloud-event specification events can be send to Direktiv in three different formats. Event ID The specification requires an event ID. Direktiv generates a random ID if not provided by the client. Binary Content Mode Th binary content mode uses headers to describe the event metadata with a \"ce-\" prefix and allows for efficient transfer and without transcoding effort. The header \"content-type\" must be set to the content-type of the body of the event. POST /api/namespaces/{namespace}/broadcast HTTP/1.1 Host: direktiv.io ce-specversion: 1.0 ce-type: com.example.event ce-id: 1234-1234-1234 ce-source: /mycontext/subcontext Content-Type: application/json; charset=utf-8 { \"hello\": \"world\" } Structured Content Mode In structured mode the whole cloudevent is in the payload. The content-type header needs to be set to \"application/cloudevents+json\". { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Batched Content Mode In batch mode multiple events can be send to direktiv. The content-type has to be \"application/cloudevents-batch+json\" and the body is a JSON array of cloud events. [ { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"C234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" }, { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"B234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } ] Other Data If unknown data arrives at the API endpoint Direktiv does not drop the data but converts it into a cloud event. The value for type is set to noncompliant and source to unknown . The payload of the original requets will be base64 encoded and added as data_base64 to the event. If the content type can be guessed or is provided in the header it will be part of the cloud event as well. { \"noncompliant\" : { \"data_base64\" : \"aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1kUXc0dzlXZ1hjUQ==\" , \"datacontenttype\" : \"text/plain\" , \"error\" : \"unknown Message encoding\" , \"id\" : \"60290f4b-3971-411a-b824-73b60eb8b72d\" , \"source\" : \"unknown\" , \"specversion\" : \"1.0\" , \"type\" : \"noncompliant\" } } Events in Flows Events in a Direktiv flow can be a start condition and initiate a flow or a workflow can wait for an event during flow execution . Direktiv can wait for single events or on AND and OR combinations of events. Event Start Type Example The following is an example of a simple start condition for a Direktiv flow. A start condition requires the type and additional context values can be provided. If context values are defined the cloud event has to match the context attribute. For matching glob values can be used. start : type : event state : helloworld event : type : io.direktiv.myevent context : myvalue : my* states : - id : helloworld type : noop log : jq(.) The above example flow would trigger if the following cloud event would arrive: { \"specversion\" : \"1.0\" , \"type\" : \"io.direktiv.myevent\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"myvalue\" : \"mydata\" } Because the defintion uses a glob pattern valid values fo myvalue could be everything starting with my . If the attribute is missing or does not start with my the workflow would not trigger. Events in Flow Example Waiting for events within a flow is similar to a start definition except the context can be used to \"link\" flows to each other via context attributes in events. Additionally it can handle timeouts if an event has not been received within a certain time. states : - id : wait-event type : consumeEvent timeout : PT1M event : type : io.direktiv.myevent context : customer : jq(.customer) catch : - error : \"direktiv.cancels.timeout.soft\" transition : timedout - id : timedout type : noop log : this event timed out","title":"General"},{"location":"events/#events","text":"Direktiv utilizes the HTTP Protocol Binding for CloudEvents , and offers two distinct ways to produce and consume events. The easiest approach is by using Direktiv's API directly in order to route your desired events. However, if you require more flexibility, Knative can assist with a more powerful and dynamic approach when it comes to eventing. What type of integration is ideal depends on which use cases are meant to be addressed by Direktiv. Independent from this integration approach system internal events within Direktiv are always supported.","title":"Events"},{"location":"events/#event-api","text":"The event API provides direct access to Direktiv's eventing system. The general API path is /api/namespaces/{namespace}/broadcast . Following the cloud-event specification events can be send to Direktiv in three different formats. Event ID The specification requires an event ID. Direktiv generates a random ID if not provided by the client.","title":"Event API"},{"location":"events/#events-in-flows","text":"Events in a Direktiv flow can be a start condition and initiate a flow or a workflow can wait for an event during flow execution . Direktiv can wait for single events or on AND and OR combinations of events.","title":"Events in Flows"},{"location":"events/filter/","text":"Although Direktiv supports Knative Eventing which supports event filtering, Direktiv adds custom event filtering as well. Direktiv's event filters can be easlily configured and added as additional event route. Javascript Filters are based on Javascript and the filter has access to an event object which can be modified or the event can be dropped based on certain requirements. Direktiv provieds one additional function nslog which adds log entries to the namespace logs. if ( event [ \"source\" ] == \"mysource\" ) { nslog ( \"rename source\" ) event [ \"source\" ] = \"newsource\" } if ( event [ \"source\" ] == \"hello\" ) { nslog ( \"drop me\" ) return null } return event The Javascript can return null which means the event will be dropped andn not handled by Direktiv or it returns the modified event object which goes into the system and will be handled by flows if there are any configured to handle it. Add Filter Assuming the above filter script is stored in filter.js it can be added to Direktiv with the following CLI command. direktivctl events set-filter -n events -a http://myserver myfilter filter.js If the command was successful the filter is configured and ready to be used. direktivctl events list-filters -n events -a http://myserver myfilter To keep the event system performant each filter creates a event route where the rule will be applied. The API path for the filter is /api/namespaces/{namespace}/broadcast/{filtername} . The filter will be applied to every event hitting that API URL. For the following event the source would be renamed. { \"specversion\" : \"1.0\" , \"type\" : \"io.direktiv.myevent\" , \"source\" : \"mysource\" , \"subject\" : \"123\" } This event would be dropped. { \"specversion\" : \"1.0\" , \"type\" : \"io.direktiv.myevent\" , \"source\" : \"hello\" , \"subject\" : \"123\" } A filter can be removed with the following command. direktivctl events delete-filter -n events -a http://myserver myfilter","title":"Filter"},{"location":"events/filter/#javascript","text":"Filters are based on Javascript and the filter has access to an event object which can be modified or the event can be dropped based on certain requirements. Direktiv provieds one additional function nslog which adds log entries to the namespace logs. if ( event [ \"source\" ] == \"mysource\" ) { nslog ( \"rename source\" ) event [ \"source\" ] = \"newsource\" } if ( event [ \"source\" ] == \"hello\" ) { nslog ( \"drop me\" ) return null } return event The Javascript can return null which means the event will be dropped andn not handled by Direktiv or it returns the modified event object which goes into the system and will be handled by flows if there are any configured to handle it.","title":"Javascript"},{"location":"events/filter/#add-filter","text":"Assuming the above filter script is stored in filter.js it can be added to Direktiv with the following CLI command. direktivctl events set-filter -n events -a http://myserver myfilter filter.js If the command was successful the filter is configured and ready to be used. direktivctl events list-filters -n events -a http://myserver myfilter To keep the event system performant each filter creates a event route where the rule will be applied. The API path for the filter is /api/namespaces/{namespace}/broadcast/{filtername} . The filter will be applied to every event hitting that API URL. For the following event the source would be renamed. { \"specversion\" : \"1.0\" , \"type\" : \"io.direktiv.myevent\" , \"source\" : \"mysource\" , \"subject\" : \"123\" } This event would be dropped. { \"specversion\" : \"1.0\" , \"type\" : \"io.direktiv.myevent\" , \"source\" : \"hello\" , \"subject\" : \"123\" } A filter can be removed with the following command. direktivctl events delete-filter -n events -a http://myserver myfilter","title":"Add Filter"},{"location":"events/cloud/","text":"Cloud A simple introduction to using events provided from Google, Amazon and Azure to send to Direktiv.","title":"General"},{"location":"events/cloud/#cloud","text":"A simple introduction to using events provided from Google, Amazon and Azure to send to Direktiv.","title":"Cloud"},{"location":"events/cloud/amazon/","text":"Amazon EventBridge We're going to go through the process of setting up a rule for 'ec2' to send events to our Direktiv service. This explains how to create an api destination and transform the aws event input to cloud event format. Note: the below tutorial assumes that the user has already created the IAM role for the EventBridge API integration as described in Amazon EventBridge User Guide From the Role create above - keep the Role Arn details as it is needed in the final step. A screenshot is shown below: Create a rule aws events put-rule --name \"direktiv-rule\" --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"]}\" The following output should appear (make sure you hold onto the ARN as it is used further down to attach a target to the rule): { \"RuleArn\" : \"<RULE_ARN>\" } Create a connection After creating an Authorization token from the Direktiv interface, create the connection using the token as follow: aws events create-connection --name direktiv-connection --authorization-type API_KEY --auth-parameters \"{\\\"ApiKeyAuthParameters\\\": {\\\"ApiKeyName\\\":\\\"direktiv-token\\\", \\\"ApiKeyValue\\\":\\\"<DIREKTIV_TOKEN>\\\"}}\" Upon creating the connection the following output from the CLI should appear. { \"ConnectionArn\" : \"<CONNECTION_ARN>\" , \"ConnectionState\" : \"AUTHORIZED\" , \"CreationTime\" : \"2021-08-04T05:28:24+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:28:24+00:00\" } We will need to use the connection arn in the next command. Create an Api-Destination aws events create-api-destination --name direktiv-api --connection-arn \"<CONNECTION_ARN>\" --invocation-endpoint https://<DIREKTIV_URL>/api/namespaces/<NAMESPACE>/broadcast --http-method POST The output should resemble this: { \"ApiDestinationArn\" : \"<API_ARN>\" , \"ApiDestinationState\" : \"ACTIVE\" , \"CreationTime\" : \"2021-08-04T05:30:50+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:30:50+00:00\" } Put Targets to the AWS EventBridge Rule Adding the targets to the EventBridge rule also requires us to define an Input Path and Input Template. aws events put-targets --rule direktiv-rule --targets '[ { \"Id\": \"direktiv-api\", \"RoleArn\": \"<ROLE_ARN>\", \"Arn\": \"<API_ARN>\", \"InputTransformer\": { \"InputPathsMap\": { \"id\":\"$.id\", \"source\":\"$.source\", \"state\":\"$.detail.state\", \"subject\":\"$.source\", \"time\":\"$.time\", \"type\":\"$.detail-type\" }, \"InputTemplate\": \" {\\\"specversion\\\":\\\"1.0\\\", \\\"id\\\":<id>, \\\"source\\\":<source>, \\\"type\\\":<type>, \\\"subject\\\":<subject>, \\\"time\\\":<time>, \\\"data\\\":<aws.events.event.json>}\" } } ]' The output (if successful) below: { \"FailedEntryCount\" : 0 , \"FailedEntries\" : [] } Input Path Map Example Input Path Map captures the EventBridge event so we can easily filter into a cloud event to send to Direktiv { \"id\" : \"$.id\" , \"source\" : \"$.source\" , \"subject\" : \"$.source\" , \"time\" : \"$.time\" , \"type\" : \"$.detail-type\" } Input Template Example The Input Template allows you to spec out what you want the JSON to look like parsing the values from the input path. { \"specversion\" : \"1.0\" , \"id\" : \"<id>\" , \"source\" : \"<source>\" , \"type\" : \"<type>\" , \"subject\" : \"<subject>\" , \"time\" : \"<time>\" , \"data\" : <aws.eve nts .eve nt .jso n > } So now when you change the state of an instance on EC2 a workflow will be triggered on Direktiv if it is listening to 'aws.ec2'. For reference, when an AWS event is generated, the default event structure (for an EC2 status change as an example) is shown below: { \"version\" : \"0\" , \"id\" : \"7bf73129-1428-4cd3-a780-95db273d1602\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"123456789012\" , \"time\" : \"2015-11-11T21:29:54Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:ec2:us-east-1:123456789012:instance/i-abcd1111\" ], \"detail\" : { \"instance-id\" : \"i-abcd1111\" , \"state\" : \"pending\" } } The CloudEvent received by Direktiv after the transformation is shown below: { \"specversion\" : \"1.0\" , \"id\" : \"f694954a-c307-368c-005a-d4279473e156\" , \"source\" : \"aws.ec2\" , \"type\" : \"EC2 Instance State-change Notification\" , \"subject\" : \"aws.ec2\" , \"time\" : \"2022-05-04T01:57:06Z\" , \"data\" : { \"version\" : \"0\" , \"id\" : \"f694954a-c307-368c-005a-d4279473e156\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"338328518639\" , \"time\" : \"2022-05-04T01:57:06Z\" , \"region\" : \"ap-southeast-2\" , \"resources\" : [ \"arn:aws:ec2:ap-southeast-2:338328518639:instance/i-0cf5a83f321fbed55\" ], \"detail\" : { \"instance-id\" : \"i-0cf5a83f321fbed55\" , \"state\" : \"pending\" } } } Testing Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : \"EC2 Instance State-change Notification\" states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Amazon"},{"location":"events/cloud/amazon/#amazon-eventbridge","text":"We're going to go through the process of setting up a rule for 'ec2' to send events to our Direktiv service. This explains how to create an api destination and transform the aws event input to cloud event format. Note: the below tutorial assumes that the user has already created the IAM role for the EventBridge API integration as described in Amazon EventBridge User Guide From the Role create above - keep the Role Arn details as it is needed in the final step. A screenshot is shown below:","title":"Amazon EventBridge"},{"location":"events/cloud/amazon/#create-a-rule","text":"aws events put-rule --name \"direktiv-rule\" --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"]}\" The following output should appear (make sure you hold onto the ARN as it is used further down to attach a target to the rule): { \"RuleArn\" : \"<RULE_ARN>\" }","title":"Create a rule"},{"location":"events/cloud/amazon/#create-a-connection","text":"After creating an Authorization token from the Direktiv interface, create the connection using the token as follow: aws events create-connection --name direktiv-connection --authorization-type API_KEY --auth-parameters \"{\\\"ApiKeyAuthParameters\\\": {\\\"ApiKeyName\\\":\\\"direktiv-token\\\", \\\"ApiKeyValue\\\":\\\"<DIREKTIV_TOKEN>\\\"}}\" Upon creating the connection the following output from the CLI should appear. { \"ConnectionArn\" : \"<CONNECTION_ARN>\" , \"ConnectionState\" : \"AUTHORIZED\" , \"CreationTime\" : \"2021-08-04T05:28:24+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:28:24+00:00\" } We will need to use the connection arn in the next command.","title":"Create a connection"},{"location":"events/cloud/amazon/#create-an-api-destination","text":"aws events create-api-destination --name direktiv-api --connection-arn \"<CONNECTION_ARN>\" --invocation-endpoint https://<DIREKTIV_URL>/api/namespaces/<NAMESPACE>/broadcast --http-method POST The output should resemble this: { \"ApiDestinationArn\" : \"<API_ARN>\" , \"ApiDestinationState\" : \"ACTIVE\" , \"CreationTime\" : \"2021-08-04T05:30:50+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:30:50+00:00\" }","title":"Create an Api-Destination"},{"location":"events/cloud/amazon/#put-targets-to-the-aws-eventbridge-rule","text":"Adding the targets to the EventBridge rule also requires us to define an Input Path and Input Template. aws events put-targets --rule direktiv-rule --targets '[ { \"Id\": \"direktiv-api\", \"RoleArn\": \"<ROLE_ARN>\", \"Arn\": \"<API_ARN>\", \"InputTransformer\": { \"InputPathsMap\": { \"id\":\"$.id\", \"source\":\"$.source\", \"state\":\"$.detail.state\", \"subject\":\"$.source\", \"time\":\"$.time\", \"type\":\"$.detail-type\" }, \"InputTemplate\": \" {\\\"specversion\\\":\\\"1.0\\\", \\\"id\\\":<id>, \\\"source\\\":<source>, \\\"type\\\":<type>, \\\"subject\\\":<subject>, \\\"time\\\":<time>, \\\"data\\\":<aws.events.event.json>}\" } } ]' The output (if successful) below: { \"FailedEntryCount\" : 0 , \"FailedEntries\" : [] }","title":"Put Targets to the AWS EventBridge Rule"},{"location":"events/cloud/amazon/#testing","text":"Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : \"EC2 Instance State-change Notification\" states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/cloud/azure/","text":"Azure EventGrid Goes through the process of setting up a storage account that listens for events on upload. Being that Azure uses native cloud events we won't need to run anything apart from the initial setup. Setup To follow along you will need access to the resource group you wish to setup in. This example includes the creation of a storage account but an existing one can be used. Create a Storage Account & Container Create a storage account under a resource group az storage account create --name direktivstoragetest --resource-group trentis-direktiv-apps-test Create a container under that storage account. You can get the --account-key by doing the following az storage account keys list --account-name direktivstoragetest az storage container create --name direktiv-container --account-name direktivstorage100 --account-key ACCOUNT-KEY Create an Event Subscription webhook-request-callback sends option request Create an event subscription attached to the storage account. az eventgrid event-subscription create \\ --name direktiv-event \\ --source-resource-id = $( az storage account show --name direktivstoragetest --resource-group trentis-direktiv-apps-test --query id --output tsv ) \\ --endpoint = https://playground.direktiv.io/api/namespaces/trent/event \\ --endpoint-type = webhook --event-delivery-schema cloudeventschemav1_0 \\ --delivery-attribute-mapping Authorization Static \"Bearer ACCESS_TOKEN\" true Testing id : listen-for-azure-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : Microsoft.Storage.BlobCreated states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Azure"},{"location":"events/cloud/azure/#azure-eventgrid","text":"Goes through the process of setting up a storage account that listens for events on upload. Being that Azure uses native cloud events we won't need to run anything apart from the initial setup.","title":"Azure EventGrid"},{"location":"events/cloud/azure/#setup","text":"To follow along you will need access to the resource group you wish to setup in. This example includes the creation of a storage account but an existing one can be used.","title":"Setup"},{"location":"events/cloud/azure/#create-a-storage-account-container","text":"Create a storage account under a resource group az storage account create --name direktivstoragetest --resource-group trentis-direktiv-apps-test Create a container under that storage account. You can get the --account-key by doing the following az storage account keys list --account-name direktivstoragetest az storage container create --name direktiv-container --account-name direktivstorage100 --account-key ACCOUNT-KEY","title":"Create a Storage Account &amp; Container"},{"location":"events/cloud/azure/#create-an-event-subscription","text":"webhook-request-callback sends option request Create an event subscription attached to the storage account. az eventgrid event-subscription create \\ --name direktiv-event \\ --source-resource-id = $( az storage account show --name direktivstoragetest --resource-group trentis-direktiv-apps-test --query id --output tsv ) \\ --endpoint = https://playground.direktiv.io/api/namespaces/trent/event \\ --endpoint-type = webhook --event-delivery-schema cloudeventschemav1_0 \\ --delivery-attribute-mapping Authorization Static \"Bearer ACCESS_TOKEN\" true","title":"Create an Event Subscription"},{"location":"events/cloud/azure/#testing","text":"id : listen-for-azure-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : Microsoft.Storage.BlobCreated states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/cloud/gcp/","text":"Google Cloud EventArc To send Google Cloud Audit log events to EventArc you will need a container service running on Cloud Run. We provide you a container located at 'gcr.io/direktiv/event-arc-listener'. That container's job is to read the cloud event it receives and relays it back to a Direktiv service. Setup Setup Audit Logs to be managed Read policy file to /tmp/policy.yaml gcloud projects get-iam-policy PROJECT_ID > /tmp/policy.yaml Add the follow section above 'bindings:' auditConfigs : - auditLogConfigs : - logType : ADMIN_READ - logType : DATA_WRITE - logType : DATA_READ service : storage.googleapis.com Set the new policy gcloud projects set-iam-policy PROJECT_ID /tmp/policy.yaml Setup Configs for Gcloud to run properly gcloud config set project PROJECT_ID gcloud config set run/region us-central1 gcloud config set run/platform managed gcloud config set eventarc/location us-central1 Configure the Cloud Run Service Using Authentication Create a secret to use as the DIREKTIV_TOKEN gcloud secrets create DIREKTIV_TOKEN \\ --replication-policy = \"automatic\" Create a file that contains the ACCESS_TOKEN generated from Direktiv that has 'namespaceEvent' privilege. I chose to create the file as '/tmp/ac'. Add the secret data to the secret gcloud secrets versions add DIREKTIV_TOKEN --data-file = /tmp/ac Create a Cloud Run Service Deploy the container to your environment gcloud beta run deploy event-arc-listener --image gcr.io/direktiv/event-arc-listener \\ --update-secrets = DIREKTIV_TOKEN = DIREKTIV_TOKEN:1 \\ --set-env-vars \"DIREKTIV_NAMESPACE=trent\" \\ --set-env-vars \"DIREKTIV_ENDPOINT=https://playground.direktiv.io\" \\ --allow-unauthenticated Create a Trigger for the Cloud Run Service Create a new trigger to listen for storage events on this project. gcloud eventarc triggers create storage-upload-trigger \\ --destination-run-service = event-arc-listener \\ --destination-run-region = us-central1 \\ --event-filters = \"type=google.cloud.audit.log.v1.written\" \\ --event-filters = \"serviceName=storage.googleapis.com\" \\ --event-filters = \"methodName=storage.objects.create\" \\ --service-account = SERVICE_ACCOUNT_ADDRESS Note: Keep in mind this trigger will take 10 minutes to work Testing Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : google.cloud.audit.log.v1.written states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Google Cloud Platform"},{"location":"events/cloud/gcp/#google-cloud-eventarc","text":"To send Google Cloud Audit log events to EventArc you will need a container service running on Cloud Run. We provide you a container located at 'gcr.io/direktiv/event-arc-listener'. That container's job is to read the cloud event it receives and relays it back to a Direktiv service.","title":"Google Cloud EventArc"},{"location":"events/cloud/gcp/#setup","text":"","title":"Setup"},{"location":"events/cloud/gcp/#setup-audit-logs-to-be-managed","text":"Read policy file to /tmp/policy.yaml gcloud projects get-iam-policy PROJECT_ID > /tmp/policy.yaml Add the follow section above 'bindings:' auditConfigs : - auditLogConfigs : - logType : ADMIN_READ - logType : DATA_WRITE - logType : DATA_READ service : storage.googleapis.com Set the new policy gcloud projects set-iam-policy PROJECT_ID /tmp/policy.yaml","title":"Setup Audit Logs to be managed"},{"location":"events/cloud/gcp/#setup-configs-for-gcloud-to-run-properly","text":"gcloud config set project PROJECT_ID gcloud config set run/region us-central1 gcloud config set run/platform managed gcloud config set eventarc/location us-central1","title":"Setup Configs for Gcloud to run properly"},{"location":"events/cloud/gcp/#configure-the-cloud-run-service","text":"","title":"Configure the Cloud Run Service"},{"location":"events/cloud/gcp/#create-a-cloud-run-service","text":"Deploy the container to your environment gcloud beta run deploy event-arc-listener --image gcr.io/direktiv/event-arc-listener \\ --update-secrets = DIREKTIV_TOKEN = DIREKTIV_TOKEN:1 \\ --set-env-vars \"DIREKTIV_NAMESPACE=trent\" \\ --set-env-vars \"DIREKTIV_ENDPOINT=https://playground.direktiv.io\" \\ --allow-unauthenticated","title":"Create a Cloud Run Service"},{"location":"events/cloud/gcp/#create-a-trigger-for-the-cloud-run-service","text":"Create a new trigger to listen for storage events on this project. gcloud eventarc triggers create storage-upload-trigger \\ --destination-run-service = event-arc-listener \\ --destination-run-region = us-central1 \\ --event-filters = \"type=google.cloud.audit.log.v1.written\" \\ --event-filters = \"serviceName=storage.googleapis.com\" \\ --event-filters = \"methodName=storage.objects.create\" \\ --service-account = SERVICE_ACCOUNT_ADDRESS Note: Keep in mind this trigger will take 10 minutes to work","title":"Create a Trigger for the Cloud Run Service"},{"location":"events/cloud/gcp/#testing","text":"Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : google.cloud.audit.log.v1.written states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/knative/example/","text":"This example uses Kafka as Knative event broker and event source and sink as well. After receiving a message from Kafka, Knative forwards it to Direktiv which subsequently initiates a flow and publishes an event back to Knative which will broker the event to a receive topic in Kafka. To run this example the following steps are required: Installing Kafka Installing Knative with Kafka Configure Kafka Source Configure Direktiv Source Configuring Kafka Sink Flow Example Versions The version numbers in this example might have changed over time. Please make sure to update them accordingly if required. Installing Kafka To enable Knative Eventing in a production environment, Knative requires the installation of an event broker. By setting up triggers and subscriptions, Knative brokers like RabbitMQ or Kafka can build an event mesh architecture. Here we will be using Kafka and the Strimzi Operator for the installation. This is a two-step process, installing the Kafka operator and creating the Kafka cluster itself. Installing Strimzi Operator kubectl create namespace kafka kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka kubectl wait --for = condition = ready pod -l name = strimzi-cluster-operator -n kafka --timeout = 300s Kafka Installation The Kafka installation instructions provided here is just an example and can not be used as-is in production environments. Please go to https://strimzi.io for full documentation. After the operator is running the following command enables KRaft for the operator. This allows an installation without Zookeeper and should simplify this setup. Enable KRaft kubectl -n kafka set env deployment/strimzi-cluster-operator STRIMZI_FEATURE_GATES = +UseKRaft kubectl wait --for = condition = ready pod -l name = strimzi-cluster-operator -n kafka --timeout = 300s The following command will create the actual Kafka cluster which will be used in this example. Create Kafka Instance cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : Kafka metadata : name : my-cluster namespace : kafka spec : kafka : version : 3.4.0 replicas : 1 listeners : - name : plain port : 9092 type : internal tls : false - name : tls port : 9093 type : internal tls : true config : offsets.topic.replication.factor : 1 transaction.state.log.replication.factor : 1 transaction.state.log.min.isr : 1 default.replication.factor : 1 min.insync.replicas : 1 inter.broker.protocol.version : \"3.4\" storage : type : ephemeral zookeeper : replicas : 1 storage : type : ephemeral EOF Knative with Kafka To use Kafka as the underlying mechanism for message brokering Knative needs to be configured during installation. The YAML here will create Knative Eventing instance with the required settings. Knative Eventing with Kafka cat <<-EOF | kubectl apply -f - --- apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1 default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 EOF This installation requires the Knative Kafka controller and data plane as well. This can be installed with two kubectl commands. Knative Kafka Dependencies kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-controller.yaml sleep 3 kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-broker.yaml The last step is to create the actual broker. The following two commands are creating the broker configuration and the broker using the configuration. Kafka Configuration cat <<-EOF | kubectl apply -f - --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : default.topic.partitions : \"10\" default.topic.replication.factor : \"1\" bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" EOF Creating Broker cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : Kafka name : default namespace : knative-eventing spec : config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing EOF The Kafka broker is now up and running. The setup can be tested with kubectl . Working Knative Eventing kubectl get brokers.eventing.knative.dev NAME URL AGE READY REASON default http://kafka-broker-ingress.knative-eventing.svc.cluster.local/default/default 16m True Configuring Kafka Source Kafka will be an event source and a sink in this example. Therefore we need two channels. One channel sending messages and a second channeld to receive the outcome of the whole message process. Sender & Receiver Topics cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : sender-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : receiver-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF kubectl get kafkatopics.kafka.strimzi.io -n kafka With that setup a Kafka source can be installed which will trigger the event flow. This YAML creates the source which sends all messages to the Kafka broker. This shows the decoupling of the events. The producer or sender is unaware of the receiver(s) of the message. Install Kafka Source kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-source.yaml cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : direktiv-kafka-source namespace : knative-eventing spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - sender-topic sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF Working Kafka Source kubectl get kafkasources.sources.knative.dev NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE direktiv-kafka-source [ \"sender-topic\" ] [ \"my-cluster-kafka-bootstrap.kafka:9092\" ] True 4m16s With this source enabled Knative can receive events but it requires a trigger to have another system consume the event. A trigger is a simple mechnism in Knative to \"forward\" certain events to subscribers. In this YAML there is a trigger filter defined this trigger consumes all events of type dev.knative.kafka.event and forwards it to Direktiv's direktiv-eventing service. The uri value specifies the target namespace in Direktiv. For more information about eventing filters visit the Knative documentation page about filters . Trigger to Direktiv cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-in namespace : knative-eventing spec : broker : default filter : attributes : type : dev.knative.kafka.event subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /direktiv EOF This setup can already send events to a namespace called direktiv if data arrives at the sender-topic topic in Kafka. This can be easily tested if the namespace direktiv already exists in Direktiv. To test it we start a pod which connects to the sender topic. Kafka Client Pod kubectl -n kafka run kafka-producer -ti --image = quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic sender-topic After running the pod add JSON into the command prompt, e.g. {} . This sends the JSON object to Kafka. Knative's broker will pick up the message and execute the trigger for Direktiv. The event will appear on the direktiv namespace dashboard. Configuring Direktiv Source To connect Direktiv back to Knative we need to install direktiv-knative-source . This source listens to events generated in Direktiv and pushes them to Knative. In this example the message is pushed back to the broker which can then use triggers to distribute the event. The required argument for this source is the direktiv URI within the cluster, e.g. direktiv-flow.default:3333 . Direktiv Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source namespace : knative-eventing spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF Configuring Kafka Sink The last step is to create a Kafka sink which consumes the event coming from Direktiv. This closes the communication cycle from Kafka to Direktiv and back to Kafka again. For this to work a Kafka sink has to be installed. Kafka Sink Installation kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-sink.yaml cat <<-EOF | kubectl apply -f - --- apiVersion: eventing.knative.dev/v1alpha1 kind: KafkaSink metadata: name: direktiv-kafka-sink namespace: knative-eventing spec: topic: receiver-topic bootstrapServers: - my-cluster-kafka-bootstrap.kafka:9092 EOF Sink Topic Send a message to the receiver-topic if the sink reports an error about a missing topic: kubectl -n kafka run kafka-receiver -ti --image=quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic receiver-topic After installing the sink a trigger is required to tie them together. A filter can applied to that trigger as well. In this case the trigger accepts events if the type of the cloudevent is myevent . Kafka Receiver Sink cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-receive namespace : knative-eventing spec : broker : default filter : attributes : type : myevent subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : direktiv-kafka-sink EOF Flow After all components are installed and connected a flow in Direktiv is required to actually transfrom the message and send it back. The example flow in the direktiv namespace here will listen to all dev.knative.kafka.event events and return the event under the new attribute x . Simple Flow start : type : event state : tellme event : type : dev.knative.kafka.event states : - id : tellme type : generateEvent event : type : myevent source : Direktiv data : x : jq(.\"dev.knative.kafka.event\".data) With that setup a new message e.g. \"Hello\" on the sender-topic queue should show as { \"x\": { \"Hello\" }} in the receiver topic. Please make sure to send valid JSON because this is being used as the data playload for the event. Listen to Receiver Topic kubectl -n kafka run kafka-consumer -ti --image = quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic receiver-topic --from-beginning","title":"Kafka Example"},{"location":"events/knative/example/#installing-kafka","text":"To enable Knative Eventing in a production environment, Knative requires the installation of an event broker. By setting up triggers and subscriptions, Knative brokers like RabbitMQ or Kafka can build an event mesh architecture. Here we will be using Kafka and the Strimzi Operator for the installation. This is a two-step process, installing the Kafka operator and creating the Kafka cluster itself. Installing Strimzi Operator kubectl create namespace kafka kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka kubectl wait --for = condition = ready pod -l name = strimzi-cluster-operator -n kafka --timeout = 300s Kafka Installation The Kafka installation instructions provided here is just an example and can not be used as-is in production environments. Please go to https://strimzi.io for full documentation. After the operator is running the following command enables KRaft for the operator. This allows an installation without Zookeeper and should simplify this setup. Enable KRaft kubectl -n kafka set env deployment/strimzi-cluster-operator STRIMZI_FEATURE_GATES = +UseKRaft kubectl wait --for = condition = ready pod -l name = strimzi-cluster-operator -n kafka --timeout = 300s The following command will create the actual Kafka cluster which will be used in this example. Create Kafka Instance cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : Kafka metadata : name : my-cluster namespace : kafka spec : kafka : version : 3.4.0 replicas : 1 listeners : - name : plain port : 9092 type : internal tls : false - name : tls port : 9093 type : internal tls : true config : offsets.topic.replication.factor : 1 transaction.state.log.replication.factor : 1 transaction.state.log.min.isr : 1 default.replication.factor : 1 min.insync.replicas : 1 inter.broker.protocol.version : \"3.4\" storage : type : ephemeral zookeeper : replicas : 1 storage : type : ephemeral EOF","title":"Installing Kafka"},{"location":"events/knative/example/#knative-with-kafka","text":"To use Kafka as the underlying mechanism for message brokering Knative needs to be configured during installation. The YAML here will create Knative Eventing instance with the required settings. Knative Eventing with Kafka cat <<-EOF | kubectl apply -f - --- apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1 default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 EOF This installation requires the Knative Kafka controller and data plane as well. This can be installed with two kubectl commands. Knative Kafka Dependencies kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-controller.yaml sleep 3 kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-broker.yaml The last step is to create the actual broker. The following two commands are creating the broker configuration and the broker using the configuration. Kafka Configuration cat <<-EOF | kubectl apply -f - --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : default.topic.partitions : \"10\" default.topic.replication.factor : \"1\" bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" EOF Creating Broker cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : Kafka name : default namespace : knative-eventing spec : config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing EOF The Kafka broker is now up and running. The setup can be tested with kubectl . Working Knative Eventing kubectl get brokers.eventing.knative.dev NAME URL AGE READY REASON default http://kafka-broker-ingress.knative-eventing.svc.cluster.local/default/default 16m True","title":"Knative with Kafka"},{"location":"events/knative/example/#configuring-kafka-source","text":"Kafka will be an event source and a sink in this example. Therefore we need two channels. One channel sending messages and a second channeld to receive the outcome of the whole message process. Sender & Receiver Topics cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : sender-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : receiver-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF kubectl get kafkatopics.kafka.strimzi.io -n kafka With that setup a Kafka source can be installed which will trigger the event flow. This YAML creates the source which sends all messages to the Kafka broker. This shows the decoupling of the events. The producer or sender is unaware of the receiver(s) of the message. Install Kafka Source kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-source.yaml cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : direktiv-kafka-source namespace : knative-eventing spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - sender-topic sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF Working Kafka Source kubectl get kafkasources.sources.knative.dev NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE direktiv-kafka-source [ \"sender-topic\" ] [ \"my-cluster-kafka-bootstrap.kafka:9092\" ] True 4m16s With this source enabled Knative can receive events but it requires a trigger to have another system consume the event. A trigger is a simple mechnism in Knative to \"forward\" certain events to subscribers. In this YAML there is a trigger filter defined this trigger consumes all events of type dev.knative.kafka.event and forwards it to Direktiv's direktiv-eventing service. The uri value specifies the target namespace in Direktiv. For more information about eventing filters visit the Knative documentation page about filters . Trigger to Direktiv cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-in namespace : knative-eventing spec : broker : default filter : attributes : type : dev.knative.kafka.event subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /direktiv EOF This setup can already send events to a namespace called direktiv if data arrives at the sender-topic topic in Kafka. This can be easily tested if the namespace direktiv already exists in Direktiv. To test it we start a pod which connects to the sender topic. Kafka Client Pod kubectl -n kafka run kafka-producer -ti --image = quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic sender-topic After running the pod add JSON into the command prompt, e.g. {} . This sends the JSON object to Kafka. Knative's broker will pick up the message and execute the trigger for Direktiv. The event will appear on the direktiv namespace dashboard.","title":"Configuring Kafka Source"},{"location":"events/knative/example/#configuring-direktiv-source","text":"To connect Direktiv back to Knative we need to install direktiv-knative-source . This source listens to events generated in Direktiv and pushes them to Knative. In this example the message is pushed back to the broker which can then use triggers to distribute the event. The required argument for this source is the direktiv URI within the cluster, e.g. direktiv-flow.default:3333 . Direktiv Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source namespace : knative-eventing spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF","title":"Configuring Direktiv Source"},{"location":"events/knative/example/#configuring-kafka-sink","text":"The last step is to create a Kafka sink which consumes the event coming from Direktiv. This closes the communication cycle from Kafka to Direktiv and back to Kafka again. For this to work a Kafka sink has to be installed. Kafka Sink Installation kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-sink.yaml cat <<-EOF | kubectl apply -f - --- apiVersion: eventing.knative.dev/v1alpha1 kind: KafkaSink metadata: name: direktiv-kafka-sink namespace: knative-eventing spec: topic: receiver-topic bootstrapServers: - my-cluster-kafka-bootstrap.kafka:9092 EOF Sink Topic Send a message to the receiver-topic if the sink reports an error about a missing topic: kubectl -n kafka run kafka-receiver -ti --image=quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic receiver-topic After installing the sink a trigger is required to tie them together. A filter can applied to that trigger as well. In this case the trigger accepts events if the type of the cloudevent is myevent . Kafka Receiver Sink cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-receive namespace : knative-eventing spec : broker : default filter : attributes : type : myevent subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : direktiv-kafka-sink EOF","title":"Configuring Kafka Sink"},{"location":"events/knative/example/#flow","text":"After all components are installed and connected a flow in Direktiv is required to actually transfrom the message and send it back. The example flow in the direktiv namespace here will listen to all dev.knative.kafka.event events and return the event under the new attribute x . Simple Flow start : type : event state : tellme event : type : dev.knative.kafka.event states : - id : tellme type : generateEvent event : type : myevent source : Direktiv data : x : jq(.\"dev.knative.kafka.event\".data) With that setup a new message e.g. \"Hello\" on the sender-topic queue should show as { \"x\": { \"Hello\" }} in the receiver topic. Please make sure to send valid JSON because this is being used as the data playload for the event. Listen to Receiver Topic kubectl -n kafka run kafka-consumer -ti --image = quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic receiver-topic --from-beginning","title":"Flow"},{"location":"events/knative/knative/","text":"Direktiv provides a sink and a source for integration into Knative Eventing . Knative uses a broker to relay events between systems and the Kafka example shows how to use Kafka as broker. This section however explains the concept via direct connections between sinks and sources. Preparing Direktiv Knative requires a sink to send events to Direktiv. Direktiv comes with a ready-to-use Knative sink but it has to be enabled. This can be done during installation or afterward with an helm upgrade . The following configuration in Direktiv's value.yaml adds the required sink service. Enabling Eventing eventing : enabled : true Upgrade Direktiv helm upgrade -f direktiv.yaml -n direktiv direktiv direktiv/direktiv After that change there is an additional service direktiv-eventing available in Direktiv's namespace. Knative Installation During the default installation Knative's operator has been installed an makes installing Knative eventing an easy task with the default settings. Operator Installation kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.9.4/operator.yaml Create Eventing Namespace kubectl create ns knative-eventing Install Default Knative Eventing kubectl apply -f - <<EOF apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing EOF Default Installation The default installation uses an in-memory channel which is not recommended in production use because it is best-effort. Simple Ping Source An easy way to test test the installation is to install a \"ping\" source. This is one of many sources provided by the Knative project. The examples below are almost identical except the uri parameter. Direktiv uses this to define the target namespaces. If the value is empty or / it will send the event to all namespace. If it contains a value e.g. /mynamespace it will send it to that namespace only. Event filters can be defined with a query parameter filter , e.g. /mynamespace?filter=myfilter . Events For All Namespaces kubectl apply -f - <<EOF apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : my-ping namespace : default spec : schedule : \"*/1 * * * *\" contentType : application/json data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : v1 kind : Service name : direktiv-eventing EOF Events For One Namespace kubectl apply -f - <<EOF apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : my-ping namespace : default spec : schedule : \"*/1 * * * *\" contentType : application/json data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /hello EOF","title":"Eventing"},{"location":"events/knative/knative/#preparing-direktiv","text":"Knative requires a sink to send events to Direktiv. Direktiv comes with a ready-to-use Knative sink but it has to be enabled. This can be done during installation or afterward with an helm upgrade . The following configuration in Direktiv's value.yaml adds the required sink service. Enabling Eventing eventing : enabled : true Upgrade Direktiv helm upgrade -f direktiv.yaml -n direktiv direktiv direktiv/direktiv After that change there is an additional service direktiv-eventing available in Direktiv's namespace.","title":"Preparing Direktiv"},{"location":"events/knative/knative/#knative-installation","text":"During the default installation Knative's operator has been installed an makes installing Knative eventing an easy task with the default settings. Operator Installation kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.9.4/operator.yaml Create Eventing Namespace kubectl create ns knative-eventing Install Default Knative Eventing kubectl apply -f - <<EOF apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing EOF Default Installation The default installation uses an in-memory channel which is not recommended in production use because it is best-effort.","title":"Knative Installation"},{"location":"events/knative/knative/#simple-ping-source","text":"An easy way to test test the installation is to install a \"ping\" source. This is one of many sources provided by the Knative project. The examples below are almost identical except the uri parameter. Direktiv uses this to define the target namespaces. If the value is empty or / it will send the event to all namespace. If it contains a value e.g. /mynamespace it will send it to that namespace only. Event filters can be defined with a query parameter filter , e.g. /mynamespace?filter=myfilter . Events For All Namespaces kubectl apply -f - <<EOF apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : my-ping namespace : default spec : schedule : \"*/1 * * * *\" contentType : application/json data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : v1 kind : Service name : direktiv-eventing EOF Events For One Namespace kubectl apply -f - <<EOF apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : my-ping namespace : default spec : schedule : \"*/1 * * * *\" contentType : application/json data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /hello EOF","title":"Simple Ping Source"},{"location":"events/knative/vmware/","text":"Connecting Direktiv and VMWare vSphere or ESXi via Knative Eventing is a very simple process because VMWare provides a Knative eventing source for their products. If Knative Eventing is configured with Direktiv there are just three simple steps required to connect these components. VMWare Version This example has been tested with 7.x and 8.x Installing VMWare Tanzu Sources The VMWare sources can be directly installed from the source repository with a kubectl command. Apply Tanzu Source kubectl apply -f https://github.com/vmware-tanzu/sources-for-knative/releases/download/v0.36.3/release.yaml VMWare Source Version Please check for the latest version of the VMWare sources After running the command there shouild be three pods available in the namespace vmware-sources : horizon-source-webhook vsphere-source-webhook horizon-source-controller Creating Credentials To connect to vSphere or ESXi the source needs the credentials and connectivity information. It requires a kubernetes secrets which will be consumed later by the actual source. VMWare Secret kubectl apply -f - <<EOF apiVersion : v1 kind : Secret metadata : name : vsphere-credentials namespace : vmware-sources type : kubernetes.io/basic-auth stringData : username : root password : MySecretPassword EOF The next stpe is to create the actual source. It requires the address and the reference to the crednetials used. The sink is the default Direktiv sink. The namespace of the sink might need to be adjusted to fit the installation namespace. Create Source kubectl apply -f - <<EOF apiVersion : sources.tanzu.vmware.com/v1alpha1 kind : VSphereSource metadata : name : source namespace : vmware-sources spec : # Where to fetch the events, and how to auth. address : https://192.168.220.128 skipTLSVerify : true secretRef : name : vsphere-credentials # Where to send the events. sink : ref : apiVersion : v1 kind : Service name : direktiv-eventing namespace : default uri : /hello?filter=test # sending to namespace vmware # Adjust checkpointing and event replay behavior checkpointConfig : maxAgeSeconds : 300 periodSeconds : 10 # Set the CloudEvent data encoding scheme to JSON payloadEncoding : application/json EOF Testing And Filtering By default there should be enough events generated within VMWare to see incoming events on the monitoring page of Direktiv. These events can be e.g. of type com.vmware.vsphere.VmBeingCreatedEvent.v0 or com.vmware.vsphere.VmStartingEvent.v0 . Because there a many events it might be worthwhile to filter them. For thie filtering there are two options. Either use the native Knative trigger/filter mechanism or Direktiv's built-in event filter. Direktiv supports Javascript filters for events and this example will rename logout events, drop login events and pass through all other events as-is. Direktiv Filter nslog ( \"event incoming\" ) if ( event [ \"type\" ] == \"com.vmware.vsphere.UserLogoutSessionEvent.v0\" ) { nslog ( \"logout type\" ) event [ \"type\" ] = \"vmware-logout\" } if ( event [ \"type\" ] == \"com.vmware.vsphere.UserLoginSessionEvent.v0\" ) { nslog ( \"drop login session\" ) return null } return event Applying Filter direktivctl events set-filter -n MYNAMESPACE -a http://DIREKTIV_SERVER filterName filter.js","title":"VMware Example"},{"location":"events/knative/vmware/#installing-vmware-tanzu-sources","text":"The VMWare sources can be directly installed from the source repository with a kubectl command. Apply Tanzu Source kubectl apply -f https://github.com/vmware-tanzu/sources-for-knative/releases/download/v0.36.3/release.yaml VMWare Source Version Please check for the latest version of the VMWare sources After running the command there shouild be three pods available in the namespace vmware-sources : horizon-source-webhook vsphere-source-webhook horizon-source-controller","title":"Installing VMWare Tanzu Sources"},{"location":"events/knative/vmware/#creating-credentials","text":"To connect to vSphere or ESXi the source needs the credentials and connectivity information. It requires a kubernetes secrets which will be consumed later by the actual source. VMWare Secret kubectl apply -f - <<EOF apiVersion : v1 kind : Secret metadata : name : vsphere-credentials namespace : vmware-sources type : kubernetes.io/basic-auth stringData : username : root password : MySecretPassword EOF The next stpe is to create the actual source. It requires the address and the reference to the crednetials used. The sink is the default Direktiv sink. The namespace of the sink might need to be adjusted to fit the installation namespace. Create Source kubectl apply -f - <<EOF apiVersion : sources.tanzu.vmware.com/v1alpha1 kind : VSphereSource metadata : name : source namespace : vmware-sources spec : # Where to fetch the events, and how to auth. address : https://192.168.220.128 skipTLSVerify : true secretRef : name : vsphere-credentials # Where to send the events. sink : ref : apiVersion : v1 kind : Service name : direktiv-eventing namespace : default uri : /hello?filter=test # sending to namespace vmware # Adjust checkpointing and event replay behavior checkpointConfig : maxAgeSeconds : 300 periodSeconds : 10 # Set the CloudEvent data encoding scheme to JSON payloadEncoding : application/json EOF","title":"Creating Credentials"},{"location":"events/knative/vmware/#testing-and-filtering","text":"By default there should be enough events generated within VMWare to see incoming events on the monitoring page of Direktiv. These events can be e.g. of type com.vmware.vsphere.VmBeingCreatedEvent.v0 or com.vmware.vsphere.VmStartingEvent.v0 . Because there a many events it might be worthwhile to filter them. For thie filtering there are two options. Either use the native Knative trigger/filter mechanism or Direktiv's built-in event filter. Direktiv supports Javascript filters for events and this example will rename logout events, drop login events and pass through all other events as-is. Direktiv Filter nslog ( \"event incoming\" ) if ( event [ \"type\" ] == \"com.vmware.vsphere.UserLogoutSessionEvent.v0\" ) { nslog ( \"logout type\" ) event [ \"type\" ] = \"vmware-logout\" } if ( event [ \"type\" ] == \"com.vmware.vsphere.UserLoginSessionEvent.v0\" ) { nslog ( \"drop login session\" ) return null } return event Applying Filter direktivctl events set-filter -n MYNAMESPACE -a http://DIREKTIV_SERVER filterName filter.js","title":"Testing And Filtering"},{"location":"examples/aws/","text":"AWS Examples AWS Examples on Github These examples should how you can communicate with AWS using the aws images. There are two examples, one is how to run a ec2 instance and the other is how to upload a file to a s3 bucket. These examples require the following namespace secrets to be set: ACCESS_KEY SECRET_ACCESS_KEY The flow will use these secrets to configure AWS access. Run EC2 Instance Flow Example This flow will create a new t2.small instance on ec2 ap-southeast-2 region. The flow uses the 'awsgo' action which executes the cli command passed in the command input property. Start AWS Instance direktiv_api : workflow/v1 functions : - id : aws-cli image : direktiv/aws-cli:dev type : knative-workflow states : - id : start-instance type : action action : secrets : [ \"ACCESS_KEY\" , \"ACCESS_SECRET\" ] function : aws-cli input : access-key : jq(.secrets.ACCESS_KEY) secret-key : jq(.secrets.ACCESS_SECRET) region : ap-southeast-2 commands : - command : aws ec2 run-instances --image-id ami-07620139298af599e --instance-type t2.small Upload File to S3 Bucket Example This flow will upload a file to a S3 bucket. The file name and data are set in the input. The input property fileData can be a url-encoded base64 string or a standard base64 string. Start AWS Instance direktiv_api : workflow/v1 functions : - id : s3 image : direktiv/aws-cli:dev type : knative-workflow states : - id : validate-input type : validate transform : 'jq(. + {fileData: .fileData | split(\"base64,\")[-1]})' schema : type : object required : - fileName - fileData properties : fileName : title : Filename description : Filename to be set in S3 bucket type : string fileData : title : File description : File to upload type : string format : data-url transition : store # stores the uploaded file as binary - id : store type : setter variables : - key : data scope : workflow mimeType : application/octet-stream value : 'jq(.fileData)' transition : upload-file - id : upload-file type : action action : function : s3 secrets : [ \"ACCESS_KEY\" , \"ACCESS_SECRET\" ] files : - key : data scope : workflow as : jq(.fileName) input : access-key : jq(.secrets.ACCESS_KEY) secret-key : jq(.secrets.ACCESS_SECRET) region : ap-southeast-2 commands : - command : aws s3 cp jq(.fileName) s3://direktiv/ Input { \"fileData\" : \"SGVsbG8sIHdvcmxkIQ==\" , \"fileName\" : \"message.txt\" }","title":"AWS Examples"},{"location":"examples/aws/#aws-examples","text":"AWS Examples on Github These examples should how you can communicate with AWS using the aws images. There are two examples, one is how to run a ec2 instance and the other is how to upload a file to a s3 bucket. These examples require the following namespace secrets to be set: ACCESS_KEY SECRET_ACCESS_KEY The flow will use these secrets to configure AWS access.","title":"AWS Examples"},{"location":"examples/aws/#run-ec2-instance-flow-example","text":"This flow will create a new t2.small instance on ec2 ap-southeast-2 region. The flow uses the 'awsgo' action which executes the cli command passed in the command input property. Start AWS Instance direktiv_api : workflow/v1 functions : - id : aws-cli image : direktiv/aws-cli:dev type : knative-workflow states : - id : start-instance type : action action : secrets : [ \"ACCESS_KEY\" , \"ACCESS_SECRET\" ] function : aws-cli input : access-key : jq(.secrets.ACCESS_KEY) secret-key : jq(.secrets.ACCESS_SECRET) region : ap-southeast-2 commands : - command : aws ec2 run-instances --image-id ami-07620139298af599e --instance-type t2.small","title":"Run EC2 Instance Flow Example"},{"location":"examples/aws/#upload-file-to-s3-bucket-example","text":"This flow will upload a file to a S3 bucket. The file name and data are set in the input. The input property fileData can be a url-encoded base64 string or a standard base64 string. Start AWS Instance direktiv_api : workflow/v1 functions : - id : s3 image : direktiv/aws-cli:dev type : knative-workflow states : - id : validate-input type : validate transform : 'jq(. + {fileData: .fileData | split(\"base64,\")[-1]})' schema : type : object required : - fileName - fileData properties : fileName : title : Filename description : Filename to be set in S3 bucket type : string fileData : title : File description : File to upload type : string format : data-url transition : store # stores the uploaded file as binary - id : store type : setter variables : - key : data scope : workflow mimeType : application/octet-stream value : 'jq(.fileData)' transition : upload-file - id : upload-file type : action action : function : s3 secrets : [ \"ACCESS_KEY\" , \"ACCESS_SECRET\" ] files : - key : data scope : workflow as : jq(.fileName) input : access-key : jq(.secrets.ACCESS_KEY) secret-key : jq(.secrets.ACCESS_SECRET) region : ap-southeast-2 commands : - command : aws s3 cp jq(.fileName) s3://direktiv/ Input { \"fileData\" : \"SGVsbG8sIHdvcmxkIQ==\" , \"fileName\" : \"message.txt\" }","title":"Upload File to S3 Bucket Example"},{"location":"examples/conditional-states/","text":"Conditional State Conditional State on Github This example demonstrates the use of a switch state to conditional transition to different states based on a jq expression. To show this, the example below is a flow that either approves or rejects a loan depending on the provided credit score and required minimum credit score. Simple Switch Statement direktiv_api : workflow/v1 description : | Conditionally transition to states depending if input credit score is higher or lower than creditMinRequired. states : - id : validate-input type : validate schema : type : object required : - creditScore - creditMinRequired properties : creditMinRequired : type : number title : Minimum credit score description : minimum credit score required for approval default : 500 creditScore : type : number description : credit score of user title : Credit Score transition : check-credit # # Check if the user's threshold is above minimum credit requirements. # If credit score meets requirements transition to approve-loan. Otherwise # transition to reject-loan. # - id : check-credit type : switch conditions : - condition : jq(.creditScore > .creditMinRequired) transition : approve-loan defaultTransition : reject-loan - id : reject-loan type : noop transform : 'jq({ \"msg\": \"You have been rejected for this loan\" })' - id : approve-loan type : noop transform : 'jq({ \"msg\": \"You have been approved for this loan\" })' Input { \"creditMinRequired\" : 500 , \"creditScore\" : 600 } Output { \"msg\" : \"You have been approved for this loan\" }","title":"Conditional State"},{"location":"examples/conditional-states/#conditional-state","text":"Conditional State on Github This example demonstrates the use of a switch state to conditional transition to different states based on a jq expression. To show this, the example below is a flow that either approves or rejects a loan depending on the provided credit score and required minimum credit score. Simple Switch Statement direktiv_api : workflow/v1 description : | Conditionally transition to states depending if input credit score is higher or lower than creditMinRequired. states : - id : validate-input type : validate schema : type : object required : - creditScore - creditMinRequired properties : creditMinRequired : type : number title : Minimum credit score description : minimum credit score required for approval default : 500 creditScore : type : number description : credit score of user title : Credit Score transition : check-credit # # Check if the user's threshold is above minimum credit requirements. # If credit score meets requirements transition to approve-loan. Otherwise # transition to reject-loan. # - id : check-credit type : switch conditions : - condition : jq(.creditScore > .creditMinRequired) transition : approve-loan defaultTransition : reject-loan - id : reject-loan type : noop transform : 'jq({ \"msg\": \"You have been rejected for this loan\" })' - id : approve-loan type : noop transform : 'jq({ \"msg\": \"You have been approved for this loan\" })' Input { \"creditMinRequired\" : 500 , \"creditScore\" : 600 } Output { \"msg\" : \"You have been approved for this loan\" }","title":"Conditional State"},{"location":"examples/counter-persistent-data/","text":"Counter Counter on Github A simple example that shows how to store a counter as a flow variable for persistent data. Any state data can be set to a variable to be used in later instances. If the variable does not exist it is empty but is getting created the first time it will be stored. Counter Example direktiv_api : workflow/v1 description : \"Simple Counter getter and setter variable example\" states : # # Get flow counter variable and increment value # - id : counter-get type : getter transition : counter-set variables : - key : counter scope : workflow transform : 'jq(. += {\"newCounter\": (.var.counter + 1)})' # # Set workflow counter variable # - id : counter-set type : setter variables : - key : counter scope : workflow value : 'jq(.newCounter)' Output Output { \"newCounter\" : 1 , \"var\" : { \"counter\" : 0 } }","title":"Counter"},{"location":"examples/counter-persistent-data/#counter","text":"Counter on Github A simple example that shows how to store a counter as a flow variable for persistent data. Any state data can be set to a variable to be used in later instances. If the variable does not exist it is empty but is getting created the first time it will be stored. Counter Example direktiv_api : workflow/v1 description : \"Simple Counter getter and setter variable example\" states : # # Get flow counter variable and increment value # - id : counter-get type : getter transition : counter-set variables : - key : counter scope : workflow transform : 'jq(. += {\"newCounter\": (.var.counter + 1)})' # # Set workflow counter variable # - id : counter-set type : setter variables : - key : counter scope : workflow value : 'jq(.newCounter)'","title":"Counter"},{"location":"examples/counter-persistent-data/#output","text":"Output { \"newCounter\" : 1 , \"var\" : { \"counter\" : 0 } }","title":"Output"},{"location":"examples/cron/","text":"Cron Cron on Github Direktiv flows can have different start actions. This can be a direct call or waiting for events. Another way of executing flows is the cron start definition. Cron direktiv_api : workflow/v1 start : type : scheduled cron : '* * * * *' # Trigger a new instance every minute. states : - id : run type : noop log : Run Cron","title":"Cron"},{"location":"examples/cron/#cron","text":"Cron on Github Direktiv flows can have different start actions. This can be a direct call or waiting for events. Another way of executing flows is the cron start definition. Cron direktiv_api : workflow/v1 start : type : scheduled cron : '* * * * *' # Trigger a new instance every minute. states : - id : run type : noop log : Run Cron","title":"Cron"},{"location":"examples/foreach/","text":"Foreach Foreach on Github The foreach state requires the array attribute to loop over. The difference to other states is that the data in the action of the foreach function is not getting the state data of the flow but the values provided in the array. Simple Foreach This is the most basic example. It shows that each action call in the foreach loop has it's own object during execution. In the flow scope there is a variable .names . But the array definition uses jq to iterate through .names and creates a list of JSON objects with the variable name . This means that each action only sees an object with the value name and has no access to names . Simple Foreach direktiv_api : workflow/v1 functions : - id : echo image : direktiv/echo:dev type : knative-workflow states : - id : data type : noop log : preparing foreach data transform : names : - hello - world - goodbye transition : foreach - id : foreach type : foreach array : 'jq([.names[] | { name: . }])' action : function : echo input : 'jq(.)' The output for this flow should be something like the following: Output { \"names\" : [ \"hello\" , \"world\" , \"goodbye\" ], \"return\" : [ { \"name\" : \"hello\" }, { \"name\" : \"world\" }, { \"name\" : \"goodbye\" } ] } Foreach with JQ This examples shows how to use JQ for a more complex foreach scenario. It generates an array based on .data in the first state. The JQ command is storing the state data .otherdata in the variable od . This result will be piped into the actual array generation with .data[] . In this case it is more obvious how each foreach action gets it's own JSON object. In this case the JQ command sets the name to the name in the array, time to the actual time with the JQ time function. The last attribute otherdata passes the original value from the flow state data into the action. JQ Foreach direktiv_api : workflow/v1 functions : - id : echo image : direktiv/echo:dev type : knative-workflow states : - id : data type : noop transform : data : - name : key1 value : value1 - name : key2 value : value2 - name : key3 value : value3 otherdata : somedata transition : foreach - id : foreach type : foreach array : 'jq(.otherdata as $od | [.data[] | { name: .name, time: now, otherdata: $od }])' action : function : echo input : 'jq(.)' Output { \"data\" : [ { \"name\" : \"key1\" , \"value\" : \"value1\" }, { \"name\" : \"key2\" , \"value\" : \"value2\" }, { \"name\" : \"key3\" , \"value\" : \"value3\" } ], \"otherdata\" : \"somedata\" , \"return\" : [ { \"name\" : \"key1\" , \"otherdata\" : \"somedata\" , \"time\" : 1680972341.2246315 }, { \"name\" : \"key2\" , \"otherdata\" : \"somedata\" , \"time\" : 1680972341.224634 }, { \"name\" : \"key3\" , \"otherdata\" : \"somedata\" , \"time\" : 1680972341.2246354 } ] } Foreach with JS This example uses Javascript to achieve the same outcome. If data structures are getting too complex it might be better to use Javascript for readability. If Javascript is used Direktiv passes in an object data which contains the flow state. Data can be accessed in the usual way like data[\"otherdata\"] . In the case of a foreach the Javascript function needs to return an array. JS Foreach direktiv_api : workflow/v1 functions : - id : echo image : direktiv/echo:dev type : knative-workflow states : - id : data type : noop transform : data : - name : key1 value : value1 - name : key2 value : value2 - name : key3 value : value3 otherdata : somedata transition : foreach - id : foreach type : foreach array : | js( // empty array const items = [] // loop over \"data\" attribute created in first state of flow for (let i = 0; i < data[\"data\"].length; i++) { // create object and set attributes item = new Object(); item.name = data[\"data\"][i][\"name\"] item.time = Date.now() item.otherdata = data[\"otherdata\"] // add item items[i] = item } // return array of items return items ) action : function : echo input : 'jq(.)'","title":"Foreach"},{"location":"examples/foreach/#foreach","text":"Foreach on Github The foreach state requires the array attribute to loop over. The difference to other states is that the data in the action of the foreach function is not getting the state data of the flow but the values provided in the array.","title":"Foreach"},{"location":"examples/foreach/#simple-foreach","text":"This is the most basic example. It shows that each action call in the foreach loop has it's own object during execution. In the flow scope there is a variable .names . But the array definition uses jq to iterate through .names and creates a list of JSON objects with the variable name . This means that each action only sees an object with the value name and has no access to names . Simple Foreach direktiv_api : workflow/v1 functions : - id : echo image : direktiv/echo:dev type : knative-workflow states : - id : data type : noop log : preparing foreach data transform : names : - hello - world - goodbye transition : foreach - id : foreach type : foreach array : 'jq([.names[] | { name: . }])' action : function : echo input : 'jq(.)' The output for this flow should be something like the following: Output { \"names\" : [ \"hello\" , \"world\" , \"goodbye\" ], \"return\" : [ { \"name\" : \"hello\" }, { \"name\" : \"world\" }, { \"name\" : \"goodbye\" } ] }","title":"Simple Foreach"},{"location":"examples/foreach/#foreach-with-jq","text":"This examples shows how to use JQ for a more complex foreach scenario. It generates an array based on .data in the first state. The JQ command is storing the state data .otherdata in the variable od . This result will be piped into the actual array generation with .data[] . In this case it is more obvious how each foreach action gets it's own JSON object. In this case the JQ command sets the name to the name in the array, time to the actual time with the JQ time function. The last attribute otherdata passes the original value from the flow state data into the action. JQ Foreach direktiv_api : workflow/v1 functions : - id : echo image : direktiv/echo:dev type : knative-workflow states : - id : data type : noop transform : data : - name : key1 value : value1 - name : key2 value : value2 - name : key3 value : value3 otherdata : somedata transition : foreach - id : foreach type : foreach array : 'jq(.otherdata as $od | [.data[] | { name: .name, time: now, otherdata: $od }])' action : function : echo input : 'jq(.)' Output { \"data\" : [ { \"name\" : \"key1\" , \"value\" : \"value1\" }, { \"name\" : \"key2\" , \"value\" : \"value2\" }, { \"name\" : \"key3\" , \"value\" : \"value3\" } ], \"otherdata\" : \"somedata\" , \"return\" : [ { \"name\" : \"key1\" , \"otherdata\" : \"somedata\" , \"time\" : 1680972341.2246315 }, { \"name\" : \"key2\" , \"otherdata\" : \"somedata\" , \"time\" : 1680972341.224634 }, { \"name\" : \"key3\" , \"otherdata\" : \"somedata\" , \"time\" : 1680972341.2246354 } ] }","title":"Foreach with JQ"},{"location":"examples/foreach/#foreach-with-js","text":"This example uses Javascript to achieve the same outcome. If data structures are getting too complex it might be better to use Javascript for readability. If Javascript is used Direktiv passes in an object data which contains the flow state. Data can be accessed in the usual way like data[\"otherdata\"] . In the case of a foreach the Javascript function needs to return an array. JS Foreach direktiv_api : workflow/v1 functions : - id : echo image : direktiv/echo:dev type : knative-workflow states : - id : data type : noop transform : data : - name : key1 value : value1 - name : key2 value : value2 - name : key3 value : value3 otherdata : somedata transition : foreach - id : foreach type : foreach array : | js( // empty array const items = [] // loop over \"data\" attribute created in first state of flow for (let i = 0; i < data[\"data\"].length; i++) { // create object and set attributes item = new Object(); item.name = data[\"data\"][i][\"name\"] item.time = Date.now() item.otherdata = data[\"otherdata\"] // add item items[i] = item } // return array of items return items ) action : function : echo input : 'jq(.)'","title":"Foreach with JS"},{"location":"examples/gcp-vm-destroy/","text":"Self-Destroying VM in GCP Self-Destroying VM in GCP on Github This example shows how to create virtual machines (VM) in Google cloud and delete after a certain time. This can be used for build processes or creating test instances. This example requires a service account JSON key in Google Cloud. It consist of three workflows. The create flow is responsible for creating the virtual machine. This example uses a Google Cloud VM but conceptually it works with every cloud provider. This flow returns all the important information about the created machine. At the end it starts a subflow which waits for an event to delete the virtual machine. If that event does not arrive and times out, the delete process starts even in the absence of that event. This flow has a validate state at the beginning and a transform to set defaults for the virtual machine creation. Create VM Flow direktiv_api : workflow/v1 functions : - id : gcp image : gcr.io/direktiv/functions/gcp:1.0 type : knative-workflow - id : deleter type : subflow workflow : deleter.yaml states : # validate input for flow - id : input type : validate schema : title : Create GCP VM type : object required : [ \"name\" ] properties : name : type : string title : VM Name disk : type : string title : Disk Size zone : type : string title : Zone machine : type : string title : Machine Type tags : type : array items : type : string transform : name : jq(.name) disk : jq(.disk // \"10GB\") zone : jq(.zone // \"us-west2-a\") machine : jq(.machine // \"e2-standard-16\") tags : jq(.tags // []) transition : gcp # create vm with parameters provided - id : gcp type : action action : function : gcp secrets : [ \"gcpJSONKey\" , \"gcpProject\" , \"gcpAccount\" ] input : account : jq(.secrets.gcpAccount) project : jq(.secrets.gcpProject) key : jq(.secrets.gcpJSONKey | @base64 ) commands : - command : gcloud compute instances create jq(.name) --boot-disk-size jq(.disk) --zone jq(.zone) --machine-type jq(.machine) jq(if .tags then \"--tags \" + (.tags | join(\",\")) end) --format=json transition : load-delete # start the delete flow - id : load-delete type : action async : true action : function : deleter input : name : jq(.name) zone : jq(.zone) The timeout defines how long that flow waits before it starts to delete the virtual machine. Delete Flow direktiv_api : workflow/v1 functions : - id : gcp image : gcr.io/direktiv/functions/gcp:1.0 type : knative-workflow states : # waits for the delete event, if it times out it deletes the VM anyways - id : wait type : consumeEvent timeout : PT1H log : waiting for delete event for jq(.name) event : type : io.direktiv.gcp.vm.delete context : name : jq(.name) transition : check-instance catch : - error : \"direktiv.cancels.timeout.soft\" transition : check-instance # lists instances to check if there is something to delete - id : check-instance type : action action : function : gcp secrets : [ \"gcpJSONKey\" , \"gcpProject\" , \"gcpAccount\" ] input : account : jq(.secrets.gcpAccount) project : jq(.secrets.gcpProject) key : jq(.secrets.gcpJSONKey | @base64 ) commands : - command : gcloud compute instances list --filter=\"name=jq(.name)\" --format json transition : length-check # if the previous state returns a VM it proceeds to deleting - id : length-check type : switch conditions : - condition : 'jq(.return.gcp[0].result | length > 0)' transition : delete - id : delete type : action action : function : gcp secrets : [ \"gcpJSONKey\" , \"gcpProject\" , \"gcpAccount\" ] input : account : jq(.secrets.gcpAccount) project : jq(.secrets.gcpProject) key : jq(.secrets.gcpJSONKey | @base64 ) commands : - command : gcloud compute instances delete jq(.name) --zone=jq(.zone) -q The virtual machine can be deleted by sending an event. This can come from outside via an API call or within a flow with the generateEvent state. Trigger Event Example direktiv_api : workflow/v1 states : - id : a type : generateEvent event : type : io.direktiv.gcp.vm.delete source : direktiv context : name : jq(.name)","title":"Self-Destroying VM in GCP"},{"location":"examples/gcp-vm-destroy/#self-destroying-vm-in-gcp","text":"Self-Destroying VM in GCP on Github This example shows how to create virtual machines (VM) in Google cloud and delete after a certain time. This can be used for build processes or creating test instances. This example requires a service account JSON key in Google Cloud. It consist of three workflows. The create flow is responsible for creating the virtual machine. This example uses a Google Cloud VM but conceptually it works with every cloud provider. This flow returns all the important information about the created machine. At the end it starts a subflow which waits for an event to delete the virtual machine. If that event does not arrive and times out, the delete process starts even in the absence of that event. This flow has a validate state at the beginning and a transform to set defaults for the virtual machine creation. Create VM Flow direktiv_api : workflow/v1 functions : - id : gcp image : gcr.io/direktiv/functions/gcp:1.0 type : knative-workflow - id : deleter type : subflow workflow : deleter.yaml states : # validate input for flow - id : input type : validate schema : title : Create GCP VM type : object required : [ \"name\" ] properties : name : type : string title : VM Name disk : type : string title : Disk Size zone : type : string title : Zone machine : type : string title : Machine Type tags : type : array items : type : string transform : name : jq(.name) disk : jq(.disk // \"10GB\") zone : jq(.zone // \"us-west2-a\") machine : jq(.machine // \"e2-standard-16\") tags : jq(.tags // []) transition : gcp # create vm with parameters provided - id : gcp type : action action : function : gcp secrets : [ \"gcpJSONKey\" , \"gcpProject\" , \"gcpAccount\" ] input : account : jq(.secrets.gcpAccount) project : jq(.secrets.gcpProject) key : jq(.secrets.gcpJSONKey | @base64 ) commands : - command : gcloud compute instances create jq(.name) --boot-disk-size jq(.disk) --zone jq(.zone) --machine-type jq(.machine) jq(if .tags then \"--tags \" + (.tags | join(\",\")) end) --format=json transition : load-delete # start the delete flow - id : load-delete type : action async : true action : function : deleter input : name : jq(.name) zone : jq(.zone) The timeout defines how long that flow waits before it starts to delete the virtual machine. Delete Flow direktiv_api : workflow/v1 functions : - id : gcp image : gcr.io/direktiv/functions/gcp:1.0 type : knative-workflow states : # waits for the delete event, if it times out it deletes the VM anyways - id : wait type : consumeEvent timeout : PT1H log : waiting for delete event for jq(.name) event : type : io.direktiv.gcp.vm.delete context : name : jq(.name) transition : check-instance catch : - error : \"direktiv.cancels.timeout.soft\" transition : check-instance # lists instances to check if there is something to delete - id : check-instance type : action action : function : gcp secrets : [ \"gcpJSONKey\" , \"gcpProject\" , \"gcpAccount\" ] input : account : jq(.secrets.gcpAccount) project : jq(.secrets.gcpProject) key : jq(.secrets.gcpJSONKey | @base64 ) commands : - command : gcloud compute instances list --filter=\"name=jq(.name)\" --format json transition : length-check # if the previous state returns a VM it proceeds to deleting - id : length-check type : switch conditions : - condition : 'jq(.return.gcp[0].result | length > 0)' transition : delete - id : delete type : action action : function : gcp secrets : [ \"gcpJSONKey\" , \"gcpProject\" , \"gcpAccount\" ] input : account : jq(.secrets.gcpAccount) project : jq(.secrets.gcpProject) key : jq(.secrets.gcpJSONKey | @base64 ) commands : - command : gcloud compute instances delete jq(.name) --zone=jq(.zone) -q The virtual machine can be deleted by sending an event. This can come from outside via an API call or within a flow with the generateEvent state. Trigger Event Example direktiv_api : workflow/v1 states : - id : a type : generateEvent event : type : io.direktiv.gcp.vm.delete source : direktiv context : name : jq(.name)","title":"Self-Destroying VM in GCP"},{"location":"examples/greeting-event-listener/","text":"Event-based Workflow Event-based Workflow on Github This example demonstrates a flow that waits for a cloud event with type greetingcloudevent . When the event is received, a state will be triggered using the data provided by the event. Because this flow has a start of type event, directly executing this flow is not necessary. To trigger the listener flow, a second flow will be created to generate the cloud event. The generate-greeting flow generates the greetingcloudevent that the eventbased-greeting flow is waiting for. Listener Workflow # Example Input: # This input is a cloud event and was generated from the greeting-generate flow. # { # \"greetingcloudevent\": { # \"data\": { # \"name\": \"Trent\" # }, # \"datacontenttype\": \"application/json\", # \"id\": \"2638e2d6-754e-409f-9038-f725e0d9d0af\", # \"source\": \"Direktiv\", # \"specversion\": \"1.0\", # \"type\": \"greetingcloudevent\" # } # } # # Example Output # { # \"return\": { # \"greeting\": \"Welcome to Direktiv, World!\" # } # } direktiv_api : workflow/v1 description : | Passively listen for cloud events where the type equals \"greetingcloudevent\" and then execute a action state to call the direktiv/greeting action, which 'greets' the user specified in the \"name\" field of the input provided to the flow. Because this flow has a start of type event, directly executing this flow is not necessary. # # Start of type event definition sets the flow to be executed when a event # is triggered with the defined type 'greetingcloudevent' # start : type : event state : greeter event : type : greetingcloudevent functions : - id : hello-world image : direktiv/hello-world:dev type : knative-workflow states : - id : greeter type : action log : jq(.greetingcloudevent.data.name) action : function : hello-world input : name : jq(.greetingcloudevent.data.name) transform : 'jq({ \"greeting\": .return.\"hello-world\" })' Output { \"return\" : { \"greeting\" : \"Welcome to Direktiv, World!\" } } Generator Workflow direktiv_api : workflow/v1 description : | Generate a cloud with of type \"greetingcloudevent\" with name data as input. states : # Example Generated Cloud Event: # { # \"greetingcloudevent\": { # \"data\": { # \"name\": \"World\" # }, # \"datacontenttype\": \"application/json\", # \"id\": \"2638e2d6-754e-409f-9038-f725e0d9d0af\", # \"source\": \"Direktiv\", # \"specversion\": \"1.0\", # \"type\": \"greetingcloudevent\" # } # } - id : gen type : generateEvent event : type : greetingcloudevent source : Direktiv data : name : \"World\"","title":"Event-based Workflow"},{"location":"examples/greeting-event-listener/#event-based-workflow","text":"Event-based Workflow on Github This example demonstrates a flow that waits for a cloud event with type greetingcloudevent . When the event is received, a state will be triggered using the data provided by the event. Because this flow has a start of type event, directly executing this flow is not necessary. To trigger the listener flow, a second flow will be created to generate the cloud event. The generate-greeting flow generates the greetingcloudevent that the eventbased-greeting flow is waiting for. Listener Workflow # Example Input: # This input is a cloud event and was generated from the greeting-generate flow. # { # \"greetingcloudevent\": { # \"data\": { # \"name\": \"Trent\" # }, # \"datacontenttype\": \"application/json\", # \"id\": \"2638e2d6-754e-409f-9038-f725e0d9d0af\", # \"source\": \"Direktiv\", # \"specversion\": \"1.0\", # \"type\": \"greetingcloudevent\" # } # } # # Example Output # { # \"return\": { # \"greeting\": \"Welcome to Direktiv, World!\" # } # } direktiv_api : workflow/v1 description : | Passively listen for cloud events where the type equals \"greetingcloudevent\" and then execute a action state to call the direktiv/greeting action, which 'greets' the user specified in the \"name\" field of the input provided to the flow. Because this flow has a start of type event, directly executing this flow is not necessary. # # Start of type event definition sets the flow to be executed when a event # is triggered with the defined type 'greetingcloudevent' # start : type : event state : greeter event : type : greetingcloudevent functions : - id : hello-world image : direktiv/hello-world:dev type : knative-workflow states : - id : greeter type : action log : jq(.greetingcloudevent.data.name) action : function : hello-world input : name : jq(.greetingcloudevent.data.name) transform : 'jq({ \"greeting\": .return.\"hello-world\" })' Output { \"return\" : { \"greeting\" : \"Welcome to Direktiv, World!\" } } Generator Workflow direktiv_api : workflow/v1 description : | Generate a cloud with of type \"greetingcloudevent\" with name data as input. states : # Example Generated Cloud Event: # { # \"greetingcloudevent\": { # \"data\": { # \"name\": \"World\" # }, # \"datacontenttype\": \"application/json\", # \"id\": \"2638e2d6-754e-409f-9038-f725e0d9d0af\", # \"source\": \"Direktiv\", # \"specversion\": \"1.0\", # \"type\": \"greetingcloudevent\" # } # } - id : gen type : generateEvent event : type : greetingcloudevent source : Direktiv data : name : \"World\"","title":"Event-based Workflow"},{"location":"examples/greeting/","text":"Greeting Example Greeting Example on Github This simple example flow uses a single action state to call the hello-world action, which 'greets' the user specified in the \"name\" field of the input provided to the flow. The validate state ensures the input is valid. Greeter Flow # Example Input: # { # \"name\": \"World\" # } # # Example Output: # The results of this action will contain a greeting addressed to the provided name. # { # \"return\": { # \"greeting\": \"Welcome to Direktiv, World!\" # } # } direktiv_api : workflow/v1 description : | Execute a action state to call the direktiv/greeting action, which 'greets' the user specified in the \"name\" field of the input provided to the flow. functions : - id : greeter image : direktiv/hello-world:dev type : knative-workflow states : - id : validate-input type : validate schema : type : object required : - name properties : name : type : string description : Name to greet title : Name transition : greeter # # Execute greeter action. # - id : greeter type : action log : jq(.) action : function : greeter input : name : jq(.name) transform : 'jq({ \"greeting\": .return.\"hello-world\" })' Input { \"name\" : \"World\" } The results of this action will contain a greeting addressed to the provided name. Output { \"greeting\" : \"Hello World\" }","title":"Greeting Example"},{"location":"examples/greeting/#greeting-example","text":"Greeting Example on Github This simple example flow uses a single action state to call the hello-world action, which 'greets' the user specified in the \"name\" field of the input provided to the flow. The validate state ensures the input is valid. Greeter Flow # Example Input: # { # \"name\": \"World\" # } # # Example Output: # The results of this action will contain a greeting addressed to the provided name. # { # \"return\": { # \"greeting\": \"Welcome to Direktiv, World!\" # } # } direktiv_api : workflow/v1 description : | Execute a action state to call the direktiv/greeting action, which 'greets' the user specified in the \"name\" field of the input provided to the flow. functions : - id : greeter image : direktiv/hello-world:dev type : knative-workflow states : - id : validate-input type : validate schema : type : object required : - name properties : name : type : string description : Name to greet title : Name transition : greeter # # Execute greeter action. # - id : greeter type : action log : jq(.) action : function : greeter input : name : jq(.name) transform : 'jq({ \"greeting\": .return.\"hello-world\" })' Input { \"name\" : \"World\" } The results of this action will contain a greeting addressed to the provided name. Output { \"greeting\" : \"Hello World\" }","title":"Greeting Example"},{"location":"examples/input-convert/","text":"Convert Input Convert Input on Github This example show how to handle input which is not JSON. This example uses a XLSX file and converts it to JSON to be used in the workflow. If Direktiv gets non-JSON input, in this case a binary file, it encodes it as Base64 and starts the workflow with a an input variable containing the binary file. Convert Flow direktiv_api : workflow/v1 functions : - id : csvkit image : direktiv/csvkit:dev type : knative-workflow # Fetch base64 input and store it workflow variable states : - id : set type : setter log : jq(.) variables : - key : in.xlsx # mark this a binary file mimeType : application/octet-stream # for non-JSON input the data ends up as base64 in .input value : 'jq(.input)' scope : workflow transition : convert # Takes the workflow variable and converts it - id : convert type : action action : function : csvkit files : - key : in.xlsx scope : workflow input : commands : - command : bash -c 'in2csv in.xlsx > out.csv' - command : csvjson out.csv transform : json : jq(.return.csvkit[1].result[0]) Push Data to Flow curl -XPOST --data-binary @data.xlsx http://MYSERVER/api/namespaces/examples/tree/input-convert/workflow?op=wait","title":"Convert Input"},{"location":"examples/input-convert/#convert-input","text":"Convert Input on Github This example show how to handle input which is not JSON. This example uses a XLSX file and converts it to JSON to be used in the workflow. If Direktiv gets non-JSON input, in this case a binary file, it encodes it as Base64 and starts the workflow with a an input variable containing the binary file. Convert Flow direktiv_api : workflow/v1 functions : - id : csvkit image : direktiv/csvkit:dev type : knative-workflow # Fetch base64 input and store it workflow variable states : - id : set type : setter log : jq(.) variables : - key : in.xlsx # mark this a binary file mimeType : application/octet-stream # for non-JSON input the data ends up as base64 in .input value : 'jq(.input)' scope : workflow transition : convert # Takes the workflow variable and converts it - id : convert type : action action : function : csvkit files : - key : in.xlsx scope : workflow input : commands : - command : bash -c 'in2csv in.xlsx > out.csv' - command : csvjson out.csv transform : json : jq(.return.csvkit[1].result[0]) Push Data to Flow curl -XPOST --data-binary @data.xlsx http://MYSERVER/api/namespaces/examples/tree/input-convert/workflow?op=wait","title":"Convert Input"},{"location":"examples/request-external-api/","text":"Request API Request API on Github This example shows how we can write a flow to communicate with a external API service. In this flowflow we will use the Direktiv request image to make a HTTP GET request to https://fakerapi.it/ and fetch the details of a fake person. A transform is also used to clean up the returned value from the action, but it can be commented out to see the full return value. API Request # Example Output: # { # \"person\": { # \"address\": { # \"buildingNumber\": \"8422\", # \"city\": \"Ashleytown\", # \"country\": \"Ethiopia\", # \"county_code\": \"AD\", # \"id\": 0, # \"latitude\": -21.509297, # \"longitude\": -48.162169, # \"street\": \"47933 Kennedi View Apt. 395\", # \"streetName\": \"Margie Stream\", # \"zipcode\": \"44788\" # }, # \"birthday\": \"1944-09-28\", # \"email\": \"qmetz@gmail.com\", # \"firstname\": \"Gabriella\", # \"gender\": \"female\", # \"id\": 1, # \"image\": \"http://placeimg.com/640/480/people\", # \"lastname\": \"Steuber\", # \"phone\": \"+5542223225627\", # \"website\": \"http://wiza.com\" # } # } direktiv_api : workflow/v1 description : | Execute a HTTP request to generate a persons details from the fake data API fakerapi. functions : - id : http-request image : direktiv/http-request:dev type : knative-workflow states : # # HTTP GET Fake person from fakerapi # Transform data to get data out of body # - id : get-fake-persons transform : \"jq({person: .return[0].result.data[0]})\" type : action action : function : http-request input : method : \"GET\" url : \"https://fakerapi.it/api/v1/persons?_quantity=1\" Output { \"person\" : { \"address\" : { \"buildingNumber\" : \"8422\" , \"city\" : \"Ashleytown\" , \"country\" : \"Ethiopia\" , \"county_code\" : \"AD\" , \"id\" : 0 , \"latitude\" : -21.509297 , \"longitude\" : -48.162169 , \"street\" : \"47933 Kennedi View Apt. 395\" , \"streetName\" : \"Margie Stream\" , \"zipcode\" : \"44788\" }, \"birthday\" : \"1944-09-28\" , \"email\" : \"qmetz@gmail.com\" , \"firstname\" : \"Gabriella\" , \"gender\" : \"female\" , \"id\" : 1 , \"image\" : \"http://placeimg.com/640/480/people\" , \"lastname\" : \"Steuber\" , \"phone\" : \"+5542223225627\" , \"website\" : \"http://wiza.com\" } }","title":"Request API"},{"location":"examples/request-external-api/#request-api","text":"Request API on Github This example shows how we can write a flow to communicate with a external API service. In this flowflow we will use the Direktiv request image to make a HTTP GET request to https://fakerapi.it/ and fetch the details of a fake person. A transform is also used to clean up the returned value from the action, but it can be commented out to see the full return value. API Request # Example Output: # { # \"person\": { # \"address\": { # \"buildingNumber\": \"8422\", # \"city\": \"Ashleytown\", # \"country\": \"Ethiopia\", # \"county_code\": \"AD\", # \"id\": 0, # \"latitude\": -21.509297, # \"longitude\": -48.162169, # \"street\": \"47933 Kennedi View Apt. 395\", # \"streetName\": \"Margie Stream\", # \"zipcode\": \"44788\" # }, # \"birthday\": \"1944-09-28\", # \"email\": \"qmetz@gmail.com\", # \"firstname\": \"Gabriella\", # \"gender\": \"female\", # \"id\": 1, # \"image\": \"http://placeimg.com/640/480/people\", # \"lastname\": \"Steuber\", # \"phone\": \"+5542223225627\", # \"website\": \"http://wiza.com\" # } # } direktiv_api : workflow/v1 description : | Execute a HTTP request to generate a persons details from the fake data API fakerapi. functions : - id : http-request image : direktiv/http-request:dev type : knative-workflow states : # # HTTP GET Fake person from fakerapi # Transform data to get data out of body # - id : get-fake-persons transform : \"jq({person: .return[0].result.data[0]})\" type : action action : function : http-request input : method : \"GET\" url : \"https://fakerapi.it/api/v1/persons?_quantity=1\" Output { \"person\" : { \"address\" : { \"buildingNumber\" : \"8422\" , \"city\" : \"Ashleytown\" , \"country\" : \"Ethiopia\" , \"county_code\" : \"AD\" , \"id\" : 0 , \"latitude\" : -21.509297 , \"longitude\" : -48.162169 , \"street\" : \"47933 Kennedi View Apt. 395\" , \"streetName\" : \"Margie Stream\" , \"zipcode\" : \"44788\" }, \"birthday\" : \"1944-09-28\" , \"email\" : \"qmetz@gmail.com\" , \"firstname\" : \"Gabriella\" , \"gender\" : \"female\" , \"id\" : 1 , \"image\" : \"http://placeimg.com/640/480/people\" , \"lastname\" : \"Steuber\" , \"phone\" : \"+5542223225627\" , \"website\" : \"http://wiza.com\" } }","title":"Request API"},{"location":"examples/scripting/","text":"Scripting Scripting on Github To use scripts like Python, Javascript, Powershell etc. A script can be loaded from flow or namespace variables. These files can be provided to the actions and executed. If the namespace is synced via Git a naming convention adds variables to worklflows. If a file has a prefix of a flow it will be added as variable. This enables Direktiv to use it as script in a flow, e.g.: myflow.yaml myflow.yaml.script.sh In the above example there would be a script.sh flow variable. The following flow example uses Python but any file type can be used, even binaries. Python Flow direktiv_api : workflow/v1 functions : - id : python image : direktiv/python:dev type : knative-workflow states : - id : python type : action action : function : python # use AWS key and secret secrets : [ \"AWS_ACCESS_KEY_ID\" , \"AWS_SECRET_ACCESS_KEY\" ] files : - key : python.py scope : workflow input : commands : - command : pip install boto3 - command : python3 python.py envs : - name : AWS_ACCESS_KEY_ID value : jq(.secrets.AWS_ACCESS_KEY_ID) - name : AWS_SECRET_ACCESS_KEY value : jq(.secrets.AWS_SECRET_ACCESS_KEY) - command : cat out.json transform : regions : jq(.return.python[1].result) python.py import boto3 import os import json session = boto3 . session . Session () regions = {} client = boto3 . client ( 'ec2' , region_name = 'us-east-1' ) ec2_regions = [ region [ 'RegionName' ] for region in client . describe_regions ()[ 'Regions' ]] for region in ec2_regions : print ( \"executing region \" + region ) vs = [] ec2_resource = session . resource ( 'ec2' , region_name = region ) for volume in ec2_resource . volumes . filter (): if volume . state == 'available' : vs . append ( volume . id ) if len ( vs ) > 0 : regions [ region ] = vs print ( \"added \" + str ( len ( vs )) + \"volumes for region \" + region ) # writes json to a file with open ( 'out.json' , 'w' ) as out_file : json . dump ( regions , out_file ) # writes json to a workflow variable with open ( 'out/workflow/out.json' , 'w' ) as out_file : json . dump ( regions , out_file )","title":"Scripting"},{"location":"examples/scripting/#scripting","text":"Scripting on Github To use scripts like Python, Javascript, Powershell etc. A script can be loaded from flow or namespace variables. These files can be provided to the actions and executed. If the namespace is synced via Git a naming convention adds variables to worklflows. If a file has a prefix of a flow it will be added as variable. This enables Direktiv to use it as script in a flow, e.g.: myflow.yaml myflow.yaml.script.sh In the above example there would be a script.sh flow variable. The following flow example uses Python but any file type can be used, even binaries. Python Flow direktiv_api : workflow/v1 functions : - id : python image : direktiv/python:dev type : knative-workflow states : - id : python type : action action : function : python # use AWS key and secret secrets : [ \"AWS_ACCESS_KEY_ID\" , \"AWS_SECRET_ACCESS_KEY\" ] files : - key : python.py scope : workflow input : commands : - command : pip install boto3 - command : python3 python.py envs : - name : AWS_ACCESS_KEY_ID value : jq(.secrets.AWS_ACCESS_KEY_ID) - name : AWS_SECRET_ACCESS_KEY value : jq(.secrets.AWS_SECRET_ACCESS_KEY) - command : cat out.json transform : regions : jq(.return.python[1].result) python.py import boto3 import os import json session = boto3 . session . Session () regions = {} client = boto3 . client ( 'ec2' , region_name = 'us-east-1' ) ec2_regions = [ region [ 'RegionName' ] for region in client . describe_regions ()[ 'Regions' ]] for region in ec2_regions : print ( \"executing region \" + region ) vs = [] ec2_resource = session . resource ( 'ec2' , region_name = region ) for volume in ec2_resource . volumes . filter (): if volume . state == 'available' : vs . append ( volume . id ) if len ( vs ) > 0 : regions [ region ] = vs print ( \"added \" + str ( len ( vs )) + \"volumes for region \" + region ) # writes json to a file with open ( 'out.json' , 'w' ) as out_file : json . dump ( regions , out_file ) # writes json to a workflow variable with open ( 'out/workflow/out.json' , 'w' ) as out_file : json . dump ( regions , out_file )","title":"Scripting"},{"location":"examples/solving-math-expressions/","text":"Solving Math Expressions Solving Math Expressions on Github This example shows how we can iterate over data using the ForEach state. Which executes an action that solves a math expression. The flow data input are the expressions you want to solve as a string array. The example demonstrates the use of an action isolate to solve a number of mathematical expressions using a foreach state. For each expression in the input array, the isolate will be run once. Solver Flow # Example Input: # { # \"expressions\": [ # \"4+10\", # \"15-14\", # \"100*3\", # \"200/2\" # ] # } # # Example Output: # The results of this foreach loop will be a json array of strings that have the solved answers. # { # \"solved\": [ # \"14\", # \"1\", # \"300\", # \"100\" # ] # } direktiv_api : workflow/v1 description : | Executes an action that solves a math expression. The workflow data input are the expressions you want to solve as a string array. functions : - id : solve-math-expression image : direktiv/bash:dev type : knative-workflow states : - id : validate-input type : validate schema : type : object required : - expressions properties : expressions : type : array description : expressions to solve title : Expressions items : type : string transition : solve # # Execute solve action. # - id : solve type : foreach array : 'jq([.expressions[] | { expression: . }])' action : function : solve-math-expression input : commands : - command : bash -c \"echo $((jq(.expression)))\" transform : 'jq({ solved: [.return[] | .bash[0].result ] })' Input { \"expressions\" : [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] } The results of this foreach loop will be a json array of strings that have the solved answers. Output { \"solved\" : [ \"14\" , \"1\" , \"300\" , \"100\" ] } Note: The array for a foreach state must be passed as an array of objects. This is why to iterate over the expressions string array, we must pipe it and construct a new array of objects using [.expressions[] | { expression: . }] . jq: .expressions [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] jq: [.expressions[] | { expression: . }] [ { \"expression\" : \"4+10\" }, { \"expression\" : \"15-14\" }, { \"expression\" : \"100*3\" }, { \"expression\" : \"200/2\" } ]","title":"Solving Math Expressions"},{"location":"examples/solving-math-expressions/#solving-math-expressions","text":"Solving Math Expressions on Github This example shows how we can iterate over data using the ForEach state. Which executes an action that solves a math expression. The flow data input are the expressions you want to solve as a string array. The example demonstrates the use of an action isolate to solve a number of mathematical expressions using a foreach state. For each expression in the input array, the isolate will be run once. Solver Flow # Example Input: # { # \"expressions\": [ # \"4+10\", # \"15-14\", # \"100*3\", # \"200/2\" # ] # } # # Example Output: # The results of this foreach loop will be a json array of strings that have the solved answers. # { # \"solved\": [ # \"14\", # \"1\", # \"300\", # \"100\" # ] # } direktiv_api : workflow/v1 description : | Executes an action that solves a math expression. The workflow data input are the expressions you want to solve as a string array. functions : - id : solve-math-expression image : direktiv/bash:dev type : knative-workflow states : - id : validate-input type : validate schema : type : object required : - expressions properties : expressions : type : array description : expressions to solve title : Expressions items : type : string transition : solve # # Execute solve action. # - id : solve type : foreach array : 'jq([.expressions[] | { expression: . }])' action : function : solve-math-expression input : commands : - command : bash -c \"echo $((jq(.expression)))\" transform : 'jq({ solved: [.return[] | .bash[0].result ] })' Input { \"expressions\" : [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] } The results of this foreach loop will be a json array of strings that have the solved answers. Output { \"solved\" : [ \"14\" , \"1\" , \"300\" , \"100\" ] } Note: The array for a foreach state must be passed as an array of objects. This is why to iterate over the expressions string array, we must pipe it and construct a new array of objects using [.expressions[] | { expression: . }] .","title":"Solving Math Expressions"},{"location":"examples/subflows/","text":"Subflows Subflows on Github Direktiv can use containers as actions but can also call subflows in the same way. It uses the same parameters and provides the same functionality. Parent Flow direktiv_api : workflow/v1 functions : # Define subflow function - id : sub workflow : subflow type : subflow # Call subflow with input values states : - id : call-sub type : action action : function : sub input : key : value Subflow direktiv_api : workflow/v1 states : - id : print type : noop log : jq(.)","title":"Subflows"},{"location":"examples/subflows/#subflows","text":"Subflows on Github Direktiv can use containers as actions but can also call subflows in the same way. It uses the same parameters and provides the same functionality. Parent Flow direktiv_api : workflow/v1 functions : # Define subflow function - id : sub workflow : subflow type : subflow # Call subflow with input values states : - id : call-sub type : action action : function : sub input : key : value Subflow direktiv_api : workflow/v1 states : - id : print type : noop log : jq(.)","title":"Subflows"},{"location":"examples/variable-mime-type/","text":"Variable Mime Type Example Variable Mime Type Example on Github All variables have an associated mime type to distinguish the content type of its value. This example will show two examples, and the special behaviour that happens when mimeType is text/plain or application/octet-stream . Storing a string as a raw plaintext variable. By default (mimeType=application/json) all variables are treated as JSON values. So this means even if you store a string in a variable, it's value is stored with quotes wrapped around it. JSON String Data direktiv_api : workflow/v1 description : | Store the workflow variable 'StringVar' as a json encoded string. states : # # Set StringVar Value: # \"hello\\nworld\" # - id : set-var type : setter variables : - key : StringVar scope : workflow value : | hello world JSON String Variable \"hello\\nworld\" If the data is YAML it will be converted to JSON in the variable. JSON Data direktiv_api : workflow/v1 description : | Store the workflow variable 'StringVar' as a json. states : # # Set StringVar Value: # \"hello\\nworld\" # - id : set-var type : setter variables : - key : StringVar scope : workflow value : - key : value JSON Variable [{ \"key\" : \"value\" }] There are certain scenarios where you would not want to store the variable with its quotes. To do this all need to do is simply set the mimeType to text/plain or text/plain; charset=utf-8 . This will store the variable as a raw string without quotes. Plain Text direktiv_api : workflow/v1 description : | Store the workflow variable 'StringVar' as a plaintext string. states : # # Set StringVar Value: # hello # world # - id : set-var type : setter variables : - key : StringVar scope : workflow mimeType : 'text/plain' value : | hello world Variable - StringVar Value Plain Text Variable hello world Auto-Decoding Base64 string Another special behaviour is that it's also possible to auto decode a base64 string by setting the mimeType to application/octet-stream . This is used for binaries like Excel files, images etc. Base64 Variable direktiv_api : workflow/v1 description : | Auto decode base64 string and store the resulting value as the workflow variable 'MessageVar'. states : # # Set MessageVar Value: # hello from direktiv # - id : set-var type : setter variables : - key : MessageVar scope : workflow value : 'aGVsbG8gZnJvbSBkaXJla3Rpdg==' mimeType : 'application/octet-stream' Variable - MessageVar Value Binary Data hello fr om direk t iv These are the only two mime types with special behaviour. Any other mimeType will be treated internally by the default JSON behaviour. The default value for mimeType is application/json","title":"Variable Mime Type Example"},{"location":"examples/variable-mime-type/#variable-mime-type-example","text":"Variable Mime Type Example on Github All variables have an associated mime type to distinguish the content type of its value. This example will show two examples, and the special behaviour that happens when mimeType is text/plain or application/octet-stream .","title":"Variable Mime Type Example"},{"location":"examples/variable-mime-type/#storing-a-string-as-a-raw-plaintext-variable","text":"By default (mimeType=application/json) all variables are treated as JSON values. So this means even if you store a string in a variable, it's value is stored with quotes wrapped around it. JSON String Data direktiv_api : workflow/v1 description : | Store the workflow variable 'StringVar' as a json encoded string. states : # # Set StringVar Value: # \"hello\\nworld\" # - id : set-var type : setter variables : - key : StringVar scope : workflow value : | hello world JSON String Variable \"hello\\nworld\" If the data is YAML it will be converted to JSON in the variable. JSON Data direktiv_api : workflow/v1 description : | Store the workflow variable 'StringVar' as a json. states : # # Set StringVar Value: # \"hello\\nworld\" # - id : set-var type : setter variables : - key : StringVar scope : workflow value : - key : value JSON Variable [{ \"key\" : \"value\" }] There are certain scenarios where you would not want to store the variable with its quotes. To do this all need to do is simply set the mimeType to text/plain or text/plain; charset=utf-8 . This will store the variable as a raw string without quotes. Plain Text direktiv_api : workflow/v1 description : | Store the workflow variable 'StringVar' as a plaintext string. states : # # Set StringVar Value: # hello # world # - id : set-var type : setter variables : - key : StringVar scope : workflow mimeType : 'text/plain' value : | hello world","title":"Storing a string as a raw plaintext variable."},{"location":"examples/variable-mime-type/#auto-decoding-base64-string","text":"Another special behaviour is that it's also possible to auto decode a base64 string by setting the mimeType to application/octet-stream . This is used for binaries like Excel files, images etc. Base64 Variable direktiv_api : workflow/v1 description : | Auto decode base64 string and store the resulting value as the workflow variable 'MessageVar'. states : # # Set MessageVar Value: # hello from direktiv # - id : set-var type : setter variables : - key : MessageVar scope : workflow value : 'aGVsbG8gZnJvbSBkaXJla3Rpdg==' mimeType : 'application/octet-stream'","title":"Auto-Decoding Base64 string"},{"location":"examples/variables/","text":"Variable Scopes Variable Scopes on Github Variable can be set on different scopes. Later in the flow they can be accessed within the same scope. The following scopes are available. instance: Only valid during the execution of the flow workflow: Stored as workflow variable and can be accessed from every intsance of the flow namespace: Namespace global scope and every workflow in the namespace can access it This example uses a setter state to set a variable in the instance scope. The second state set a workflow variable with the special output folder out in actions. Values can be stored in out/<SCOPE> and will be set after executing the action. The last state uses a transform to return the variables. Set Variables direktiv_api : workflow/v1 functions : - id : bash image : direktiv/bash:dev type : knative-workflow states : # Sets the variable in instance scope - id : set-value type : setter variables : - key : x scope : instance value : This is my value transition : set-value-fn # Sets the variable in workflow scope with writing to the special \"out\" folder - id : set-value-fn type : action action : function : bash input : commands : - command : bash -c 'echo \\\"my fn value\\\" > out/workflow/y' transition : get-values # fetch values - id : get-values type : getter variables : - key : x scope : instance - key : y scope : workflow transform : my-x : jq(.var.x) my-y : jq(.var.y)","title":"Variable Scopes"},{"location":"examples/variables/#variable-scopes","text":"Variable Scopes on Github Variable can be set on different scopes. Later in the flow they can be accessed within the same scope. The following scopes are available. instance: Only valid during the execution of the flow workflow: Stored as workflow variable and can be accessed from every intsance of the flow namespace: Namespace global scope and every workflow in the namespace can access it This example uses a setter state to set a variable in the instance scope. The second state set a workflow variable with the special output folder out in actions. Values can be stored in out/<SCOPE> and will be set after executing the action. The last state uses a transform to return the variables. Set Variables direktiv_api : workflow/v1 functions : - id : bash image : direktiv/bash:dev type : knative-workflow states : # Sets the variable in instance scope - id : set-value type : setter variables : - key : x scope : instance value : This is my value transition : set-value-fn # Sets the variable in workflow scope with writing to the special \"out\" folder - id : set-value-fn type : action action : function : bash input : commands : - command : bash -c 'echo \\\"my fn value\\\" > out/workflow/y' transition : get-values # fetch values - id : get-values type : getter variables : - key : x scope : instance - key : y scope : workflow transform : my-x : jq(.var.x) my-y : jq(.var.y)","title":"Variable Scopes"},{"location":"getting_started/","text":"If you're beginning your journey with Direktiv, there are two easy ways to do so. You can opt for a full installation or take the simpler route and use a Docker container that is fully equipped with everything required to get started - including a nested Kubernetes instance - or by utilizing a Multipass virtual machine setup. Docker (Linux only) The Docker image works on Linux only and can be used for easy development of flows on a local machine. Running Docker Image docker run --privileged -p 8080 :80 -ti direktiv/direktiv-kube Multipass (Linux, Mac, Windows) For Windows and Mac users in particular there is a Multipass cloud-init script to set up a Direktiv instance for testing and development. Running Multipass multipass launch --cpus 4 --disk 20G --memory 6G --name direktiv --cloud-init https://raw.githubusercontent.com/direktiv/direktiv/main/build/docker/all/multipass/init.yaml Warning multipass does not work in a VPN. The VPN needs to be turned off for this example installation. The development section has more details how to configure these instances and how to use them.","title":"Getting Started"},{"location":"getting_started/#docker-linux-only","text":"The Docker image works on Linux only and can be used for easy development of flows on a local machine. Running Docker Image docker run --privileged -p 8080 :80 -ti direktiv/direktiv-kube","title":"Docker (Linux only)"},{"location":"getting_started/#multipass-linux-mac-windows","text":"For Windows and Mac users in particular there is a Multipass cloud-init script to set up a Direktiv instance for testing and development. Running Multipass multipass launch --cpus 4 --disk 20G --memory 6G --name direktiv --cloud-init https://raw.githubusercontent.com/direktiv/direktiv/main/build/docker/all/multipass/init.yaml Warning multipass does not work in a VPN. The VPN needs to be turned off for this example installation. The development section has more details how to configure these instances and how to use them.","title":"Multipass (Linux, Mac, Windows)"},{"location":"getting_started/conditional-transitions/","text":"Conditional Transitions Oftentimes a flow needs to be a little bit smarter than an immutable sequence of states. That's when conditional transitions are required. For these cases Direktiv provides a switch state which can route the flow based on conditions. Each condition can route the flow to a different state but there can be a defaultTransition to transition to if none of the conditions are true. Loop Demo direktiv_api : workflow/v1 functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : ifelse type : switch defaultTransition : done conditions : - condition : jq(.names) transition : poster - id : poster type : action action : function : httprequest input : method : POST url : https://jsonplaceholder.typicode.com/posts content : name : jq(.names[0]) transform : jq(del(.names[0])) transition : ifelse - id : done type : noop transform : done Input { \"names\" : [ \"Michael\" , \"Thomas\" , \"Kevin\" ] } Output { \"done\" : \"yes\" } In this example the switch state will transition to poster until the list of names is empty, at which point the flow will transition to the default transition done . Switch State The Switch State can make decisions about where to transition to next based on the instance data by evaluating a number of jq expressions and checking the results. Here's an example switch state definition: - id : ifelse type : switch conditions : - condition : 'jq(.person.age > 18)' transition : accept #transform: - condition : 'jq(.person.age != nil)' transition : reject #transform: defaultTransition : failure #defaultTransform: Each of the conditions will be evaluated in the order it appears by running the jq command in condition . Any result other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a successful match. If no conditions match the default transition will be used. Other Conditional Transitions The Switch State is not the only way to do conditional transitions. The eventsXor state also transitions conditionally based on which CloudEvent was received. All states can also define handlers for catching various types of errors. Loops By transitioning to a state that has already happened it's possible to create loops in flow instances. In this example we have got a type of range loop, iterating over the contents of an array. Direktiv sets limits for the number of transitions an instance can make in order to protect itself from infinitely-looping flows. This is an example only and in this case a foreach is a better solution.","title":"Conditional Transitions"},{"location":"getting_started/conditional-transitions/#conditional-transitions","text":"Oftentimes a flow needs to be a little bit smarter than an immutable sequence of states. That's when conditional transitions are required. For these cases Direktiv provides a switch state which can route the flow based on conditions. Each condition can route the flow to a different state but there can be a defaultTransition to transition to if none of the conditions are true. Loop Demo direktiv_api : workflow/v1 functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : ifelse type : switch defaultTransition : done conditions : - condition : jq(.names) transition : poster - id : poster type : action action : function : httprequest input : method : POST url : https://jsonplaceholder.typicode.com/posts content : name : jq(.names[0]) transform : jq(del(.names[0])) transition : ifelse - id : done type : noop transform : done Input { \"names\" : [ \"Michael\" , \"Thomas\" , \"Kevin\" ] } Output { \"done\" : \"yes\" } In this example the switch state will transition to poster until the list of names is empty, at which point the flow will transition to the default transition done .","title":"Conditional Transitions"},{"location":"getting_started/conditional-transitions/#switch-state","text":"The Switch State can make decisions about where to transition to next based on the instance data by evaluating a number of jq expressions and checking the results. Here's an example switch state definition: - id : ifelse type : switch conditions : - condition : 'jq(.person.age > 18)' transition : accept #transform: - condition : 'jq(.person.age != nil)' transition : reject #transform: defaultTransition : failure #defaultTransform: Each of the conditions will be evaluated in the order it appears by running the jq command in condition . Any result other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a successful match. If no conditions match the default transition will be used.","title":"Switch State"},{"location":"getting_started/conditional-transitions/#other-conditional-transitions","text":"The Switch State is not the only way to do conditional transitions. The eventsXor state also transitions conditionally based on which CloudEvent was received. All states can also define handlers for catching various types of errors.","title":"Other Conditional Transitions"},{"location":"getting_started/conditional-transitions/#loops","text":"By transitioning to a state that has already happened it's possible to create loops in flow instances. In this example we have got a type of range loop, iterating over the contents of an array. Direktiv sets limits for the number of transitions an instance can make in order to protect itself from infinitely-looping flows. This is an example only and in this case a foreach is a better solution.","title":"Loops"},{"location":"getting_started/error-handling/","text":"Error Handling One obvious use for loops is to retry some logic if an error occurs, but there's no need to design looping flow because Direktiv has configurable error catching & retrying available on every action-based state. This will be discussed in a later article. Handling errors can be an important part of a flow. Demo direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : do-request type : action action : function : http-request input : url : http://doesnotexist.xy retries : max_attempts : 2 delay : PT5S multiplier : 2.0 codes : [ \".*\" ] In this example a request is being made to an URL. This URL does not exist to simulate the retry mechanism. It uses the multiplier to try within 5 seconds the first time and 10 seconds the second time. Catchable Errors Errors that occur during instance execution usually are considered \"catchable\". Any flow state may optionally define error catchers, and if a catchable error is raised Direktiv will check to see if any catchers can handle it. Errors have a \"code\", which is a string formatted in a style similar to a domain name. Error catchers can explicitly catch a single error code or they can use * wildcards in their error codes to catch ranges of errors. Setting the error catcher to just \" * \" means it will handle any error, so long as no catcher defined higher up in the list has already caught it. If no catcher is able to handle an error, the flow will fail immediately. direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : do-request type : action action : function : http-request input : url : http://doesnotexist.xy retries : max_attempts : 2 delay : PT5S multiplier : 2.0 codes : [ \".*\" ] catch : - error : \"direktiv.retries.exceeded\" transition : handle-error - id : handle-error type : noop log : this did not work In this case the flow catches the failed retries and transitions to handle-error and the flow finished successful. Every other error will mark the flow execution as failed. Uncatchable Errors Rarely, some errors are considered \"uncatchable\", but generally an uncatchable error becomes catchable if escalated to a calling flow. One example of this is the error triggered by Direktiv if a flow fails to complete within its maximum timeout. If a flow fails to complete within its maximum timeout it will not be given an opportunity to catch the error and continue running. But if that flow is running as a subflow its parentflow will be able to detect and handle that error. Retries Action definitions may optionally define a retry strategy. If a retry strategy is defined the catcher's transition won't be used and no error will be escalated for retryable errors until all retries have failed. A retry strategy might look like the following: retry : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] In this example you can see that a maximum number of attempts is defined, alongside an initial delay between attempts and a multiplication factor to apply to the delay between subsequent attempts. Recovery Flows sometimes perform actions which may need to be reverted or undone if the flow as a whole cannot complete successfully. Solving these problems requires careful use of error catchers and transitions. Cause Errors Sometimes it is important to fail the flow with a custom error. This is possible with the error state. This can used e.g. in switch states. direktiv_api : workflow/v1 states : - id : a type : switch defaultTransition : fail conditions : - condition : 'jq(.y == true)' - id : fail type : error error : badinput message : 'value y not set' In this example if the payload does not contain y: true the flow fails. The error throwns badinput is thrown and the flow failed. The error badinput could be caught by a parent flow.","title":"Error Handling"},{"location":"getting_started/error-handling/#error-handling","text":"One obvious use for loops is to retry some logic if an error occurs, but there's no need to design looping flow because Direktiv has configurable error catching & retrying available on every action-based state. This will be discussed in a later article. Handling errors can be an important part of a flow.","title":"Error Handling"},{"location":"getting_started/error-handling/#demo","text":"direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : do-request type : action action : function : http-request input : url : http://doesnotexist.xy retries : max_attempts : 2 delay : PT5S multiplier : 2.0 codes : [ \".*\" ] In this example a request is being made to an URL. This URL does not exist to simulate the retry mechanism. It uses the multiplier to try within 5 seconds the first time and 10 seconds the second time.","title":"Demo"},{"location":"getting_started/error-handling/#catchable-errors","text":"Errors that occur during instance execution usually are considered \"catchable\". Any flow state may optionally define error catchers, and if a catchable error is raised Direktiv will check to see if any catchers can handle it. Errors have a \"code\", which is a string formatted in a style similar to a domain name. Error catchers can explicitly catch a single error code or they can use * wildcards in their error codes to catch ranges of errors. Setting the error catcher to just \" * \" means it will handle any error, so long as no catcher defined higher up in the list has already caught it. If no catcher is able to handle an error, the flow will fail immediately. direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : do-request type : action action : function : http-request input : url : http://doesnotexist.xy retries : max_attempts : 2 delay : PT5S multiplier : 2.0 codes : [ \".*\" ] catch : - error : \"direktiv.retries.exceeded\" transition : handle-error - id : handle-error type : noop log : this did not work In this case the flow catches the failed retries and transitions to handle-error and the flow finished successful. Every other error will mark the flow execution as failed.","title":"Catchable Errors"},{"location":"getting_started/error-handling/#uncatchable-errors","text":"Rarely, some errors are considered \"uncatchable\", but generally an uncatchable error becomes catchable if escalated to a calling flow. One example of this is the error triggered by Direktiv if a flow fails to complete within its maximum timeout. If a flow fails to complete within its maximum timeout it will not be given an opportunity to catch the error and continue running. But if that flow is running as a subflow its parentflow will be able to detect and handle that error.","title":"Uncatchable Errors"},{"location":"getting_started/error-handling/#retries","text":"Action definitions may optionally define a retry strategy. If a retry strategy is defined the catcher's transition won't be used and no error will be escalated for retryable errors until all retries have failed. A retry strategy might look like the following: retry : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] In this example you can see that a maximum number of attempts is defined, alongside an initial delay between attempts and a multiplication factor to apply to the delay between subsequent attempts.","title":"Retries"},{"location":"getting_started/error-handling/#recovery","text":"Flows sometimes perform actions which may need to be reverted or undone if the flow as a whole cannot complete successfully. Solving these problems requires careful use of error catchers and transitions.","title":"Recovery"},{"location":"getting_started/error-handling/#cause-errors","text":"Sometimes it is important to fail the flow with a custom error. This is possible with the error state. This can used e.g. in switch states. direktiv_api : workflow/v1 states : - id : a type : switch defaultTransition : fail conditions : - condition : 'jq(.y == true)' - id : fail type : error error : badinput message : 'value y not set' In this example if the payload does not contain y: true the flow fails. The error throwns badinput is thrown and the flow failed. The error badinput could be caught by a parent flow.","title":"Cause Errors"},{"location":"getting_started/events/","text":"Events Direktiv has built-in support for CloudEvents, which can be a great way to interact with flows. The following flow has a start condition based on events. It would only start if an event of type com.github.pull.create arrives with the source set to https://github.com/cloudevents/spec/pull Example Workflow direktiv_api : workflow/v1 start : type : event event : type : com.github.pull.create context : source : https://github.com/cloudevents/spec/pull functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : notify type : action action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.\"com.github.pull.create\")' CloudEvents CloudEvents are specification for describing event data in a common way. They're JSON objects with a number of required fields, some optional fields, and a payload. Sample Cloudevent { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } CloudEvents can be sent via the API to a namespace or generated by flow within Direktiv, to be handled by any number of interested receivers on that namespace. Start Types The most common use for events in Direktiv is to have external services generate CloudEvents and send them to Direktiv to trigger your flows. But to make your flows trigger on an event you need to register the flow's interest in the event by adding the appropriate start type to your workflow definition: direktiv_api : workflow/v1 start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" In this example a new instance will be created whenever a cloudevent is received that has the matching type and source values. Two other event-based start types exist in Direktiv: the eventsXor , and the eventsAnd . The eventsXor registers an interest in multiple events and will trigger a new instance as soon as any one of them is received. The eventsAnd also registers an interest in multiple events, but will only trigger once all have been received. Event Payloads Whenever an event is received its payload will be added to the instance data under a field with the same name as the event \"type\". This allows for a uniform approach to accepting events that supports single events, eventsXor, and eventsAnd. The payload itself consists of the full cloudevent including attributes, extension context attributes and data. Instances Waiting for Events Triggering flows is not the only thing you can do with events. Flows can be constructed to run some logic and then wait for an event before proceeding. Like the event-based start types, there are three event consuming states: consumeEvent , eventsXor , and eventsAnd . Waiting Within A Flow - id : wait-event type : consumeEvent event : type : com.github.pull.create context : source : \"https://github.com/cloudevents/spec/pull\" repository : 'jq(.repo)' timeout : PT5M transform : 'jq(.\"com.github.pull.create\")' transition : next-state Timeouts It's rarely a good idea to leave a flow waiting indefinitely. Direktiv allows you to define timeouts in ISO8601 format when waiting on an event. If the state is not ready to proceed before the timeout has elapsed an error will be thrown. It's possible to catch the error direktiv.cancels.timeout.soft . The timeout field is not required, but Direktiv caps the maximum timeout whether specified or not to prevent flows from living forever. The default timeout is 15 minutes. Context Event-consuming states have a context field. The context field can restrict which events are considered matches by requiring an exact match on a CloudEvent context field. This can be used to link certain events to e.g. customer ids or transaction ids. GenerateEvent State Flows can generate events for their namespace. The fields for this state are fairly self-explanatory. Here's an example: - id : gen-event type : generateEvent event : type : \"my.custom.event\" source : \"direktiv\" data : 'jq(.)' datacontenttype : \"application/json\" If the jq command that populates the data field outputs a plain base64 encoded string and the datacontenttype field is set to anything other than application/json Direktiv will decode the string before sending the event.","title":"Events in Direktiv"},{"location":"getting_started/events/#events","text":"Direktiv has built-in support for CloudEvents, which can be a great way to interact with flows. The following flow has a start condition based on events. It would only start if an event of type com.github.pull.create arrives with the source set to https://github.com/cloudevents/spec/pull Example Workflow direktiv_api : workflow/v1 start : type : event event : type : com.github.pull.create context : source : https://github.com/cloudevents/spec/pull functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : notify type : action action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.\"com.github.pull.create\")'","title":"Events"},{"location":"getting_started/events/#cloudevents","text":"CloudEvents are specification for describing event data in a common way. They're JSON objects with a number of required fields, some optional fields, and a payload. Sample Cloudevent { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } CloudEvents can be sent via the API to a namespace or generated by flow within Direktiv, to be handled by any number of interested receivers on that namespace.","title":"CloudEvents"},{"location":"getting_started/events/#start-types","text":"The most common use for events in Direktiv is to have external services generate CloudEvents and send them to Direktiv to trigger your flows. But to make your flows trigger on an event you need to register the flow's interest in the event by adding the appropriate start type to your workflow definition: direktiv_api : workflow/v1 start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" In this example a new instance will be created whenever a cloudevent is received that has the matching type and source values. Two other event-based start types exist in Direktiv: the eventsXor , and the eventsAnd . The eventsXor registers an interest in multiple events and will trigger a new instance as soon as any one of them is received. The eventsAnd also registers an interest in multiple events, but will only trigger once all have been received.","title":"Start Types"},{"location":"getting_started/events/#event-payloads","text":"Whenever an event is received its payload will be added to the instance data under a field with the same name as the event \"type\". This allows for a uniform approach to accepting events that supports single events, eventsXor, and eventsAnd. The payload itself consists of the full cloudevent including attributes, extension context attributes and data.","title":"Event Payloads"},{"location":"getting_started/events/#instances-waiting-for-events","text":"Triggering flows is not the only thing you can do with events. Flows can be constructed to run some logic and then wait for an event before proceeding. Like the event-based start types, there are three event consuming states: consumeEvent , eventsXor , and eventsAnd . Waiting Within A Flow - id : wait-event type : consumeEvent event : type : com.github.pull.create context : source : \"https://github.com/cloudevents/spec/pull\" repository : 'jq(.repo)' timeout : PT5M transform : 'jq(.\"com.github.pull.create\")' transition : next-state","title":"Instances Waiting for Events"},{"location":"getting_started/events/#generateevent-state","text":"Flows can generate events for their namespace. The fields for this state are fairly self-explanatory. Here's an example: - id : gen-event type : generateEvent event : type : \"my.custom.event\" source : \"direktiv\" data : 'jq(.)' datacontenttype : \"application/json\" If the jq command that populates the data field outputs a plain base64 encoded string and the datacontenttype field is set to anything other than application/json Direktiv will decode the string before sending the event.","title":"GenerateEvent State"},{"location":"getting_started/functions-intro/","text":"Introduction to Functions Flows wouldn't be very powerful if they were limited to just the predefined states. That's why Direktiv can run \"functions\" which are basically serverless containers or even a separate flow, referred to as a subflow . Direktiv uses the action state to provide this functionality. Example direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : getter type : action action : function : http-request input : url : \"https://jsonplaceholder.typicode.com/todos/1\" If the flow requires function to run they need to be defined in the functions section. There are different types of functions and the type is specified in the type attribute. The value can be one of the following: Function Types knative-workflow This function is for the flow only and will not be re-used across different flows. This type requires an image name to use. The image name should point to a valid container image in a remote registry like Docker Hub, GCR, Azure etc. Two additional attributes can be provided: size : Sometimes functions need a different size in terms of CPU and memory. Possible values are small , medium and large . The definition of those values can be configured in Direktiv's configuration files via Helm chart. cmd : The function can use a different command in the container if it is supported by the function container. knative-namespace If a function is used frequently by different flows it can be shared across flows with this type. They can be created under Services in the user interface or via API. - id : http-request service : request type : knative-namespace For these types a scale attribute can be defined on creation of the service which sets the minimum instances to run in the cluster and therefore reducing or eliminating the warm-up time and the function can run immediately. subflow In Direktiv a function can be a subflows as well. The behaviour is the same as calling serverless container functions. If used the flow provides the input for the subflow and accepts the response of the subflow as result. - id : http-request workflow : my-subflow type : subflow Input Value The input value for the function is set in input in. This YAML object under input will be send as JSON to the function container and can a multi-level nested object as well. Different containers require different inputs depending on their functionality. This concept is important for custom functions . - id : getter type : action action : function : http-request input : url : \"https://jsonplaceholder.typicode.com/todos/1\" Return Value Every time a function is called the response is stored in return in the state data and can be processed via e.g. transform or switch . The next function call overwrites the return value so if data is required from a function accross multiple states it needs to be stored with a transition. In the example above the state data after executing the flow would have an additional JSON object with information about the headers and the content of the HTTP request in the return attribute. { \"return\" : [ { \"code\" : 200 , \"headers\" : { \"Access-Control-Allow-Credentials\" : [ \"true\" ], \"Age\" : [ \"20706\" ] }, \"result\" : { \"completed\" : false , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"userId\" : 1 }, \"status\" : \"200 OK\" , \"success\" : true } ] } Store Value As mentioned earlier, every return of a function is getting overwritten with the next function call. Therefore it is important to store the data in the state if it is needed later in the flow. This can be done with a simple transform at the end of the action state. In the example here we store only the response status. direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : getter type : action action : function : http-request input : url : \"https://jsonplaceholder.typicode.com/todos/1\" transform : status : jq(.return[0].status) The http request function returns an array so the JQ command would be .return[0] to get the 0th item from it and the .status fetches the status of that item. More function can be found at apps.direktiv.io . Foreach Another way to call functions is the foreach function. This is useful if an array of objects need to be processed the same way, e.g. executing multiple http requests. direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : getter type : foreach array : |- jq( [ { \"url\": \"https://jsonplaceholder.typicode.com/todos/1\"}, { \"url\": \"https://www.direktiv.io\"}] ) action : function : http-request input : url : jq(.url) This foreach call the same function but uses an array of objects. There are a few simple requirements for foreach states. The array has to be a list of objects not e.g. an array of strings. The input attribute has only access to the object it is iterating over at that time. It does not have access to state data at all. Parallel The parallel execution can be used if the flow needs to execute functions in parallel with the same state data. An example would be quality gates during a release process where functional tests and load test can potentially be run in parallel. direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow - id : python image : gcr.io/direktiv/functions/python:1.0 type : knative-workflow states : - id : execute-both type : parallel mode : and actions : - function : http-request input : url : https://www.direktiv.io - function : python input : commands : - command : python3 -c 'import os;print(os.environ[\"hello\"])' envs : - name : hello value : world Additionally a mode attribute can be set to either or or and to define if all actions need to return successfully or only one.","title":"Introduction to Functions"},{"location":"getting_started/functions-intro/#introduction-to-functions","text":"Flows wouldn't be very powerful if they were limited to just the predefined states. That's why Direktiv can run \"functions\" which are basically serverless containers or even a separate flow, referred to as a subflow . Direktiv uses the action state to provide this functionality.","title":"Introduction to Functions"},{"location":"getting_started/functions-intro/#function-types","text":"","title":"Function Types"},{"location":"getting_started/functions-intro/#input-value","text":"The input value for the function is set in input in. This YAML object under input will be send as JSON to the function container and can a multi-level nested object as well. Different containers require different inputs depending on their functionality. This concept is important for custom functions . - id : getter type : action action : function : http-request input : url : \"https://jsonplaceholder.typicode.com/todos/1\"","title":"Input Value"},{"location":"getting_started/functions-intro/#return-value","text":"Every time a function is called the response is stored in return in the state data and can be processed via e.g. transform or switch . The next function call overwrites the return value so if data is required from a function accross multiple states it needs to be stored with a transition. In the example above the state data after executing the flow would have an additional JSON object with information about the headers and the content of the HTTP request in the return attribute. { \"return\" : [ { \"code\" : 200 , \"headers\" : { \"Access-Control-Allow-Credentials\" : [ \"true\" ], \"Age\" : [ \"20706\" ] }, \"result\" : { \"completed\" : false , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"userId\" : 1 }, \"status\" : \"200 OK\" , \"success\" : true } ] }","title":"Return Value"},{"location":"getting_started/functions-intro/#store-value","text":"As mentioned earlier, every return of a function is getting overwritten with the next function call. Therefore it is important to store the data in the state if it is needed later in the flow. This can be done with a simple transform at the end of the action state. In the example here we store only the response status. direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : getter type : action action : function : http-request input : url : \"https://jsonplaceholder.typicode.com/todos/1\" transform : status : jq(.return[0].status) The http request function returns an array so the JQ command would be .return[0] to get the 0th item from it and the .status fetches the status of that item. More function can be found at apps.direktiv.io .","title":"Store Value"},{"location":"getting_started/functions-intro/#foreach","text":"Another way to call functions is the foreach function. This is useful if an array of objects need to be processed the same way, e.g. executing multiple http requests. direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : getter type : foreach array : |- jq( [ { \"url\": \"https://jsonplaceholder.typicode.com/todos/1\"}, { \"url\": \"https://www.direktiv.io\"}] ) action : function : http-request input : url : jq(.url) This foreach call the same function but uses an array of objects. There are a few simple requirements for foreach states. The array has to be a list of objects not e.g. an array of strings. The input attribute has only access to the object it is iterating over at that time. It does not have access to state data at all.","title":"Foreach"},{"location":"getting_started/functions-intro/#parallel","text":"The parallel execution can be used if the flow needs to execute functions in parallel with the same state data. An example would be quality gates during a release process where functional tests and load test can potentially be run in parallel. direktiv_api : workflow/v1 functions : - id : http-request image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow - id : python image : gcr.io/direktiv/functions/python:1.0 type : knative-workflow states : - id : execute-both type : parallel mode : and actions : - function : http-request input : url : https://www.direktiv.io - function : python input : commands : - command : python3 -c 'import os;print(os.environ[\"hello\"])' envs : - name : hello value : world Additionally a mode attribute can be set to either or or and to define if all actions need to return successfully or only one.","title":"Parallel"},{"location":"getting_started/namespaces/","text":"Direktiv namespaces allow you the flexibility to divide projects, teams or use-cases. These spaces are totally seperate and independent of each other in terms of e.g. flows, secrets and services. You can easily create a namespace using the user interface or through an API call. Namespaces come in two different types. The standard version only stores data in Direktiv, while the mirror namespaces use Git as their source of truth for configuration and flows. It is recommended to use Git-backed namespaces for projects but for this guide a standard namespace will suffice. Create Standard Namespace curl -X PUT \"http://localhost:8080/api/namespaces/demo\" Response { \"namespace\" : { \"createdAt\" : \"2023-02-23T08:47:05.490124153Z\" , \"updatedAt\" : \"2023-02-23T08:47:05.490124801Z\" , \"name\" : \"demo\" , \"oid\" : \"\" } } Server Name Please adjust the server name to your environment if you are not using the all-in-one image for this \"Getting Started\" guide. Create Mirror (Git) Namespace To create a Git namespace Direktiv requires at least the two attributes url and ref . The ref value is the tag, branch or commit to use as the base whereas the url points to the Git repository to use. If there are only those two attributes provided the access to the repository needs to be public . Public Git curl -X PUT http://localhost:8080/api/namespaces/demo \\ --data-binary @- << EOF { \"url\": \"https://github.com/direktiv/direktiv-examples.git\", \"ref\": \"main\" } EOF If it is a private repository Direktiv requires either passphrase , which can be Github or Gitlab token or a publicKey / privateKey combination where the public key is registered with the Git instance. Private Git with Token curl -X PUT http://localhost:8080/api/namespaces/demo \\ --data-binary @- << EOF { \"url\": \"https://github.com/direktiv/direktiv-examples.git\", \"ref\": \"main\", \"passphrase\": \"abhsh2763gshs\" } EOF GitLab Passphrases GitLab requires a username for the token. The username needs to be prepended like username:glpat-152zshj2756 Delete Namespace Delting a namepsace with the API is very simple. The command requires the recursive attribute if there is already content in the namespace. curl -X DELETE http://localhost:8080/api/namespaces/demo?recursive = true","title":"Namespaces"},{"location":"getting_started/namespaces/#create-standard-namespace","text":"curl -X PUT \"http://localhost:8080/api/namespaces/demo\" Response { \"namespace\" : { \"createdAt\" : \"2023-02-23T08:47:05.490124153Z\" , \"updatedAt\" : \"2023-02-23T08:47:05.490124801Z\" , \"name\" : \"demo\" , \"oid\" : \"\" } } Server Name Please adjust the server name to your environment if you are not using the all-in-one image for this \"Getting Started\" guide.","title":"Create Standard Namespace"},{"location":"getting_started/namespaces/#create-mirror-git-namespace","text":"To create a Git namespace Direktiv requires at least the two attributes url and ref . The ref value is the tag, branch or commit to use as the base whereas the url points to the Git repository to use. If there are only those two attributes provided the access to the repository needs to be public . Public Git curl -X PUT http://localhost:8080/api/namespaces/demo \\ --data-binary @- << EOF { \"url\": \"https://github.com/direktiv/direktiv-examples.git\", \"ref\": \"main\" } EOF If it is a private repository Direktiv requires either passphrase , which can be Github or Gitlab token or a publicKey / privateKey combination where the public key is registered with the Git instance. Private Git with Token curl -X PUT http://localhost:8080/api/namespaces/demo \\ --data-binary @- << EOF { \"url\": \"https://github.com/direktiv/direktiv-examples.git\", \"ref\": \"main\", \"passphrase\": \"abhsh2763gshs\" } EOF GitLab Passphrases GitLab requires a username for the token. The username needs to be prepended like username:glpat-152zshj2756","title":"Create Mirror (Git) Namespace"},{"location":"getting_started/namespaces/#delete-namespace","text":"Delting a namepsace with the API is very simple. The command requires the recursive attribute if there is already content in the namespace. curl -X DELETE http://localhost:8080/api/namespaces/demo?recursive = true","title":"Delete Namespace"},{"location":"getting_started/persistent-data/","text":"Persistent Data Direktiv supports storing and retrieving data that is persisted beyond the scope of a single state or flow instance. This article shows how to store and retrieve these variables. Demo direktiv_api : workflow/v1 states : - id : a type : getter variables : - key : x scope : workflow transform : 'jq(.var.x += 1)' transition : b - id : b type : setter variables : - key : x scope : workflow value : 'jq(.var.x)' This demo increments a counter each time the flow is executed. It gets the variable x from workflow scope and increments it vi jq . The secons state stores the data in the same variable on the same scope. Scopes There are three scopes for storing persistent data: instance , workflow , and namespace . Data stored in the instance scope only exists for the duration of the running flow instance. Data stored in the workflow scope exists until the flow definition is deleted, and is accessible to all instances of that flow. Data stored in the namespace scope exists until the namespace itself is deleted, and is accessible to all instances of all flows originating on that namespace. Setter State The Setter State can be used to store any number of variables. Each variable must be explicitly scoped, and the value stored for a variable is generated by the output of a jq query. direktiv_api : workflow/v1 states : - id : a type : setter variables : - key : MyVar scope : namespace value : 'Hello' The only way to delete a stored value is to set it to null . direktiv_api : workflow/v1 states : - id : a type : setter variables : - key : MyVar scope : namespace value : Getter State The Getter State is used to retrieve any number of variables in persistent storage. Each variable must be explicitly scoped, and the value retrieved will be stored under .var.KEY where KEY is the variable's name. - id : a type : getter variables : - key : x scope : namespace A key doesn't need to exist in storage to return successfully, but the value returned will be null if it doesn't exist. Concurrency Direktiv makes no effort to guarantee any thread-safety on persistent data. Multiple instances that interact with the same variable may have inconsistent results. Getting & Setting from Functions Getting Accessing persistent data from within a function is a fairly straightforward process. The request that the custom function receives from Direktiv contains a header 'Direktiv-TempDir', which contains all of the variables specified in the function definition. The as , key , scope , and type fields can all play a role in the placement and naming of files within this directory: key The key used to select a variable from within the flow definition. If no as field is provided, the file on a custom function will correspond to the value of key . scope Which scope to get the variable from: instance , workflow , or namespace . Defaults to instance if omitted. as An optional field used to set the name of the file as it appears on the isolate. type plain The variable data inside of the file will be written 'as-is'. base64 If the variable is stored as base64-encoded data, it will be decoded before being written to the file system. tar If the variable is a valid tar archive, a directory will be created instead of a file, with the contents of the tar archive populating it. tar.gz Similar to tar , this will result in a populated directory being created from a valid .tar.gz file. For example, given the following state definition, a directory named 'myFiles' should exist within the directory specified by the Direktiv-TempDir header. Assuming that this header has a value of /mnt/shared/example , the following structure would be expected: - id : get image : localhost:5000/iv-getter:v1 files : - key : \"myFiles\" scope : instance type : tar /mnt/shared/example/ \u2514\u2500\u2500 myFiles \u2514\u2500\u2500 file-1 \u2514\u2500\u2500 file-2 \u2514\u2500\u2500 file-3 Setting From within a function running on Direktiv, variables can be set by sending a POST request: POST http://localhost:8889/var?aid=<EXAMPLE>&scope=instance&key=myFiles Body: <VARIABLE DATA> query parameters aid The action ID, found from the Direktiv-ActionID header of the request being served by the isolate. scope The scope for which the variable is set ( namespace , workflow , or instance ) key The key used by subsequent actions to access the variable. An alternative approach is to write files into certain directories. The direktiv sidecar will store those files as variables. There are three different folders for the three different scopes. For the above example they would be: /mnt/shared/example/out/instance /mnt/shared/example/out/workflow /mnt/shared/example/out/namespace Files under these folders will be stored with their names under the scope of the folder. Diretories will be stored as tar.gz files.","title":"Persistent Data"},{"location":"getting_started/persistent-data/#persistent-data","text":"Direktiv supports storing and retrieving data that is persisted beyond the scope of a single state or flow instance. This article shows how to store and retrieve these variables.","title":"Persistent Data"},{"location":"getting_started/persistent-data/#demo","text":"direktiv_api : workflow/v1 states : - id : a type : getter variables : - key : x scope : workflow transform : 'jq(.var.x += 1)' transition : b - id : b type : setter variables : - key : x scope : workflow value : 'jq(.var.x)' This demo increments a counter each time the flow is executed. It gets the variable x from workflow scope and increments it vi jq . The secons state stores the data in the same variable on the same scope.","title":"Demo"},{"location":"getting_started/persistent-data/#scopes","text":"There are three scopes for storing persistent data: instance , workflow , and namespace . Data stored in the instance scope only exists for the duration of the running flow instance. Data stored in the workflow scope exists until the flow definition is deleted, and is accessible to all instances of that flow. Data stored in the namespace scope exists until the namespace itself is deleted, and is accessible to all instances of all flows originating on that namespace.","title":"Scopes"},{"location":"getting_started/persistent-data/#setter-state","text":"The Setter State can be used to store any number of variables. Each variable must be explicitly scoped, and the value stored for a variable is generated by the output of a jq query. direktiv_api : workflow/v1 states : - id : a type : setter variables : - key : MyVar scope : namespace value : 'Hello' The only way to delete a stored value is to set it to null . direktiv_api : workflow/v1 states : - id : a type : setter variables : - key : MyVar scope : namespace value :","title":"Setter State"},{"location":"getting_started/persistent-data/#getter-state","text":"The Getter State is used to retrieve any number of variables in persistent storage. Each variable must be explicitly scoped, and the value retrieved will be stored under .var.KEY where KEY is the variable's name. - id : a type : getter variables : - key : x scope : namespace A key doesn't need to exist in storage to return successfully, but the value returned will be null if it doesn't exist.","title":"Getter State"},{"location":"getting_started/persistent-data/#concurrency","text":"Direktiv makes no effort to guarantee any thread-safety on persistent data. Multiple instances that interact with the same variable may have inconsistent results.","title":"Concurrency"},{"location":"getting_started/persistent-data/#getting-setting-from-functions","text":"","title":"Getting &amp; Setting from Functions"},{"location":"getting_started/scheduling/","text":"Scheduling Sometimes a flow needs to run periodically. Direktiv supports scheduling based on \"cron\". The cron is one of the start definitions . Demo direktiv_api : workflow/v1 start : type : scheduled cron : \"* 0/2 * * *\" functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Start Types Flow definitions can have one of many different start types. If the start section left out entirely, causes it to default , which is appropriate for a direct-invoke/subflow flow. start : type : scheduled cron : \"0 */2 * * *\" Direktiv supports valid cron expressions and prevents scheduled flows from being directly invoked or used as a subflow, which is why this example does not specify any input data. Scheduled flows can not accept any payloads. Active/Inactive Flows Every flow definition can be considered \"active\" or \"inactive\". Being \"active\" doesn't mean that there's an instance running right now, it means that Direktiv will allow instances to be created from it. This setting is part of the API, not a part of the flow definition. With scheduled flows this is a useful setting. It can toggle the schedule on and off without modifying the flow definition itself. Cron Cron is a time-based job scheduler in Unix-like operating systems. Direktiv doesn't run cron, but it does borrow their syntax and expressions for scheduling. In the example above the cron expression is \" 0 */2 * * * \". This tells Direktiv to run the flow once every two hours. There are many great resources online to help creating custom cron expressions.","title":"Scheduling"},{"location":"getting_started/scheduling/#scheduling","text":"Sometimes a flow needs to run periodically. Direktiv supports scheduling based on \"cron\". The cron is one of the start definitions .","title":"Scheduling"},{"location":"getting_started/scheduling/#demo","text":"direktiv_api : workflow/v1 start : type : scheduled cron : \"* 0/2 * * *\" functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\"","title":"Demo"},{"location":"getting_started/scheduling/#start-types","text":"Flow definitions can have one of many different start types. If the start section left out entirely, causes it to default , which is appropriate for a direct-invoke/subflow flow. start : type : scheduled cron : \"0 */2 * * *\" Direktiv supports valid cron expressions and prevents scheduled flows from being directly invoked or used as a subflow, which is why this example does not specify any input data. Scheduled flows can not accept any payloads.","title":"Start Types"},{"location":"getting_started/scheduling/#activeinactive-flows","text":"Every flow definition can be considered \"active\" or \"inactive\". Being \"active\" doesn't mean that there's an instance running right now, it means that Direktiv will allow instances to be created from it. This setting is part of the API, not a part of the flow definition. With scheduled flows this is a useful setting. It can toggle the schedule on and off without modifying the flow definition itself.","title":"Active/Inactive Flows"},{"location":"getting_started/scheduling/#cron","text":"Cron is a time-based job scheduler in Unix-like operating systems. Direktiv doesn't run cron, but it does borrow their syntax and expressions for scheduling. In the example above the cron expression is \" 0 */2 * * * \". This tells Direktiv to run the flow once every two hours. There are many great resources online to help creating custom cron expressions.","title":"Cron"},{"location":"getting_started/secrets-registries/","text":"Secrets & Registries Many flows require sensitive information such as passwords or authentication tokens to access third-party APIs. This article shows the best way to handle sensitive data such as this so that they don not need to be stored as plaintext in flow definitions. Additionally this article shows how to pull containers from a private repository. Stored secrets can be requested in a function via the secrets attribute and is available as .secrets.SECRETNAME Secrets direktiv_api : workflow/v1 functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : secrets : [ \"secretToken\" ] function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" headers : \"Content-type\" : \"application/json; charset=UTF-8\" \"Authorization\" : \"bearer jq(.secrets.secretToken)\" Registries Direktiv can store authentication information for a container repositories on a namespace-by-namespace basis. Creating secrets can be done via the Direktiv API or web interface in the settings page. With the relevant registry defined, functions referencing containers on that registry become accessible. For example, if a registry was created via the api with the following curl command: curl -X 'POST' \\ 'URL/api/functions/registries/namespaces/NAMESPACE' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data\": \"admin:8QwFLg%D$qg*\", \"reg\": \"https://index.docker.io\" }' This registry would be used automatically by Direktiv when running the flow in the demo. Example Google Artifact Registry To use the Google Artifact Registry a service account with a key is required. How to create a service account and generate a key is documented here . The keed needs to be in base64 format. On linux it can be converted with the following command: base64 -w 0 mykey-da8c8b573601.json > base64google.json Please make sure that there are no line wraps in the base64 file. For base64 encoded files the username is _json_key_base64 . Example details for this registry would be something like the following: Key Value URL https://us-central1-docker.pkg.dev Username _json_key_base64 Password ewogICJ0eXBlIjo...WlJkMWhqK1RRRF Note If a registry is created after a service, the service will need to be recreated to use the latest registry. Secrets Similar to how registry tokens are stored, arbitrary secrets can also be stored. That includes passwords, API tokens, certificates, or anything else. Secrets are stored on a namespace-by-namespace basis as key-value pairs. Secreats can be defined with the Direktiv API or web interface. Wherever actions appear in flow definitions there's always an optional secrets field. For every secret named in this field, Direktiv will find and decrypt the relevant secret from your namespace and add it to the data from which the action input is generated just before running the jq command that generates that logic. This means your jq commands can reference your secret and place it wherever it needs to be. Direktiv discards the secret-enriched data after generating the action input, so the secrets won't naturally appear in your instance output or logs. But once Direktiv passes that data to your action it has no control over how it's used. It's up to you to ensure your action doesn't log sensitive information and doesn't send sensitive information where it shouldn't go. IMPORTANT: Be especially wary of subflows. Try to avoid passing secrets to subflows if you can, subflows can reference secrets the same way as their parents after all. Remember, your secret-enriched data will become the input for a subflow, which means it will be logged. It's also stored in that subflow's instance data and could be passed around automatically if you're not careful. If your subflow doesn't strip secrets out before it terminates those secrets could also end up in the caller's return object. Security Registry tokens and secrets are stored individually encrypted within Direktiv's database. Each namespace gets its own unique encryption keys, and the decryption key is stored in a different database. For the online Direktiv, these two databases are on different machines and are firewalled apart from one another, and all internal traffic is encrypted. These measures minimize the risk of damaging data breaches, but we still recommend using tokens rather than passwords wherever possible.","title":"Secrets & Registries"},{"location":"getting_started/secrets-registries/#secrets-registries","text":"Many flows require sensitive information such as passwords or authentication tokens to access third-party APIs. This article shows the best way to handle sensitive data such as this so that they don not need to be stored as plaintext in flow definitions. Additionally this article shows how to pull containers from a private repository. Stored secrets can be requested in a function via the secrets attribute and is available as .secrets.SECRETNAME Secrets direktiv_api : workflow/v1 functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : secrets : [ \"secretToken\" ] function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" headers : \"Content-type\" : \"application/json; charset=UTF-8\" \"Authorization\" : \"bearer jq(.secrets.secretToken)\"","title":"Secrets &amp; Registries"},{"location":"getting_started/secrets-registries/#registries","text":"Direktiv can store authentication information for a container repositories on a namespace-by-namespace basis. Creating secrets can be done via the Direktiv API or web interface in the settings page. With the relevant registry defined, functions referencing containers on that registry become accessible. For example, if a registry was created via the api with the following curl command: curl -X 'POST' \\ 'URL/api/functions/registries/namespaces/NAMESPACE' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data\": \"admin:8QwFLg%D$qg*\", \"reg\": \"https://index.docker.io\" }' This registry would be used automatically by Direktiv when running the flow in the demo.","title":"Registries"},{"location":"getting_started/secrets-registries/#secrets","text":"Similar to how registry tokens are stored, arbitrary secrets can also be stored. That includes passwords, API tokens, certificates, or anything else. Secrets are stored on a namespace-by-namespace basis as key-value pairs. Secreats can be defined with the Direktiv API or web interface. Wherever actions appear in flow definitions there's always an optional secrets field. For every secret named in this field, Direktiv will find and decrypt the relevant secret from your namespace and add it to the data from which the action input is generated just before running the jq command that generates that logic. This means your jq commands can reference your secret and place it wherever it needs to be. Direktiv discards the secret-enriched data after generating the action input, so the secrets won't naturally appear in your instance output or logs. But once Direktiv passes that data to your action it has no control over how it's used. It's up to you to ensure your action doesn't log sensitive information and doesn't send sensitive information where it shouldn't go. IMPORTANT: Be especially wary of subflows. Try to avoid passing secrets to subflows if you can, subflows can reference secrets the same way as their parents after all. Remember, your secret-enriched data will become the input for a subflow, which means it will be logged. It's also stored in that subflow's instance data and could be passed around automatically if you're not careful. If your subflow doesn't strip secrets out before it terminates those secrets could also end up in the caller's return object.","title":"Secrets"},{"location":"getting_started/secrets-registries/#security","text":"Registry tokens and secrets are stored individually encrypted within Direktiv's database. Each namespace gets its own unique encryption keys, and the decryption key is stored in a different database. For the online Direktiv, these two databases are on different machines and are firewalled apart from one another, and all internal traffic is encrypted. These measures minimize the risk of damaging data breaches, but we still recommend using tokens rather than passwords wherever possible.","title":"Security"},{"location":"getting_started/states/","text":"Direktiv Flows are YAML-based definitions of states connected in a directed acyclic graph (DAG). During runtime, the flow controls how execution progresses and which states are being called. It provides different state types to allow e.g. decision making, execute functions, event triggering and subflow calls. During execution data will be stored as JSON which can be accessed or modified in any given state. Direktiv takes any kind of input to start the process off and returns a result as JSON output once finished. Direktiv Flow Workflow definition It is best practices for all workflows to begin with the following line, so that tools can identify it as a Direktiv workflow: direktiv_api : workflow/v1 All states for a flow are listed under states . Every flow must have at least one state. The first state under states will be executed first and all subsequent states need to be connected via transitions. If a state has no transition attribute the flow ends at that point of the execution. Simple State direktiv_api : workflow/v1 states : - id : hello type : noop log : this is the log transform : hello : world The above flow contains a single noop (\"no operation\") and shows the common attributes in all available states within Dirketiv. When the flow is getting executed Direktiv creates an instance of that flow definition and tracks the progress and state data of that instance. The output of that flow would be the following: { \"hello\" : \"world\" } State ID - id : hello Every state has to have its own identifier. The state identifier is used in logging and to define transitions, which will come up in a later example when we define more than one state. A state identifier must be unique within the flow definition. State Type type : noop There are many state types that do all sorts of different things. It is required to provide the state type. Log log : this is the log Every state has the log attribute and the content of the log attribute will be stored in the logs of the instance. Transform Command transform : hello : world Any state may optionally define a \"transform\" to modify the state data. The transform can add and delete data in the state or even wipe all data in the state . Simple Transition A transition attribute in a state instructs Direktiv to move to the next state. Transitions can also be conditional or during error handling but the following is a simple sequential transition. states : - id : hello type : noop log : this is the log transform : hello : world transition : next-step - id : next-step type : noop log : last-step - id : last-step type : noop log : second stage","title":"Flows & States"},{"location":"getting_started/states/#workflow-definition","text":"It is best practices for all workflows to begin with the following line, so that tools can identify it as a Direktiv workflow: direktiv_api : workflow/v1 All states for a flow are listed under states . Every flow must have at least one state. The first state under states will be executed first and all subsequent states need to be connected via transitions. If a state has no transition attribute the flow ends at that point of the execution.","title":"Workflow definition"},{"location":"getting_started/states/#simple-state","text":"direktiv_api : workflow/v1 states : - id : hello type : noop log : this is the log transform : hello : world The above flow contains a single noop (\"no operation\") and shows the common attributes in all available states within Dirketiv. When the flow is getting executed Direktiv creates an instance of that flow definition and tracks the progress and state data of that instance. The output of that flow would be the following: { \"hello\" : \"world\" }","title":"Simple State"},{"location":"getting_started/states/#simple-transition","text":"A transition attribute in a state instructs Direktiv to move to the next state. Transitions can also be conditional or during error handling but the following is a simple sequential transition. states : - id : hello type : noop log : this is the log transform : hello : world transition : next-step - id : next-step type : noop log : last-step - id : last-step type : noop log : second stage","title":"Simple Transition"},{"location":"getting_started/subflows/","text":"Subflows Just like scripting or programming, with Direktiv it's possible to organize your logic into reusable modules. Anytime a flow is invoked by another we it is called subflow. A subflow can be called like actions and it uses the same parameters as functions. Subflow 'checker' direktiv_api : workflow/v1 functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : # validates input and protects flow from wrong input data - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify # run http request - id : notify type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" content : input : jq(.) transition : check-results # check if http code is 200 - id : check-results type : switch conditions : - condition : 'jq(.return[0].code != 200)' transition : throw defaultTransform : result : jq(.return[0].code) # throw an error if not 200 response code - id : throw type : error error : notification.lint message : \"not 200 response\" Parent Flow direktiv_api : workflow/v1 functions : - id : checker-sub type : subflow # relative reference to subflow workflow : checker states : - id : notify type : action action : function : checker-sub input : contact : hello payload : data Output { \"return\" : { \"result\" : 200 } }","title":"Subflows"},{"location":"getting_started/subflows/#subflows","text":"Just like scripting or programming, with Direktiv it's possible to organize your logic into reusable modules. Anytime a flow is invoked by another we it is called subflow. A subflow can be called like actions and it uses the same parameters as functions. Subflow 'checker' direktiv_api : workflow/v1 functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : # validates input and protects flow from wrong input data - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify # run http request - id : notify type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" content : input : jq(.) transition : check-results # check if http code is 200 - id : check-results type : switch conditions : - condition : 'jq(.return[0].code != 200)' transition : throw defaultTransform : result : jq(.return[0].code) # throw an error if not 200 response code - id : throw type : error error : notification.lint message : \"not 200 response\" Parent Flow direktiv_api : workflow/v1 functions : - id : checker-sub type : subflow # relative reference to subflow workflow : checker states : - id : notify type : action action : function : checker-sub input : contact : hello payload : data Output { \"return\" : { \"result\" : 200 } }","title":"Subflows"},{"location":"getting_started/transforms/","text":"Transforms & JQ/JS Every flow instance always has something called the \"Instance Data\", which is a JSON object that is used to pass data around. Almost everywhere a transition can happen in a flow definition a transform can also happen allowing the author to filter, enrich, or otherwise modify the instance data. Transforms can be static, as seen in previous parts of this guide, or use JQ or Javascript to dynamically change it. JQ introduction Direktiv uses JQ , JSON query language, to dynamically change data within the system. It is used in transformations, transitions , logs or function calls . JQ Hints Setting Defaults : JQ throws an error if the value you are accessing is empty. It is easy to set a default value with JQ like the following example: - id: hello type: noop transform: hello: jq ( .myvalue // \"world\" ) Multi-Line : Sometimes JQ can be hard to read if it is too long. YAML provides an easy way to use multi-line input. - id: hello type: noop transform: hello: | - jq ( if .mydata // \"myvalue\" == \"hello\" then \"it is hello\" elif . == \"world\" then \"it is world\" else \"none of the above\" end ) The transform field can contain a valid jq command, which will be applied to the existing instance data to generate a new JSON object that will entirely replace it. Note that only a JSON object will be considered a valid output from this jq command: jq is capable of outputting primitives and arrays, but these are not acceptable output for a transform . Transforms can be wrapped in 'jq()' or jq() . The difference between the two is that one instructs YAML more explicitly what's in the string. This can be important if you use jq commands containing braces, for example: jq({a: 1}) . Because if this is not explicitly quoted, YAML interprets it incorrectly and throws errors. The quoted form is always valid and generally safer. Hint The UI provides a JQ playground to write andf test JQ queries. JS introduction An alternative to JQ is Javascript. Direktiv provides a data Javascript object which can be modified to change state data. It assumes the script runs in a function and the Javascript section needs to return data even if it is an empty string. A null value is not allowed. - id : hello type : noop transform : epoch : js(return Date.now()) Javascript snippets have access to the state data as well. The state data in that object is accessible through regular Javascript commands. - id : hello type : noop transform : |- js( data[\"hello\"] = \"world\" return data ) First Transform Although a transform can use jq or js to modify data plain YAML can be used to do the transform. The following example does such a static transform. This can be used to e.g. set-up defaults or a basic object to work with in that flow. direktiv_api : workflow/v1 states : - id : transform1 type : noop transform : number : 5 objects : - key1 : value1 - key2 : value2 Resulting Instance Data { \"number\" : 5 , \"objects\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ] } Second Transform The second transform enriches the existing instance data by adding a new field to it. Command direktiv_api : workflow/v1 states : - id : transform1 type : noop transform : number : 5 objects : - key1 : value1 - key2 : value2 transition : transform2 - id : transform2 type : noop transform : 'jq(.multiplier = 10)' Resulting Instance Data { \"multiplier\" : 10 , \"number\" : 5 , \"objects\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ] } Third Transform The third transform multiplies two fields to produce a new field, then pipes the results into another command that deletes two fields. Command direktiv_api : workflow/v1 states : - id : transform1 type : noop transform : number : 5 objects : - key1 : value1 - key2 : value2 transition : transform2 - id : transform2 type : noop transform : 'jq(.multiplier = 10)' transition : transform3 - id : transform3 type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' Resulting Instance Data { \"objects\" : [ { \"key1\" : \"value1\" }, { \"key2\" : \"value2\" } ], \"result\" : 50 } Fourth Transform The fourth transform selects a child object nested within the instance data and makes that into the new instance data. Command direktiv_api : workflow/v1 states : - id : transform1 type : noop transform : number : 5 objects : - key1 : value1 - key2 : value2 transition : transform2 - id : transform2 type : noop transform : 'jq(.multiplier = 10)' transition : transform3 - id : transform3 type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : transform4 - id : transform4 type : noop transform : 'jq(.objects[0])' Resulting Instance Data { \"key1\" : \"value1\" }","title":"Transforms & JQ/JS"},{"location":"getting_started/transforms/#transforms-jqjs","text":"Every flow instance always has something called the \"Instance Data\", which is a JSON object that is used to pass data around. Almost everywhere a transition can happen in a flow definition a transform can also happen allowing the author to filter, enrich, or otherwise modify the instance data. Transforms can be static, as seen in previous parts of this guide, or use JQ or Javascript to dynamically change it.","title":"Transforms &amp; JQ/JS"},{"location":"getting_started/transforms/#jq-introduction","text":"Direktiv uses JQ , JSON query language, to dynamically change data within the system. It is used in transformations, transitions , logs or function calls . JQ Hints Setting Defaults : JQ throws an error if the value you are accessing is empty. It is easy to set a default value with JQ like the following example: - id: hello type: noop transform: hello: jq ( .myvalue // \"world\" ) Multi-Line : Sometimes JQ can be hard to read if it is too long. YAML provides an easy way to use multi-line input. - id: hello type: noop transform: hello: | - jq ( if .mydata // \"myvalue\" == \"hello\" then \"it is hello\" elif . == \"world\" then \"it is world\" else \"none of the above\" end ) The transform field can contain a valid jq command, which will be applied to the existing instance data to generate a new JSON object that will entirely replace it. Note that only a JSON object will be considered a valid output from this jq command: jq is capable of outputting primitives and arrays, but these are not acceptable output for a transform . Transforms can be wrapped in 'jq()' or jq() . The difference between the two is that one instructs YAML more explicitly what's in the string. This can be important if you use jq commands containing braces, for example: jq({a: 1}) . Because if this is not explicitly quoted, YAML interprets it incorrectly and throws errors. The quoted form is always valid and generally safer. Hint The UI provides a JQ playground to write andf test JQ queries.","title":"JQ introduction"},{"location":"getting_started/transforms/#js-introduction","text":"An alternative to JQ is Javascript. Direktiv provides a data Javascript object which can be modified to change state data. It assumes the script runs in a function and the Javascript section needs to return data even if it is an empty string. A null value is not allowed. - id : hello type : noop transform : epoch : js(return Date.now()) Javascript snippets have access to the state data as well. The state data in that object is accessible through regular Javascript commands. - id : hello type : noop transform : |- js( data[\"hello\"] = \"world\" return data )","title":"JS introduction"},{"location":"getting_started/transitions/","text":"Input data and transitions , in particular conditional transitions, are an important part in Direktiv. As previously shown a state can define a transition as the next state in the flow. If there is no transition defined the flow ends at that point in the execution. So far the examples have only shown sequential transition but here there will be a conditional transition based on input data of the flow. Conditional Transition To execute conditional transitions Direktiv provides a switch which makes decisions about where to transition to next based on the instance data by evaluating a number of jq or js expressions and checking the results. direktiv_api : workflow/v1 states : - id : ifelse type : switch conditions : - condition : 'jq(.age > 17)' transition : accepted - condition : 'jq(.age != null)' transition : rejected defaultTransition : failure - id : accepted type : noop transform : message : request accepted - id : rejected type : noop transform : message : rejected based on age - id : failure type : error error : age.error message : no age provided Each of the conditions will be evaluated in the order it appears by running the jq command in condition . Any result other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a successful match. If no conditions match the default transition will be used. Transform Each condition has a transform attribute and there is a defaultTransform so every condition can modfiy the state data if there is a successful match. Running the above example will always go to the failure state because no input data has been provided for this flow. In this case the failure state is an error state which marks the flow as failed. More about errors can be found in the error handling section . Input Data To make the above example more useful the flow needs input data. Input data in Direktiv will never be empty. If the flow is called with no data it will be executed with an empty JSON object {} . If the payload is in JSON format it will be base64 encoded and provided with the attribute input . { \"input\" : \"T1hSisBaSE64Data==\" } The above flow can be called with a simple JSON providing a value for age. { \"age\" : 18 } The curl command to call the flow via the API is the following. Please adjust the flow and server name if required. curl -X POST http://localhost:8080/api/namespaces/demo/tree/MYWORKFLOWNAME?op = wait \\ --data-binary @- << EOF { \"age\": 18 } EOF The response is always the last state data of a flow. Because the final states include a transform the response of the flow would be the transformed data. { \"message\" : \"request accepted\" }","title":"Input & Transitions"},{"location":"getting_started/transitions/#conditional-transition","text":"To execute conditional transitions Direktiv provides a switch which makes decisions about where to transition to next based on the instance data by evaluating a number of jq or js expressions and checking the results. direktiv_api : workflow/v1 states : - id : ifelse type : switch conditions : - condition : 'jq(.age > 17)' transition : accepted - condition : 'jq(.age != null)' transition : rejected defaultTransition : failure - id : accepted type : noop transform : message : request accepted - id : rejected type : noop transform : message : rejected based on age - id : failure type : error error : age.error message : no age provided Each of the conditions will be evaluated in the order it appears by running the jq command in condition . Any result other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a successful match. If no conditions match the default transition will be used. Transform Each condition has a transform attribute and there is a defaultTransform so every condition can modfiy the state data if there is a successful match. Running the above example will always go to the failure state because no input data has been provided for this flow. In this case the failure state is an error state which marks the flow as failed. More about errors can be found in the error handling section .","title":"Conditional Transition"},{"location":"getting_started/transitions/#input-data","text":"To make the above example more useful the flow needs input data. Input data in Direktiv will never be empty. If the flow is called with no data it will be executed with an empty JSON object {} . If the payload is in JSON format it will be base64 encoded and provided with the attribute input . { \"input\" : \"T1hSisBaSE64Data==\" } The above flow can be called with a simple JSON providing a value for age. { \"age\" : 18 } The curl command to call the flow via the API is the following. Please adjust the flow and server name if required. curl -X POST http://localhost:8080/api/namespaces/demo/tree/MYWORKFLOWNAME?op = wait \\ --data-binary @- << EOF { \"age\": 18 } EOF The response is always the last state data of a flow. Because the final states include a transform the response of the flow would be the transformed data. { \"message\" : \"request accepted\" }","title":"Input Data"},{"location":"getting_started/validating/","text":"Validating Input In some cases it is important to validate the state of the flow. This can be done as the first state in the flow to protect the flow from rogue data or within the flow to check the state data before proceeding. Direktiv is using JSON schema to validate the state data. Check Attribute direktiv_api : workflow/v1 states : - id : data type : noop transform : name : Michael transition : check - id : check type : validate schema : type : object required : - name properties : name : type : string The above example will succedd because the attribute name is set and it is a string, in this case Michael . If the the value would be an integer the flow would fail. Failed Attribute direktiv_api : workflow/v1 states : - id : start type : validate schema : type : object required : - name properties : name : type : string JSON schema is used to validate JSON structures and not contents. It can not validate if the value of name has a certain content. If that is a requirement Direktiv's switch statement has to be used. First State Direktiv can generate a form if the validate state is the first state in the flow. Validate Form direktiv_api : workflow/v1 states : - id : start type : validate schema : type : object required : - name properties : name : type : string title : Name description : Please enter your name default : My Name In this example the default attribute is used and it is shown in the form. Direktiv can not set defaults via API or in a running flow. It is for form generation only. If defaults are required jq can be used like jq(.name // \"Michael\") . The following validate would ask for the name but set it to Michael if it is empty. Setting Defaults direktiv_api : workflow/v1 states : - id : start type : validate schema : type : object properties : name : type : string title : Name description : Please enter your name transition : next-state - id : next-state type : noop transform : 'jq(. + { name: (.name // \"Michael\") })'","title":"Validating Input"},{"location":"getting_started/validating/#validating-input","text":"In some cases it is important to validate the state of the flow. This can be done as the first state in the flow to protect the flow from rogue data or within the flow to check the state data before proceeding. Direktiv is using JSON schema to validate the state data. Check Attribute direktiv_api : workflow/v1 states : - id : data type : noop transform : name : Michael transition : check - id : check type : validate schema : type : object required : - name properties : name : type : string The above example will succedd because the attribute name is set and it is a string, in this case Michael . If the the value would be an integer the flow would fail. Failed Attribute direktiv_api : workflow/v1 states : - id : start type : validate schema : type : object required : - name properties : name : type : string JSON schema is used to validate JSON structures and not contents. It can not validate if the value of name has a certain content. If that is a requirement Direktiv's switch statement has to be used.","title":"Validating Input"},{"location":"getting_started/validating/#first-state","text":"Direktiv can generate a form if the validate state is the first state in the flow. Validate Form direktiv_api : workflow/v1 states : - id : start type : validate schema : type : object required : - name properties : name : type : string title : Name description : Please enter your name default : My Name In this example the default attribute is used and it is shown in the form. Direktiv can not set defaults via API or in a running flow. It is for form generation only. If defaults are required jq can be used like jq(.name // \"Michael\") . The following validate would ask for the name but set it to Michael if it is empty. Setting Defaults direktiv_api : workflow/v1 states : - id : start type : validate schema : type : object properties : name : type : string title : Name description : Please enter your name transition : next-state - id : next-state type : noop transform : 'jq(. + { name: (.name // \"Michael\") })'","title":"First State"},{"location":"getting_started/advanced/making-functions/","text":"Making Custom Functions If a custom function is required in case there are no Direktiv functions available it is easy to create those. Direktiv is pulling the images defined in the functions section and executes the container in the flow. They can be in any repository. Custom Function functions : - id : custom image : mycompany/customfunction type : knative-workflow The custom container just need to implement a few things. The most important requirement is to listen to port 8080 . The data in the flow will be posted to the container on that port. Input - id : notify type : action action : function : custom input : hello : world In the above example Direktiv would post JSON { \"hello\": \"world\" } to the function. The function can use his date to execute whatever it needs to do. After that the function has to return in JSON formart. Direktiv will fetch the response and add it to the state data in the return attribute. The flow can use that data and proceed. Reporting Errors If something goes wrong a function can report an error to the calling flow instance by adding HTTP headers to the response. If these headers are populated the execution of the function will be considered a failure regardless of what's stored in response data. The headers to report errors are: Direktiv-ErrorCode and Direktiv-ErrorMessage . If an error message is defined without defining an error code the calling flow instance will be marked as \"crashed\" without exposing any helpful information, so it's important to always define both. Errors raised by functions are always 'catchable' by their error codes. Error Headers \"Direktiv-ErrorCode\" : \"myapp.input\" , \"Direktiv-ErrorMessage\" : \"Missing 'customerId' property in JSON input.\" Logging Logging for functions is a simple HTTP POST or GET request to the address: http://localhost:8889/log?aid=$ACTIONID If POST is used the body of the request is getting logged for GET requests add a log request parameter. The important parameter is $ACTIONID. Each requests gets an action id header which identifies the flow instance. This parameter has to be passed back to attach the log to the instance. This information is passed in as in the initial request ( Direktiv-ActionID ). Examples Dotnet Python FastAPI Golang Java Node Python Rust","title":"Custom Functions"},{"location":"getting_started/advanced/making-functions/#making-custom-functions","text":"If a custom function is required in case there are no Direktiv functions available it is easy to create those. Direktiv is pulling the images defined in the functions section and executes the container in the flow. They can be in any repository. Custom Function functions : - id : custom image : mycompany/customfunction type : knative-workflow The custom container just need to implement a few things. The most important requirement is to listen to port 8080 . The data in the flow will be posted to the container on that port. Input - id : notify type : action action : function : custom input : hello : world In the above example Direktiv would post JSON { \"hello\": \"world\" } to the function. The function can use his date to execute whatever it needs to do. After that the function has to return in JSON formart. Direktiv will fetch the response and add it to the state data in the return attribute. The flow can use that data and proceed.","title":"Making Custom Functions"},{"location":"getting_started/advanced/making-functions/#reporting-errors","text":"If something goes wrong a function can report an error to the calling flow instance by adding HTTP headers to the response. If these headers are populated the execution of the function will be considered a failure regardless of what's stored in response data. The headers to report errors are: Direktiv-ErrorCode and Direktiv-ErrorMessage . If an error message is defined without defining an error code the calling flow instance will be marked as \"crashed\" without exposing any helpful information, so it's important to always define both. Errors raised by functions are always 'catchable' by their error codes. Error Headers \"Direktiv-ErrorCode\" : \"myapp.input\" , \"Direktiv-ErrorMessage\" : \"Missing 'customerId' property in JSON input.\"","title":"Reporting Errors"},{"location":"getting_started/advanced/making-functions/#logging","text":"Logging for functions is a simple HTTP POST or GET request to the address: http://localhost:8889/log?aid=$ACTIONID If POST is used the body of the request is getting logged for GET requests add a log request parameter. The important parameter is $ACTIONID. Each requests gets an action id header which identifies the flow instance. This parameter has to be passed back to attach the log to the instance. This information is passed in as in the initial request ( Direktiv-ActionID ).","title":"Logging"},{"location":"getting_started/advanced/making-functions/#examples","text":"Dotnet Python FastAPI Golang Java Node Python Rust","title":"Examples"},{"location":"getting_started/advanced/metadata/","text":"Metadata If Direktiv flow are consumed by external applications metadata can be used to request the state of a flow. It is data which can be set by the flow and requested via API. This in particular useful if the flow is executed asynchronously. Executing Flow Asynchronously A flow can be started with a simple API call. By default this is done asynchronously and can be called e.g. via shell with curl: curl -X POST http://<DIREKTIV-ADDRESS>/api/namespaces/<NAMESPACE>/tree/<FLOW-NAME>?op=execute This call would return information about the started flow and it looks like the following: Workflow Info { \"namespace\" : \"asdas\" , \"instance\" : \"24d6b04b-6e3c-47ba-a300-462f02c8fcae\" } To request the metadata the instance attribute is the value required for subsequent requests to fetch the metadata. curl http://10.100.91.85/api/namespaces/<NAMESPACE>/instances/<INSTANCE ID FROM THE PREVIOUS CALL>/metadata | jq -r .data | base64 -d The following flow with just delay states can be used to test the result of the metadata call. Metadata Flow Example direktiv_api : workflow/v1 states : - id : step1 type : delay metadata : state : waiting at the moment at state one duration : PT30S transition : step2 - id : step2 type : delay metadata : state : waiting at the moment at state two duration : PT30S transition : step3 - id : step3 type : delay metadata : state : waiting at the moment at state three duration : PT30S","title":"Metadata"},{"location":"getting_started/advanced/metadata/#metadata","text":"If Direktiv flow are consumed by external applications metadata can be used to request the state of a flow. It is data which can be set by the flow and requested via API. This in particular useful if the flow is executed asynchronously.","title":"Metadata"},{"location":"getting_started/advanced/metadata/#executing-flow-asynchronously","text":"A flow can be started with a simple API call. By default this is done asynchronously and can be called e.g. via shell with curl: curl -X POST http://<DIREKTIV-ADDRESS>/api/namespaces/<NAMESPACE>/tree/<FLOW-NAME>?op=execute This call would return information about the started flow and it looks like the following: Workflow Info { \"namespace\" : \"asdas\" , \"instance\" : \"24d6b04b-6e3c-47ba-a300-462f02c8fcae\" } To request the metadata the instance attribute is the value required for subsequent requests to fetch the metadata. curl http://10.100.91.85/api/namespaces/<NAMESPACE>/instances/<INSTANCE ID FROM THE PREVIOUS CALL>/metadata | jq -r .data | base64 -d The following flow with just delay states can be used to test the result of the metadata call. Metadata Flow Example direktiv_api : workflow/v1 states : - id : step1 type : delay metadata : state : waiting at the moment at state one duration : PT30S transition : step2 - id : step2 type : delay metadata : state : waiting at the moment at state two duration : PT30S transition : step3 - id : step3 type : delay metadata : state : waiting at the moment at state three duration : PT30S","title":"Executing Flow Asynchronously"},{"location":"getting_started/advanced/timeout/","text":"Timeouts Direktiv supports timeouts on different levels. The main reason for having timeouts is to avoid having long-running or orphaned flows. The default timeout for flows and actions is 15 minutes. Flow Timeouts There is a general flow time out setting which controls how Direktiv will try to gracefully stop or interrupt the flow and eventually kill the flow if that is not possible. The time has to be provided in ISO8601 format. direktiv_api : workflow/v1 timeouts : interrupt : PT20M kill : PT30M states : - id : nothing type : noop log : I'm doing nothing State Timeouts Every state has a timeout attribute as well. This is in particular interesting for functions and the action state. If the timeout is triggered in an action Direktiv sends an interrupt to the action and it is up to the function to handle it. Action Timeouts direktiv_api : workflow/v1 functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : timeout-test type : action # this flow fails if it exceeds 10 seconds timeout : PT10S action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Events Another use for timeouts is events. There are three states consuming events: consumeEvent , eventsAnd and eventsXor . If the timeout is reached the flow fails or the error can be caught and handled. Event Wait And Timeout direktiv_api : workflow/v1 states : - id : something type : noop transition : consume - id : consume type : consumeEvent # wait for the event for one minute, otherwise fail timeout : PT1M event : type : com.github.pull.create context : subject : '123' Catching Timeouts Timeouts in actions can be caught and the error thrown is direktiv.cancels.timeout.soft . Based on that error the flow can be re-routed. Catch Timeout direktiv_api : workflow/v1 states : - id : something type : noop transition : consume - id : consume type : consumeEvent timeout : PT1S event : type : com.github.pull.create context : subject : '123' catch : - error : \"direktiv.cancels.timeout.soft\" transition : handle-error - id : handle-error type : noop log : error handling Timeouts in States Direktiv is not automatically calculating timeouts. If an action has a 30 minute timeout the flow timeout has to be increased as well to cater for long-running actions.","title":"Timeouts"},{"location":"getting_started/advanced/timeout/#timeouts","text":"Direktiv supports timeouts on different levels. The main reason for having timeouts is to avoid having long-running or orphaned flows. The default timeout for flows and actions is 15 minutes.","title":"Timeouts"},{"location":"getting_started/advanced/timeout/#flow-timeouts","text":"There is a general flow time out setting which controls how Direktiv will try to gracefully stop or interrupt the flow and eventually kill the flow if that is not possible. The time has to be provided in ISO8601 format. direktiv_api : workflow/v1 timeouts : interrupt : PT20M kill : PT30M states : - id : nothing type : noop log : I'm doing nothing","title":"Flow Timeouts"},{"location":"getting_started/advanced/timeout/#state-timeouts","text":"Every state has a timeout attribute as well. This is in particular interesting for functions and the action state. If the timeout is triggered in an action Direktiv sends an interrupt to the action and it is up to the function to handle it. Action Timeouts direktiv_api : workflow/v1 functions : - id : httprequest image : gcr.io/direktiv/functions/http-request:1.0 type : knative-workflow states : - id : timeout-test type : action # this flow fails if it exceeds 10 seconds timeout : PT10S action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\"","title":"State Timeouts"},{"location":"getting_started/advanced/timeout/#events","text":"Another use for timeouts is events. There are three states consuming events: consumeEvent , eventsAnd and eventsXor . If the timeout is reached the flow fails or the error can be caught and handled. Event Wait And Timeout direktiv_api : workflow/v1 states : - id : something type : noop transition : consume - id : consume type : consumeEvent # wait for the event for one minute, otherwise fail timeout : PT1M event : type : com.github.pull.create context : subject : '123'","title":"Events"},{"location":"getting_started/advanced/timeout/#catching-timeouts","text":"Timeouts in actions can be caught and the error thrown is direktiv.cancels.timeout.soft . Based on that error the flow can be re-routed. Catch Timeout direktiv_api : workflow/v1 states : - id : something type : noop transition : consume - id : consume type : consumeEvent timeout : PT1S event : type : com.github.pull.create context : subject : '123' catch : - error : \"direktiv.cancels.timeout.soft\" transition : handle-error - id : handle-error type : noop log : error handling Timeouts in States Direktiv is not automatically calculating timeouts. If an action has a 30 minute timeout the flow timeout has to be increased as well to cater for long-running actions.","title":"Catching Timeouts"},{"location":"installation/","text":"Installation Direktiv is using Helm charts for installation. For a basic installation there are only two dependencies. A PostgreSQL database and Knative . Optional dependencies are Linkerd as service mesh and monitoring and tracing tools, e.g. backends for Direktiv's Opentelemetry configuration. The following diagram shows a high-level architecture of Direktiv and the required and optional components. The following sections explain how to install each component in a local cluster: Kubernetes Linkerd Postgres Direktiv Knative Run Docker Image For testing there is a \"all-in-one\" Docker image available. It contains all required components already installed and can be used for testing or development. It has a container registry installed on port 31212 as well which can be used to push local images. Direktiv Docker Container docker run --privileged -p 8080 :80 -ti direktiv/direktiv-kube The docker image has additional environment variables which can add other functionalities and configurations: APIKEY: Set an API key for the application HTTPS_PROXY: Sets the HTTPS_PROXY environment variable HTTP_PROXY: Sets the HTTP_PROXY environment variable NO_PROXY: Sets the NO_PROXY environment variable EVENTING: Enables Knative eventing DEBUG: Prints k3s output to stdout Direktiv Docker Container with API Key and Registry docker run -e APIKEY = 123 --privileged -p 8080 :80 -p 31212 :31212 -ti direktiv/direktiv-kube","title":"Install"},{"location":"installation/#installation","text":"Direktiv is using Helm charts for installation. For a basic installation there are only two dependencies. A PostgreSQL database and Knative . Optional dependencies are Linkerd as service mesh and monitoring and tracing tools, e.g. backends for Direktiv's Opentelemetry configuration. The following diagram shows a high-level architecture of Direktiv and the required and optional components. The following sections explain how to install each component in a local cluster: Kubernetes Linkerd Postgres Direktiv Knative","title":"Installation"},{"location":"installation/database/","text":"Database Direktiv requires a PostgreSQL 13+ database. It acts as datastore as well as pub/sub system between Direktiv's components. It has been tested with Postgres offerings from cloud providers as well as on-premise installations. It is recommended to use a managed Postgres service from cloud providers. If that is not possible Postgres can be installed in Kubernetes as well. To install a Postgres instance in Kubernetes we are using Percona's Postgres operator. The following section will provide exmaples for different installation scenarios from basic testing setups to more complex high-availability configurations. For inidividual changes please visit the Percona Operator documentation page. Installing the Operator The operator is provided as Helm chart and the installation is straighforward. Add Direktiv's helm chart repository and run the installation command. Install Postgres Operator helm repo add percona https://percona.github.io/percona-helm-charts/ helm install -n postgres pg-operator percona/pg-operator --wait Backup Ports For the backup to work properly port 2022 needs to be open between the nodes Creating a Postgres Instance Basic Configuration This basic configuration is good for small instances and testing. It creates weekly backups and keeps the last 4 backups. Direktiv connects directly to the Database without connection pooling. Basic Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/basic.yaml Basic Database Configuration apiVersion : pgv2.percona.com/v2 kind : PerconaPGCluster metadata : name : direktiv-cluster namespace : postgres # finalizers: # - percona.com/delete-pvc spec : crVersion : 2.3.0 users : - name : direktiv databases : - direktiv - name : postgres image : perconalab/percona-postgresql-operator:main-ppg14-postgres imagePullPolicy : Always postgresVersion : 14 port : 5432 instances : - name : instance1 replicas : 1 dataVolumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi proxy : pgBouncer : replicas : 1 image : perconalab/percona-postgresql-operator:main-ppg14-pgbouncer backups : pgbackrest : image : perconalab/percona-postgresql-operator:main-ppg14-pgbackrest global : # Keep 4 Backups repo1-retention-full : \"4\" repo1-retention-full-type : count manual : repoName : repo1 options : - --type=full repos : - name : repo1 schedules : full : \"0 0 * * 6\" volume : volumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi pmm : enabled : false image : percona/pmm-client:2.37.0 High-Availability High-Availabilty can be achieved by scaling the database replicas. The following example has added daily differential backups and pod anti-affinity to spread the pods across the cluster. If anti-affinity is used the cluster needs to have the same number of nodes and database replicas. Basic Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/ha.yaml High-Availability Configuration apiVersion : pgv2.percona.com/v2 kind : PerconaPGCluster metadata : name : direktiv-cluster namespace : postgres spec : crVersion : 2.3.0 users : - name : direktiv databases : - direktiv - name : postgres image : perconalab/percona-postgresql-operator:main-ppg14-postgres imagePullPolicy : Always postgresVersion : 14 port : 5432 instances : - name : instance1 replicas : 2 resources : limits : cpu : 2.0 memory : 4Gi dataVolumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 4Gi topologySpreadConstraints : - maxSkew : 1 topologyKey : kubernetes.io/hostname whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : postgres-operator.crunchydata.com/instance-set : instance1 proxy : pgBouncer : replicas : 2 image : perconalab/percona-postgresql-operator:main-ppg14-pgbouncer affinity : podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 podAffinityTerm : labelSelector : matchLabels : postgres-operator.crunchydata.com/cluster : keycloakdb postgres-operator.crunchydata.com/role : pgbouncer topologyKey : kubernetes.io/hostname backups : pgbackrest : image : perconalab/percona-postgresql-operator:main-ppg14-pgbackrest global : repo1-retention-full : \"4\" repo1-retention-full-type : count manual : repoName : repo1 options : - --type=full repos : - name : repo1 schedules : full : \"0 0 * * 6\" differential : \"0 1 * * 1-6\" volume : volumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 4Gi pmm : enabled : false image : percona/pmm-client:2.37.0 High-Availability with S3 Backup Percona's Postgres operator can store backups in AWS, Azure and Google Cloud as well. The following example shows how to use AWS S3 as backup storage. A secret is required for the S3 backend with the appropriate permission. This requires a s3.conf file with the S3 key and secret. s3.conf [ global ] repo1-s3-key = MYKEY repo1-s3-key-secret = MYSECRET After creating the file adding the secret is a simple kubectl command: Create S3 Secret kubectl create secret generic -n postgres direktiv-pgbackrest-secret --from-file = s3.conf To test if the values are correct run the following command: Show S3 Secrets kubectl get secret -n postgres direktiv-pgbackrest-secret -o go-template = '{{ index .data \"s3.conf\" | base64decode }}' High-Availabilty can be achieved by scaling the database replicas. The following example has added daily differential backups and pod anti-affinity and topology spread constraints to spread the pods across the cluster. If anti-affinity is used the cluster needs to have the same number of nodes and database replicas. S3 Backup Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/s3.yaml S3 Configuration apiVersion : pgv2.percona.com/v2 kind : PerconaPGCluster metadata : name : direktiv-cluster namespace : postgres spec : crVersion : 2.3.0 users : - name : direktiv databases : - direktiv - name : postgres image : perconalab/percona-postgresql-operator:main-ppg14-postgres imagePullPolicy : Always postgresVersion : 14 port : 5432 instances : - name : instance1 replicas : 1 dataVolumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi proxy : pgBouncer : replicas : 1 image : perconalab/percona-postgresql-operator:main-ppg14-pgbouncer backups : pgbackrest : image : perconalab/percona-postgresql-operator:main-ppg14-pgbackrest global : repo1-retention-full : \"4\" repo1-retention-full-type : time configuration : - secret : name : direktiv-pgbackrest-secret manual : repoName : repo1 options : - --type=full repos : - name : repo1 s3 : bucket : direktiv-backup endpoint : \"https://eu-central-1.linodeobjects.com\" region : \"US\" schedules : full : \"0 1 * * 0\" pmm : enabled : false image : percona/pmm-client:2.37.0 Connection-Pooling Connection pooling help scaling and maintaining availability between your application and the database. The Postgres Operator provides pgBouncer as connection pooling mechanism. If it is a multi-node cluster the pgBouncer replicas can be increased and spread across the cluster with pod anit-affinity rules. Direktiv can connect to the Postgres instances as well as to pgBouncer . Getting Database Secrets Direktiv will need the database connection information during installation with a Helm chart. It is a good start for an installation YAML to have this information. It can be easily done by running a simple script: Database Configuration (No Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Database Configuration (With Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Restore from S3 It is always recommended to test the backup and restore before using Direktiv in production. To restore from S3 is a straightforward process. The first step is to pick the backup used for the restore process. It can be found under pgbackrest/backup/db in the bucket used for backups in S3. It looks like this: 20221023-042407F . There are two differtent scenarios to consider. The first one is a restore for an existing database. This can be a restore of as certain backup or a point-in-time recovery. Restore From S3 (Same Database) ... apiVersion : pgv2.percona.com/v2 kind : PerconaPGRestore metadata : name : restore namespace : postgres spec : pgCluster : direktiv-cluster repoName : repo1 options : - --set=20230914-061517F # point-in-time recovery alternative # - --type=time # - --target=\"2023-06-09 14:15:11-04\" The second scenario is if the whole database has been destroyed and it is a restore to a new database instance. In this case a datasource attribute has to be added to define the source for the backup. Restore From S3 (New Database) ... dataSource : postgresCluster : clusterName : direktiv repoName : repo1 options : - --set=20221023-042407F ... Additional Information For more information visit Percona's documentation about backup and restore . Restore from PVC In case the database is not using S3 backups the backups need to be stored in a safe location in case of loss of the node storing the backups. The data has to be transferred via e.g. scp using a cron job. A restore of an existing database can be achieved with a simple restore attribute in the database configuration YAML mentioned in the S3 restore section of this documentation. The process is different if the backup node has been destroyed. It is important to not do this restore procedure if a backup is already running. The backup needs to be rescheduled to execute it without running a backup in parallel. Identify Backup PVC The first step to store the backup is to identify the node where the backup is stored. Identify PV kubectl get pv NAME CAPACITY ... CLAIM # This is the backup node pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 1Gi ... postgres/direktiv-cluster-repo1 pvc-ce9bb226-1038-49bf-bed6-e6d0188b228c 1Gi ... postgres/direktiv-cluster-instance1-q9sm-pgdata Describing the PV shows the node where the data is stored and the directory of the data. This directory needs to be stored in a safe location for a later restore. Identify PV kubectl describe pv pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 Name: pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 ... Claim: postgres/direktiv-repo1 ... Node Affinity: Required Terms: # Node where the data is stored Term 0 : kubernetes.io/hostname in [ db2 ] Message: Source: Type: HostPath ( bare host directory volume ) # Data directory on the node Path: /var/lib/rancher/k3s/storage/pvc-80ae5325-8b27-4695-b6df-b362dd946cb7_postgres_direktiv-repo1 HostPathType: DirectoryOrCreate Copy Data To restore the database a backup has to be selected. The available backups are in <Backup Directory>/<Old PVC Name>/backup/db . The directory will look like the following: Backup Directory drwxr-x--- 7 root root 4096 Okt 23 08 :11 . drwxr-x--- 3 root root 4096 Okt 23 08 :11 .. drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -060801F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -060901F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -061001F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -061101F drwxr-x--- 3 root root 4096 Okt 23 08 :11 backup.history -rw-r----- 1 root root 2792 Okt 23 08 :11 backup.info -rw-r----- 1 root root 2792 Okt 23 08 :11 backup.info.copy lrwxrwxrwx 1 root root 16 Okt 23 08 :11 latest -> 20221023 -061101F The next step is to identify where the new backup folder is located. It is exactly the same procedure as used in the copying process. The archive and backup have to be copied into the new backup directory of the new cluster. Copy Folders sudo cp -Rf <Backup Directory>/archive /var/lib/rancher/k3s/storage/<New PV Directory> sudo cp -Rf <Backup Directory>/backup /var/lib/rancher/k3s/storage/<New PV Directory> sudo chown -R 26 :tape /var/lib/rancher/k3s/storage/<New PV Directory> The selected restore needs to be configured in the database configuration YAML and applied with kubectl apply -f mydb.yaml . Restore from PVC ... spec : dataSource : postgresCluster : clusterName : direktiv repoName : repo1 options : - --set=20221023-075501F - --archive-mode=off ... Update Password If this is a new installation of the database the password will be overwritten and the command to generate the direktiv.yaml file is incorrect. Therefore it is advised to update the password to the password in the Kubernetes secret and update Direktiv with the new password. Update User Password # get the old password kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode # execute in pod kubectl exec -n postgres --stdin --tty direktiv-cluster-instance1-<POD-ID> -- psql # update user password ALTER USER direktiv WITH PASSWORD '<PASSWORD FROM FIRST COMMAND>' ; # exit \\q Helpful Commands Fetch Master Instance kubectl -n postgres get pods \\ --selector = postgres-operator.crunchydata.com/role = master \\ -o jsonpath = '{.items[*].metadata.labels.postgres-operator\\.crunchydata\\.com/instance}' Cluster Information kubectl -n postgres describe postgrescluster direktiv Use psql in Database Instance kubectl exec -n postgres --stdin --tty direktiv-cluster-instance1-<ID> -- psql","title":"Database"},{"location":"installation/database/#database","text":"Direktiv requires a PostgreSQL 13+ database. It acts as datastore as well as pub/sub system between Direktiv's components. It has been tested with Postgres offerings from cloud providers as well as on-premise installations. It is recommended to use a managed Postgres service from cloud providers. If that is not possible Postgres can be installed in Kubernetes as well. To install a Postgres instance in Kubernetes we are using Percona's Postgres operator. The following section will provide exmaples for different installation scenarios from basic testing setups to more complex high-availability configurations. For inidividual changes please visit the Percona Operator documentation page.","title":"Database"},{"location":"installation/database/#installing-the-operator","text":"The operator is provided as Helm chart and the installation is straighforward. Add Direktiv's helm chart repository and run the installation command. Install Postgres Operator helm repo add percona https://percona.github.io/percona-helm-charts/ helm install -n postgres pg-operator percona/pg-operator --wait Backup Ports For the backup to work properly port 2022 needs to be open between the nodes","title":"Installing the Operator"},{"location":"installation/database/#creating-a-postgres-instance","text":"","title":"Creating a Postgres Instance"},{"location":"installation/database/#restore-from-s3","text":"It is always recommended to test the backup and restore before using Direktiv in production. To restore from S3 is a straightforward process. The first step is to pick the backup used for the restore process. It can be found under pgbackrest/backup/db in the bucket used for backups in S3. It looks like this: 20221023-042407F . There are two differtent scenarios to consider. The first one is a restore for an existing database. This can be a restore of as certain backup or a point-in-time recovery. Restore From S3 (Same Database) ... apiVersion : pgv2.percona.com/v2 kind : PerconaPGRestore metadata : name : restore namespace : postgres spec : pgCluster : direktiv-cluster repoName : repo1 options : - --set=20230914-061517F # point-in-time recovery alternative # - --type=time # - --target=\"2023-06-09 14:15:11-04\" The second scenario is if the whole database has been destroyed and it is a restore to a new database instance. In this case a datasource attribute has to be added to define the source for the backup. Restore From S3 (New Database) ... dataSource : postgresCluster : clusterName : direktiv repoName : repo1 options : - --set=20221023-042407F ... Additional Information For more information visit Percona's documentation about backup and restore .","title":"Restore from S3"},{"location":"installation/database/#restore-from-pvc","text":"In case the database is not using S3 backups the backups need to be stored in a safe location in case of loss of the node storing the backups. The data has to be transferred via e.g. scp using a cron job. A restore of an existing database can be achieved with a simple restore attribute in the database configuration YAML mentioned in the S3 restore section of this documentation. The process is different if the backup node has been destroyed. It is important to not do this restore procedure if a backup is already running. The backup needs to be rescheduled to execute it without running a backup in parallel.","title":"Restore from PVC"},{"location":"installation/database/#helpful-commands","text":"Fetch Master Instance kubectl -n postgres get pods \\ --selector = postgres-operator.crunchydata.com/role = master \\ -o jsonpath = '{.items[*].metadata.labels.postgres-operator\\.crunchydata\\.com/instance}' Cluster Information kubectl -n postgres describe postgrescluster direktiv Use psql in Database Instance kubectl exec -n postgres --stdin --tty direktiv-cluster-instance1-<ID> -- psql","title":"Helpful Commands"},{"location":"installation/direktiv/","text":"Direktiv Direktiv requires a few components to run. At least the database has to be installed before proceeding with this part of the installation. Linkerd Database Knative Direktiv The following is a two-step process. First Knative is installed. Knative is responsible to execute Direktiv's serverless functions. It comes pre-configured to work with Direktiv. Knative Knative is an essential part of Direktiv and can be installed with Knative's operator. The following command installs this operator in the default namespace. Install Knative Operator kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.9.4/operator.yaml After the deployment of the operator a new instance of Knative Serving can be created. Direktiv requires a certain configuration for Knative to work. There are two examples of configurations in the (Github repository). The first one is the standard configuration and the other one is an example with proxy settings . Install Knative kubectl create ns knative-serving kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/knative/basic.yaml Direktiv supports Contour as network component. Install Contour kubectl apply --filename https://github.com/knative/net-contour/releases/download/knative-v1.9.3/contour.yaml This installs Contour in two namespaces contour-internal and contour-external . The second namespace is not needed for Direktiv to run and might even block the ingress controller from getting an external IP. This can be deleted with: Delete Contour External kubectl delete namespace contour-external Direktiv Firstly, create a direktiv.yaml file which contains all of the database connectivity and secret information created during the database setup: Direktiv Database Configuration database : # -- database host host : \"direktiv-ha.postgres.svc\" # -- database port port : 5432 # -- database user user : \"direktiv\" # -- database password password : \"direktivdirektiv\" # -- database name, auto created if it does not exist name : \"direktiv\" # -- sslmode for database sslmode : require Database Configuration (No Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Database Configuration (With Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Using this direktiv.yaml configuration, deploy the direktiv helm chart: helm install -f direktiv.yaml -n direktiv direktiv direktiv/direktiv Direktiv should now be running. Run this to get the IP of the UI: kubectl -n direktiv get services direktiv-ingress-nginx-controller --output jsonpath = '{.status.loadBalancer.ingress[0].ip}' For more configuration options go to Direktiv's helm charts .","title":"Direktiv"},{"location":"installation/direktiv/#direktiv","text":"Direktiv requires a few components to run. At least the database has to be installed before proceeding with this part of the installation. Linkerd Database Knative Direktiv The following is a two-step process. First Knative is installed. Knative is responsible to execute Direktiv's serverless functions. It comes pre-configured to work with Direktiv.","title":"Direktiv"},{"location":"installation/direktiv/#knative","text":"Knative is an essential part of Direktiv and can be installed with Knative's operator. The following command installs this operator in the default namespace. Install Knative Operator kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.9.4/operator.yaml After the deployment of the operator a new instance of Knative Serving can be created. Direktiv requires a certain configuration for Knative to work. There are two examples of configurations in the (Github repository). The first one is the standard configuration and the other one is an example with proxy settings . Install Knative kubectl create ns knative-serving kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/knative/basic.yaml Direktiv supports Contour as network component. Install Contour kubectl apply --filename https://github.com/knative/net-contour/releases/download/knative-v1.9.3/contour.yaml This installs Contour in two namespaces contour-internal and contour-external . The second namespace is not needed for Direktiv to run and might even block the ingress controller from getting an external IP. This can be deleted with: Delete Contour External kubectl delete namespace contour-external","title":"Knative"},{"location":"installation/direktiv/#direktiv_1","text":"Firstly, create a direktiv.yaml file which contains all of the database connectivity and secret information created during the database setup: Direktiv Database Configuration database : # -- database host host : \"direktiv-ha.postgres.svc\" # -- database port port : 5432 # -- database user user : \"direktiv\" # -- database password password : \"direktivdirektiv\" # -- database name, auto created if it does not exist name : \"direktiv\" # -- sslmode for database sslmode : require Database Configuration (No Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Database Configuration (With Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Using this direktiv.yaml configuration, deploy the direktiv helm chart: helm install -f direktiv.yaml -n direktiv direktiv direktiv/direktiv Direktiv should now be running. Run this to get the IP of the UI: kubectl -n direktiv get services direktiv-ingress-nginx-controller --output jsonpath = '{.status.loadBalancer.ingress[0].ip}' For more configuration options go to Direktiv's helm charts .","title":"Direktiv"},{"location":"installation/kubernetes/","text":"Kubernetes Direktiv is a cloud-native solution requiring Kubernetes to run. It is working with all Kubernetes offerings of the major cloud providers as well as on-premise Kubernetes installations. The easiest way to install a Kubernetes cluster for Kubernetes is using k3s . The following section explains how to install k3s on-premise. The minimum version is 1.24. k3s Direktiv supports Kubernetes offerings from all major cloud providers and requires Kubernets 1.24+ to be installed. Direktiv supports Kubernetes setups with a single node, seperate server and agents nodes as well as small setups with nodes acting both as server and agent. The following section describes the installation with k3s . Single Node Setup A single node setup requires no further configuration and k3s can be used with the default settings. This setup disables Traefik to be replaced with Nginx during the installation. If proxy configuration is required please read the proxy setup section . One Node Setup curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode = 644 k3s Version If an error message occurs during installation, e.g. resource mapping not found for name: \"linkerd-heartbeat\" namespace: \"linkerd\" from \"\": no matches for kind \"CronJob\" in version \"batch/v1beta1\" it is most likely the wrong k3s version. To keep k3S small there is only a subset of Kubernetes APIs available. Please try to update k3s to the latest version or at least 1.24 Multi Node Setup For production use it is recommended to run Direktiv in a multi-node environment. The k3s documentation page provides a lot of information about configuration and installation options. The following is a quick installation instruction to setup a three node cluster with nodes action as servers and agents. Server configuration In a multi-node environment the nodes have to communicate with each other. Therefore certain ports between those nodes have to be open. The following table shows the ports required to be accessible (incoming) for the nodes to enable this. On some Linux distributions firewall changes have to be applied. Please see k3s installation guide for detailed installation instructions. Protocol Port Source Description TCP 6443 k3s agent nodes Kubernetes API Server UDP 8472 k3s server and agent nodes VXLAN TCP 10250 k3s server and agent nodes Kubelet metrics TCP 2379-2380 k3s server nodes Required for HA with embedded etcd only Firewall changes (Centos/RedHat): Example Firewall Changes Centos/RedHat sudo firewall-cmd --permanent --add-port=6443/tcp sudo firewall-cmd --permanent --add-port=10250/tcp sudo firewall-cmd --permanent --add-port=8472/udp sudo firewall-cmd --permanent --add-port=2379-2380/tcp sudo firewall-cmd --reload Additional Centos/RedHat Instructions https://rancher.com/docs/k3s/latest/en/advanced/#additional-preparation-for-red-hat-centos-enterprise-linux An additional Kubernetes requirement is to disable swap on the nodes. This change need to be applied permanently to survive reboots. This might be achieved differently on different Linux distributions. Disable Swap sudo swapoff -a sudo sed -e '/swap/s/^/#/g' -i /etc/fstab Node Installation k3s provides a script to install k3s. It is recommended to use it for installation. The configuration can be done via environment variables during installation. For Direktiv the default ingress controller (Traefik) needs to be disabled because Nginx will be used. For installations using the embedded etcd the first server node requires the '--cluster-init' flag. Initial Node curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644 --cluster-init\" sh - Loadbalancer To use MetalLB add --disable servicelb to the arguments, e.g. for on-premise installation. It is not needed if the cluster is installed in a cloud environment like AWS, GCP or Azure. To add nodes to the cluster the node token is required, which is saved under /var/lib/rancher/k3s/server/node-token . With this token additional nodes can be added. Additional Nodes curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 sh - MetalLB In a on-premise environment a Kubernetes bare-metal load-balancer is required. The following example shows the use of MetalLB . k3s load-balancer needs to be disabled with --disable servicelb for this to work. Disable Loadbalancer curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 sh - To install MetalLB add the Helm repository and configure the avaiable IPs. helm repo add metallb https://metallb.github.io/metallb helm install metallb metallb/metallb MetalLB needs an IP pool to serve IP address. During the installation this pool can be configured with fhe following example YAML file: MetalLB IP Pool Configuration apiVersion : metallb.io/v1beta1 kind : IPAddressPool metadata : name : myip namespace : default spec : addresses : - 192.168.0.199/32 --- apiVersion : metallb.io/v1beta1 kind : L2Advertisement metadata : name : ipadvertise namespace : default Proxy Setup K3s will download container images during installation and runtime. For the downloads of those internet connectivity is required. If the nodes are behind a proxy server the Linux environment variables need to provided to the service, e.g.: Proxy Settings for k3s curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 HTTP_PROXY = \"http://192.168.1.10:3128\" HTTPS_PROXY = \"http://192.168.1.10:3128\" NO_PROXY = \"localhost,127.0.0.1,svc,.cluster.local,192.168.1.100,192.168.1.101,192.168.1.102,10.0.0.0/8\" sh - Alternatively the environment variables HTTP_PROXY , HTTPS_PROXY and NO_PROXY can be set and k3s will automatically add them to the service configuration file.","title":"Kubernetes"},{"location":"installation/kubernetes/#kubernetes","text":"Direktiv is a cloud-native solution requiring Kubernetes to run. It is working with all Kubernetes offerings of the major cloud providers as well as on-premise Kubernetes installations. The easiest way to install a Kubernetes cluster for Kubernetes is using k3s . The following section explains how to install k3s on-premise. The minimum version is 1.24.","title":"Kubernetes"},{"location":"installation/kubernetes/#k3s","text":"Direktiv supports Kubernetes offerings from all major cloud providers and requires Kubernets 1.24+ to be installed. Direktiv supports Kubernetes setups with a single node, seperate server and agents nodes as well as small setups with nodes acting both as server and agent. The following section describes the installation with k3s .","title":"k3s"},{"location":"installation/kubernetes/#proxy-setup","text":"K3s will download container images during installation and runtime. For the downloads of those internet connectivity is required. If the nodes are behind a proxy server the Linux environment variables need to provided to the service, e.g.: Proxy Settings for k3s curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 HTTP_PROXY = \"http://192.168.1.10:3128\" HTTPS_PROXY = \"http://192.168.1.10:3128\" NO_PROXY = \"localhost,127.0.0.1,svc,.cluster.local,192.168.1.100,192.168.1.101,192.168.1.102,10.0.0.0/8\" sh - Alternatively the environment variables HTTP_PROXY , HTTPS_PROXY and NO_PROXY can be set and k3s will automatically add them to the service configuration file.","title":"Proxy Setup"},{"location":"installation/linkerd/","text":"Linkerd (Optional) Linkerd is a lightweight service mesh for Kubernetes and can be used in Direktiv as a mechanism to secure communication between the components. Linkerd can enable mTLS between the core Direktiv pods as well as the containers running in a flow. The installation of Linkerd is optional. The easiest way to install Linkerd is via Helm . Creating Certificates The identity component of Linkerd requires setting up a trust anchor certificate, and an issuer certificate with its key. The following script starts a container and generates the certificates needed: Generating Linkerd Certificates certDir = $( exe = 'step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 87600h --no-password --insecure \\ --ca ca.crt --ca-key ca.key' ; \\ sudo docker run --mount \"type=bind,src= $( pwd ) ,dst=/home/step\" -i smallstep/step-cli /bin/bash -c \" $exe \" ; \\ echo $( pwd )) ; Permissions The directory where the certificates are located is stored in $certDir. If there are permission problems, please try a different directory with write permissions. Install with Helm After creating the certificates the certificate folder should be located at $certDir. The expiry date provided during installation has to be the same as the value for the certificates (in this case: one year). The following script installs Linkerd with the previously generated certificates: Install Linkerd CRDs helm repo add linkerd https://helm.linkerd.io/stable ; helm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace Install Linkerd helm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM = $certDir /ca.crt \\ --set-file identity.issuer.tls.crtPEM = $certDir /issuer.crt \\ --set-file identity.issuer.tls.keyPEM = $certDir /issuer.key \\ linkerd/linkerd-control-plane --wait Annotate Namespaces To use the service mesh (and, in particular, the mTLS communication) between pods within a Direktiv cluster the namespaces need to be annotated for Linkerd to inject its proxy. The default namespace to annotate is direktiv . Create Namespace kubectl create namespace direktiv Annotate Namespace kubectl annotate ns --overwrite = true direktiv linkerd.io/inject = enabled","title":"Linkerd"},{"location":"installation/linkerd/#linkerd-optional","text":"Linkerd is a lightweight service mesh for Kubernetes and can be used in Direktiv as a mechanism to secure communication between the components. Linkerd can enable mTLS between the core Direktiv pods as well as the containers running in a flow. The installation of Linkerd is optional. The easiest way to install Linkerd is via Helm .","title":"Linkerd (Optional)"},{"location":"installation/linkerd/#creating-certificates","text":"The identity component of Linkerd requires setting up a trust anchor certificate, and an issuer certificate with its key. The following script starts a container and generates the certificates needed: Generating Linkerd Certificates certDir = $( exe = 'step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 87600h --no-password --insecure \\ --ca ca.crt --ca-key ca.key' ; \\ sudo docker run --mount \"type=bind,src= $( pwd ) ,dst=/home/step\" -i smallstep/step-cli /bin/bash -c \" $exe \" ; \\ echo $( pwd )) ; Permissions The directory where the certificates are located is stored in $certDir. If there are permission problems, please try a different directory with write permissions.","title":"Creating Certificates"},{"location":"installation/linkerd/#install-with-helm","text":"After creating the certificates the certificate folder should be located at $certDir. The expiry date provided during installation has to be the same as the value for the certificates (in this case: one year). The following script installs Linkerd with the previously generated certificates: Install Linkerd CRDs helm repo add linkerd https://helm.linkerd.io/stable ; helm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace Install Linkerd helm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM = $certDir /ca.crt \\ --set-file identity.issuer.tls.crtPEM = $certDir /issuer.crt \\ --set-file identity.issuer.tls.keyPEM = $certDir /issuer.key \\ linkerd/linkerd-control-plane --wait","title":"Install with Helm"},{"location":"installation/linkerd/#annotate-namespaces","text":"To use the service mesh (and, in particular, the mTLS communication) between pods within a Direktiv cluster the namespaces need to be annotated for Linkerd to inject its proxy. The default namespace to annotate is direktiv . Create Namespace kubectl create namespace direktiv Annotate Namespace kubectl annotate ns --overwrite = true direktiv linkerd.io/inject = enabled","title":"Annotate Namespaces"},{"location":"installation/summary/","text":"Quick Install This is a list of \"copy&paste\" commands which creates a one node Direktiv cluster. K3s curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION = v1.24.7+k3s1 sh -s - --disable traefik --write-kubeconfig-mode = 644 Linkerd Create certificates certDir = $( exe = 'step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 87600h --no-password --insecure \\ --ca ca.crt --ca-key ca.key' ; \\ sudo docker run --mount \"type=bind,src= $( pwd ) ,dst=/home/step\" -i smallstep/step-cli /bin/bash -c \" $exe \" ; \\ echo $( pwd )) ; Install Linkerd helm repo add linkerd https://helm.linkerd.io/stable helm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace helm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM = $certDir /ca.crt \\ --set-file identity.issuer.tls.crtPEM = $certDir /issuer.crt \\ --set-file identity.issuer.tls.keyPEM = $certDir /issuer.key \\ linkerd/linkerd-control-plane --wait Annotate the Namespace kubectl annotate ns --overwrite = true direktiv linkerd.io/inject = enabled Database helm repo add percona https://percona.github.io/percona-helm-charts/ helm install -n postgres pg-operator percona/pg-operator --wait kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/basic.yaml Knative kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.9.4/operator.yaml kubectl create ns knative-serving kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/knative/basic.yaml kubectl delete ns contour-external Direktiv echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml helm install -f direktiv.yaml -n direktiv direktiv direktiv/direktiv Get IP of Direktiv kubectl -n direktiv get services direktiv-ingress-nginx-controller --output jsonpath = '{.status.loadBalancer.ingress[0].ip}'","title":"Quick Install"},{"location":"installation/summary/#quick-install","text":"This is a list of \"copy&paste\" commands which creates a one node Direktiv cluster.","title":"Quick Install"},{"location":"installation/summary/#k3s","text":"curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION = v1.24.7+k3s1 sh -s - --disable traefik --write-kubeconfig-mode = 644","title":"K3s"},{"location":"installation/summary/#linkerd","text":"","title":"Linkerd"},{"location":"installation/summary/#database","text":"helm repo add percona https://percona.github.io/percona-helm-charts/ helm install -n postgres pg-operator percona/pg-operator --wait kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/basic.yaml","title":"Database"},{"location":"installation/summary/#knative","text":"kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.9.4/operator.yaml kubectl create ns knative-serving kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/knative/basic.yaml kubectl delete ns contour-external","title":"Knative"},{"location":"installation/summary/#direktiv","text":"echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml helm install -f direktiv.yaml -n direktiv direktiv direktiv/direktiv","title":"Direktiv"},{"location":"spec/TODO/","text":"Spec Documentation TODOs YAML Section Inclusions: Logging Metadata Errors Section about secrets. Section about events. Section about errors & error handling. Large section explaining the spec for containers and everything related to actions. Expand variables section. Explain function files and events.","title":"Spec Documentation TODOs"},{"location":"spec/TODO/#spec-documentation-todos","text":"YAML Section Inclusions: Logging Metadata Errors Section about secrets. Section about events. Section about errors & error handling. Large section explaining the spec for containers and everything related to actions. Expand variables section. Explain function files and events.","title":"Spec Documentation TODOs"},{"location":"spec/instance-data/input/","text":"Workflow Input When a workflow is triggered and spawns a new instance it may do so with some starting value for its instance data . Here's everything you need to know about workflow input. API / Subflow If a workflow is invoked directly, either through the API or as a subflow to another instance, its input is passed in as-is. So if you call the workflow with the following input: { \"msg\" : \"Hello, world!\" } Then the instance data for the workflow will, be the same: { \"msg\" : \"Hello, world!\" } That is, unless the input data isn't a valid JSON object. If the input is valid JSON but not an object, as in the following example, it is wrapped within an object automatically under the property .input . So this input: [ 1 , 2 , 3 ] Becomes: { \"input\" : [ 1 , 2 , 3 ] } If the input data isn't valid JSON at all, it is treated as binary data. Binary data is converted into a base64 encoded string and passed into the instance the same way as above. This input: Hello, world! Becomes: { \"input\" : \"SGVsbG8sIHdvcmxkIQo=\" } This treatment of binary data allows workflows to handle non-JSON inputs. Common examples include XML and form data. Just use a function to extract the information needed from these other formats and convert them to JSON. One thing to keep in mind that might trip you up: if you provide no input data whatsoever that's not valid JSON. It is valid binary data, which means this input: Becomes: { \"input\" : \"\" } An empty string is a valid base64 representation of zero bytes. CRON By their nature, scheduled workflows have empty input. They will always be: {} This doesn't mean they have to do exactly the same thing each time, it just means you need to get a little creative. For example, begin your workflow by loading data from variables or by using an action that grabs data from an external source. CloudEvents Events Workflows that are triggered by receiving one or more events will include the received event(s) in their input data. Each received event will appear in the instance data under a property with the same value as their event type, to allow workflows to distinguish between events. For an instance triggered with the following event: { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } The instance input data becomes: { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } } If an event's payload is JSON it should be directly addressable, rather than being embedded within a string. Large Inputs Like instance data, input data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 128 MiB.","title":"Input"},{"location":"spec/instance-data/input/#workflow-input","text":"When a workflow is triggered and spawns a new instance it may do so with some starting value for its instance data . Here's everything you need to know about workflow input.","title":"Workflow Input"},{"location":"spec/instance-data/input/#api-subflow","text":"If a workflow is invoked directly, either through the API or as a subflow to another instance, its input is passed in as-is. So if you call the workflow with the following input: { \"msg\" : \"Hello, world!\" } Then the instance data for the workflow will, be the same: { \"msg\" : \"Hello, world!\" } That is, unless the input data isn't a valid JSON object. If the input is valid JSON but not an object, as in the following example, it is wrapped within an object automatically under the property .input . So this input: [ 1 , 2 , 3 ] Becomes: { \"input\" : [ 1 , 2 , 3 ] } If the input data isn't valid JSON at all, it is treated as binary data. Binary data is converted into a base64 encoded string and passed into the instance the same way as above. This input: Hello, world! Becomes: { \"input\" : \"SGVsbG8sIHdvcmxkIQo=\" } This treatment of binary data allows workflows to handle non-JSON inputs. Common examples include XML and form data. Just use a function to extract the information needed from these other formats and convert them to JSON. One thing to keep in mind that might trip you up: if you provide no input data whatsoever that's not valid JSON. It is valid binary data, which means this input: Becomes: { \"input\" : \"\" } An empty string is a valid base64 representation of zero bytes.","title":"API / Subflow"},{"location":"spec/instance-data/input/#cron","text":"By their nature, scheduled workflows have empty input. They will always be: {} This doesn't mean they have to do exactly the same thing each time, it just means you need to get a little creative. For example, begin your workflow by loading data from variables or by using an action that grabs data from an external source.","title":"CRON"},{"location":"spec/instance-data/input/#cloudevents-events","text":"Workflows that are triggered by receiving one or more events will include the received event(s) in their input data. Each received event will appear in the instance data under a property with the same value as their event type, to allow workflows to distinguish between events. For an instance triggered with the following event: { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } The instance input data becomes: { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } } If an event's payload is JSON it should be directly addressable, rather than being embedded within a string.","title":"CloudEvents Events"},{"location":"spec/instance-data/input/#large-inputs","text":"Like instance data, input data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 128 MiB.","title":"Large Inputs"},{"location":"spec/instance-data/instance-data/","text":"Instance Data Every workflow instance has its own instance data , which is data that is exclusively accessible to the instance, and only changeable by the instance. JSON Objects Instance data is represented in JSON form, and is always a valid JSON object . This detail is important because it means that not everything that is valid JSON can be valid instance data. This is valid instance data: {} So is this: { \"list\" : [ 1 , 2 , 3 ] } And this: { \"a\" : 5 , \"b\" : \"6\" , \"c\" : true , \"d\" : { \"list\" : [ 7 , \"8\" ] } } But this are not valid instance data, even though it is valid JSON: true Neither is this: \"Hello, world!\" Nor is this: [{ \"a\" : 5 }] Another way of looking at it: it's not valid instance data unless it's valid JSON beginning with { and ending with } . Size Limit The size of instance data is measured in terms of the length (in bytes) of its JSON representation. For technical reasons, there is an enforced upper limit allowed for this maximum size. This limit can vary according to the specific configuration of a Direktiv installation, but the default is 128 MiB. Lifecycle The starting value for an instance's data is set based on what triggered the workflow to spawn a new instance. See Workflow Input . Afterwards, the instance may manipulate the data in predictable ways according to the instructions in the workflow definition. The main way to change instance data is through Transforms . Other operations can also contribute to the instance data. Actions may return results, error handling may save error information, event listeners save received events, and variable getters can retrieve data saved elsewhere and add it to the instance data. After the final operation of an instance is executed the instance data becomes the instance's output data. See Instance Output . Output data is viewable by the API, and is also returned to the caller if the instance was executed as a subflow to another workflow.","title":"Instance Data"},{"location":"spec/instance-data/instance-data/#instance-data","text":"Every workflow instance has its own instance data , which is data that is exclusively accessible to the instance, and only changeable by the instance.","title":"Instance Data"},{"location":"spec/instance-data/instance-data/#json-objects","text":"Instance data is represented in JSON form, and is always a valid JSON object . This detail is important because it means that not everything that is valid JSON can be valid instance data. This is valid instance data: {} So is this: { \"list\" : [ 1 , 2 , 3 ] } And this: { \"a\" : 5 , \"b\" : \"6\" , \"c\" : true , \"d\" : { \"list\" : [ 7 , \"8\" ] } } But this are not valid instance data, even though it is valid JSON: true Neither is this: \"Hello, world!\" Nor is this: [{ \"a\" : 5 }] Another way of looking at it: it's not valid instance data unless it's valid JSON beginning with { and ending with } .","title":"JSON Objects"},{"location":"spec/instance-data/instance-data/#size-limit","text":"The size of instance data is measured in terms of the length (in bytes) of its JSON representation. For technical reasons, there is an enforced upper limit allowed for this maximum size. This limit can vary according to the specific configuration of a Direktiv installation, but the default is 128 MiB.","title":"Size Limit"},{"location":"spec/instance-data/instance-data/#lifecycle","text":"The starting value for an instance's data is set based on what triggered the workflow to spawn a new instance. See Workflow Input . Afterwards, the instance may manipulate the data in predictable ways according to the instructions in the workflow definition. The main way to change instance data is through Transforms . Other operations can also contribute to the instance data. Actions may return results, error handling may save error information, event listeners save received events, and variable getters can retrieve data saved elsewhere and add it to the instance data. After the final operation of an instance is executed the instance data becomes the instance's output data. See Instance Output . Output data is viewable by the API, and is also returned to the caller if the instance was executed as a subflow to another workflow.","title":"Lifecycle"},{"location":"spec/instance-data/output/","text":"Instance Output Unlike instance data, which is not accessible to anything other than the instance itself, instance output is exposed via API. It is also returned to caller instance if the workflow was invoked as a subflow. When an instance completes it saves its instance data as its output, which indirectly exposes the instance data. Normally this is the desired behaviour, but it can present a security risk if handled incorrectly. Workflows should take steps to trim things they don't mean to return before terminating using transforms . Large Outputs Like instance data, output data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 128 MiB.","title":"Output"},{"location":"spec/instance-data/output/#instance-output","text":"Unlike instance data, which is not accessible to anything other than the instance itself, instance output is exposed via API. It is also returned to caller instance if the workflow was invoked as a subflow. When an instance completes it saves its instance data as its output, which indirectly exposes the instance data. Normally this is the desired behaviour, but it can present a security risk if handled incorrectly. Workflows should take steps to trim things they don't mean to return before terminating using transforms .","title":"Instance Output"},{"location":"spec/instance-data/output/#large-outputs","text":"Like instance data, output data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 128 MiB.","title":"Large Outputs"},{"location":"spec/instance-data/security/","text":"Security There are a number of security concerns to keep in mind with instance data: Input & Output Data Is Remembered A copy of the starting value is saved separately so that instances can be replayed. It also helps to debug workflows. It is worth keeping this in mind. Even if your workflow deletes sensitive fields, they are still stored somewhere they could be reused or read later. Likewise, output data is saved. It is returned to parent instances if the instance was executed as a subflow. Both input and output data is requestable via API. For a good way to handle most sensitive data, consider using secrets. Input Validation To protect your workflows from behaving in unexpected ways, including intentional exploits by attackers, it is good practice to validate your input data before acting upon it. That is why we recommend beginning every workflow with a validate state. .private As a basic precaution, anything stored under .private is redacted over the APIs that retrieve instance input and output data. This data is still usable by the instance. Could still be returned to a parent instance. It is still stored in the database in plaintext. And there is nothing preventing you from transforming it or passing it somewhere that exposes this information. Use this feature with caution.","title":"Security"},{"location":"spec/instance-data/security/#security","text":"There are a number of security concerns to keep in mind with instance data:","title":"Security"},{"location":"spec/instance-data/security/#input-output-data-is-remembered","text":"A copy of the starting value is saved separately so that instances can be replayed. It also helps to debug workflows. It is worth keeping this in mind. Even if your workflow deletes sensitive fields, they are still stored somewhere they could be reused or read later. Likewise, output data is saved. It is returned to parent instances if the instance was executed as a subflow. Both input and output data is requestable via API. For a good way to handle most sensitive data, consider using secrets.","title":"Input &amp; Output Data Is Remembered"},{"location":"spec/instance-data/security/#input-validation","text":"To protect your workflows from behaving in unexpected ways, including intentional exploits by attackers, it is good practice to validate your input data before acting upon it. That is why we recommend beginning every workflow with a validate state.","title":"Input Validation"},{"location":"spec/instance-data/security/#private","text":"As a basic precaution, anything stored under .private is redacted over the APIs that retrieve instance input and output data. This data is still usable by the instance. Could still be returned to a parent instance. It is still stored in the database in plaintext. And there is nothing preventing you from transforming it or passing it somewhere that exposes this information. Use this feature with caution.","title":".private"},{"location":"spec/instance-data/structured-jx/","text":"Structured JX Many fields of the workflow definition are described as \"Structured JX\". That's a name we use for fields that support complex and powerful query logic that we'll describe in greater detail here. JQ Since instance data is represented as JSON, the most natural way to work with that data is with the powerful JSON query language called jq. Whenever a string appears within a Structured JX field that includes jq(...) , everything between the brackets is evaluated as a jq query against the instance data. Then the entire jq(...) part is replaced by the results of that query. Note: YAML allows for strings without quotation marks, but this should be avoided when using Structured JX. The characters in the queries will commonly be interpreted in unintended ways by the YAML parser. If the jq(...) part constitutes the entirety of the string then the entire string is replaced by whatever data type was returned. If not, the results are marshalled into a JSON string and substituted into the parent string. The one exception to this rule is if the returned data type is a string, in which case it is substituted as-is without marshalling into JSON. This enables you to build strings without filling them with quotation marks. Example 1 Instance Data { \"a\" : [ 1 , 2 , 3 ] } Structured JX 'jq(.a)' Evaluated Result [ 1 , 2 , 3 ] Example 2 Instance Data { \"a\" : [ 1 , 2 , 3 ] } Structured JX 'a: jq(.a)' Evaluated Result \"a: [1, 2, 3]\" Example 3 Instance Data { \"a\" : \"hello\" } Structured JX 'a: jq(.a)' Evaluated Result \"a: hello\" JS JQ isn't the only option available to interact with the instance data. Javascript is also supported using js(...) in a very similar way. Entire Javascript scripts can be embedded in strings within Structured JX. Note: YAML supports several ways of including large or multi-line strings. But each of these ways is treated a little bit differently by the YAML parser. To preserve newlines, we recommend using the | form. With Javascript this often necessary. When writing scripts this way, the instance data is copied and exposed to the script in an object called data . Example 1 JQ transform : 'jq({x: 5})' Analogous Javascript transform : | js( items = new Object() items.x = 5 return items ) Example 2 JQ transform : 'jq({x: .a})' Analogous Javascript transform : | js( items = new Object() items.x = data['a'] return items ) YAML So far we've seen how you can use jq or Javascript to produce a value for your Structured JX field, but it's also possible to use neither, or both. The \"Structured\" part of Structured JX is so named because you don't have to provide a single string. You can provide any type of data you like. The entirety of what is provided will be converted from its YAML representation to a JSON representation. And then every field within will be searched recursively for embedded jq/Javascript. Example Instance Data Before Transform { \"a\" : [ 1 , 2 , 3 ] } Transform tranform : x : 'jq(.a)' y : | js( var output = data['a'].map((x) => {return ++x;}) return output ) z : 5 listA : [ \"a\" , \"b\" , \"c\" ] listB : - d - e - f obj : i : 10 j : 'jq(.a[2])' Evaluated Result { \"listA\" : [ \"a\" , \"b\" , \"c\" ], \"listB\" : [ \"d\" , \"e\" , \"f\" ], \"obj\" : { \"i\" : 10 , \"j\" : 3 }, \"x\" : [ 1 , 2 , 3 ], \"y\" : [ 2 , 3 , 4 ], \"z\" : 5 }","title":"Structured JX"},{"location":"spec/instance-data/structured-jx/#structured-jx","text":"Many fields of the workflow definition are described as \"Structured JX\". That's a name we use for fields that support complex and powerful query logic that we'll describe in greater detail here.","title":"Structured JX"},{"location":"spec/instance-data/structured-jx/#jq","text":"Since instance data is represented as JSON, the most natural way to work with that data is with the powerful JSON query language called jq. Whenever a string appears within a Structured JX field that includes jq(...) , everything between the brackets is evaluated as a jq query against the instance data. Then the entire jq(...) part is replaced by the results of that query. Note: YAML allows for strings without quotation marks, but this should be avoided when using Structured JX. The characters in the queries will commonly be interpreted in unintended ways by the YAML parser. If the jq(...) part constitutes the entirety of the string then the entire string is replaced by whatever data type was returned. If not, the results are marshalled into a JSON string and substituted into the parent string. The one exception to this rule is if the returned data type is a string, in which case it is substituted as-is without marshalling into JSON. This enables you to build strings without filling them with quotation marks.","title":"JQ"},{"location":"spec/instance-data/structured-jx/#js","text":"JQ isn't the only option available to interact with the instance data. Javascript is also supported using js(...) in a very similar way. Entire Javascript scripts can be embedded in strings within Structured JX. Note: YAML supports several ways of including large or multi-line strings. But each of these ways is treated a little bit differently by the YAML parser. To preserve newlines, we recommend using the | form. With Javascript this often necessary. When writing scripts this way, the instance data is copied and exposed to the script in an object called data .","title":"JS"},{"location":"spec/instance-data/structured-jx/#example-2_1","text":"JQ transform : 'jq({x: .a})' Analogous Javascript transform : | js( items = new Object() items.x = data['a'] return items )","title":"Example 2"},{"location":"spec/instance-data/structured-jx/#yaml","text":"So far we've seen how you can use jq or Javascript to produce a value for your Structured JX field, but it's also possible to use neither, or both. The \"Structured\" part of Structured JX is so named because you don't have to provide a single string. You can provide any type of data you like. The entirety of what is provided will be converted from its YAML representation to a JSON representation. And then every field within will be searched recursively for embedded jq/Javascript.","title":"YAML"},{"location":"spec/instance-data/transforms/","text":"Transforms Whenever an instance finishes executing a state there is an opportunity to perform a Transform. Usually with a field called transform , but sometimes in other forms. The switch state also has a defaultTransform , for example. All transforms use structured jx , giving you powerful options to enrich, sanitize, or modify instance data. All transforms must produce output that remains valid instance data , otherwise an error will be thrown: direktiv.jq.notObject . Examples Here are some common use-case helpful examples of transforms. Completely Replacing Instance Data Instance Data Before Transform { \"msg\" : \"Hello, world! } Transform Snippet - id : snippet type : noop transform : x : 5 Instance Data After Transform { \"x\" : 5 } Replacing A Subset Of Instance Data Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(.a = 5 | .b = 6)' Instance Data After Transform { \"a\" : 5 , \"b\" : 6 , \"c\" : 3 } Deleteing A Subset of Instance Data Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(del(.a) | del(.b))' Instance Data After Transform { \"c\" : 3 } Adding A New Value. Instance Data Before Transform { \"a\" : 1 } Transform Snippet - id : snippet type : noop transform : 'jq(.b = 2)' Instance Data After Transform { \"a\" : 1 , \"b\" : 2 } Renaming A Subset of Instance Data Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(.x = .a | del(.a))' Instance Data After Transform { \"b\" : 2 , \"c\" : 3 , \"x\" : 1 }","title":"Transforms"},{"location":"spec/instance-data/transforms/#transforms","text":"Whenever an instance finishes executing a state there is an opportunity to perform a Transform. Usually with a field called transform , but sometimes in other forms. The switch state also has a defaultTransform , for example. All transforms use structured jx , giving you powerful options to enrich, sanitize, or modify instance data. All transforms must produce output that remains valid instance data , otherwise an error will be thrown: direktiv.jq.notObject .","title":"Transforms"},{"location":"spec/instance-data/transforms/#examples","text":"Here are some common use-case helpful examples of transforms.","title":"Examples"},{"location":"spec/variables/system/","text":"System Variables In addition to the standard scopes, there is a special system scope. This scope is a utility to make miscellaneous information accessible to an instance. The following special variables exist in the system scope: Key Description instance Returns the instance ID of the running instance. uuid Returns a randomly generated UUID. epoch Returns the current time in unix/epoch format.","title":"System"},{"location":"spec/variables/system/#system-variables","text":"In addition to the standard scopes, there is a special system scope. This scope is a utility to make miscellaneous information accessible to an instance. The following special variables exist in the system scope: Key Description instance Returns the instance ID of the running instance. uuid Returns a randomly generated UUID. epoch Returns the current time in unix/epoch format.","title":"System Variables"},{"location":"spec/variables/variables/","text":"Variables Direktiv can store data separately to instance data . An instance can read and change its instance data at will so you might wonder why this separation needs to exist, but it turns out variables solve a number of problems: Efficiently passing around large datasets or files to actions, especially ones that exceed instance data size limits. Persisting data between instances of a workflow. Sharing data between different workflows. Scopes All variables belong to a scope. The scopes are instance , workflow , namespace , and file . Instance scoped variables are only accessible to the singular instance that created them. Workflow scoped variables can be used and shared between multiple instances of the same workflow. Namespace scoped variables are available to all instances of all workflows on the namespace. For these first three scopes, variables are identified by a name, and each name is unique within its scope. The file scope is treated similarly. It looks for a file in the namespace's filetree to use. This means file scoped variables are available to all instances of all workflows on the namespace, which is similar to namespace scoped variables. However, instead of using just a name, you need to reference them by a filepath. Additionally, the file scope is considered read-only: instances do not have permission to delete, modify, or create files on the filetree. States Two types of states in the workflow spec interact directly with variables: getter and setter . Files Due to size limitations on action inputs and instance data it can sometimes be impossible to pass data to actions without using variables. Actions can interact with variables directly, loading them onto their file-system and sometimes creating/changing variables as well.","title":"Variables"},{"location":"spec/variables/variables/#variables","text":"Direktiv can store data separately to instance data . An instance can read and change its instance data at will so you might wonder why this separation needs to exist, but it turns out variables solve a number of problems: Efficiently passing around large datasets or files to actions, especially ones that exceed instance data size limits. Persisting data between instances of a workflow. Sharing data between different workflows.","title":"Variables"},{"location":"spec/variables/variables/#scopes","text":"All variables belong to a scope. The scopes are instance , workflow , namespace , and file . Instance scoped variables are only accessible to the singular instance that created them. Workflow scoped variables can be used and shared between multiple instances of the same workflow. Namespace scoped variables are available to all instances of all workflows on the namespace. For these first three scopes, variables are identified by a name, and each name is unique within its scope. The file scope is treated similarly. It looks for a file in the namespace's filetree to use. This means file scoped variables are available to all instances of all workflows on the namespace, which is similar to namespace scoped variables. However, instead of using just a name, you need to reference them by a filepath. Additionally, the file scope is considered read-only: instances do not have permission to delete, modify, or create files on the filetree.","title":"Scopes"},{"location":"spec/variables/variables/#states","text":"Two types of states in the workflow spec interact directly with variables: getter and setter .","title":"States"},{"location":"spec/variables/variables/#files","text":"Due to size limitations on action inputs and instance data it can sometimes be impossible to pass data to actions without using variables. Actions can interact with variables directly, loading them onto their file-system and sometimes creating/changing variables as well.","title":"Files"},{"location":"spec/workflow-yaml/action/","text":"Action State - id : a type : action action : function : myfunc input : 'jq(.x)' ActionStateDefinition The action state is the simplest and most common way to call a function or invoke a workflow to act as a subflow. See Actions . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to action . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no async If set to true , the workflow execution will continue without waiting for the action to return. boolean no action Defines the action to perform. ActionDefinition yes","title":"action"},{"location":"spec/workflow-yaml/action/#action-state","text":"- id : a type : action action : function : myfunc input : 'jq(.x)'","title":"Action State"},{"location":"spec/workflow-yaml/action/#actionstatedefinition","text":"The action state is the simplest and most common way to call a function or invoke a workflow to act as a subflow. See Actions . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to action . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no async If set to true , the workflow execution will continue without waiting for the action to return. boolean no action Defines the action to perform. ActionDefinition yes","title":"ActionStateDefinition"},{"location":"spec/workflow-yaml/actions/","text":"Actions - id : a type : action action : function : myfunc input : 'jq(.x)' ActionDefinition Parameter Description Type Required function Name of the referenced function. See FunctionDefinition . string yes input Selects or generates the data to send as input to the function. Structured JQ no secrets Defines a list of secrets to temporarily add to the instance data under .secrets , before evaluating the input . []string no retries []RetryPolicyDefinition no files Determines a list of files to load onto the function's file-system from variables. Only valid if the referenced function supports it. []FunctionFileDefinition no RetryPolicyDefinition - id : a type : action action : function : myfunc input : 'jq(.x)' retries : codes : [ \".*\" ] max_attempts : 3 delay : PT3S multiplier : 1.5 Parameter Description Type Required codes A list of \"glob\" patterns that will be compared to catchable error codes returned by the function to determine if this retry policy applies. []string yes max_attempts Maximum number of retry attempts. If the function has been retried this many times or more when this policy is invoked the retry will be skipped, and instead the error will be escalated to the state's error handling logic. integer yes delay ISO8601 duration string giving a time delay between retry attempts. string no multiplier Value by which the delay is multiplied after each attempt. float no FunctionFileDefinition - id : a type : action action : function : myfunc input : 'jq(.x)' files : - key : VAR_A scope : namespace as : a Some function types support loading variable directly from storage onto their file-systems. This object defines what variable to load and what to save it as. Parameter Description Type Required key Identifies which variable to load into a file. string yes scope Specifies the scope from which to load the variable. VariableScopeDefinition no as Names the resulting file. If left unspecified, the key will be used instead. string no VariableScopeDefinition Every variable exists within a single scope. The scope dictates what can access it and how persistent it is. There are four defined scopes : instance workflow namespace file","title":"Actions"},{"location":"spec/workflow-yaml/actions/#actions","text":"- id : a type : action action : function : myfunc input : 'jq(.x)'","title":"Actions"},{"location":"spec/workflow-yaml/actions/#actiondefinition","text":"Parameter Description Type Required function Name of the referenced function. See FunctionDefinition . string yes input Selects or generates the data to send as input to the function. Structured JQ no secrets Defines a list of secrets to temporarily add to the instance data under .secrets , before evaluating the input . []string no retries []RetryPolicyDefinition no files Determines a list of files to load onto the function's file-system from variables. Only valid if the referenced function supports it. []FunctionFileDefinition no","title":"ActionDefinition"},{"location":"spec/workflow-yaml/actions/#retrypolicydefinition","text":"- id : a type : action action : function : myfunc input : 'jq(.x)' retries : codes : [ \".*\" ] max_attempts : 3 delay : PT3S multiplier : 1.5 Parameter Description Type Required codes A list of \"glob\" patterns that will be compared to catchable error codes returned by the function to determine if this retry policy applies. []string yes max_attempts Maximum number of retry attempts. If the function has been retried this many times or more when this policy is invoked the retry will be skipped, and instead the error will be escalated to the state's error handling logic. integer yes delay ISO8601 duration string giving a time delay between retry attempts. string no multiplier Value by which the delay is multiplied after each attempt. float no","title":"RetryPolicyDefinition"},{"location":"spec/workflow-yaml/actions/#functionfiledefinition","text":"- id : a type : action action : function : myfunc input : 'jq(.x)' files : - key : VAR_A scope : namespace as : a Some function types support loading variable directly from storage onto their file-systems. This object defines what variable to load and what to save it as. Parameter Description Type Required key Identifies which variable to load into a file. string yes scope Specifies the scope from which to load the variable. VariableScopeDefinition no as Names the resulting file. If left unspecified, the key will be used instead. string no","title":"FunctionFileDefinition"},{"location":"spec/workflow-yaml/actions/#variablescopedefinition","text":"Every variable exists within a single scope. The scope dictates what can access it and how persistent it is. There are four defined scopes : instance workflow namespace file","title":"VariableScopeDefinition"},{"location":"spec/workflow-yaml/consume-event/","text":"ConsumeEvent State - id : a type : consumeEvent timeout : PT15M event : type : com.github.pull.create context : subject : '123' ConsumeEventStateDefinition To pause the workflow and wait until a CloudEvents event is received before proceeding, the consumeEvent is the simplest state that can be used. It is one of three states that can do so, along with eventsAnd and eventsXor . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to consumeEvent . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no event Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes ConsumeEventDefinition The StartEventDefinition is a structure shared by various event-consuming states. Parameter Description Type Required type Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own type context value. string yes context Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" must be strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. Structured JQ no The received data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following data: CloudEvents Event { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Input Data { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } }","title":"consumeEvent"},{"location":"spec/workflow-yaml/consume-event/#consumeevent-state","text":"- id : a type : consumeEvent timeout : PT15M event : type : com.github.pull.create context : subject : '123'","title":"ConsumeEvent State"},{"location":"spec/workflow-yaml/consume-event/#consumeeventstatedefinition","text":"To pause the workflow and wait until a CloudEvents event is received before proceeding, the consumeEvent is the simplest state that can be used. It is one of three states that can do so, along with eventsAnd and eventsXor . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to consumeEvent . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no event Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"ConsumeEventStateDefinition"},{"location":"spec/workflow-yaml/consume-event/#consumeeventdefinition","text":"The StartEventDefinition is a structure shared by various event-consuming states. Parameter Description Type Required type Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own type context value. string yes context Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" must be strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. Structured JQ no The received data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following data: CloudEvents Event { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Input Data { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } }","title":"ConsumeEventDefinition"},{"location":"spec/workflow-yaml/delay/","text":"Delay State - id : a type : delay duration : PT10S DelayStateDefinition If the workflow needs to pause for a specific length of time, the delay state is usually the simplest way to do that. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to delay . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no duration An ISO8601 duration string. string yes","title":"delay"},{"location":"spec/workflow-yaml/delay/#delay-state","text":"- id : a type : delay duration : PT10S","title":"Delay State"},{"location":"spec/workflow-yaml/delay/#delaystatedefinition","text":"If the workflow needs to pause for a specific length of time, the delay state is usually the simplest way to do that. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to delay . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no duration An ISO8601 duration string. string yes","title":"DelayStateDefinition"},{"location":"spec/workflow-yaml/error/","text":"Error State - id : a type : error error : badinput message : 'Missing or invalid value for required input.' ErrorStateDefinition When workflow logic end up in a failure mode, the error state can be used to mark the instance as failed. This allows the instance to report what went wrong to the caller, which may then be handled or reported appropriately. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to error . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no error A short descriptive error code that can be caught by a parent workflow. string yes message Generates a more detailed message or object that can contain instance data, to provide more information for users. Structured JQ yes","title":"error"},{"location":"spec/workflow-yaml/error/#error-state","text":"- id : a type : error error : badinput message : 'Missing or invalid value for required input.'","title":"Error State"},{"location":"spec/workflow-yaml/error/#errorstatedefinition","text":"When workflow logic end up in a failure mode, the error state can be used to mark the instance as failed. This allows the instance to report what went wrong to the caller, which may then be handled or reported appropriately. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to error . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no error A short descriptive error code that can be caught by a parent workflow. string yes message Generates a more detailed message or object that can contain instance data, to provide more information for users. Structured JQ yes","title":"ErrorStateDefinition"},{"location":"spec/workflow-yaml/errors/","text":"Errors Errors can happen for many reasons. Direktiv allows you to catch and handle these errors using a common field 'catch'. This field takes an array of ErrorCatchDefinition objects, each specifying one or more errors that apply and where to transition to next in order to handle them. When an error is thrown, the list of error catchers is evaluated in order until a match is found. If no match is found, the instance fails. direktiv_api : workflow/v1 states : - id : a type : consumeEvent timeout : PT5S event : type : com.github.pull.create catch : - error : \"direktiv.cancels.timeout.soft\" transition : handle-error - id : handle-error type : noop log : handling error ErrorCatchDefinition Parameter Description Type Required error Specified what error code(s) this catcher applies to. This should be a \"glob\" pattern that will be compared to catchable error codes to determine if this retry policy applies. string yes transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no","title":"Errors"},{"location":"spec/workflow-yaml/errors/#errors","text":"Errors can happen for many reasons. Direktiv allows you to catch and handle these errors using a common field 'catch'. This field takes an array of ErrorCatchDefinition objects, each specifying one or more errors that apply and where to transition to next in order to handle them. When an error is thrown, the list of error catchers is evaluated in order until a match is found. If no match is found, the instance fails. direktiv_api : workflow/v1 states : - id : a type : consumeEvent timeout : PT5S event : type : com.github.pull.create catch : - error : \"direktiv.cancels.timeout.soft\" transition : handle-error - id : handle-error type : noop log : handling error","title":"Errors"},{"location":"spec/workflow-yaml/errors/#errorcatchdefinition","text":"Parameter Description Type Required error Specified what error code(s) this catcher applies to. This should be a \"glob\" pattern that will be compared to catchable error codes to determine if this retry policy applies. string yes transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no","title":"ErrorCatchDefinition"},{"location":"spec/workflow-yaml/events-and/","text":"EventsAnd State - id : a type : eventsAnd timeout : PT15M events : - type : com.github.pull.create context : subject : '123' - type : com.github.pull.delete context : subject : '123' EventsAndStateDefinition To pause the workflow and wait until multiple CloudEvents events are received before proceeding, the eventsAnd is used. Every listed event must be received for the state to complete. If there are multiple events of the same type a index number will be added to the duplicate cloudevent types. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to eventsAnd . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no events Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"eventsAnd"},{"location":"spec/workflow-yaml/events-and/#eventsand-state","text":"- id : a type : eventsAnd timeout : PT15M events : - type : com.github.pull.create context : subject : '123' - type : com.github.pull.delete context : subject : '123'","title":"EventsAnd State"},{"location":"spec/workflow-yaml/events-and/#eventsandstatedefinition","text":"To pause the workflow and wait until multiple CloudEvents events are received before proceeding, the eventsAnd is used. Every listed event must be received for the state to complete. If there are multiple events of the same type a index number will be added to the duplicate cloudevent types. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to eventsAnd . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no events Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"EventsAndStateDefinition"},{"location":"spec/workflow-yaml/events-xor/","text":"EventsXor State direktiv_api : workflow/v1 states : - id : a type : eventsXor timeout : PT15M events : - event : type : com.github.pull.create context : subject : '123' transition : received transform : hello : world - event : type : com.github.pull.delete context : subject : '123' transition : received - id : received type : noop EventsXorStateDefinition To pause the workflow and wait until one of multiple CloudEvents events is received before proceeding, the eventsXor state might be used. Any event match received will cause this state to complete. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to eventsXor . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no events Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"eventsXor"},{"location":"spec/workflow-yaml/events-xor/#eventsxor-state","text":"direktiv_api : workflow/v1 states : - id : a type : eventsXor timeout : PT15M events : - event : type : com.github.pull.create context : subject : '123' transition : received transform : hello : world - event : type : com.github.pull.delete context : subject : '123' transition : received - id : received type : noop","title":"EventsXor State"},{"location":"spec/workflow-yaml/events-xor/#eventsxorstatedefinition","text":"To pause the workflow and wait until one of multiple CloudEvents events is received before proceeding, the eventsXor state might be used. Any event match received will cause this state to complete. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to eventsXor . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no events Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"EventsXorStateDefinition"},{"location":"spec/workflow-yaml/foreach/","text":"Foreach State - id : data type : noop transform : names : - hello - world transition : a - id : a type : foreach array : 'jq([.names[] | {name: .}])' action : function : echo input : 'jq(.name)' ForeachStateDefinition The foreach state is a convenient way to divide some data and then perform an action on each element in parallel. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to foreach . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no array Selects or generates an array, from which each element will be separately acted upon. The action.input will be evaluated against each element in this array, rather than the usual instance data. Structured JQ yes action Defines the action to perform. ActionDefinition yes","title":"foreach"},{"location":"spec/workflow-yaml/foreach/#foreach-state","text":"- id : data type : noop transform : names : - hello - world transition : a - id : a type : foreach array : 'jq([.names[] | {name: .}])' action : function : echo input : 'jq(.name)'","title":"Foreach State"},{"location":"spec/workflow-yaml/foreach/#foreachstatedefinition","text":"The foreach state is a convenient way to divide some data and then perform an action on each element in parallel. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to foreach . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no array Selects or generates an array, from which each element will be separately acted upon. The action.input will be evaluated against each element in this array, rather than the usual instance data. Structured JQ yes action Defines the action to perform. ActionDefinition yes","title":"ForeachStateDefinition"},{"location":"spec/workflow-yaml/functions/","text":"Functions FunctionDefinition Functions refer to anything executable by Direktiv as a unit of logic within a subflow that isn't otherwise part of basic state functionality. Usually this means either a purpose-built container or another workflow executed as a subflow. In some cases functions can be extensively configured, and they are often reused repeatedly within a workflow. To manage the size of Direktiv workflow definitions functions are predefined as much as possible and referenced when called. These are the currently available function types: Functions FunctionDefinition NamespacedKnativeFunctionDefinition WorkflowKnativeFunctionDefinition ContainerSizeDefinition SubflowFunctionDefinition The following example demonstrate how to define and reference a function within a workflow: Workflow direktiv_api : workflow/v1 description : | A basic demonstration of functions. functions : - type : knative-workflow id : request image : direktiv/request:latest size : small states : - id : getter type : action action : function : request input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Input {} Output { \"return\" : { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"completed\" : false } } NamespacedKnativeFunctionDefinition A knative-namespace refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service configured to be available on the namespace. This function type supports files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to knative-namespace . string yes id A unique identifier for the function within the workflow definition. string yes service URI to a function on the namespace. string yes WorkflowKnativeFunctionDefinition A knative-workflow refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service that Direktiv can create on-demand for the exclusive use by this workflow. This function type supports files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to knative-workflow . string yes id A unique identifier for the function within the workflow definition. string yes image URI to a knative-workflow compliant container. string yes size Specifies the container size. ContainerSizeDefinition no cmd Custom command to execute within the container. string no ContainerSizeDefinition When functions use containers you may be able to specify what size the container should be. This is done using one of three keywords, each representing a different size preset defined in Direktiv's configuration files: small medium large SubflowFunctionDefinition A subflow refers to a function that is actually another workflow. The other workflow is called with some input and its output is returned to this workflow. This function type does not support files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to subflow . string yes id A unique identifier for the function within the workflow definition. string yes workflow URI to a workflow within the same namespace. string yes","title":"Functions"},{"location":"spec/workflow-yaml/functions/#functions","text":"","title":"Functions"},{"location":"spec/workflow-yaml/functions/#functiondefinition","text":"Functions refer to anything executable by Direktiv as a unit of logic within a subflow that isn't otherwise part of basic state functionality. Usually this means either a purpose-built container or another workflow executed as a subflow. In some cases functions can be extensively configured, and they are often reused repeatedly within a workflow. To manage the size of Direktiv workflow definitions functions are predefined as much as possible and referenced when called. These are the currently available function types: Functions FunctionDefinition NamespacedKnativeFunctionDefinition WorkflowKnativeFunctionDefinition ContainerSizeDefinition SubflowFunctionDefinition The following example demonstrate how to define and reference a function within a workflow: Workflow direktiv_api : workflow/v1 description : | A basic demonstration of functions. functions : - type : knative-workflow id : request image : direktiv/request:latest size : small states : - id : getter type : action action : function : request input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Input {} Output { \"return\" : { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"completed\" : false } }","title":"FunctionDefinition"},{"location":"spec/workflow-yaml/generate-event/","text":"GenerateEvent State - id : a type : generateEvent event : type : myeventtype source : myeventsource data : hello : world datacontenttype : application/json GenerateEventStateDefinition Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to generateEvent . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no delay ISO8601 duration string defining how long to hold the event before broadcasting it. string no event Defines the event to generate. GenerateEventDefinition yes GenerateEventDefinition Parameter Description Type Required type Sets the CloudEvents event type. string yes source Sets the CloudEvents event source. string yes data Defines the content of the payload for the CloudEvents event. Structured JQ no datacontenttype An RFC2046 string specifying the payload content type. string no context If defined, must evaluate to an object of key-value pairs. These will be used to define CloudEvents event context data. Structured JQ no","title":"generateEvent"},{"location":"spec/workflow-yaml/generate-event/#generateevent-state","text":"- id : a type : generateEvent event : type : myeventtype source : myeventsource data : hello : world datacontenttype : application/json","title":"GenerateEvent State"},{"location":"spec/workflow-yaml/generate-event/#generateeventstatedefinition","text":"Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to generateEvent . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no delay ISO8601 duration string defining how long to hold the event before broadcasting it. string no event Defines the event to generate. GenerateEventDefinition yes","title":"GenerateEventStateDefinition"},{"location":"spec/workflow-yaml/generate-event/#generateeventdefinition","text":"Parameter Description Type Required type Sets the CloudEvents event type. string yes source Sets the CloudEvents event source. string yes data Defines the content of the payload for the CloudEvents event. Structured JQ no datacontenttype An RFC2046 string specifying the payload content type. string no context If defined, must evaluate to an object of key-value pairs. These will be used to define CloudEvents event context data. Structured JQ no","title":"GenerateEventDefinition"},{"location":"spec/workflow-yaml/getter/","text":"Getter State - id : a type : setter variables : - key : x scope : workflow mimeType : application/json value : Hello World transition : b - id : b type : getter variables : - key : x scope : workflow GetterStateDefinition To load variables, use the getter state. See Variables . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to getter . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no variables Defines variables to load. []VariableGetterDefinition yes VariableGetterDefinition Parameter Description Type Required key Variable name. Structured JQ yes scope Selects the scope to which the variable belongs. If undefined, defaults to instance . See Variables . yes no as Names the resulting data. If left unspecified, the key will be used instead. string no","title":"getter"},{"location":"spec/workflow-yaml/getter/#getter-state","text":"- id : a type : setter variables : - key : x scope : workflow mimeType : application/json value : Hello World transition : b - id : b type : getter variables : - key : x scope : workflow","title":"Getter State"},{"location":"spec/workflow-yaml/getter/#getterstatedefinition","text":"To load variables, use the getter state. See Variables . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to getter . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no variables Defines variables to load. []VariableGetterDefinition yes","title":"GetterStateDefinition"},{"location":"spec/workflow-yaml/getter/#variablegetterdefinition","text":"Parameter Description Type Required key Variable name. Structured JQ yes scope Selects the scope to which the variable belongs. If undefined, defaults to instance . See Variables . yes no as Names the resulting data. If left unspecified, the key will be used instead. string no","title":"VariableGetterDefinition"},{"location":"spec/workflow-yaml/logging/","text":"Logging All states can write to instance logs via a common field log . This field uses structured jx to support querying instance data and inserting it into the logs. - id : a type : noop log : 'Hello, world!'","title":"Logging"},{"location":"spec/workflow-yaml/logging/#logging","text":"All states can write to instance logs via a common field log . This field uses structured jx to support querying instance data and inserting it into the logs. - id : a type : noop log : 'Hello, world!'","title":"Logging"},{"location":"spec/workflow-yaml/metadata/","text":"Instance Metadata Instance metadata is a way to monitor an instance. An instance can update its metadata at any time, replacing it with whatever information it needs to expose via the API. All states can write to instance metadata via a common field metadata . This field uses structured jx to support querying instance data and inserting it into the metadata. direktiv_api : workflow/v1 states : - id : a type : delay duration : PT1M metadata : workflow-data : jq(.)","title":"Instance Metadata"},{"location":"spec/workflow-yaml/metadata/#instance-metadata","text":"Instance metadata is a way to monitor an instance. An instance can update its metadata at any time, replacing it with whatever information it needs to expose via the API. All states can write to instance metadata via a common field metadata . This field uses structured jx to support querying instance data and inserting it into the metadata. direktiv_api : workflow/v1 states : - id : a type : delay duration : PT1M metadata : workflow-data : jq(.)","title":"Instance Metadata"},{"location":"spec/workflow-yaml/noop/","text":"Noop State - id : a type : noop NoopStateDefinition Often workflows need to do something that can be achieved using logic built into most state types. For example, to log something, or to transform the instance data by running a jq command. In many cases this can be done by an existing state within the workflow, but sometimes it's necessary to split it out into a separate state. The noop state exists for this purpose. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to noop . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no","title":"noop"},{"location":"spec/workflow-yaml/noop/#noop-state","text":"- id : a type : noop","title":"Noop State"},{"location":"spec/workflow-yaml/noop/#noopstatedefinition","text":"Often workflows need to do something that can be achieved using logic built into most state types. For example, to log something, or to transform the instance data by running a jq command. In many cases this can be done by an existing state within the workflow, but sometimes it's necessary to split it out into a separate state. The noop state exists for this purpose. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to noop . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no","title":"NoopStateDefinition"},{"location":"spec/workflow-yaml/parallel/","text":"Parallel State - id : a type : parallel mode : and actions : - function : myfunc input : 'jq(.x)' - function : myfunc input : 'jq(.y)' ParallelStateDefinition The parallel state is an alternative to the action state when a workflow can perform multiple threads of logic simultaneously. The values in return is an array of the returns of the individual actions. In mode or the first response is set in the array and the other actions are set to null . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to parallel . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no mode If defined, must be either and or or . The default is and . This setting determines whether the state is considered successfully completed only if all threads have returned without error ( and ) or as soon as any single thread returns without error ( or ). string no actions Defines the action to perform. []ActionDefinition yes","title":"parallel"},{"location":"spec/workflow-yaml/parallel/#parallel-state","text":"- id : a type : parallel mode : and actions : - function : myfunc input : 'jq(.x)' - function : myfunc input : 'jq(.y)'","title":"Parallel State"},{"location":"spec/workflow-yaml/parallel/#parallelstatedefinition","text":"The parallel state is an alternative to the action state when a workflow can perform multiple threads of logic simultaneously. The values in return is an array of the returns of the individual actions. In mode or the first response is set in the array and the other actions are set to null . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to parallel . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no mode If defined, must be either and or or . The default is and . This setting determines whether the state is considered successfully completed only if all threads have returned without error ( and ) or as soon as any single thread returns without error ( or ). string no actions Defines the action to perform. []ActionDefinition yes","title":"ParallelStateDefinition"},{"location":"spec/workflow-yaml/setter/","text":"Setter State - id : a type : setter variables : - key : x scope : workflow mimeType : text/plain value : 'jq(.x)' SetterStateDefinition To create or change variables, use the setter state. See Variables . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to setter . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no variables Defines variables to push. []VariableSetterDefinition yes VariableSetterDefinition Parameter Description Type Required key Variable name. Structured JQ yes scope Selects the scope to which the variable belongs. If undefined, defaults to instance . See Variables . yes no mimeType Store a MIME type with the variable. If left undefined, it will default to application/json . Two specific MIME types cause this state to behave differently: text/plain and application/octet-stream . If the value evaluates to a JSON string the MIME type is text/plain , that string will be stored in plaintext (without JSON quotes and escapes). If if the value is a JSON string containing base64 encoded data and the MIME type is application/octet-stream , the base64 data will be decoded and stored as binary data. Structured JQ no value Select or generate the data to store. Structured JQ yes","title":"setter"},{"location":"spec/workflow-yaml/setter/#setter-state","text":"- id : a type : setter variables : - key : x scope : workflow mimeType : text/plain value : 'jq(.x)'","title":"Setter State"},{"location":"spec/workflow-yaml/setter/#setterstatedefinition","text":"To create or change variables, use the setter state. See Variables . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to setter . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no variables Defines variables to push. []VariableSetterDefinition yes","title":"SetterStateDefinition"},{"location":"spec/workflow-yaml/setter/#variablesetterdefinition","text":"Parameter Description Type Required key Variable name. Structured JQ yes scope Selects the scope to which the variable belongs. If undefined, defaults to instance . See Variables . yes no mimeType Store a MIME type with the variable. If left undefined, it will default to application/json . Two specific MIME types cause this state to behave differently: text/plain and application/octet-stream . If the value evaluates to a JSON string the MIME type is text/plain , that string will be stored in plaintext (without JSON quotes and escapes). If if the value is a JSON string containing base64 encoded data and the MIME type is application/octet-stream , the base64 data will be decoded and stored as binary data. Structured JQ no value Select or generate the data to store. Structured JQ yes","title":"VariableSetterDefinition"},{"location":"spec/workflow-yaml/starts/","text":"Starts StartDefinition A StartDefinition may be defined using one of the following, depending on the desired behaviour: Starts StartDefinition DefaultStartDefinition ScheduledStartDefinition EventStartDefinition EventsXorStartDefinition EventsAndStartDefinition StartEventDefinition If omitted from the workflow definition the DefaultStartDefinition will be used, which means the workflow will only be executed when called. DefaultStartDefinition The default start definition is used for workflows that should only execute when called. This means subflows, workflows triggered by scripts, and workflows triggered manually by humans. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to default . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no ScheduledStartDefinition The scheduled start definition is used for workflows that should execute at regularly defined times. Scheduled workflow can be manually triggered for convenience and testing. They never have input data, so accurate testing should use {} as input. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to scheduled . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no cron Defines the time(s) when the workflow should execute using a CRON expression. string yes Example (snippet) start : type : scheduled cron : '* * * * *' # Trigger a new instance every minute. EventStartDefinition The event start definition is used for workflows that should be executed whenever a relevant CloudEvents event is received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to event . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no event Defines what events can trigger the workflow. StartEventDefinition yes EventsXorStartDefinition The event \"xor\" start definition is used for workflows that should be executed whenever one of multiple possible CloudEvents events is received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to eventsXor . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no events Defines what events can trigger the workflow. []StartEventDefinition yes EventsAndStartDefinition The event \"and\" start definition is used for workflows that should be executed when multiple matching CloudEvents events are received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to eventsAnd . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no lifespan An ISO8601 duration string. Sets the maximum duration an event can be stored before being discarded while waiting for other events. string no events Defines what events can trigger the workflow. []StartEventDefinition yes StartEventDefinition The StartEventDefinition is a structure shared by various start definitions involving events. Parameter Description Type Required type Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own type context value. string yes context Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" are strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. object no The input data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following input data in a workflow triggered by a single event: CloudEvents Event { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Input Data { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } }","title":"Triggers / Starts"},{"location":"spec/workflow-yaml/starts/#starts","text":"","title":"Starts"},{"location":"spec/workflow-yaml/starts/#startdefinition","text":"A StartDefinition may be defined using one of the following, depending on the desired behaviour: Starts StartDefinition DefaultStartDefinition ScheduledStartDefinition EventStartDefinition EventsXorStartDefinition EventsAndStartDefinition StartEventDefinition If omitted from the workflow definition the DefaultStartDefinition will be used, which means the workflow will only be executed when called.","title":"StartDefinition"},{"location":"spec/workflow-yaml/states/","text":"States StateDefinition A StateDefinition may be defined using one of the following, depending on the desired behaviour: action consumeEvent delay error eventsAnd eventsXor foreach generateEvent getter noop parallel setter switch validate","title":"States"},{"location":"spec/workflow-yaml/states/#states","text":"","title":"States"},{"location":"spec/workflow-yaml/states/#statedefinition","text":"A StateDefinition may be defined using one of the following, depending on the desired behaviour: action consumeEvent delay error eventsAnd eventsXor foreach generateEvent getter noop parallel setter switch validate","title":"StateDefinition"},{"location":"spec/workflow-yaml/switch/","text":"Switch State - id : a type : switch defaultTransform : 'jq(del(.x))' defaultTransition : b conditions : - condition : 'jq(.y == true)' transform : 'jq(.x)' transition : c - condition : 'jq(.z == true)' transform : 'jq(.x)' transition : d SwitchStateDefinition To change the behaviour of a workflow based on the instance data, use a switch state. This state does nothing except choose between any number of different possible state transitions. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to switch . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no defaultTransform If defined, modifies the instance's data upon completing the state logic. But only if none of the conditions are met. See StateTransforms . Structured JQ no defaultTransition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. But only if none of the conditions are met. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no conditions List of conditions, which are evaluated in-order until a match is found. []SwitchConditionDefinition yes SwitchConditionDefinition Parameter Description Type Required condition Selects or generates the data used to determine if condition is met. The condition is considered met if the result is anything other than null , false , {} , [] , \"\" , or 0 . Structured JQ yes transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, matching this condition terminates the workflow. string no","title":"switch"},{"location":"spec/workflow-yaml/switch/#switch-state","text":"- id : a type : switch defaultTransform : 'jq(del(.x))' defaultTransition : b conditions : - condition : 'jq(.y == true)' transform : 'jq(.x)' transition : c - condition : 'jq(.z == true)' transform : 'jq(.x)' transition : d","title":"Switch State"},{"location":"spec/workflow-yaml/switch/#switchstatedefinition","text":"To change the behaviour of a workflow based on the instance data, use a switch state. This state does nothing except choose between any number of different possible state transitions. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to switch . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no defaultTransform If defined, modifies the instance's data upon completing the state logic. But only if none of the conditions are met. See StateTransforms . Structured JQ no defaultTransition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. But only if none of the conditions are met. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no conditions List of conditions, which are evaluated in-order until a match is found. []SwitchConditionDefinition yes","title":"SwitchStateDefinition"},{"location":"spec/workflow-yaml/switch/#switchconditiondefinition","text":"Parameter Description Type Required condition Selects or generates the data used to determine if condition is met. The condition is considered met if the result is anything other than null , false , {} , [] , \"\" , or 0 . Structured JQ yes transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, matching this condition terminates the workflow. string no","title":"SwitchConditionDefinition"},{"location":"spec/workflow-yaml/timeouts/","text":"Timeouts TimeoutsDefinition In addition to any timeouts applied on a state-by-state basis, every workflow has two global timeouts that begin ticking from the moment the workflow starts. This is where you can configure these timeouts differently to their defaults. Parameter Description Type Required interrupt An ISO8601 duration string. Sets the time to wait before throwing a catchable direktiv.cancels.timeout.soft error. Consider this a soft timeout. string no kill An ISO8601 duration string. Sets the time to wait before throwing an uncatchable direktiv.cancels.timeout.hard error. This is a hard timeout. string no Workflow Timeout direktiv_api : workflow/v1 timeouts : interrupt : PT60M kill : PT30M functions : - type : knative-workflow id : request image : direktiv/request:latest size : small states : - id : getter type : action action : function : request input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\"","title":"Timeouts"},{"location":"spec/workflow-yaml/timeouts/#timeouts","text":"","title":"Timeouts"},{"location":"spec/workflow-yaml/timeouts/#timeoutsdefinition","text":"In addition to any timeouts applied on a state-by-state basis, every workflow has two global timeouts that begin ticking from the moment the workflow starts. This is where you can configure these timeouts differently to their defaults. Parameter Description Type Required interrupt An ISO8601 duration string. Sets the time to wait before throwing a catchable direktiv.cancels.timeout.soft error. Consider this a soft timeout. string no kill An ISO8601 duration string. Sets the time to wait before throwing an uncatchable direktiv.cancels.timeout.hard error. This is a hard timeout. string no Workflow Timeout direktiv_api : workflow/v1 timeouts : interrupt : PT60M kill : PT30M functions : - type : knative-workflow id : request image : direktiv/request:latest size : small states : - id : getter type : action action : function : request input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\"","title":"TimeoutsDefinition"},{"location":"spec/workflow-yaml/validate/","text":"Validate State - id : a type : validate schema : title : Files type : object properties : firstname : type : string description : Your first name title : First Name ValidateStateDefinition Since workflows receive external input it may be necessary to check that instance data is valid. The validate state exists for this purpose. If this state is the first state in the flow the UI will generate a input form based on the specification. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to validate . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no subject Selects or generates the data that will be compared to the schema . If undefined, it will be default to 'jq(.)' . Structured JQ no schema A YAMLified representation of a JSON Schema that defines whether the subject is considered valid. object yes","title":"validate"},{"location":"spec/workflow-yaml/validate/#validate-state","text":"- id : a type : validate schema : title : Files type : object properties : firstname : type : string description : Your first name title : First Name","title":"Validate State"},{"location":"spec/workflow-yaml/validate/#validatestatedefinition","text":"Since workflows receive external input it may be necessary to check that instance data is valid. The validate state exists for this purpose. If this state is the first state in the flow the UI will generate a input form based on the specification. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to validate . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no subject Selects or generates the data that will be compared to the schema . If undefined, it will be default to 'jq(.)' . Structured JQ no schema A YAMLified representation of a JSON Schema that defines whether the subject is considered valid. object yes","title":"ValidateStateDefinition"},{"location":"spec/workflow-yaml/workflow/","text":"Workflow Definition Direktiv Workflow Definition This document describes the rules for Direktiv workflow definition files. These files are written in YAML and dictate the behaviour of a workflow running on Direktiv. Workflow direktiv_api : workflow/v1 description : | A simple \"Hello, world\" demonstration. states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' Input {} Output { \"msg\" : \"Hello, world!\" } Workflows have inputs and outputs, usually in JSON. Where examples appear in this document they will often be accompanied by inputs and outputs as seen above. WorkflowDefinition This is the top-level structure of a Direktiv workflow definition. All workflows must have one. Parameter Description Type Required direktiv_api Set it to 'workflow/v1' to reduce ambiguity, enabling tools to better identify this file as a workflow. string no url Link to further information. string no description Short description of the workflow. string no functions List of function definitions for use by function-based states . []FunctionDefinition no start Configuration for how the workflow should start. StartDefinition no states List of all possible workflow states. []StateDefinition yes timeouts Configuration of workflow-level timeouts. TimeoutsDefinition no","title":"Workflow"},{"location":"spec/workflow-yaml/workflow/#workflow-definition","text":"","title":"Workflow Definition"},{"location":"spec/workflow-yaml/workflow/#direktiv-workflow-definition","text":"This document describes the rules for Direktiv workflow definition files. These files are written in YAML and dictate the behaviour of a workflow running on Direktiv. Workflow direktiv_api : workflow/v1 description : | A simple \"Hello, world\" demonstration. states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' Input {} Output { \"msg\" : \"Hello, world!\" } Workflows have inputs and outputs, usually in JSON. Where examples appear in this document they will often be accompanied by inputs and outputs as seen above.","title":"Direktiv Workflow Definition"}]}