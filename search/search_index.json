{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Quickstart Starting the Server Getting a local playground environment can be easily done with Docker. The following command starts a docker container with kubernetes. On startup it can take a few minutes to download all images. When the installation is done all pods should show \"Running\" or \"Completed\". docker run --privileged -p 8080:80 -ti direktiv/direktiv-kube For proxy usage: docker run --privileged -p 8080 :80 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \".default,10.0.0.0/8,172.0.0.0/8,localhost\" direktiv/direktiv-kube Testing Direktiv : To test the installation create a namespace in Direktiv with the following command: $curl -X PUT \"http://localhost:8080/api/namespaces/demo\" { \"namespace\": { \"createdAt\": \"2021-10-06T00:03:22.444884147Z\", \"updatedAt\": \"2021-10-06T00:03:22.444884447Z\", \"name\": \"demo\", \"oid\": \"\" } } The command curl \"http://localhost:8080/api/namespaces\" will return the just created namespace. Kubernetes installation For instructions on how to install in a pre-existing Kubernetes environment, following the installation instructions . Workflow specification The below example is the minimal configuration needed for a workflow, following the workflow language specification : id : helloworld states : - id : hello type : noop transform : msg : \"Hello jq(.name)!\" Creating and Running a Workflow The following script does everything required to run the first workflow. This includes creating a namespace & workflow and running the workflow the first time. $ curl -X PUT \"http://localhost:8080/api/namespaces/demo\" { \"namespace\" : { \"createdAt\" : \"2021-10-06T00:03:22.444884147Z\" , \"updatedAt\" : \"2021-10-06T00:03:22.444884447Z\" , \"name\" : \"demo\" , \"oid\" : \"\" } } $ cat > helloworld.yml <<- EOF states: - id: hello type: noop transform: msg: \"Hello, jq(.name)!\" EOF $ curl -vv -X PUT --data-binary \"@helloworld.yml\" \"http://localhost:8080/api/namespaces/demo/tree/helloworld?op=create-workflow\" { \"namespace\" : \"demo\" , \"node\" : { ... } , \"revision\" : { ... } } $ cat > input.json <<- EOF { \"name\": \"Alan\" } EOF $ curl -vv -X POST --data-binary \"@input.json\" \"http://localhost:8080/api/namespaces/demo/tree/helloworld?op=wait\" { \"msg\" : \"Hello, Alan!\" } Next steps For more complex examples review the Getting Started section of the documentation. See Also The direktiv.io website. The direktiv.io repository. The Godoc library documentation.","title":"Quickstart"},{"location":"#quickstart","text":"","title":"Quickstart"},{"location":"#starting-the-server","text":"Getting a local playground environment can be easily done with Docker. The following command starts a docker container with kubernetes. On startup it can take a few minutes to download all images. When the installation is done all pods should show \"Running\" or \"Completed\". docker run --privileged -p 8080:80 -ti direktiv/direktiv-kube For proxy usage: docker run --privileged -p 8080 :80 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \".default,10.0.0.0/8,172.0.0.0/8,localhost\" direktiv/direktiv-kube Testing Direktiv : To test the installation create a namespace in Direktiv with the following command: $curl -X PUT \"http://localhost:8080/api/namespaces/demo\" { \"namespace\": { \"createdAt\": \"2021-10-06T00:03:22.444884147Z\", \"updatedAt\": \"2021-10-06T00:03:22.444884447Z\", \"name\": \"demo\", \"oid\": \"\" } } The command curl \"http://localhost:8080/api/namespaces\" will return the just created namespace.","title":"Starting the Server"},{"location":"#kubernetes-installation","text":"For instructions on how to install in a pre-existing Kubernetes environment, following the installation instructions .","title":"Kubernetes installation"},{"location":"#workflow-specification","text":"The below example is the minimal configuration needed for a workflow, following the workflow language specification : id : helloworld states : - id : hello type : noop transform : msg : \"Hello jq(.name)!\"","title":"Workflow specification"},{"location":"#creating-and-running-a-workflow","text":"The following script does everything required to run the first workflow. This includes creating a namespace & workflow and running the workflow the first time. $ curl -X PUT \"http://localhost:8080/api/namespaces/demo\" { \"namespace\" : { \"createdAt\" : \"2021-10-06T00:03:22.444884147Z\" , \"updatedAt\" : \"2021-10-06T00:03:22.444884447Z\" , \"name\" : \"demo\" , \"oid\" : \"\" } } $ cat > helloworld.yml <<- EOF states: - id: hello type: noop transform: msg: \"Hello, jq(.name)!\" EOF $ curl -vv -X PUT --data-binary \"@helloworld.yml\" \"http://localhost:8080/api/namespaces/demo/tree/helloworld?op=create-workflow\" { \"namespace\" : \"demo\" , \"node\" : { ... } , \"revision\" : { ... } } $ cat > input.json <<- EOF { \"name\": \"Alan\" } EOF $ curl -vv -X POST --data-binary \"@input.json\" \"http://localhost:8080/api/namespaces/demo/tree/helloworld?op=wait\" { \"msg\" : \"Hello, Alan!\" }","title":"Creating and Running a Workflow"},{"location":"#next-steps","text":"For more complex examples review the Getting Started section of the documentation.","title":"Next steps"},{"location":"#see-also","text":"The direktiv.io website. The direktiv.io repository. The Godoc library documentation.","title":"See Also"},{"location":"api/","text":"Direktiv API Direktiv Open API Specification Direktiv Documentation can be found at https://docs.direktiv.io/ Informations Version 1.0.0 Contact info@direktiv.io Content negotiation URI Schemes http https Consumes application/json text/plain Produces application/json text/event-stream Access control Security Schemes api_key (header: KEY) Type : apikey Security Requirements api_key All endpoints directory Method URI Name Summary PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory create directory Create a Directory global_services Method URI Name Summary POST /api/functions create global service Create Global Service DELETE /api/functions/{serviceName}/revisions/{revisionGeneration} delete global revision Delete Global Service Revision DELETE /api/functions/{serviceName} delete global service Delete Global Service GET /api/functions/{serviceName} get global service Get Global Service Details GET /api/functions get global service list Get Global Services List GET /api/functions/{serviceName}/revisions/{revisionGeneration}/pods list global service revision pods Get Global Service Revision Pods List POST /api/functions/{serviceName} update global service Create Global Service Revision PATCH /api/functions/{serviceName} update global service traffic Update Global Service Traffic GET /api/functions/{serviceName}/revisions/{revisionGeneration} watch global service revision Watch Global Service Revision GET /api/functions/{serviceName}/revisions watch global service revision list Watch Global Service Revision List instances Method URI Name Summary POST /api/namespaces/{namespace}/instances/{instance}/cancel cancel instance Cancel a Pending Instance GET /api/namespaces/{namespace}/instances/{instance} get instance Get a Instance GET /api/namespaces/{namespace}/instances/{instance}/input get instance input Get a Instance Input GET /api/namespaces/{namespace}/instances get instance list Get List Instances GET /api/namespaces/{namespace}/instances/{instance}/output get instance output Get a Instance Output logs Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=logs get workflow logs Get Workflow Level Logs GET /api/namespaces/{namespace}/instances/{instance}/logs instance logs Gets Instance Logs GET /api/namespaces/{namespace}/logs namespace logs Gets Namespace Level Logs GET /api/logs server logs Get Direktiv Server Logs metrics Method URI Name Summary GET /api/namespaces/{namespace}/metrics/failed namespace metrics failed Gets Namespace Failed Workflow Instances Metrics GET /api/namespaces/{namespace}/metrics/invoked namespace metrics invoked Gets Namespace Invoked Workflow Metrics GET /api/namespaces/{namespace}/metrics/milliseconds namespace metrics milliseconds Gets Namespace Workflow Timing Metrics GET /api/namespaces/{namespace}/metrics/successful namespace metrics successful Gets Namespace Successful Workflow Instances Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked workflow metrics invoked Gets Invoked Workflow Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed workflow metrics milliseconds Gets Workflow Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds workflow metrics state milliseconds Gets a Workflow State Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful workflow metrics successful Gets Successful Workflow Metrics namespace_services Method URI Name Summary POST /api/functions/namespaces/{namespace} create namespace service Create Namespace Service DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} delete namespace revision Delete Namespace Service Revision DELETE /api/functions/namespaces/{namespace}/function/{serviceName} delete namespace service Delete Namespace Service GET /api/functions/namespaces/{namespace}/function/{serviceName} get namespace service Get Namespace Service Details GET /api/functions/namespaces/{namespace} get namespace service list Get Namespace Services List GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods list namespace service revision pods Get Namespace Service Revision Pods List POST /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service Create Namespace Service Revision PATCH /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service traffic Update Namespace Service Traffic GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} watch namespace service revision Watch Namespace Service Revision GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions watch namespace service revision list Watch Namespace Service Revision List namespaces Method URI Name Summary PUT /api/namespaces/{namespace} create namespace Creates a namespace DELETE /api/namespaces/{namespace} delete namespace Delete a namespace GET /api/namespaces/{namespace}/config get namespace config Gets a namespace config GET /api/namespaces get namespaces Gets the list of namespaces PATCH /api/namespaces/{namespace}/config set namespace config Sets a namespace config node Method URI Name Summary DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node delete node Delete a node GET /api/namespaces/{namespace}/tree/{nodePath} get nodes Get List of Namespace Nodes other Method URI Name Summary POST /api/namespaces/{namespace}/broadcast broadcast cloudevent Broadcast Cloud Event POST /api/jq jq playground JQ Playground api to test jq queries GET /api/version version Returns version information for servers in the cluster. registries Method URI Name Summary POST /api/functions/registries/private create global private registry Create a Global Container Registry POST /api/functions/registries/global create global registry Create a Global Container Registry POST /api/functions/registries/namespaces/{namespace} create registry Create a Namespace Container Registry DELETE /api/functions/registries/private delete global private registry Delete a Global Container Registry DELETE /api/functions/registries/global delete global registry Delete a global Container Registry DELETE /api/functions/registries/namespaces/{namespace} delete registry Delete a Namespace Container Registry GET /api/functions/registries/private get global private registries Get List of Global Private Registries GET /api/functions/registries/global get global registries Get List of Global Registries GET /api/functions/registries/namespaces/{namespace} get registries Get List of Namespace Registries secrets Method URI Name Summary PUT /api/namespaces/{namespace}/secrets/{secret} create secret Create a Namespace Secret DELETE /api/namespaces/{namespace}/secrets/{secret} delete secret Delete a Namespace Secret GET /api/namespaces/{namespace}/secrets get secrets Get List of Namespace Secrets variables Method URI Name Summary DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} delete instance variable Delete a Instance Variable DELETE /api/namespaces/{namespace}/vars/{variable} delete namespace variable Delete a Namespace Variable DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var delete workflow variable Delete a Workflow Variable GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} get instance variable Get a Instance Variable GET /api/namespaces/{namespace}/instances/{instance}/vars get instance variables Get List of Instance Variable GET /api/namespaces/{namespace}/vars/{variable} get namespace variable Get a Namespace Variable GET /api/namespaces/{namespace}/vars get namespace variables Get Namespace Variable List GET /api/namespaces/{namespace}/tree/{workflow}?op=var get workflow variable Get a Workflow Variable GET /api/namespaces/{namespace}/tree/{workflow}?op=vars get workflow variables Get List of Workflow Variables PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} set instance variable Set a Instance Variable PUT /api/namespaces/{namespace}/vars/{variable} set namespace variable Set a Namespace Variable PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var set workflow variable Set a Workflow Variable workflow_services Method URI Name Summary GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function get workflow service Get Workflow Service Details GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision get workflow service revision Get Workflow Service Revision GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions get workflow service revision list Get Workflow Service Revision List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods list workflow service revision pods Get Workflow Service Revision Pods List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services list workflow services Get Workflow Services List workflows Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow Await Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow body Await Execute a Workflow With Body PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow create workflow Create a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=execute execute workflow Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging set workflow cloud event logs Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle toggle workflow Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow update workflow Update a Workflow Paths Await Execute a Workflow ( awaitExecuteWorkflow ) GET /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string. All responses Code Status Description Has headers Schema 200 OK successfully executed workflow schema Responses 200 - successfully executed workflow Status: OK Schema Await Execute a Workflow With Body ( awaitExecuteWorkflowBody ) POST /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow with optionally some input provided in the request body as json. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters. Input query parameters are only read if the request has no body. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string. Workflow Input body interface{} interface{} \u2713 The input of this workflow instance All responses Code Status Description Has headers Schema 200 OK successfully executed workflow schema Responses 200 - successfully executed workflow Status: OK Schema Broadcast Cloud Event ( broadcastCloudevent ) POST /api/namespaces/{namespace}/broadcast Broadcast a cloud event to a namespace. Cloud events posted to this api will be picked up by any workflows listening to the same event type on the namescape. The body of this request should follow the cloud event core specification defined at https://github.com/cloudevents/spec . Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace cloudevent body interface{} interface{} \u2713 Cloud Event request to be sent. All responses Code Status Description Has headers Schema 200 OK successfully sent cloud event schema Responses 200 - successfully sent cloud event Status: OK Schema Cancel a Pending Instance ( cancelInstance ) POST /api/namespaces/{namespace}/instances/{instance}/cancel Cancel a currently pending instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully cancelled instance schema Responses 200 - successfully cancelled instance Status: OK Schema Create a Directory ( createDirectory ) PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory Creates a directory at the target path. Parameters Name Source Type Go type Separator Required Default Description directory path string string \u2713 path to target directory namespace path string string \u2713 target namespace op query string string \u2713 \"create-directory\" the operation for the api All responses Code Status Description Has headers Schema 200 OK directory has been created schema default an error has occurred schema Responses 200 - directory has been created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create a Global Container Registry ( createGlobalPrivateRegistry ) POST /api/functions/registries/private Create a global container registry. Global Private registries are only available to global services. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN Parameters Name Source Type Go type Separator Required Default Description Registry Payload body CreateGlobalPrivateRegistryBody CreateGlobalPrivateRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully created global private registry schema Responses 200 - successfully created global private registry Status: OK Schema Inlined models CreateGlobalPrivateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL Create a Global Container Registry ( createGlobalRegistry ) POST /api/functions/registries/global Create a global container registry. Global registries are available to all services. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN Parameters Name Source Type Go type Separator Required Default Description Registry Payload body CreateGlobalRegistryBody CreateGlobalRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully created global registry schema Responses 200 - successfully created global registry Status: OK Schema Inlined models CreateGlobalRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL Create Global Service ( createGlobalService ) POST /api/functions Creates global scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/docs/walkthrough/using-functions.html Parameters Name Source Type Go type Separator Required Default Description Service body CreateGlobalServiceBody CreateGlobalServiceBody \u2713 Payload that contains information on new service All responses Code Status Description Has headers Schema 200 OK successfully created service schema Responses 200 - successfully created service Status: OK Schema Inlined models CreateGlobalServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size string string \u2713 Size of created service pods Creates a namespace ( createNamespace ) PUT /api/namespaces/{namespace} Creates a new namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to create All responses Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema Responses 200 - namespace has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create Namespace Service ( createNamespaceService ) POST /api/functions/namespaces/{namespace} Creates namespace scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/docs/walkthrough/using-functions.html Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Service body CreateNamespaceServiceBody CreateNamespaceServiceBody \u2713 Payload that contains information on new service All responses Code Status Description Has headers Schema 200 OK successfully created service schema Responses 200 - successfully created service Status: OK Schema Inlined models CreateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size string string \u2713 Size of created service pods Create a Namespace Container Registry ( createRegistry ) POST /api/functions/registries/namespaces/{namespace} Create a namespace container registry. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body CreateRegistryBody CreateRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully created namespace registry schema Responses 200 - successfully created namespace registry Status: OK Schema Inlined models CreateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL Create a Namespace Secret ( createSecret ) PUT /api/namespaces/{namespace}/secrets/{secret} Create a namespace secret. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret Secret Payload body string string \u2713 Payload that contains secret data. All responses Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema Responses 200 - namespace has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create a Workflow ( createWorkflow ) PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow Creates a workflow at the target path. The body of this request should contain the workflow yaml. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"create-workflow\" the operation for the api workflow data body string string Payload that contains the direktiv workflow yaml to create. All responses Code Status Description Has headers Schema 200 OK successfully created workflow schema default an error has occurred schema Responses 200 - successfully created workflow Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete a Global Container Registry ( deleteGlobalPrivateRegistry ) DELETE /api/functions/registries/private Delete a global container registry. Global Private registries are only available to global services. Parameters Name Source Type Go type Separator Required Default Description Registry Payload body DeleteGlobalPrivateRegistryBody DeleteGlobalPrivateRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully delete global private registry schema Responses 200 - successfully delete global private registry Status: OK Schema Inlined models DeleteGlobalPrivateRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL Delete a global Container Registry ( deleteGlobalRegistry ) DELETE /api/functions/registries/global Delete a Global container registry Global registries are available to all services. Parameters Name Source Type Go type Separator Required Default Description Registry Payload body DeleteGlobalRegistryBody DeleteGlobalRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully delete global registry schema Responses 200 - successfully delete global registry Status: OK Schema Inlined models DeleteGlobalRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL Delete Global Service Revision ( deleteGlobalRevision ) DELETE /api/functions/{serviceName}/revisions/{revisionGeneration} Delete a global scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003'. Note: Revisions with traffic cannot be deleted. Parameters Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service revision schema Responses 200 - successfully deleted service revision Status: OK Schema Delete Global Service ( deleteGlobalService ) DELETE /api/functions/{serviceName} Deletes global scoped knative service and all its revisions. Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service schema Responses 200 - successfully deleted service Status: OK Schema Delete a Instance Variable ( deleteInstanceVariable ) DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Delete a instance variable. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted instance variable schema Responses 200 - successfully deleted instance variable Status: OK Schema Delete a namespace ( deleteNamespace ) DELETE /api/namespaces/{namespace} Delete a namespace. A namespace will not delete by default if it has any child resources (workflows, etc...). Deleting the namespace with all its children can be done using the recursive query parameter. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to delete recursive query boolean bool recursively deletes all child resources All responses Code Status Description Has headers Schema 200 OK namespace has been successfully deleted schema default an error has occurred schema Responses 200 - namespace has been successfully deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete Namespace Service Revision ( deleteNamespaceRevision ) DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Delete a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: Revisions with traffic cannot be deleted. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service revision schema Responses 200 - successfully deleted service revision Status: OK Schema Delete Namespace Service ( deleteNamespaceService ) DELETE /api/functions/namespaces/{namespace}/function/{serviceName} Deletes namespace scoped knative service and all its revisions. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service schema Responses 200 - successfully deleted service Status: OK Schema Delete a Namespace Variable ( deleteNamespaceVariable ) DELETE /api/namespaces/{namespace}/vars/{variable} Delete a namespace variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted namespace variable schema Responses 200 - successfully deleted namespace variable Status: OK Schema Delete a node ( deleteNode ) DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node Creates a directory at the target path. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace node path string string \u2713 path to target node op query string string \u2713 \"delete-node\" the operation for the api All responses Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema Responses 200 - node has been deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete a Namespace Container Registry ( deleteRegistry ) DELETE /api/functions/registries/namespaces/{namespace} Delete a namespace container registry Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body DeleteRegistryBody DeleteRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully delete namespace registry schema Responses 200 - successfully delete namespace registry Status: OK Schema Inlined models DeleteRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL Delete a Namespace Secret ( deleteSecret ) DELETE /api/namespaces/{namespace}/secrets/{secret} Delete a namespace secret. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret All responses Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema Responses 200 - namespace has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete a Workflow Variable ( deleteWorkflowVariable ) DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var Delete a workflow variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted workflow variable schema Responses 200 - successfully deleted workflow variable Status: OK Schema Execute a Workflow ( executeWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=execute Executes a workflow with optionally some input provided in the request body as json. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"execute\" the operation for the api Workflow Input body interface{} interface{} \u2713 The input of this workflow instance All responses Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema Responses 200 - node has been deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get List of Global Private Registries ( getGlobalPrivateRegistries ) GET /api/functions/registries/private Gets the list of global private registries. Global Private registries are only available to global services. All responses Code Status Description Has headers Schema 200 OK successfully got global private registries schema Responses 200 - successfully got global private registries Status: OK Schema Get List of Global Registries ( getGlobalRegistries ) GET /api/functions/registries/global Gets the list of global registries. Global registries are available to all services. All responses Code Status Description Has headers Schema 200 OK successfully got global registries schema Responses 200 - successfully got global registries Status: OK Schema Get Global Service Details ( getGlobalService ) GET /api/functions/{serviceName} Get details of a global scoped knative service. Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got service details schema Responses 200 - successfully got service details Status: OK Schema Get Global Services List ( getGlobalServiceList ) GET /api/functions Gets a list of global knative services. All responses Code Status Description Has headers Schema 200 OK successfully got services list schema Responses 200 - successfully got services list Status: OK Schema Get a Instance ( getInstance ) GET /api/namespaces/{namespace}/instances/{instance} Gets the details of a executed workflow instance in this namespace. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance schema default an error has occurred schema Responses 200 - successfully got instance Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get a Instance Input ( getInstanceInput ) GET /api/namespaces/{namespace}/instances/{instance}/input Gets the input an instance was provided when executed. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance input schema Responses 200 - successfully got instance input Status: OK Schema Get List Instances ( getInstanceList ) GET /api/namespaces/{namespace}/instances Gets a list of instances in a namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace instances schema Responses 200 - successfully got namespace instances Status: OK Schema Get a Instance Output ( getInstanceOutput ) GET /api/namespaces/{namespace}/instances/{instance}/output Gets the output an instance was provided when executed. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance output schema Responses 200 - successfully got instance output Status: OK Schema Get a Instance Variable ( getInstanceVariable ) GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Get the value sorted in a instance variable. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got instance variable schema Responses 200 - successfully got instance variable Status: OK Schema Get List of Instance Variable ( getInstanceVariables ) GET /api/namespaces/{namespace}/instances/{instance}/vars Gets a list of variables in a instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance variables schema Responses 200 - successfully got instance variables Status: OK Schema Gets a namespace config ( getNamespaceConfig ) GET /api/namespaces/{namespace}/config Gets a namespace config. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update All responses Code Status Description Has headers Schema 200 OK successfully got namespace config schema Responses 200 - successfully got namespace config Status: OK Schema Get Namespace Service Details ( getNamespaceService ) GET /api/functions/namespaces/{namespace}/function/{serviceName} Get details of a namespace scoped knative service. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got service details schema Responses 200 - successfully got service details Status: OK Schema Get Namespace Services List ( getNamespaceServiceList ) GET /api/functions/namespaces/{namespace} Gets a list of namespace knative services. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got services list schema Responses 200 - successfully got services list Status: OK Schema Get a Namespace Variable ( getNamespaceVariable ) GET /api/namespaces/{namespace}/vars/{variable} Get the value sorted in a namespace variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got namespace variable schema Responses 200 - successfully got namespace variable Status: OK Schema Get Namespace Variable List ( getNamespaceVariables ) GET /api/namespaces/{namespace}/vars Gets a list of variables in a namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace variables schema Responses 200 - successfully got namespace variables Status: OK Schema Gets the list of namespaces ( getNamespaces ) GET /api/namespaces Gets the list of namespaces. All responses Code Status Description Has headers Schema 200 OK successfully got list of namespaces schema Responses 200 - successfully got list of namespaces Status: OK Schema Get List of Namespace Nodes ( getNodes ) GET /api/namespaces/{namespace}/tree/{nodePath} Gets Workflow and Directory Nodes at nodePath. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace nodePath path string string \u2713 target path in tree All responses Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema Responses 200 - successfully got namespace nodes Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get List of Namespace Registries ( getRegistries ) GET /api/functions/registries/namespaces/{namespace} Gets the list of namespace registries. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace registries schema Responses 200 - successfully got namespace registries Status: OK Schema Get List of Namespace Secrets ( getSecrets ) GET /api/namespaces/{namespace}/secrets Gets the list of namespace secrets. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema Responses 200 - successfully got namespace nodes Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get Workflow Level Logs ( getWorkflowLogs ) GET /api/namespaces/{namespace}/tree/{workflow}?op=logs Get workflow level logs. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow logs schema Responses 200 - successfully got workflow logs Status: OK Schema Get Workflow Service Details ( getWorkflowService ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function Get a workflow scoped knative service details. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version All responses Code Status Description Has headers Schema 200 OK successfully got service details schema Responses 200 - successfully got service details Status: OK Schema Get Workflow Service Revision ( getWorkflowServiceRevision ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision Get a workflow scoped knative service revision. This will return details on a single revision. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got service revision details schema Responses 200 - successfully got service revision details Status: OK Schema Get Workflow Service Revision List ( getWorkflowServiceRevisionList ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions Get the revision list of a workflow scoped knative service. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version All responses Code Status Description Has headers Schema 200 OK successfully got service revisions schema Responses 200 - successfully got service revisions Status: OK Schema Get a Workflow Variable ( getWorkflowVariable ) GET /api/namespaces/{namespace}/tree/{workflow}?op=var Get the value sorted in a workflow variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got workflow variable schema Responses 200 - successfully got workflow variable Status: OK Schema Get List of Workflow Variables ( getWorkflowVariables ) GET /api/namespaces/{namespace}/tree/{workflow}?op=vars Gets a list of variables in a workflow. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow variables schema Responses 200 - successfully got workflow variables Status: OK Schema Gets Instance Logs ( instanceLogs ) GET /api/namespaces/{namespace}/instances/{instance}/logs Gets the logs of an executed instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance id namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance logs schema default an error has occurred schema Responses 200 - successfully got instance logs Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse JQ Playground api to test jq queries ( jqPlayground ) POST /api/jq JQ Playground is a sandbox where you can test jq queries with custom data. Parameters Name Source Type Go type Separator Required Default Description JQ payload body JqPlaygroundBody JqPlaygroundBody \u2713 Payload that contains both the JSON data to manipulate and jq query. All responses Code Status Description Has headers Schema 200 OK jq query was successful schema 400 Bad Request the request was invalid schema 500 Internal Server Error an unexpected internal error occurred schema Responses 200 - jq query was successful Status: OK Schema 400 - the request was invalid Status: Bad Request Schema 500 - an unexpected internal error occurred Status: Internal Server Error Schema Inlined models JqPlaygroundBody Properties Name Type Go type Required Default Description Example data string string \u2713 JSON data encoded in base64 query string string \u2713 jq query to manipulate JSON data Get Global Service Revision Pods List ( listGlobalServiceRevisionPods ) GET /api/functions/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a global scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003' . Parameters Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema Responses 200 - successfully got list of a service revision pods Status: OK Schema Get Namespace Service Revision Pods List ( listNamespaceServiceRevisionPods ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a namespace scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema Responses 200 - successfully got list of a service revision pods Status: OK Schema Get Workflow Service Revision Pods List ( listWorkflowServiceRevisionPods ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods List a revisions pods of a workflow scoped knative service. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema Responses 200 - successfully got list of a service revision pods Status: OK Schema Get Workflow Services List ( listWorkflowServices ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services Gets a list of workflow knative services. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got services list schema Responses 200 - successfully got services list Status: OK Schema Gets Namespace Level Logs ( namespaceLogs ) GET /api/namespaces/{namespace}/logs Gets Namespace Level Logs. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace logs schema Responses 200 - successfully got namespace logs Status: OK Schema Gets Namespace Failed Workflow Instances Metrics ( namespaceMetricsFailed ) GET /api/namespaces/{namespace}/metrics/failed Get metrics of failed workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Invoked Workflow Metrics ( namespaceMetricsInvoked ) GET /api/namespaces/{namespace}/metrics/invoked Get metrics of invoked workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Workflow Timing Metrics ( namespaceMetricsMilliseconds ) GET /api/namespaces/{namespace}/metrics/milliseconds Get timing metrics of workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Successful Workflow Instances Metrics ( namespaceMetricsSuccessful ) GET /api/namespaces/{namespace}/metrics/successful Get metrics of successful workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Get Direktiv Server Logs ( serverLogs ) GET /api/logs Gets Direktiv Server Logs. All responses Code Status Description Has headers Schema 200 OK successfully got server logs schema default an error has occurred schema Responses 200 - successfully got server logs Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Set a Instance Variable ( setInstanceVariable ) PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Set the value sorted in a instance variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set instance variable schema Responses 200 - successfully set instance variable Status: OK Schema Sets a namespace config ( setNamespaceConfig ) PATCH /api/namespaces/{namespace}/config Sets a namespace config. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update Config Payload body SetNamespaceConfigBody SetNamespaceConfigBody Payload that contains the config information to set. Note: This payload only need to contain the properities you wish to set. All responses Code Status Description Has headers Schema 200 OK namespace config has been successfully been updated schema Responses 200 - namespace config has been successfully been updated Status: OK Schema Inlined models SetNamespaceConfigBody Properties Name Type Go type Required Default Description Example broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace Set a Namespace Variable ( setNamespaceVariable ) PUT /api/namespaces/{namespace}/vars/{variable} Set the value sorted in a namespace variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set namespace variable schema Responses 200 - successfully set namespace variable Status: OK Schema Set Cloud Event for Workflow to Log to ( setWorkflowCloudEventLogs ) POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging Set Cloud Event for Workflow to Log to. When configured type direktiv.instanceLog cloud events will be generated with the logger parameter set to the configured value. Workflows can be configured to generate cloud events on their namespace anything the log parameter produces data. Please find more information on this topic here: https://docs.direktiv.io/docs/examples/logging.html Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Cloud Event Logger body SetWorkflowCloudEventLogsBody SetWorkflowCloudEventLogsBody \u2713 Cloud event logger to target All responses Code Status Description Has headers Schema 200 OK successfully update workflow schema Responses 200 - successfully update workflow Status: OK Schema Inlined models SetWorkflowCloudEventLogsBody Properties Name Type Go type Required Default Description Example logger string string \u2713 Target Cloud Event Set a Workflow Variable ( setWorkflowVariable ) PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var Set the value sorted in a workflow variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set workflow variable schema Responses 200 - successfully set workflow variable Status: OK Schema Set Cloud Event for Workflow to Log to ( toggleWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle Toggle's whether or not a workflow is active. Disabled workflows cannot be invoked. This includes start event and scheduled workflows. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Workflow Live Status body ToggleWorkflowBody ToggleWorkflowBody \u2713 Whether or not the workflow is alive or disabled All responses Code Status Description Has headers Schema 200 OK successfully updated workflow live status schema Responses 200 - successfully updated workflow live status Status: OK Schema Inlined models ToggleWorkflowBody Properties Name Type Go type Required Default Description Example live boolean bool \u2713 Workflow live status Create Global Service Revision ( updateGlobalService ) POST /api/functions/{serviceName} Creates a new global scoped knative service revision Revisions are created with a traffic percentage. This percentage controls how much traffic will be directed to this revision. Traffic can be set to 100 to direct all traffic. Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name Service body UpdateGlobalServiceBody UpdateGlobalServiceBody \u2713 Payload that contains information on service revision All responses Code Status Description Has headers Schema 200 OK successfully created service revision schema Responses 200 - successfully created service revision Status: OK Schema Inlined models UpdateGlobalServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods trafficPercent integer int64 \u2713 Traffic percentage new revision will use Update Global Service Traffic ( updateGlobalServiceTraffic ) PATCH /api/functions/{serviceName} Update Global Service traffic directed to each revision, traffic can only be configured between two revisions. All other revisions will bet set to 0 traffic. Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name Service Traffic body UpdateGlobalServiceTrafficBody UpdateGlobalServiceTrafficBody \u2713 Payload that contains information on service traffic All responses Code Status Description Has headers Schema 200 OK successfully updated service traffic schema Responses 200 - successfully updated service traffic Status: OK Schema Inlined models UpdateGlobalServiceTrafficBody Properties Name Type Go type Required Default Description Example values [] UpdateGlobalServiceTrafficParamsBodyValuesItems0 []*models.UpdateGlobalServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateGlobalServiceTrafficParamsBodyValuesItems0 Properties Name Type Go type Required Default Description Example percent integer int64 Target traffice percentage revision string string Target service revision Create Namespace Service Revision ( updateNamespaceService ) POST /api/functions/namespaces/{namespace}/function/{serviceName} Creates a new namespace scoped knative service revision. Revisions are created with a traffic percentage. This percentage controls how much traffic will be directed to this revision. Traffic can be set to 100 to direct all traffic. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service body UpdateNamespaceServiceBody UpdateNamespaceServiceBody \u2713 Payload that contains information on service revision All responses Code Status Description Has headers Schema 200 OK successfully created service revision schema Responses 200 - successfully created service revision Status: OK Schema Inlined models UpdateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods trafficPercent integer int64 \u2713 Traffic percentage new revision will use Update Namespace Service Traffic ( updateNamespaceServiceTraffic ) PATCH /api/functions/namespaces/{namespace}/function/{serviceName} Update Namespace Service traffic directed to each revision, traffic can only be configured between two revisions. All other revisions will bet set to 0 traffic. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service Traffic body UpdateNamespaceServiceTrafficBody UpdateNamespaceServiceTrafficBody \u2713 Payload that contains information on service traffic All responses Code Status Description Has headers Schema 200 OK successfully updated service traffic schema Responses 200 - successfully updated service traffic Status: OK Schema Inlined models UpdateNamespaceServiceTrafficBody Properties Name Type Go type Required Default Description Example values [] UpdateNamespaceServiceTrafficParamsBodyValuesItems0 []*models.UpdateNamespaceServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateNamespaceServiceTrafficParamsBodyValuesItems0 Properties Name Type Go type Required Default Description Example percent integer int64 Target traffice percentage revision string string Target service revision Update a Workflow ( updateWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow Updates a workflow at the target path. The body of this request should contain the workflow yaml you want to update to. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow workflow data body string string Payload that contains the updated direktiv workflow yaml. All responses Code Status Description Has headers Schema 200 OK successfully updated workflow schema Responses 200 - successfully updated workflow Status: OK Schema Returns version information for servers in the cluster. ( version ) GET /api/version Returns version information for servers in the cluster. All responses Code Status Description Has headers Schema 200 OK version query was successful schema Responses 200 - version query was successful Status: OK Schema Watch Global Service Revision ( watchGlobalServiceRevision ) GET /api/functions/{serviceName}/revisions/{revisionGeneration} Watch a global scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revision schema Responses 200 - successfully watching service revision Status: OK Schema Watch Global Service Revision List ( watchGlobalServiceRevisionList ) GET /api/functions/{serviceName}/revisions Watch the revision list of a global scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revisions schema Responses 200 - successfully watching service revisions Status: OK Schema Watch Namespace Service Revision ( watchNamespaceServiceRevision ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Watch a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revision schema Responses 200 - successfully watching service revision Status: OK Schema Watch Namespace Service Revision List ( watchNamespaceServiceRevisionList ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions Watch the revision list of a namespace scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revisions schema Responses 200 - successfully watching service revisions Status: OK Schema Gets Invoked Workflow Metrics ( workflowMetricsInvoked ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked Get metrics of invoked workflow instances. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets Workflow Time Metrics ( workflowMetricsMilliseconds ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed Get the timing metrics of a workflow's instance. This returns a total sum of the milliseconds a workflow has been executed for. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets a Workflow State Time Metrics ( workflowMetricsStateMilliseconds ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds Get the state timing metrics of a workflow's instance. This returns the timing of individual states in a workflow. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets Successful Workflow Metrics ( workflowMetricsSuccessful ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful Get metrics of a workflow, where the instance was successful. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Models CreateGlobalPrivateRegistryBody CreateGlobalPrivateRegistryBody create global private registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL CreateGlobalRegistryBody CreateGlobalRegistryBody create global registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL CreateGlobalServiceBody CreateGlobalServiceBody create global service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v12\",\"minScale\":\"1\",\"name\":\"fast-request\",\"size\":\"small\"} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Name string string \u2713 Name of new service Size string string \u2713 Size of created service pods CreateNamespaceServiceBody CreateNamespaceServiceBody create namespace service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v12\",\"minScale\":\"1\",\"name\":\"fast-request\",\"size\":\"small\"} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Name string string \u2713 Name of new service Size string string \u2713 Size of created service pods CreateRegistryBody CreateRegistryBody create registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL DeleteGlobalPrivateRegistryBody DeleteGlobalPrivateRegistryBody delete global private registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL DeleteGlobalRegistryBody DeleteGlobalRegistryBody delete global registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL DeleteRegistryBody DeleteRegistryBody delete registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL ErrorResponse Properties Name Type Go type Required Default Description Example Error string string StatusCode int64 (formatted integer) int64 JqPlaygroundBody JqPlaygroundBody jq playground body Example {\"data\":\"eyJhIjogMSwgImIiOiAyLCAiYyI6IDQsICJkIjogN30=\",\"query\":\"map(select(. \\u003e= 2))\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 JSON data encoded in base64 Query string string \u2713 jq query to manipulate JSON data OkBody OkBody OkBody is an arbitrary placeholder response that represents an ok response body OkBody SetNamespaceConfigBody SetNamespaceConfigBody set namespace config body Example {\"broadcast\":{\"directory.create\":false,\"directory.delete\":false,\"instance.failed\":false,\"instance.started\":false,\"instance.success\":false,\"instance.variable.create\":false,\"instance.variable.delete\":false,\"instance.variable.update\":false,\"namespace.variable.create\":false,\"namespace.variable.delete\":false,\"namespace.variable.update\":false,\"workflow.create\":false,\"workflow.delete\":false,\"workflow.update\":false,\"workflow.variable.create\":false,\"workflow.variable.delete\":false,\"workflow.variable.update\":false}} Properties Name Type Go type Required Default Description Example Broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace SetWorkflowCloudEventLogsBody SetWorkflowCloudEventLogsBody set workflow cloud event logs body Example {\"logger\":\"mylog\"} Properties Name Type Go type Required Default Description Example Logger string string \u2713 Target Cloud Event ToggleWorkflowBody ToggleWorkflowBody toggle workflow body Example {\"live\":false} Properties Name Type Go type Required Default Description Example Live boolean bool \u2713 Workflow live status UpdateGlobalServiceBody UpdateGlobalServiceBody update global service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v10\",\"minScale\":\"1\",\"size\":\"small\",\"trafficPercent\":50} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Size string string \u2713 Size of created service pods TrafficPercent int64 (formatted integer) int64 \u2713 Traffic percentage new revision will use UpdateGlobalServiceTrafficBody UpdateGlobalServiceTrafficBody update global service traffic body Example {\"values\":[{\"percent\":60,\"revision\":\"global-fast-request-00002\"},{\"percent\":40,\"revision\":\"global-fast-request-00001\"}]} Properties Name Type Go type Required Default Description Example Values [] UpdateGlobalServiceTrafficParamsBodyValuesItems0 []*UpdateGlobalServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateGlobalServiceTrafficParamsBodyValuesItems0 UpdateGlobalServiceTrafficParamsBodyValuesItems0 update global service traffic params body values items0 Properties Name Type Go type Required Default Description Example Percent int64 (formatted integer) int64 Target traffice percentage Revision string string Target service revision UpdateNamespaceServiceBody UpdateNamespaceServiceBody update namespace service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v10\",\"minScale\":\"1\",\"size\":\"small\",\"trafficPercent\":50} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Size string string \u2713 Size of created service pods TrafficPercent int64 (formatted integer) int64 \u2713 Traffic percentage new revision will use UpdateNamespaceServiceTrafficBody UpdateNamespaceServiceTrafficBody update namespace service traffic body Example {\"values\":[{\"percent\":60,\"revision\":\"namespace-direktiv-fast-request-00002\"},{\"percent\":40,\"revision\":\"namespace-direktiv-fast-request-00001\"}]} Properties Name Type Go type Required Default Description Example Values [] UpdateNamespaceServiceTrafficParamsBodyValuesItems0 []*UpdateNamespaceServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateNamespaceServiceTrafficParamsBodyValuesItems0 UpdateNamespaceServiceTrafficParamsBodyValuesItems0 update namespace service traffic params body values items0 Properties Name Type Go type Required Default Description Example Percent int64 (formatted integer) int64 Target traffice percentage Revision string string Target service revision UpdateServiceRequest UpdateServiceRequest UpdateServiceRequest UpdateServiceRequest update service request Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 image MinScale int32 (formatted integer) int32 \u2713 minScale Size int32 (formatted integer) int32 \u2713 size TrafficPercent int64 (formatted integer) int64 \u2713 trafficPercent updateServiceRequest UpdateServiceRequest UpdateServiceRequest update service request Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 image MinScale int32 (formatted integer) int32 \u2713 minScale Size int32 (formatted integer) int32 \u2713 size TrafficPercent int64 (formatted integer) int64 \u2713 trafficPercent","title":"API"},{"location":"api/#direktiv-api","text":"Direktiv Open API Specification Direktiv Documentation can be found at https://docs.direktiv.io/","title":"Direktiv API"},{"location":"api/#informations","text":"","title":"Informations"},{"location":"api/#version","text":"1.0.0","title":"Version"},{"location":"api/#contact","text":"info@direktiv.io","title":"Contact"},{"location":"api/#content-negotiation","text":"","title":"Content negotiation"},{"location":"api/#uri-schemes","text":"http https","title":"URI Schemes"},{"location":"api/#consumes","text":"application/json text/plain","title":"Consumes"},{"location":"api/#produces","text":"application/json text/event-stream","title":"Produces"},{"location":"api/#access-control","text":"","title":"Access control"},{"location":"api/#security-schemes","text":"","title":"Security Schemes"},{"location":"api/#api_key-header-key","text":"Type : apikey","title":"api_key (header: KEY)"},{"location":"api/#security-requirements","text":"api_key","title":"Security Requirements"},{"location":"api/#all-endpoints","text":"","title":"All endpoints"},{"location":"api/#directory","text":"Method URI Name Summary PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory create directory Create a Directory","title":"directory"},{"location":"api/#global_services","text":"Method URI Name Summary POST /api/functions create global service Create Global Service DELETE /api/functions/{serviceName}/revisions/{revisionGeneration} delete global revision Delete Global Service Revision DELETE /api/functions/{serviceName} delete global service Delete Global Service GET /api/functions/{serviceName} get global service Get Global Service Details GET /api/functions get global service list Get Global Services List GET /api/functions/{serviceName}/revisions/{revisionGeneration}/pods list global service revision pods Get Global Service Revision Pods List POST /api/functions/{serviceName} update global service Create Global Service Revision PATCH /api/functions/{serviceName} update global service traffic Update Global Service Traffic GET /api/functions/{serviceName}/revisions/{revisionGeneration} watch global service revision Watch Global Service Revision GET /api/functions/{serviceName}/revisions watch global service revision list Watch Global Service Revision List","title":"global_services"},{"location":"api/#instances","text":"Method URI Name Summary POST /api/namespaces/{namespace}/instances/{instance}/cancel cancel instance Cancel a Pending Instance GET /api/namespaces/{namespace}/instances/{instance} get instance Get a Instance GET /api/namespaces/{namespace}/instances/{instance}/input get instance input Get a Instance Input GET /api/namespaces/{namespace}/instances get instance list Get List Instances GET /api/namespaces/{namespace}/instances/{instance}/output get instance output Get a Instance Output","title":"instances"},{"location":"api/#logs","text":"Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=logs get workflow logs Get Workflow Level Logs GET /api/namespaces/{namespace}/instances/{instance}/logs instance logs Gets Instance Logs GET /api/namespaces/{namespace}/logs namespace logs Gets Namespace Level Logs GET /api/logs server logs Get Direktiv Server Logs","title":"logs"},{"location":"api/#metrics","text":"Method URI Name Summary GET /api/namespaces/{namespace}/metrics/failed namespace metrics failed Gets Namespace Failed Workflow Instances Metrics GET /api/namespaces/{namespace}/metrics/invoked namespace metrics invoked Gets Namespace Invoked Workflow Metrics GET /api/namespaces/{namespace}/metrics/milliseconds namespace metrics milliseconds Gets Namespace Workflow Timing Metrics GET /api/namespaces/{namespace}/metrics/successful namespace metrics successful Gets Namespace Successful Workflow Instances Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked workflow metrics invoked Gets Invoked Workflow Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed workflow metrics milliseconds Gets Workflow Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds workflow metrics state milliseconds Gets a Workflow State Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful workflow metrics successful Gets Successful Workflow Metrics","title":"metrics"},{"location":"api/#namespace_services","text":"Method URI Name Summary POST /api/functions/namespaces/{namespace} create namespace service Create Namespace Service DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} delete namespace revision Delete Namespace Service Revision DELETE /api/functions/namespaces/{namespace}/function/{serviceName} delete namespace service Delete Namespace Service GET /api/functions/namespaces/{namespace}/function/{serviceName} get namespace service Get Namespace Service Details GET /api/functions/namespaces/{namespace} get namespace service list Get Namespace Services List GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods list namespace service revision pods Get Namespace Service Revision Pods List POST /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service Create Namespace Service Revision PATCH /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service traffic Update Namespace Service Traffic GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} watch namespace service revision Watch Namespace Service Revision GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions watch namespace service revision list Watch Namespace Service Revision List","title":"namespace_services"},{"location":"api/#namespaces","text":"Method URI Name Summary PUT /api/namespaces/{namespace} create namespace Creates a namespace DELETE /api/namespaces/{namespace} delete namespace Delete a namespace GET /api/namespaces/{namespace}/config get namespace config Gets a namespace config GET /api/namespaces get namespaces Gets the list of namespaces PATCH /api/namespaces/{namespace}/config set namespace config Sets a namespace config","title":"namespaces"},{"location":"api/#node","text":"Method URI Name Summary DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node delete node Delete a node GET /api/namespaces/{namespace}/tree/{nodePath} get nodes Get List of Namespace Nodes","title":"node"},{"location":"api/#other","text":"Method URI Name Summary POST /api/namespaces/{namespace}/broadcast broadcast cloudevent Broadcast Cloud Event POST /api/jq jq playground JQ Playground api to test jq queries GET /api/version version Returns version information for servers in the cluster.","title":"other"},{"location":"api/#registries","text":"Method URI Name Summary POST /api/functions/registries/private create global private registry Create a Global Container Registry POST /api/functions/registries/global create global registry Create a Global Container Registry POST /api/functions/registries/namespaces/{namespace} create registry Create a Namespace Container Registry DELETE /api/functions/registries/private delete global private registry Delete a Global Container Registry DELETE /api/functions/registries/global delete global registry Delete a global Container Registry DELETE /api/functions/registries/namespaces/{namespace} delete registry Delete a Namespace Container Registry GET /api/functions/registries/private get global private registries Get List of Global Private Registries GET /api/functions/registries/global get global registries Get List of Global Registries GET /api/functions/registries/namespaces/{namespace} get registries Get List of Namespace Registries","title":"registries"},{"location":"api/#secrets","text":"Method URI Name Summary PUT /api/namespaces/{namespace}/secrets/{secret} create secret Create a Namespace Secret DELETE /api/namespaces/{namespace}/secrets/{secret} delete secret Delete a Namespace Secret GET /api/namespaces/{namespace}/secrets get secrets Get List of Namespace Secrets","title":"secrets"},{"location":"api/#variables","text":"Method URI Name Summary DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} delete instance variable Delete a Instance Variable DELETE /api/namespaces/{namespace}/vars/{variable} delete namespace variable Delete a Namespace Variable DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var delete workflow variable Delete a Workflow Variable GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} get instance variable Get a Instance Variable GET /api/namespaces/{namespace}/instances/{instance}/vars get instance variables Get List of Instance Variable GET /api/namespaces/{namespace}/vars/{variable} get namespace variable Get a Namespace Variable GET /api/namespaces/{namespace}/vars get namespace variables Get Namespace Variable List GET /api/namespaces/{namespace}/tree/{workflow}?op=var get workflow variable Get a Workflow Variable GET /api/namespaces/{namespace}/tree/{workflow}?op=vars get workflow variables Get List of Workflow Variables PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} set instance variable Set a Instance Variable PUT /api/namespaces/{namespace}/vars/{variable} set namespace variable Set a Namespace Variable PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var set workflow variable Set a Workflow Variable","title":"variables"},{"location":"api/#workflow_services","text":"Method URI Name Summary GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function get workflow service Get Workflow Service Details GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision get workflow service revision Get Workflow Service Revision GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions get workflow service revision list Get Workflow Service Revision List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods list workflow service revision pods Get Workflow Service Revision Pods List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services list workflow services Get Workflow Services List","title":"workflow_services"},{"location":"api/#workflows","text":"Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow Await Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow body Await Execute a Workflow With Body PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow create workflow Create a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=execute execute workflow Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging set workflow cloud event logs Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle toggle workflow Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow update workflow Update a Workflow","title":"workflows"},{"location":"api/#paths","text":"","title":"Paths"},{"location":"api/#await-execute-a-workflow-awaitexecuteworkflow","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters.","title":" Await Execute a Workflow (awaitExecuteWorkflow)"},{"location":"api/#parameters","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string.","title":"Parameters"},{"location":"api/#all-responses","text":"Code Status Description Has headers Schema 200 OK successfully executed workflow schema","title":"All responses"},{"location":"api/#responses","text":"","title":"Responses"},{"location":"api/#200-successfully-executed-workflow","text":"Status: OK","title":" 200 - successfully executed workflow"},{"location":"api/#schema","text":"","title":" Schema"},{"location":"api/#await-execute-a-workflow-with-body-awaitexecuteworkflowbody","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow with optionally some input provided in the request body as json. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters. Input query parameters are only read if the request has no body.","title":" Await Execute a Workflow With Body (awaitExecuteWorkflowBody)"},{"location":"api/#parameters_1","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string. Workflow Input body interface{} interface{} \u2713 The input of this workflow instance","title":"Parameters"},{"location":"api/#all-responses_1","text":"Code Status Description Has headers Schema 200 OK successfully executed workflow schema","title":"All responses"},{"location":"api/#responses_1","text":"","title":"Responses"},{"location":"api/#200-successfully-executed-workflow_1","text":"Status: OK","title":" 200 - successfully executed workflow"},{"location":"api/#schema_1","text":"","title":" Schema"},{"location":"api/#broadcast-cloud-event-broadcastcloudevent","text":"POST /api/namespaces/{namespace}/broadcast Broadcast a cloud event to a namespace. Cloud events posted to this api will be picked up by any workflows listening to the same event type on the namescape. The body of this request should follow the cloud event core specification defined at https://github.com/cloudevents/spec .","title":" Broadcast Cloud Event (broadcastCloudevent)"},{"location":"api/#parameters_2","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace cloudevent body interface{} interface{} \u2713 Cloud Event request to be sent.","title":"Parameters"},{"location":"api/#all-responses_2","text":"Code Status Description Has headers Schema 200 OK successfully sent cloud event schema","title":"All responses"},{"location":"api/#responses_2","text":"","title":"Responses"},{"location":"api/#200-successfully-sent-cloud-event","text":"Status: OK","title":" 200 - successfully sent cloud event"},{"location":"api/#schema_2","text":"","title":" Schema"},{"location":"api/#cancel-a-pending-instance-cancelinstance","text":"POST /api/namespaces/{namespace}/instances/{instance}/cancel Cancel a currently pending instance.","title":" Cancel a Pending Instance (cancelInstance)"},{"location":"api/#parameters_3","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_3","text":"Code Status Description Has headers Schema 200 OK successfully cancelled instance schema","title":"All responses"},{"location":"api/#responses_3","text":"","title":"Responses"},{"location":"api/#200-successfully-cancelled-instance","text":"Status: OK","title":" 200 - successfully cancelled instance"},{"location":"api/#schema_3","text":"","title":" Schema"},{"location":"api/#create-a-directory-createdirectory","text":"PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory Creates a directory at the target path.","title":" Create a Directory (createDirectory)"},{"location":"api/#parameters_4","text":"Name Source Type Go type Separator Required Default Description directory path string string \u2713 path to target directory namespace path string string \u2713 target namespace op query string string \u2713 \"create-directory\" the operation for the api","title":"Parameters"},{"location":"api/#all-responses_4","text":"Code Status Description Has headers Schema 200 OK directory has been created schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_4","text":"","title":"Responses"},{"location":"api/#200-directory-has-been-created","text":"Status: OK","title":" 200 - directory has been created"},{"location":"api/#schema_4","text":"OkBody","title":" Schema"},{"location":"api/#default-response","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_5","text":"ErrorResponse","title":" Schema"},{"location":"api/#create-a-global-container-registry-createglobalprivateregistry","text":"POST /api/functions/registries/private Create a global container registry. Global Private registries are only available to global services. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN","title":" Create a Global Container Registry (createGlobalPrivateRegistry)"},{"location":"api/#parameters_5","text":"Name Source Type Go type Separator Required Default Description Registry Payload body CreateGlobalPrivateRegistryBody CreateGlobalPrivateRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_5","text":"Code Status Description Has headers Schema 200 OK successfully created global private registry schema","title":"All responses"},{"location":"api/#responses_5","text":"","title":"Responses"},{"location":"api/#200-successfully-created-global-private-registry","text":"Status: OK","title":" 200 - successfully created global private registry"},{"location":"api/#schema_6","text":"","title":" Schema"},{"location":"api/#inlined-models","text":"CreateGlobalPrivateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#create-a-global-container-registry-createglobalregistry","text":"POST /api/functions/registries/global Create a global container registry. Global registries are available to all services. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN","title":" Create a Global Container Registry (createGlobalRegistry)"},{"location":"api/#parameters_6","text":"Name Source Type Go type Separator Required Default Description Registry Payload body CreateGlobalRegistryBody CreateGlobalRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_6","text":"Code Status Description Has headers Schema 200 OK successfully created global registry schema","title":"All responses"},{"location":"api/#responses_6","text":"","title":"Responses"},{"location":"api/#200-successfully-created-global-registry","text":"Status: OK","title":" 200 - successfully created global registry"},{"location":"api/#schema_7","text":"","title":" Schema"},{"location":"api/#inlined-models_1","text":"CreateGlobalRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#create-global-service-createglobalservice","text":"POST /api/functions Creates global scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/docs/walkthrough/using-functions.html","title":" Create Global Service (createGlobalService)"},{"location":"api/#parameters_7","text":"Name Source Type Go type Separator Required Default Description Service body CreateGlobalServiceBody CreateGlobalServiceBody \u2713 Payload that contains information on new service","title":"Parameters"},{"location":"api/#all-responses_7","text":"Code Status Description Has headers Schema 200 OK successfully created service schema","title":"All responses"},{"location":"api/#responses_7","text":"","title":"Responses"},{"location":"api/#200-successfully-created-service","text":"Status: OK","title":" 200 - successfully created service"},{"location":"api/#schema_8","text":"","title":" Schema"},{"location":"api/#inlined-models_2","text":"CreateGlobalServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size string string \u2713 Size of created service pods","title":"Inlined models"},{"location":"api/#creates-a-namespace-createnamespace","text":"PUT /api/namespaces/{namespace} Creates a new namespace.","title":" Creates a namespace (createNamespace)"},{"location":"api/#parameters_8","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to create","title":"Parameters"},{"location":"api/#all-responses_8","text":"Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_8","text":"","title":"Responses"},{"location":"api/#200-namespace-has-been-successfully-created","text":"Status: OK","title":" 200 - namespace has been successfully created"},{"location":"api/#schema_9","text":"OkBody","title":" Schema"},{"location":"api/#default-response_1","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_10","text":"ErrorResponse","title":" Schema"},{"location":"api/#create-namespace-service-createnamespaceservice","text":"POST /api/functions/namespaces/{namespace} Creates namespace scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/docs/walkthrough/using-functions.html","title":" Create Namespace Service (createNamespaceService)"},{"location":"api/#parameters_9","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Service body CreateNamespaceServiceBody CreateNamespaceServiceBody \u2713 Payload that contains information on new service","title":"Parameters"},{"location":"api/#all-responses_9","text":"Code Status Description Has headers Schema 200 OK successfully created service schema","title":"All responses"},{"location":"api/#responses_9","text":"","title":"Responses"},{"location":"api/#200-successfully-created-service_1","text":"Status: OK","title":" 200 - successfully created service"},{"location":"api/#schema_11","text":"","title":" Schema"},{"location":"api/#inlined-models_3","text":"CreateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size string string \u2713 Size of created service pods","title":"Inlined models"},{"location":"api/#create-a-namespace-container-registry-createregistry","text":"POST /api/functions/registries/namespaces/{namespace} Create a namespace container registry. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN","title":" Create a Namespace Container Registry (createRegistry)"},{"location":"api/#parameters_10","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body CreateRegistryBody CreateRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_10","text":"Code Status Description Has headers Schema 200 OK successfully created namespace registry schema","title":"All responses"},{"location":"api/#responses_10","text":"","title":"Responses"},{"location":"api/#200-successfully-created-namespace-registry","text":"Status: OK","title":" 200 - successfully created namespace registry"},{"location":"api/#schema_12","text":"","title":" Schema"},{"location":"api/#inlined-models_4","text":"CreateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#create-a-namespace-secret-createsecret","text":"PUT /api/namespaces/{namespace}/secrets/{secret} Create a namespace secret.","title":" Create a Namespace Secret (createSecret)"},{"location":"api/#consumes_1","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_11","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret Secret Payload body string string \u2713 Payload that contains secret data.","title":"Parameters"},{"location":"api/#all-responses_11","text":"Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_11","text":"","title":"Responses"},{"location":"api/#200-namespace-has-been-successfully-created_1","text":"Status: OK","title":" 200 - namespace has been successfully created"},{"location":"api/#schema_13","text":"OkBody","title":" Schema"},{"location":"api/#default-response_2","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_14","text":"ErrorResponse","title":" Schema"},{"location":"api/#create-a-workflow-createworkflow","text":"PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow Creates a workflow at the target path. The body of this request should contain the workflow yaml.","title":" Create a Workflow (createWorkflow)"},{"location":"api/#consumes_2","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_12","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"create-workflow\" the operation for the api workflow data body string string Payload that contains the direktiv workflow yaml to create.","title":"Parameters"},{"location":"api/#all-responses_12","text":"Code Status Description Has headers Schema 200 OK successfully created workflow schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_12","text":"","title":"Responses"},{"location":"api/#200-successfully-created-workflow","text":"Status: OK","title":" 200 - successfully created workflow"},{"location":"api/#schema_15","text":"OkBody","title":" Schema"},{"location":"api/#default-response_3","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_16","text":"ErrorResponse","title":" Schema"},{"location":"api/#delete-a-global-container-registry-deleteglobalprivateregistry","text":"DELETE /api/functions/registries/private Delete a global container registry. Global Private registries are only available to global services.","title":" Delete a Global Container Registry (deleteGlobalPrivateRegistry)"},{"location":"api/#parameters_13","text":"Name Source Type Go type Separator Required Default Description Registry Payload body DeleteGlobalPrivateRegistryBody DeleteGlobalPrivateRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_13","text":"Code Status Description Has headers Schema 200 OK successfully delete global private registry schema","title":"All responses"},{"location":"api/#responses_13","text":"","title":"Responses"},{"location":"api/#200-successfully-delete-global-private-registry","text":"Status: OK","title":" 200 - successfully delete global private registry"},{"location":"api/#schema_17","text":"","title":" Schema"},{"location":"api/#inlined-models_5","text":"DeleteGlobalPrivateRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#delete-a-global-container-registry-deleteglobalregistry","text":"DELETE /api/functions/registries/global Delete a Global container registry Global registries are available to all services.","title":" Delete a global Container Registry (deleteGlobalRegistry)"},{"location":"api/#parameters_14","text":"Name Source Type Go type Separator Required Default Description Registry Payload body DeleteGlobalRegistryBody DeleteGlobalRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_14","text":"Code Status Description Has headers Schema 200 OK successfully delete global registry schema","title":"All responses"},{"location":"api/#responses_14","text":"","title":"Responses"},{"location":"api/#200-successfully-delete-global-registry","text":"Status: OK","title":" 200 - successfully delete global registry"},{"location":"api/#schema_18","text":"","title":" Schema"},{"location":"api/#inlined-models_6","text":"DeleteGlobalRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#delete-global-service-revision-deleteglobalrevision","text":"DELETE /api/functions/{serviceName}/revisions/{revisionGeneration} Delete a global scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003'. Note: Revisions with traffic cannot be deleted.","title":" Delete Global Service Revision (deleteGlobalRevision)"},{"location":"api/#parameters_15","text":"Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_15","text":"Code Status Description Has headers Schema 200 OK successfully deleted service revision schema","title":"All responses"},{"location":"api/#responses_15","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-service-revision","text":"Status: OK","title":" 200 - successfully deleted service revision"},{"location":"api/#schema_19","text":"","title":" Schema"},{"location":"api/#delete-global-service-deleteglobalservice","text":"DELETE /api/functions/{serviceName} Deletes global scoped knative service and all its revisions.","title":" Delete Global Service (deleteGlobalService)"},{"location":"api/#parameters_16","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_16","text":"Code Status Description Has headers Schema 200 OK successfully deleted service schema","title":"All responses"},{"location":"api/#responses_16","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-service","text":"Status: OK","title":" 200 - successfully deleted service"},{"location":"api/#schema_20","text":"","title":" Schema"},{"location":"api/#delete-a-instance-variable-deleteinstancevariable","text":"DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Delete a instance variable.","title":" Delete a Instance Variable (deleteInstanceVariable)"},{"location":"api/#parameters_17","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_17","text":"Code Status Description Has headers Schema 200 OK successfully deleted instance variable schema","title":"All responses"},{"location":"api/#responses_17","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-instance-variable","text":"Status: OK","title":" 200 - successfully deleted instance variable"},{"location":"api/#schema_21","text":"","title":" Schema"},{"location":"api/#delete-a-namespace-deletenamespace","text":"DELETE /api/namespaces/{namespace} Delete a namespace. A namespace will not delete by default if it has any child resources (workflows, etc...). Deleting the namespace with all its children can be done using the recursive query parameter.","title":" Delete a namespace (deleteNamespace)"},{"location":"api/#parameters_18","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to delete recursive query boolean bool recursively deletes all child resources","title":"Parameters"},{"location":"api/#all-responses_18","text":"Code Status Description Has headers Schema 200 OK namespace has been successfully deleted schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_18","text":"","title":"Responses"},{"location":"api/#200-namespace-has-been-successfully-deleted","text":"Status: OK","title":" 200 - namespace has been successfully deleted"},{"location":"api/#schema_22","text":"OkBody","title":" Schema"},{"location":"api/#default-response_4","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_23","text":"ErrorResponse","title":" Schema"},{"location":"api/#delete-namespace-service-revision-deletenamespacerevision","text":"DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Delete a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: Revisions with traffic cannot be deleted.","title":" Delete Namespace Service Revision (deleteNamespaceRevision)"},{"location":"api/#parameters_19","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_19","text":"Code Status Description Has headers Schema 200 OK successfully deleted service revision schema","title":"All responses"},{"location":"api/#responses_19","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-service-revision_1","text":"Status: OK","title":" 200 - successfully deleted service revision"},{"location":"api/#schema_24","text":"","title":" Schema"},{"location":"api/#delete-namespace-service-deletenamespaceservice","text":"DELETE /api/functions/namespaces/{namespace}/function/{serviceName} Deletes namespace scoped knative service and all its revisions.","title":" Delete Namespace Service (deleteNamespaceService)"},{"location":"api/#parameters_20","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_20","text":"Code Status Description Has headers Schema 200 OK successfully deleted service schema","title":"All responses"},{"location":"api/#responses_20","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-service_1","text":"Status: OK","title":" 200 - successfully deleted service"},{"location":"api/#schema_25","text":"","title":" Schema"},{"location":"api/#delete-a-namespace-variable-deletenamespacevariable","text":"DELETE /api/namespaces/{namespace}/vars/{variable} Delete a namespace variable.","title":" Delete a Namespace Variable (deleteNamespaceVariable)"},{"location":"api/#parameters_21","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_21","text":"Code Status Description Has headers Schema 200 OK successfully deleted namespace variable schema","title":"All responses"},{"location":"api/#responses_21","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-namespace-variable","text":"Status: OK","title":" 200 - successfully deleted namespace variable"},{"location":"api/#schema_26","text":"","title":" Schema"},{"location":"api/#delete-a-node-deletenode","text":"DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node Creates a directory at the target path.","title":" Delete a node (deleteNode)"},{"location":"api/#parameters_22","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace node path string string \u2713 path to target node op query string string \u2713 \"delete-node\" the operation for the api","title":"Parameters"},{"location":"api/#all-responses_22","text":"Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_22","text":"","title":"Responses"},{"location":"api/#200-node-has-been-deleted","text":"Status: OK","title":" 200 - node has been deleted"},{"location":"api/#schema_27","text":"OkBody","title":" Schema"},{"location":"api/#default-response_5","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_28","text":"ErrorResponse","title":" Schema"},{"location":"api/#delete-a-namespace-container-registry-deleteregistry","text":"DELETE /api/functions/registries/namespaces/{namespace} Delete a namespace container registry","title":" Delete a Namespace Container Registry (deleteRegistry)"},{"location":"api/#parameters_23","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body DeleteRegistryBody DeleteRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_23","text":"Code Status Description Has headers Schema 200 OK successfully delete namespace registry schema","title":"All responses"},{"location":"api/#responses_23","text":"","title":"Responses"},{"location":"api/#200-successfully-delete-namespace-registry","text":"Status: OK","title":" 200 - successfully delete namespace registry"},{"location":"api/#schema_29","text":"","title":" Schema"},{"location":"api/#inlined-models_7","text":"DeleteRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#delete-a-namespace-secret-deletesecret","text":"DELETE /api/namespaces/{namespace}/secrets/{secret} Delete a namespace secret.","title":" Delete a Namespace Secret (deleteSecret)"},{"location":"api/#parameters_24","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret","title":"Parameters"},{"location":"api/#all-responses_24","text":"Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_24","text":"","title":"Responses"},{"location":"api/#200-namespace-has-been-successfully-created_2","text":"Status: OK","title":" 200 - namespace has been successfully created"},{"location":"api/#schema_30","text":"OkBody","title":" Schema"},{"location":"api/#default-response_6","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_31","text":"ErrorResponse","title":" Schema"},{"location":"api/#delete-a-workflow-variable-deleteworkflowvariable","text":"DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var Delete a workflow variable.","title":" Delete a Workflow Variable (deleteWorkflowVariable)"},{"location":"api/#parameters_25","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_25","text":"Code Status Description Has headers Schema 200 OK successfully deleted workflow variable schema","title":"All responses"},{"location":"api/#responses_25","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-workflow-variable","text":"Status: OK","title":" 200 - successfully deleted workflow variable"},{"location":"api/#schema_32","text":"","title":" Schema"},{"location":"api/#execute-a-workflow-executeworkflow","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=execute Executes a workflow with optionally some input provided in the request body as json.","title":" Execute a Workflow (executeWorkflow)"},{"location":"api/#parameters_26","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"execute\" the operation for the api Workflow Input body interface{} interface{} \u2713 The input of this workflow instance","title":"Parameters"},{"location":"api/#all-responses_26","text":"Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_26","text":"","title":"Responses"},{"location":"api/#200-node-has-been-deleted_1","text":"Status: OK","title":" 200 - node has been deleted"},{"location":"api/#schema_33","text":"OkBody","title":" Schema"},{"location":"api/#default-response_7","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_34","text":"ErrorResponse","title":" Schema"},{"location":"api/#get-list-of-global-private-registries-getglobalprivateregistries","text":"GET /api/functions/registries/private Gets the list of global private registries. Global Private registries are only available to global services.","title":" Get List of Global Private Registries (getGlobalPrivateRegistries)"},{"location":"api/#all-responses_27","text":"Code Status Description Has headers Schema 200 OK successfully got global private registries schema","title":"All responses"},{"location":"api/#responses_27","text":"","title":"Responses"},{"location":"api/#200-successfully-got-global-private-registries","text":"Status: OK","title":" 200 - successfully got global private registries"},{"location":"api/#schema_35","text":"","title":" Schema"},{"location":"api/#get-list-of-global-registries-getglobalregistries","text":"GET /api/functions/registries/global Gets the list of global registries. Global registries are available to all services.","title":" Get List of Global Registries (getGlobalRegistries)"},{"location":"api/#all-responses_28","text":"Code Status Description Has headers Schema 200 OK successfully got global registries schema","title":"All responses"},{"location":"api/#responses_28","text":"","title":"Responses"},{"location":"api/#200-successfully-got-global-registries","text":"Status: OK","title":" 200 - successfully got global registries"},{"location":"api/#schema_36","text":"","title":" Schema"},{"location":"api/#get-global-service-details-getglobalservice","text":"GET /api/functions/{serviceName} Get details of a global scoped knative service.","title":" Get Global Service Details (getGlobalService)"},{"location":"api/#parameters_27","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_29","text":"Code Status Description Has headers Schema 200 OK successfully got service details schema","title":"All responses"},{"location":"api/#responses_29","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-details","text":"Status: OK","title":" 200 - successfully got service details"},{"location":"api/#schema_37","text":"","title":" Schema"},{"location":"api/#get-global-services-list-getglobalservicelist","text":"GET /api/functions Gets a list of global knative services.","title":" Get Global Services List (getGlobalServiceList)"},{"location":"api/#all-responses_30","text":"Code Status Description Has headers Schema 200 OK successfully got services list schema","title":"All responses"},{"location":"api/#responses_30","text":"","title":"Responses"},{"location":"api/#200-successfully-got-services-list","text":"Status: OK","title":" 200 - successfully got services list"},{"location":"api/#schema_38","text":"","title":" Schema"},{"location":"api/#get-a-instance-getinstance","text":"GET /api/namespaces/{namespace}/instances/{instance} Gets the details of a executed workflow instance in this namespace.","title":" Get a Instance (getInstance)"},{"location":"api/#parameters_28","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_31","text":"Code Status Description Has headers Schema 200 OK successfully got instance schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_31","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance","text":"Status: OK","title":" 200 - successfully got instance"},{"location":"api/#schema_39","text":"OkBody","title":" Schema"},{"location":"api/#default-response_8","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_40","text":"ErrorResponse","title":" Schema"},{"location":"api/#get-a-instance-input-getinstanceinput","text":"GET /api/namespaces/{namespace}/instances/{instance}/input Gets the input an instance was provided when executed.","title":" Get a Instance Input (getInstanceInput)"},{"location":"api/#parameters_29","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_32","text":"Code Status Description Has headers Schema 200 OK successfully got instance input schema","title":"All responses"},{"location":"api/#responses_32","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-input","text":"Status: OK","title":" 200 - successfully got instance input"},{"location":"api/#schema_41","text":"","title":" Schema"},{"location":"api/#get-list-instances-getinstancelist","text":"GET /api/namespaces/{namespace}/instances Gets a list of instances in a namespace.","title":" Get List Instances (getInstanceList)"},{"location":"api/#parameters_30","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_33","text":"Code Status Description Has headers Schema 200 OK successfully got namespace instances schema","title":"All responses"},{"location":"api/#responses_33","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-instances","text":"Status: OK","title":" 200 - successfully got namespace instances"},{"location":"api/#schema_42","text":"","title":" Schema"},{"location":"api/#get-a-instance-output-getinstanceoutput","text":"GET /api/namespaces/{namespace}/instances/{instance}/output Gets the output an instance was provided when executed.","title":" Get a Instance Output (getInstanceOutput)"},{"location":"api/#parameters_31","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_34","text":"Code Status Description Has headers Schema 200 OK successfully got instance output schema","title":"All responses"},{"location":"api/#responses_34","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-output","text":"Status: OK","title":" 200 - successfully got instance output"},{"location":"api/#schema_43","text":"","title":" Schema"},{"location":"api/#get-a-instance-variable-getinstancevariable","text":"GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Get the value sorted in a instance variable.","title":" Get a Instance Variable (getInstanceVariable)"},{"location":"api/#parameters_32","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_35","text":"Code Status Description Has headers Schema 200 OK successfully got instance variable schema","title":"All responses"},{"location":"api/#responses_35","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-variable","text":"Status: OK","title":" 200 - successfully got instance variable"},{"location":"api/#schema_44","text":"","title":" Schema"},{"location":"api/#get-list-of-instance-variable-getinstancevariables","text":"GET /api/namespaces/{namespace}/instances/{instance}/vars Gets a list of variables in a instance.","title":" Get List of Instance Variable (getInstanceVariables)"},{"location":"api/#parameters_33","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_36","text":"Code Status Description Has headers Schema 200 OK successfully got instance variables schema","title":"All responses"},{"location":"api/#responses_36","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-variables","text":"Status: OK","title":" 200 - successfully got instance variables"},{"location":"api/#schema_45","text":"","title":" Schema"},{"location":"api/#gets-a-namespace-config-getnamespaceconfig","text":"GET /api/namespaces/{namespace}/config Gets a namespace config.","title":" Gets a namespace config (getNamespaceConfig)"},{"location":"api/#parameters_34","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update","title":"Parameters"},{"location":"api/#all-responses_37","text":"Code Status Description Has headers Schema 200 OK successfully got namespace config schema","title":"All responses"},{"location":"api/#responses_37","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-config","text":"Status: OK","title":" 200 - successfully got namespace config"},{"location":"api/#schema_46","text":"","title":" Schema"},{"location":"api/#get-namespace-service-details-getnamespaceservice","text":"GET /api/functions/namespaces/{namespace}/function/{serviceName} Get details of a namespace scoped knative service.","title":" Get Namespace Service Details (getNamespaceService)"},{"location":"api/#parameters_35","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_38","text":"Code Status Description Has headers Schema 200 OK successfully got service details schema","title":"All responses"},{"location":"api/#responses_38","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-details_1","text":"Status: OK","title":" 200 - successfully got service details"},{"location":"api/#schema_47","text":"","title":" Schema"},{"location":"api/#get-namespace-services-list-getnamespaceservicelist","text":"GET /api/functions/namespaces/{namespace} Gets a list of namespace knative services.","title":" Get Namespace Services List (getNamespaceServiceList)"},{"location":"api/#parameters_36","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_39","text":"Code Status Description Has headers Schema 200 OK successfully got services list schema","title":"All responses"},{"location":"api/#responses_39","text":"","title":"Responses"},{"location":"api/#200-successfully-got-services-list_1","text":"Status: OK","title":" 200 - successfully got services list"},{"location":"api/#schema_48","text":"","title":" Schema"},{"location":"api/#get-a-namespace-variable-getnamespacevariable","text":"GET /api/namespaces/{namespace}/vars/{variable} Get the value sorted in a namespace variable.","title":" Get a Namespace Variable (getNamespaceVariable)"},{"location":"api/#parameters_37","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_40","text":"Code Status Description Has headers Schema 200 OK successfully got namespace variable schema","title":"All responses"},{"location":"api/#responses_40","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-variable","text":"Status: OK","title":" 200 - successfully got namespace variable"},{"location":"api/#schema_49","text":"","title":" Schema"},{"location":"api/#get-namespace-variable-list-getnamespacevariables","text":"GET /api/namespaces/{namespace}/vars Gets a list of variables in a namespace.","title":" Get Namespace Variable List (getNamespaceVariables)"},{"location":"api/#parameters_38","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_41","text":"Code Status Description Has headers Schema 200 OK successfully got namespace variables schema","title":"All responses"},{"location":"api/#responses_41","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-variables","text":"Status: OK","title":" 200 - successfully got namespace variables"},{"location":"api/#schema_50","text":"","title":" Schema"},{"location":"api/#gets-the-list-of-namespaces-getnamespaces","text":"GET /api/namespaces Gets the list of namespaces.","title":" Gets the list of namespaces (getNamespaces)"},{"location":"api/#all-responses_42","text":"Code Status Description Has headers Schema 200 OK successfully got list of namespaces schema","title":"All responses"},{"location":"api/#responses_42","text":"","title":"Responses"},{"location":"api/#200-successfully-got-list-of-namespaces","text":"Status: OK","title":" 200 - successfully got list of namespaces"},{"location":"api/#schema_51","text":"","title":" Schema"},{"location":"api/#get-list-of-namespace-nodes-getnodes","text":"GET /api/namespaces/{namespace}/tree/{nodePath} Gets Workflow and Directory Nodes at nodePath.","title":" Get List of Namespace Nodes (getNodes)"},{"location":"api/#parameters_39","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace nodePath path string string \u2713 target path in tree","title":"Parameters"},{"location":"api/#all-responses_43","text":"Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_43","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-nodes","text":"Status: OK","title":" 200 - successfully got namespace nodes"},{"location":"api/#schema_52","text":"OkBody","title":" Schema"},{"location":"api/#default-response_9","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_53","text":"ErrorResponse","title":" Schema"},{"location":"api/#get-list-of-namespace-registries-getregistries","text":"GET /api/functions/registries/namespaces/{namespace} Gets the list of namespace registries.","title":" Get List of Namespace Registries (getRegistries)"},{"location":"api/#parameters_40","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_44","text":"Code Status Description Has headers Schema 200 OK successfully got namespace registries schema","title":"All responses"},{"location":"api/#responses_44","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-registries","text":"Status: OK","title":" 200 - successfully got namespace registries"},{"location":"api/#schema_54","text":"","title":" Schema"},{"location":"api/#get-list-of-namespace-secrets-getsecrets","text":"GET /api/namespaces/{namespace}/secrets Gets the list of namespace secrets.","title":" Get List of Namespace Secrets (getSecrets)"},{"location":"api/#parameters_41","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_45","text":"Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_45","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-nodes_1","text":"Status: OK","title":" 200 - successfully got namespace nodes"},{"location":"api/#schema_55","text":"OkBody","title":" Schema"},{"location":"api/#default-response_10","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_56","text":"ErrorResponse","title":" Schema"},{"location":"api/#get-workflow-level-logs-getworkflowlogs","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=logs Get workflow level logs.","title":" Get Workflow Level Logs (getWorkflowLogs)"},{"location":"api/#parameters_42","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_46","text":"Code Status Description Has headers Schema 200 OK successfully got workflow logs schema","title":"All responses"},{"location":"api/#responses_46","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-logs","text":"Status: OK","title":" 200 - successfully got workflow logs"},{"location":"api/#schema_57","text":"","title":" Schema"},{"location":"api/#get-workflow-service-details-getworkflowservice","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function Get a workflow scoped knative service details. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Get Workflow Service Details (getWorkflowService)"},{"location":"api/#parameters_43","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version","title":"Parameters"},{"location":"api/#all-responses_47","text":"Code Status Description Has headers Schema 200 OK successfully got service details schema","title":"All responses"},{"location":"api/#responses_47","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-details_2","text":"Status: OK","title":" 200 - successfully got service details"},{"location":"api/#schema_58","text":"","title":" Schema"},{"location":"api/#get-workflow-service-revision-getworkflowservicerevision","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision Get a workflow scoped knative service revision. This will return details on a single revision. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'.","title":" Get Workflow Service Revision (getWorkflowServiceRevision)"},{"location":"api/#parameters_44","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_48","text":"Code Status Description Has headers Schema 200 OK successfully got service revision details schema","title":"All responses"},{"location":"api/#responses_48","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-revision-details","text":"Status: OK","title":" 200 - successfully got service revision details"},{"location":"api/#schema_59","text":"","title":" Schema"},{"location":"api/#get-workflow-service-revision-list-getworkflowservicerevisionlist","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions Get the revision list of a workflow scoped knative service.","title":" Get Workflow Service Revision List (getWorkflowServiceRevisionList)"},{"location":"api/#parameters_45","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version","title":"Parameters"},{"location":"api/#all-responses_49","text":"Code Status Description Has headers Schema 200 OK successfully got service revisions schema","title":"All responses"},{"location":"api/#responses_49","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-revisions","text":"Status: OK","title":" 200 - successfully got service revisions"},{"location":"api/#schema_60","text":"","title":" Schema"},{"location":"api/#get-a-workflow-variable-getworkflowvariable","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=var Get the value sorted in a workflow variable.","title":" Get a Workflow Variable (getWorkflowVariable)"},{"location":"api/#parameters_46","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_50","text":"Code Status Description Has headers Schema 200 OK successfully got workflow variable schema","title":"All responses"},{"location":"api/#responses_50","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-variable","text":"Status: OK","title":" 200 - successfully got workflow variable"},{"location":"api/#schema_61","text":"","title":" Schema"},{"location":"api/#get-list-of-workflow-variables-getworkflowvariables","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=vars Gets a list of variables in a workflow.","title":" Get List of Workflow Variables (getWorkflowVariables)"},{"location":"api/#parameters_47","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_51","text":"Code Status Description Has headers Schema 200 OK successfully got workflow variables schema","title":"All responses"},{"location":"api/#responses_51","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-variables","text":"Status: OK","title":" 200 - successfully got workflow variables"},{"location":"api/#schema_62","text":"","title":" Schema"},{"location":"api/#gets-instance-logs-instancelogs","text":"GET /api/namespaces/{namespace}/instances/{instance}/logs Gets the logs of an executed instance.","title":" Gets Instance Logs (instanceLogs)"},{"location":"api/#parameters_48","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance id namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_52","text":"Code Status Description Has headers Schema 200 OK successfully got instance logs schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_52","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-logs","text":"Status: OK","title":" 200 - successfully got instance logs"},{"location":"api/#schema_63","text":"OkBody","title":" Schema"},{"location":"api/#default-response_11","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_64","text":"ErrorResponse","title":" Schema"},{"location":"api/#jq-playground-api-to-test-jq-queries-jqplayground","text":"POST /api/jq JQ Playground is a sandbox where you can test jq queries with custom data.","title":" JQ Playground api to test jq queries (jqPlayground)"},{"location":"api/#parameters_49","text":"Name Source Type Go type Separator Required Default Description JQ payload body JqPlaygroundBody JqPlaygroundBody \u2713 Payload that contains both the JSON data to manipulate and jq query.","title":"Parameters"},{"location":"api/#all-responses_53","text":"Code Status Description Has headers Schema 200 OK jq query was successful schema 400 Bad Request the request was invalid schema 500 Internal Server Error an unexpected internal error occurred schema","title":"All responses"},{"location":"api/#responses_53","text":"","title":"Responses"},{"location":"api/#200-jq-query-was-successful","text":"Status: OK","title":" 200 - jq query was successful"},{"location":"api/#schema_65","text":"","title":" Schema"},{"location":"api/#400-the-request-was-invalid","text":"Status: Bad Request","title":" 400 - the request was invalid"},{"location":"api/#schema_66","text":"","title":" Schema"},{"location":"api/#500-an-unexpected-internal-error-occurred","text":"Status: Internal Server Error","title":" 500 - an unexpected internal error occurred"},{"location":"api/#schema_67","text":"","title":" Schema"},{"location":"api/#inlined-models_8","text":"JqPlaygroundBody Properties Name Type Go type Required Default Description Example data string string \u2713 JSON data encoded in base64 query string string \u2713 jq query to manipulate JSON data","title":"Inlined models"},{"location":"api/#get-global-service-revision-pods-list-listglobalservicerevisionpods","text":"GET /api/functions/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a global scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003' .","title":" Get Global Service Revision Pods List (listGlobalServiceRevisionPods)"},{"location":"api/#parameters_50","text":"Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_54","text":"Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema","title":"All responses"},{"location":"api/#responses_54","text":"","title":"Responses"},{"location":"api/#200-successfully-got-list-of-a-service-revision-pods","text":"Status: OK","title":" 200 - successfully got list of a service revision pods"},{"location":"api/#schema_68","text":"","title":" Schema"},{"location":"api/#get-namespace-service-revision-pods-list-listnamespaceservicerevisionpods","text":"GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a namespace scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'.","title":" Get Namespace Service Revision Pods List (listNamespaceServiceRevisionPods)"},{"location":"api/#parameters_51","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_55","text":"Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema","title":"All responses"},{"location":"api/#responses_55","text":"","title":"Responses"},{"location":"api/#200-successfully-got-list-of-a-service-revision-pods_1","text":"Status: OK","title":" 200 - successfully got list of a service revision pods"},{"location":"api/#schema_69","text":"","title":" Schema"},{"location":"api/#get-workflow-service-revision-pods-list-listworkflowservicerevisionpods","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods List a revisions pods of a workflow scoped knative service. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'.","title":" Get Workflow Service Revision Pods List (listWorkflowServiceRevisionPods)"},{"location":"api/#parameters_52","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_56","text":"Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema","title":"All responses"},{"location":"api/#responses_56","text":"","title":"Responses"},{"location":"api/#200-successfully-got-list-of-a-service-revision-pods_2","text":"Status: OK","title":" 200 - successfully got list of a service revision pods"},{"location":"api/#schema_70","text":"","title":" Schema"},{"location":"api/#get-workflow-services-list-listworkflowservices","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services Gets a list of workflow knative services.","title":" Get Workflow Services List (listWorkflowServices)"},{"location":"api/#parameters_53","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_57","text":"Code Status Description Has headers Schema 200 OK successfully got services list schema","title":"All responses"},{"location":"api/#responses_57","text":"","title":"Responses"},{"location":"api/#200-successfully-got-services-list_2","text":"Status: OK","title":" 200 - successfully got services list"},{"location":"api/#schema_71","text":"","title":" Schema"},{"location":"api/#gets-namespace-level-logs-namespacelogs","text":"GET /api/namespaces/{namespace}/logs Gets Namespace Level Logs.","title":" Gets Namespace Level Logs (namespaceLogs)"},{"location":"api/#parameters_54","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_58","text":"Code Status Description Has headers Schema 200 OK successfully got namespace logs schema","title":"All responses"},{"location":"api/#responses_58","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-logs","text":"Status: OK","title":" 200 - successfully got namespace logs"},{"location":"api/#schema_72","text":"","title":" Schema"},{"location":"api/#gets-namespace-failed-workflow-instances-metrics-namespacemetricsfailed","text":"GET /api/namespaces/{namespace}/metrics/failed Get metrics of failed workflows in the targeted namespace.","title":" Gets Namespace Failed Workflow Instances Metrics (namespaceMetricsFailed)"},{"location":"api/#parameters_55","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_59","text":"Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema","title":"All responses"},{"location":"api/#responses_59","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-metrics","text":"Status: OK","title":" 200 - successfully got namespace metrics"},{"location":"api/#schema_73","text":"","title":" Schema"},{"location":"api/#gets-namespace-invoked-workflow-metrics-namespacemetricsinvoked","text":"GET /api/namespaces/{namespace}/metrics/invoked Get metrics of invoked workflows in the targeted namespace.","title":" Gets Namespace Invoked Workflow Metrics (namespaceMetricsInvoked)"},{"location":"api/#parameters_56","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_60","text":"Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema","title":"All responses"},{"location":"api/#responses_60","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-metrics_1","text":"Status: OK","title":" 200 - successfully got namespace metrics"},{"location":"api/#schema_74","text":"","title":" Schema"},{"location":"api/#gets-namespace-workflow-timing-metrics-namespacemetricsmilliseconds","text":"GET /api/namespaces/{namespace}/metrics/milliseconds Get timing metrics of workflows in the targeted namespace.","title":" Gets Namespace Workflow Timing Metrics (namespaceMetricsMilliseconds)"},{"location":"api/#parameters_57","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_61","text":"Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema","title":"All responses"},{"location":"api/#responses_61","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-metrics_2","text":"Status: OK","title":" 200 - successfully got namespace metrics"},{"location":"api/#schema_75","text":"","title":" Schema"},{"location":"api/#gets-namespace-successful-workflow-instances-metrics-namespacemetricssuccessful","text":"GET /api/namespaces/{namespace}/metrics/successful Get metrics of successful workflows in the targeted namespace.","title":" Gets Namespace Successful Workflow Instances Metrics (namespaceMetricsSuccessful)"},{"location":"api/#parameters_58","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_62","text":"Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema","title":"All responses"},{"location":"api/#responses_62","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-metrics_3","text":"Status: OK","title":" 200 - successfully got namespace metrics"},{"location":"api/#schema_76","text":"","title":" Schema"},{"location":"api/#get-direktiv-server-logs-serverlogs","text":"GET /api/logs Gets Direktiv Server Logs.","title":" Get Direktiv Server Logs (serverLogs)"},{"location":"api/#all-responses_63","text":"Code Status Description Has headers Schema 200 OK successfully got server logs schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_63","text":"","title":"Responses"},{"location":"api/#200-successfully-got-server-logs","text":"Status: OK","title":" 200 - successfully got server logs"},{"location":"api/#schema_77","text":"OkBody","title":" Schema"},{"location":"api/#default-response_12","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_78","text":"ErrorResponse","title":" Schema"},{"location":"api/#set-a-instance-variable-setinstancevariable","text":"PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Set the value sorted in a instance variable. If the target variable does not exists, it will be created. Variable data can be anything.","title":" Set a Instance Variable (setInstanceVariable)"},{"location":"api/#consumes_3","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_59","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data.","title":"Parameters"},{"location":"api/#all-responses_64","text":"Code Status Description Has headers Schema 200 OK successfully set instance variable schema","title":"All responses"},{"location":"api/#responses_64","text":"","title":"Responses"},{"location":"api/#200-successfully-set-instance-variable","text":"Status: OK","title":" 200 - successfully set instance variable"},{"location":"api/#schema_79","text":"","title":" Schema"},{"location":"api/#sets-a-namespace-config-setnamespaceconfig","text":"PATCH /api/namespaces/{namespace}/config Sets a namespace config.","title":" Sets a namespace config (setNamespaceConfig)"},{"location":"api/#parameters_60","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update Config Payload body SetNamespaceConfigBody SetNamespaceConfigBody Payload that contains the config information to set. Note: This payload only need to contain the properities you wish to set.","title":"Parameters"},{"location":"api/#all-responses_65","text":"Code Status Description Has headers Schema 200 OK namespace config has been successfully been updated schema","title":"All responses"},{"location":"api/#responses_65","text":"","title":"Responses"},{"location":"api/#200-namespace-config-has-been-successfully-been-updated","text":"Status: OK","title":" 200 - namespace config has been successfully been updated"},{"location":"api/#schema_80","text":"","title":" Schema"},{"location":"api/#inlined-models_9","text":"SetNamespaceConfigBody Properties Name Type Go type Required Default Description Example broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace","title":"Inlined models"},{"location":"api/#set-a-namespace-variable-setnamespacevariable","text":"PUT /api/namespaces/{namespace}/vars/{variable} Set the value sorted in a namespace variable. If the target variable does not exists, it will be created. Variable data can be anything.","title":" Set a Namespace Variable (setNamespaceVariable)"},{"location":"api/#consumes_4","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_61","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data.","title":"Parameters"},{"location":"api/#all-responses_66","text":"Code Status Description Has headers Schema 200 OK successfully set namespace variable schema","title":"All responses"},{"location":"api/#responses_66","text":"","title":"Responses"},{"location":"api/#200-successfully-set-namespace-variable","text":"Status: OK","title":" 200 - successfully set namespace variable"},{"location":"api/#schema_81","text":"","title":" Schema"},{"location":"api/#set-cloud-event-for-workflow-to-log-to-setworkflowcloudeventlogs","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging Set Cloud Event for Workflow to Log to. When configured type direktiv.instanceLog cloud events will be generated with the logger parameter set to the configured value. Workflows can be configured to generate cloud events on their namespace anything the log parameter produces data. Please find more information on this topic here: https://docs.direktiv.io/docs/examples/logging.html","title":" Set Cloud Event for Workflow to Log to (setWorkflowCloudEventLogs)"},{"location":"api/#parameters_62","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Cloud Event Logger body SetWorkflowCloudEventLogsBody SetWorkflowCloudEventLogsBody \u2713 Cloud event logger to target","title":"Parameters"},{"location":"api/#all-responses_67","text":"Code Status Description Has headers Schema 200 OK successfully update workflow schema","title":"All responses"},{"location":"api/#responses_67","text":"","title":"Responses"},{"location":"api/#200-successfully-update-workflow","text":"Status: OK","title":" 200 - successfully update workflow"},{"location":"api/#schema_82","text":"","title":" Schema"},{"location":"api/#inlined-models_10","text":"SetWorkflowCloudEventLogsBody Properties Name Type Go type Required Default Description Example logger string string \u2713 Target Cloud Event","title":"Inlined models"},{"location":"api/#set-a-workflow-variable-setworkflowvariable","text":"PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var Set the value sorted in a workflow variable. If the target variable does not exists, it will be created. Variable data can be anything.","title":" Set a Workflow Variable (setWorkflowVariable)"},{"location":"api/#consumes_5","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_63","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable data body string string \u2713 Payload that contains variable data.","title":"Parameters"},{"location":"api/#all-responses_68","text":"Code Status Description Has headers Schema 200 OK successfully set workflow variable schema","title":"All responses"},{"location":"api/#responses_68","text":"","title":"Responses"},{"location":"api/#200-successfully-set-workflow-variable","text":"Status: OK","title":" 200 - successfully set workflow variable"},{"location":"api/#schema_83","text":"","title":" Schema"},{"location":"api/#set-cloud-event-for-workflow-to-log-to-toggleworkflow","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle Toggle's whether or not a workflow is active. Disabled workflows cannot be invoked. This includes start event and scheduled workflows.","title":" Set Cloud Event for Workflow to Log to (toggleWorkflow)"},{"location":"api/#parameters_64","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Workflow Live Status body ToggleWorkflowBody ToggleWorkflowBody \u2713 Whether or not the workflow is alive or disabled","title":"Parameters"},{"location":"api/#all-responses_69","text":"Code Status Description Has headers Schema 200 OK successfully updated workflow live status schema","title":"All responses"},{"location":"api/#responses_69","text":"","title":"Responses"},{"location":"api/#200-successfully-updated-workflow-live-status","text":"Status: OK","title":" 200 - successfully updated workflow live status"},{"location":"api/#schema_84","text":"","title":" Schema"},{"location":"api/#inlined-models_11","text":"ToggleWorkflowBody Properties Name Type Go type Required Default Description Example live boolean bool \u2713 Workflow live status","title":"Inlined models"},{"location":"api/#create-global-service-revision-updateglobalservice","text":"POST /api/functions/{serviceName} Creates a new global scoped knative service revision Revisions are created with a traffic percentage. This percentage controls how much traffic will be directed to this revision. Traffic can be set to 100 to direct all traffic.","title":" Create Global Service Revision (updateGlobalService)"},{"location":"api/#parameters_65","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name Service body UpdateGlobalServiceBody UpdateGlobalServiceBody \u2713 Payload that contains information on service revision","title":"Parameters"},{"location":"api/#all-responses_70","text":"Code Status Description Has headers Schema 200 OK successfully created service revision schema","title":"All responses"},{"location":"api/#responses_70","text":"","title":"Responses"},{"location":"api/#200-successfully-created-service-revision","text":"Status: OK","title":" 200 - successfully created service revision"},{"location":"api/#schema_85","text":"","title":" Schema"},{"location":"api/#inlined-models_12","text":"UpdateGlobalServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods trafficPercent integer int64 \u2713 Traffic percentage new revision will use","title":"Inlined models"},{"location":"api/#update-global-service-traffic-updateglobalservicetraffic","text":"PATCH /api/functions/{serviceName} Update Global Service traffic directed to each revision, traffic can only be configured between two revisions. All other revisions will bet set to 0 traffic.","title":" Update Global Service Traffic (updateGlobalServiceTraffic)"},{"location":"api/#parameters_66","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name Service Traffic body UpdateGlobalServiceTrafficBody UpdateGlobalServiceTrafficBody \u2713 Payload that contains information on service traffic","title":"Parameters"},{"location":"api/#all-responses_71","text":"Code Status Description Has headers Schema 200 OK successfully updated service traffic schema","title":"All responses"},{"location":"api/#responses_71","text":"","title":"Responses"},{"location":"api/#200-successfully-updated-service-traffic","text":"Status: OK","title":" 200 - successfully updated service traffic"},{"location":"api/#schema_86","text":"","title":" Schema"},{"location":"api/#inlined-models_13","text":"UpdateGlobalServiceTrafficBody Properties Name Type Go type Required Default Description Example values [] UpdateGlobalServiceTrafficParamsBodyValuesItems0 []*models.UpdateGlobalServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateGlobalServiceTrafficParamsBodyValuesItems0 Properties Name Type Go type Required Default Description Example percent integer int64 Target traffice percentage revision string string Target service revision","title":"Inlined models"},{"location":"api/#create-namespace-service-revision-updatenamespaceservice","text":"POST /api/functions/namespaces/{namespace}/function/{serviceName} Creates a new namespace scoped knative service revision. Revisions are created with a traffic percentage. This percentage controls how much traffic will be directed to this revision. Traffic can be set to 100 to direct all traffic.","title":" Create Namespace Service Revision (updateNamespaceService)"},{"location":"api/#parameters_67","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service body UpdateNamespaceServiceBody UpdateNamespaceServiceBody \u2713 Payload that contains information on service revision","title":"Parameters"},{"location":"api/#all-responses_72","text":"Code Status Description Has headers Schema 200 OK successfully created service revision schema","title":"All responses"},{"location":"api/#responses_72","text":"","title":"Responses"},{"location":"api/#200-successfully-created-service-revision_1","text":"Status: OK","title":" 200 - successfully created service revision"},{"location":"api/#schema_87","text":"","title":" Schema"},{"location":"api/#inlined-models_14","text":"UpdateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods trafficPercent integer int64 \u2713 Traffic percentage new revision will use","title":"Inlined models"},{"location":"api/#update-namespace-service-traffic-updatenamespaceservicetraffic","text":"PATCH /api/functions/namespaces/{namespace}/function/{serviceName} Update Namespace Service traffic directed to each revision, traffic can only be configured between two revisions. All other revisions will bet set to 0 traffic.","title":" Update Namespace Service Traffic (updateNamespaceServiceTraffic)"},{"location":"api/#parameters_68","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service Traffic body UpdateNamespaceServiceTrafficBody UpdateNamespaceServiceTrafficBody \u2713 Payload that contains information on service traffic","title":"Parameters"},{"location":"api/#all-responses_73","text":"Code Status Description Has headers Schema 200 OK successfully updated service traffic schema","title":"All responses"},{"location":"api/#responses_73","text":"","title":"Responses"},{"location":"api/#200-successfully-updated-service-traffic_1","text":"Status: OK","title":" 200 - successfully updated service traffic"},{"location":"api/#schema_88","text":"","title":" Schema"},{"location":"api/#inlined-models_15","text":"UpdateNamespaceServiceTrafficBody Properties Name Type Go type Required Default Description Example values [] UpdateNamespaceServiceTrafficParamsBodyValuesItems0 []*models.UpdateNamespaceServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateNamespaceServiceTrafficParamsBodyValuesItems0 Properties Name Type Go type Required Default Description Example percent integer int64 Target traffice percentage revision string string Target service revision","title":"Inlined models"},{"location":"api/#update-a-workflow-updateworkflow","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow Updates a workflow at the target path. The body of this request should contain the workflow yaml you want to update to.","title":" Update a Workflow (updateWorkflow)"},{"location":"api/#consumes_6","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_69","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow workflow data body string string Payload that contains the updated direktiv workflow yaml.","title":"Parameters"},{"location":"api/#all-responses_74","text":"Code Status Description Has headers Schema 200 OK successfully updated workflow schema","title":"All responses"},{"location":"api/#responses_74","text":"","title":"Responses"},{"location":"api/#200-successfully-updated-workflow","text":"Status: OK","title":" 200 - successfully updated workflow"},{"location":"api/#schema_89","text":"","title":" Schema"},{"location":"api/#returns-version-information-for-servers-in-the-cluster-version","text":"GET /api/version Returns version information for servers in the cluster.","title":" Returns version information for servers in the cluster. (version)"},{"location":"api/#all-responses_75","text":"Code Status Description Has headers Schema 200 OK version query was successful schema","title":"All responses"},{"location":"api/#responses_75","text":"","title":"Responses"},{"location":"api/#200-version-query-was-successful","text":"Status: OK","title":" 200 - version query was successful"},{"location":"api/#schema_90","text":"","title":" Schema"},{"location":"api/#watch-global-service-revision-watchglobalservicerevision","text":"GET /api/functions/{serviceName}/revisions/{revisionGeneration} Watch a global scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Watch Global Service Revision (watchGlobalServiceRevision)"},{"location":"api/#produces_1","text":"text/event-stream","title":"Produces"},{"location":"api/#parameters_70","text":"Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_76","text":"Code Status Description Has headers Schema 200 OK successfully watching service revision schema","title":"All responses"},{"location":"api/#responses_76","text":"","title":"Responses"},{"location":"api/#200-successfully-watching-service-revision","text":"Status: OK","title":" 200 - successfully watching service revision"},{"location":"api/#schema_91","text":"","title":" Schema"},{"location":"api/#watch-global-service-revision-list-watchglobalservicerevisionlist","text":"GET /api/functions/{serviceName}/revisions Watch the revision list of a global scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Watch Global Service Revision List (watchGlobalServiceRevisionList)"},{"location":"api/#produces_2","text":"text/event-stream","title":"Produces"},{"location":"api/#parameters_71","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_77","text":"Code Status Description Has headers Schema 200 OK successfully watching service revisions schema","title":"All responses"},{"location":"api/#responses_77","text":"","title":"Responses"},{"location":"api/#200-successfully-watching-service-revisions","text":"Status: OK","title":" 200 - successfully watching service revisions"},{"location":"api/#schema_92","text":"","title":" Schema"},{"location":"api/#watch-namespace-service-revision-watchnamespaceservicerevision","text":"GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Watch a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Watch Namespace Service Revision (watchNamespaceServiceRevision)"},{"location":"api/#produces_3","text":"text/event-stream","title":"Produces"},{"location":"api/#parameters_72","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_78","text":"Code Status Description Has headers Schema 200 OK successfully watching service revision schema","title":"All responses"},{"location":"api/#responses_78","text":"","title":"Responses"},{"location":"api/#200-successfully-watching-service-revision_1","text":"Status: OK","title":" 200 - successfully watching service revision"},{"location":"api/#schema_93","text":"","title":" Schema"},{"location":"api/#watch-namespace-service-revision-list-watchnamespaceservicerevisionlist","text":"GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions Watch the revision list of a namespace scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Watch Namespace Service Revision List (watchNamespaceServiceRevisionList)"},{"location":"api/#produces_4","text":"text/event-stream","title":"Produces"},{"location":"api/#parameters_73","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_79","text":"Code Status Description Has headers Schema 200 OK successfully watching service revisions schema","title":"All responses"},{"location":"api/#responses_79","text":"","title":"Responses"},{"location":"api/#200-successfully-watching-service-revisions_1","text":"Status: OK","title":" 200 - successfully watching service revisions"},{"location":"api/#schema_94","text":"","title":" Schema"},{"location":"api/#gets-invoked-workflow-metrics-workflowmetricsinvoked","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked Get metrics of invoked workflow instances.","title":" Gets Invoked Workflow Metrics (workflowMetricsInvoked)"},{"location":"api/#parameters_74","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_80","text":"Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema","title":"All responses"},{"location":"api/#responses_80","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-metrics","text":"Status: OK","title":" 200 - successfully got workflow metrics"},{"location":"api/#schema_95","text":"","title":" Schema"},{"location":"api/#gets-workflow-time-metrics-workflowmetricsmilliseconds","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed Get the timing metrics of a workflow's instance. This returns a total sum of the milliseconds a workflow has been executed for.","title":" Gets Workflow Time Metrics (workflowMetricsMilliseconds)"},{"location":"api/#parameters_75","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_81","text":"Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema","title":"All responses"},{"location":"api/#responses_81","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-metrics_1","text":"Status: OK","title":" 200 - successfully got workflow metrics"},{"location":"api/#schema_96","text":"","title":" Schema"},{"location":"api/#gets-a-workflow-state-time-metrics-workflowmetricsstatemilliseconds","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds Get the state timing metrics of a workflow's instance. This returns the timing of individual states in a workflow.","title":" Gets a Workflow State Time Metrics (workflowMetricsStateMilliseconds)"},{"location":"api/#parameters_76","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_82","text":"Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema","title":"All responses"},{"location":"api/#responses_82","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-metrics_2","text":"Status: OK","title":" 200 - successfully got workflow metrics"},{"location":"api/#schema_97","text":"","title":" Schema"},{"location":"api/#gets-successful-workflow-metrics-workflowmetricssuccessful","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful Get metrics of a workflow, where the instance was successful.","title":" Gets Successful Workflow Metrics (workflowMetricsSuccessful)"},{"location":"api/#parameters_77","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_83","text":"Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema","title":"All responses"},{"location":"api/#responses_83","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-metrics_3","text":"Status: OK","title":" 200 - successfully got workflow metrics"},{"location":"api/#schema_98","text":"","title":" Schema"},{"location":"api/#models","text":"","title":"Models"},{"location":"api/#createglobalprivateregistrybody","text":"CreateGlobalPrivateRegistryBody create global private registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL","title":" CreateGlobalPrivateRegistryBody"},{"location":"api/#createglobalregistrybody","text":"CreateGlobalRegistryBody create global registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL","title":" CreateGlobalRegistryBody"},{"location":"api/#createglobalservicebody","text":"CreateGlobalServiceBody create global service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v12\",\"minScale\":\"1\",\"name\":\"fast-request\",\"size\":\"small\"} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Name string string \u2713 Name of new service Size string string \u2713 Size of created service pods","title":" CreateGlobalServiceBody"},{"location":"api/#createnamespaceservicebody","text":"CreateNamespaceServiceBody create namespace service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v12\",\"minScale\":\"1\",\"name\":\"fast-request\",\"size\":\"small\"} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Name string string \u2713 Name of new service Size string string \u2713 Size of created service pods","title":" CreateNamespaceServiceBody"},{"location":"api/#createregistrybody","text":"CreateRegistryBody create registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL","title":" CreateRegistryBody"},{"location":"api/#deleteglobalprivateregistrybody","text":"DeleteGlobalPrivateRegistryBody delete global private registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL","title":" DeleteGlobalPrivateRegistryBody"},{"location":"api/#deleteglobalregistrybody","text":"DeleteGlobalRegistryBody delete global registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL","title":" DeleteGlobalRegistryBody"},{"location":"api/#deleteregistrybody","text":"DeleteRegistryBody delete registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL","title":" DeleteRegistryBody"},{"location":"api/#errorresponse","text":"Properties Name Type Go type Required Default Description Example Error string string StatusCode int64 (formatted integer) int64","title":" ErrorResponse"},{"location":"api/#jqplaygroundbody","text":"JqPlaygroundBody jq playground body Example {\"data\":\"eyJhIjogMSwgImIiOiAyLCAiYyI6IDQsICJkIjogN30=\",\"query\":\"map(select(. \\u003e= 2))\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 JSON data encoded in base64 Query string string \u2713 jq query to manipulate JSON data","title":" JqPlaygroundBody"},{"location":"api/#okbody","text":"OkBody OkBody is an arbitrary placeholder response that represents an ok response body OkBody","title":" OkBody"},{"location":"api/#setnamespaceconfigbody","text":"SetNamespaceConfigBody set namespace config body Example {\"broadcast\":{\"directory.create\":false,\"directory.delete\":false,\"instance.failed\":false,\"instance.started\":false,\"instance.success\":false,\"instance.variable.create\":false,\"instance.variable.delete\":false,\"instance.variable.update\":false,\"namespace.variable.create\":false,\"namespace.variable.delete\":false,\"namespace.variable.update\":false,\"workflow.create\":false,\"workflow.delete\":false,\"workflow.update\":false,\"workflow.variable.create\":false,\"workflow.variable.delete\":false,\"workflow.variable.update\":false}} Properties Name Type Go type Required Default Description Example Broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace","title":" SetNamespaceConfigBody"},{"location":"api/#setworkflowcloudeventlogsbody","text":"SetWorkflowCloudEventLogsBody set workflow cloud event logs body Example {\"logger\":\"mylog\"} Properties Name Type Go type Required Default Description Example Logger string string \u2713 Target Cloud Event","title":" SetWorkflowCloudEventLogsBody"},{"location":"api/#toggleworkflowbody","text":"ToggleWorkflowBody toggle workflow body Example {\"live\":false} Properties Name Type Go type Required Default Description Example Live boolean bool \u2713 Workflow live status","title":" ToggleWorkflowBody"},{"location":"api/#updateglobalservicebody","text":"UpdateGlobalServiceBody update global service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v10\",\"minScale\":\"1\",\"size\":\"small\",\"trafficPercent\":50} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Size string string \u2713 Size of created service pods TrafficPercent int64 (formatted integer) int64 \u2713 Traffic percentage new revision will use","title":" UpdateGlobalServiceBody"},{"location":"api/#updateglobalservicetrafficbody","text":"UpdateGlobalServiceTrafficBody update global service traffic body Example {\"values\":[{\"percent\":60,\"revision\":\"global-fast-request-00002\"},{\"percent\":40,\"revision\":\"global-fast-request-00001\"}]} Properties Name Type Go type Required Default Description Example Values [] UpdateGlobalServiceTrafficParamsBodyValuesItems0 []*UpdateGlobalServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets","title":" UpdateGlobalServiceTrafficBody"},{"location":"api/#updateglobalservicetrafficparamsbodyvaluesitems0","text":"UpdateGlobalServiceTrafficParamsBodyValuesItems0 update global service traffic params body values items0 Properties Name Type Go type Required Default Description Example Percent int64 (formatted integer) int64 Target traffice percentage Revision string string Target service revision","title":" UpdateGlobalServiceTrafficParamsBodyValuesItems0"},{"location":"api/#updatenamespaceservicebody","text":"UpdateNamespaceServiceBody update namespace service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v10\",\"minScale\":\"1\",\"size\":\"small\",\"trafficPercent\":50} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Size string string \u2713 Size of created service pods TrafficPercent int64 (formatted integer) int64 \u2713 Traffic percentage new revision will use","title":" UpdateNamespaceServiceBody"},{"location":"api/#updatenamespaceservicetrafficbody","text":"UpdateNamespaceServiceTrafficBody update namespace service traffic body Example {\"values\":[{\"percent\":60,\"revision\":\"namespace-direktiv-fast-request-00002\"},{\"percent\":40,\"revision\":\"namespace-direktiv-fast-request-00001\"}]} Properties Name Type Go type Required Default Description Example Values [] UpdateNamespaceServiceTrafficParamsBodyValuesItems0 []*UpdateNamespaceServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets","title":" UpdateNamespaceServiceTrafficBody"},{"location":"api/#updatenamespaceservicetrafficparamsbodyvaluesitems0","text":"UpdateNamespaceServiceTrafficParamsBodyValuesItems0 update namespace service traffic params body values items0 Properties Name Type Go type Required Default Description Example Percent int64 (formatted integer) int64 Target traffice percentage Revision string string Target service revision","title":" UpdateNamespaceServiceTrafficParamsBodyValuesItems0"},{"location":"api/#updateservicerequest","text":"UpdateServiceRequest UpdateServiceRequest UpdateServiceRequest update service request Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 image MinScale int32 (formatted integer) int32 \u2713 minScale Size int32 (formatted integer) int32 \u2713 size TrafficPercent int64 (formatted integer) int64 \u2713 trafficPercent","title":" UpdateServiceRequest"},{"location":"api/#updateservicerequest_1","text":"UpdateServiceRequest UpdateServiceRequest update service request Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 image MinScale int32 (formatted integer) int32 \u2713 minScale Size int32 (formatted integer) int32 \u2713 size TrafficPercent int64 (formatted integer) int64 \u2713 trafficPercent","title":" updateServiceRequest"},{"location":"hidden/","text":"Title Text here Sub Title Text here Sub Title 2 Text here Note This is a note Info This is a info Tip This is a tip Warning This is a tip Example This is a tip Danger This is a tip bubble_sort.py def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Tab 1 Tab 2 Tab1 Tab2","title":"Title"},{"location":"hidden/#title","text":"Text here","title":"Title"},{"location":"hidden/#sub-title","text":"Text here","title":"Sub Title"},{"location":"hidden/#sub-title-2","text":"Text here Note This is a note Info This is a info Tip This is a tip Warning This is a tip Example This is a tip Danger This is a tip bubble_sort.py def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Tab 1 Tab 2 Tab1 Tab2","title":"Sub Title 2"},{"location":"specification/","text":"Specification Workflow Overview Diagram The diagram below captures the workflow definition specification. This is to be used as a reference only, the full specification is described in detail in the sections below. Workflow Definition Parameter Description Type Required id Workflow unique identifier. string yes name Workflow name (metadata). string no description Workflow description (metadata). string no version Version information. string no singular Attempts to invoke this workflow will fail when an instance is running. no bool functions Workflow function definitions. []FunctionDefinition no states Workflow states. []StateDefinition no timeouts Workflow global timeouts. TimeoutDefinition no start Workflow start configuration. Start no FunctionDefinition GlobalFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"knative-global\"). string yes service The service being referenced. string yes files Workflow file definition. []FunctionFileDefinition no NamespacedFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"knative-namespace\"). string yes service The service being referenced. string yes files Workflow file definition. []FunctionFileDefinition no ReusableFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"reusable\"). string yes image Image URI. string yes files Workflow file definition. []FunctionFileDefinition no cmd Command to run in container string no size Size of virtual machine enum no scale Minimum number of instances int no A reusable function can be defined in three different sizes: \" small \"(default), \" medium \", and \" large \". These sizes control how much cpu, memory and storage a virtual machine is given for a function when their virtual machine is created. The default value for \" scale \" is 0 which means the service will be removed after a ceratin amount of time. It defines the minimum number of containers to run for this services if it is greater than 0. Size CPU Memory Storage small 1 256 MB +64 MB medium 1 512 MB +64 MB large 2 1024 MB +64 MB SubflowFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"subflow\"). enum yes workflow ID of workflow within the same namespace. string yes IsolatedFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"isolated\"). string yes image Image URI. string yes files Workflow file definition. []FunctionFileDefinition no cmd Command to run in container string no size Size of virtual machine enum no scale Minimum number of instances int no An isolated function can be defined in three different sizes: \" small \"(default), \" medium \", and \" large \". These sizes control how much cpu, memory and storage a virtual machine is given for a function when their virtual machine is created. The default value for \" scale \" is 0 which means the service will be removed after a ceratin amount of time. It defines the minimum number of containers to run for this services if it is greater than 0. Size CPU Memory Storage small 1 256 MB +64 MB medium 1 512 MB +64 MB large 2 1024 MB +64 MB FunctionFileDefinition Parameter Description Type Required key Key used to select variable. string yes scope Scope used to select variable. Defaults to 'instance', but can be 'workflow' or 'namespace'. string no as Set the filename of the file. The default is the same as the key. string no type How to treat the file. Options include 'plain', 'base64', 'tar', 'tar.gz'. string no States Common Fields Parameter Description Type Required id State unique identifier. string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no log jq command to generate data for instance-logging. string no catch Error handling. []ErrorDefinition no The id field must be unique amongst all states in the workflow, and may consist of only alphanumeric characters as well as periods, dashes, and underscores. The transform field can be a jq command string applied to the state information in order to enrich, filter, or change it. Whatever the command resolves to will completely replace the state's information. The transform will be applied immediately before the transition , so it won't change the state information before the main function of the state is performed. The transition , if provided, must be set to the id of a state within the workflow. If left unspecified, reaching this transition will end the workflow without raising an error. ErrorDefinition Parameter Description Type Required error A glob pattern to test error codes for a match. string yes transition State to transition to next. string no The error parameter can be a glob pattern to match multiple types of errors. When an error is thrown it will be compared against each ErrorDefinition in order until it finds a match. If no matches are found the workflow will immediately abort and escalate the error to any caller, unless the retry policy is ready to take over. ActionState Parameter Description Type Required id State unique identifier. string yes type State type (\"action\"). string yes action Action to perform. ActionDefinition yes async If workflow execution can continue without waiting for the action to return. boolean no timeout Duration to wait for action to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no ActionDefinition Parameter Description Type Required function Name of the referenced function. string yes input jq command to generate the input for the action. string no secrets List of secrets to temporarily add to the state data under .secrets before running the input jq command. []string no retries Retry policy. RetryDefinition no RetryDefinition Parameter Description Type Required max_attempts Maximum number of retry attempts. int yes delay Time delay between retry attempts (ISO8601). string no multiplier Value by which the delay is multiplied after each attempt. float no codes Regex patterns to specify which error codes to catch. []string yes If a retry strategy is defined, the action will be retried on any failures that statify any of the regex codes . If the retry fails max_attempts times a direktiv.retries.exceeded error will be thrown. An example definition - id : insert-into-database type : action action : function : insert-into-database-function input : customer : jq(.customer) The Action State runs another workflow as a subflow, or a function as defined in the functions section of the workflow definition. Functions may include things such as containers or direktiv virtual-machines. The input for the action is determined by an optional jq command in the input field. If unspecified, the default command is \".\" , which duplicates the entire state data. After the action has returned, whatever the results were will be stored in the state information under return . If an error occurred, it will be automatically raised, and can be handled using catch , or ignored if the desired behaviour is to abort the workflow. If async is true , the workflow will not wait for it to return before transitioning to the next state. The action will be fire-and-forget, and considered completely detached from the calling workflow. In this case, the Action State will not set the return value. ConsumeEventState Parameter Description Type Required id State unique identifier. string yes type State type (\"consumeEvent\"). string yes event Event to consume. ConsumeEventDefinition yes timeout Duration to wait to receive event (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no ConsumeEventDefinition Parameter Description Type Required type CloudEvent type. string yes context Key-value pairs for CloudEvent context values that must match. object no An example definition - id : wait-for-booking type : consumeEvent event : type : guestbooking context : source : 'bookings.*' customerId : jq(.customerId) venue : Sydney timeout : PT1H transform : jq(.customer) transition : add-booking-to-database The ConsumeEvent State is the simplest state you can use to listen for CloudEvents in the middle of a workflow (for triggering a workflow when receiving an event, see Start ). More complex event consumers include EventXor State and the EventAnd State . When a workflow reaches a ConsumeEvent State it will halt its execution until it receives a matching event, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving a matching event a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The event payload will stored at a variable with the same name as the event's type . If the payload is not valid JSON it will be base64 encoded as a string first. DelayState Parameter Description Type Required id State unique identifier. string yes type State type (\"delay\"). string yes duration Duration to delay (ISO8601). string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : sleep type : delay duration : PT1H transition : fetch-data The Delay State pauses execution of the workflow for a predefined length of time. ErrorState Parameter Description Type Required id State unique identifier. string yes type State type (\"error\"). string yes error Error code, catchable on a calling workflow. string yes message Format string to provide more context to the error. string yes args A list of jq commands to generate arguments for substitution in the message format string. []string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : error-out-of-date type : error error : validation.outOfDate message : \"food item %s is out of date\" args : - jq(.item.name) The Error State allows a subflow to throw an error, catchable by the calling workflow. The first transition to an Error State anywhere within the workflow means that a waiting caller -- if one exists -- will receive that error after this subflow returns. This doesn't prevent the Error State from transitioning to other states, which might be necessary to clean up or undo actions performed by the workflow. Subsequent transitions into Error States after the first have no effect. An error consists of two parts: an error code, and an error message. The code should be a short string can can contain alphanumeric characters, periods, dashes, and underscores. It is good practice to structure error codes similar to domain names, to make them easier to handle. The message allows you to provide extra context, and can be formatted like a printf string where each entry in args will be substituted. The args must be jq commands, allowing the state to insert state information into the error message. Alternatively the message can be generated with jq commands e.g. message: This is an error jq(.myvalue) . EventAndState Parameter Description Type Required id State unique identifier. string yes type State type (\"eventAnd\"). string yes events Events to consume. []ConsumeEventDefinition yes timeout Duration to wait to receive all events (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : event-and type : eventAnd timeout : PT1H transition : done events : - event : type : purchasePaid context : source : 'purchase.*' customerId : jq(.customerId) country : Australia - event : type : purchaseSent context : source : 'purchase.*' customerId : jq(.customerId) country : Australia When a workflow reaches an EventAnd State it will halt its execution until it receives a matching event for every event in its events list, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving matches for all required events a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The event payloads will stored in variables with the same names as each event's type . If a payload is not valid JSON it will be base64 encoded as a string first. EventXorState Parameter Description Type Required id State unique identifier. string yes type State type (\"eventXor\"). string yes events Events to consume, and what to do based on which event was received. []EventConditionDefinition yes timeout Duration to wait to receive event (ISO8601). string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no EventConditionDefinition Parameter Description Type Required event Event to consume. ConsumeEventDefinition yes transition State to transition to if this branch is selected. string no transform jq command to transform the state's data output. string no An example definition - id : event-xor type : eventXor timeout : PT1H events : - transition : \"reservation-accept\" event : type : reservationAccept context : source : \"reservation.*\" guestName : jq(.guestName) venue : \"Compu Global HMN\" - transition : \"reservation-decline\" event : type : reservationDecline context : source : \"reservation.*\" guestName : jq(.guestName) venue : \"Compu Global HMN\" When a workflow reaches an EventXor State it will halt its execution until it receives any matching event in its events list, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving matches for any required event a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The received event payload will stored in a variable with the same name as its event type . If a payload is not valid JSON it will be base64 encoded as a string first. ForeachState Parameter Description Type Required id State unique identifier. string yes type State type (\"foreach\"). string yes array jq command to produce an array of objects to loop through. string yes action Action to perform. ActionDefinition yes timeout Duration to wait for all actions to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no The ForeachState can be used to split up state data into an array and then perform an action on each element in parallel. The jq command provided in the array must produce an array or a direktiv.foreachInput error will be thrown. The jq command used to generate the input for the action will be applied to a single element from that array. The return values of each action will be included in an array stored at .return at the same index from which its input was generated. GenerateEventState Parameter Description Type Required id State unique identifier. string yes type State type (\"generateEvent\"). string yes event Event to generate. GenerateEventDefinition yes transform jq command to transform the state's data output. string no delay Duration to wait before publishing event (ISO8601). string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no GenerateEventDefinition Parameter Description Type Required type CloudEvent type. string yes source CloudEvent source. string yes data A jq command to generate the data (payload) for the produced event. string no datacontenttype An RFC 2046 string specifying the payload content type. string no context Add additional event extension context attributes (key-value). object no The GenerateEvent State will produce an event that other workflows could listen for. If the optional datacontenttype is defined and set to something other than application/json , and the jq command defined in data produces a base64 encoded string, it will be decoded before being used as the event payload. GetterState Parameter Description Type Required id State unique identifier. string yes type State type (\"getter\"). string yes variables Variables to fetch. []VariableGetterDefinition yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no VariableGetterDefinition Parameter Description Type Required key Variable name. string yes scope Variable scope (\"instance\", \"workflow\", or \"namespace\"). string yes The getter state is used to retrieve persistent data. NoopState Parameter Description Type Required id State unique identifier. string yes type State type (\"noop\"). string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : hello type : noop transform : message : \"Hello\" transition : world The No-op State exists for when nothing more than generic state functionality is required. A common use-case would be to perform a jq operation on the state data without performing another operation. ParallelState Parameter Description Type Required id State unique identifier. string yes type State type (\"parallel\"). string yes actions Actions to perform. []ActionDefinition yes mode Option types on how to complete branch execution: \"and\" (default), or \"or\". enum no timeout Duration to wait for all actions to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no catch Error handling. []ErrorDefinition no The Parallel State is an expansion on the Action State , used for running multiple actions in parallel. The state can operate in two different modes: and and or . In and mode all actions must return successfully before completing. In or mode the state can complete as soon as any one action returns without error. Return values from each of the actions will be stored in an array at .return in the order that each action is defined. If an action doesn't return but the state can still complete without errors any missing return values will be null in the array. If the timeout is reached before the state can transition a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . Any actions still running when the state transitions will be cancelled with \"best effort\" attempts. SetterState Parameter Description Type Required id State unique identifier. string yes type State type (\"setter\"). string yes variables Variables to push. []VariableSetterDefinition yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no VariableSetterDefinition Parameter Description Type Required key Variable name. string yes scope Variable scope (\"instance\", \"workflow\", or \"namespace\"). string yes value jq command to generate variable value. string yes mimeType MimeType to store variable value as. string no The setter state is used to store persistent data. A mimeType type can be provided to specify the type of content a variable is. If mimeType is not provided it will default to application/json . There are three mimeType's that are specifcically handled: * application/json - Default behaviour, value is treated as a json object. * text/plain - Value is treated as a plaintext string, no json marshalling is done. * application/octet-stream - Value is expected to be a base64 string and is stored as its decoded binary value. Read more about mimeTyps in the Examples . SwitchState Parameter Description Type Required id State unique identifier. string yes type State type (\"switch\"). string yes conditions Conditions to evaluate and determine which state to transition to next. []SwitchConditionDefinition yes defaultTransition State to transition to next if no conditions are matched. string no defaultTransform jq command to transform the state's data output. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no SwitchConditionDefinition Parameter Description Type Required condition jq command evaluated against state data. True if results are not empty. string yes transition State to transition to if this branch is selected. string no transform jq command to transform the state's data output. string no An example definition - id : decision type : switch conditions : - condition : jq(.patient.contactInfo.mobile) transition : sms transform : 'jq(. + { phone: .contact.mobile)' - condition : jq(.patient.contactInfo.landline) transition : call transform : 'jq(. + { phone: .contact.landline })' defaultTransition : email The Switch State is used to perform conditional transitions based on the current state information. A condition can be any jq command. The command will be run on the current state information and a result of anything other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a match. The list of conditions is evaluated in-order and the first match determines what happens next. If no conditions are matched the defaultTransition will be used. ValidateState Parameter Description Type Required id State unique identifier. string yes type State type (\"validate\"). string yes subject jq command to select the subject of the schema validation. Defaults to '.' if unspecified. no string schema Name of the referenced state data schema. string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : validate-input type : validate schema : type : object required : - name properties : name : type : string additionalProperties : false transition : process-request This schema is based off the following JSON Schema: { \"type\" : \"object\" , \"required\" :[ \"name\" ], \"properties\" :{ \"name\" :{ \"type\" : \"string\" } }, \"additionalProperties\" : false } The Validate State can be used to validate the structure of the state's data. The schema field takes a yaml-ified representation of a JSON Schema document. TimeoutDefinition Parameter Description Type Required interrupt Duration to wait before triggering a timeout error in the workflow (ISO8601). string no kill Duration to wait before killing the workflow (ISO8601). string no Start ScheduledStartDefinition Parameter Description Type Required type Start type (\"scheduled\"). string yes state ID of the state to use as the start state. string no cron Cron expression to schedule workflow. string no EventStartDefinition Parameter Description Type Required type Start type (\"event\"). string yes state ID of the state to use as the start state. string no event Event to listen for, which can trigger the workflow. StartEventDefinition yes StartEventDefinition Parameter Description Type Required type CloudEvent type. string yes filters Key-value regex pairs for CloudEvent context values that must match. object no EventsXorStartDefinition Parameter Description Type Required type Start type (\"eventsXor\"). string yes state ID of the state to use as the start state. string no events Event to listen for, which can trigger the workflow. []StartEventDefinition yes EventsAndStartDefinition Parameter Description Type Required type Start type (\"eventsAnd\"). string yes state ID of the state to use as the start state. string no events Event to listen for, which can trigger the workflow. []StartEventDefinition yes lifespan Maximum duration an event can be stored before being discarded while waiting for other events (ISO8601). string no correlate Context keys that must exist on every event and have matching values to be grouped together. []string no","title":"Specification"},{"location":"specification/#specification","text":"","title":"Specification"},{"location":"specification/#workflow-overview-diagram","text":"The diagram below captures the workflow definition specification. This is to be used as a reference only, the full specification is described in detail in the sections below.","title":"Workflow Overview Diagram"},{"location":"specification/#workflow-definition","text":"Parameter Description Type Required id Workflow unique identifier. string yes name Workflow name (metadata). string no description Workflow description (metadata). string no version Version information. string no singular Attempts to invoke this workflow will fail when an instance is running. no bool functions Workflow function definitions. []FunctionDefinition no states Workflow states. []StateDefinition no timeouts Workflow global timeouts. TimeoutDefinition no start Workflow start configuration. Start no","title":"Workflow Definition"},{"location":"specification/#functiondefinition","text":"","title":"FunctionDefinition"},{"location":"specification/#globalfunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"knative-global\"). string yes service The service being referenced. string yes files Workflow file definition. []FunctionFileDefinition no","title":"GlobalFunctionDefinition"},{"location":"specification/#namespacedfunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"knative-namespace\"). string yes service The service being referenced. string yes files Workflow file definition. []FunctionFileDefinition no","title":"NamespacedFunctionDefinition"},{"location":"specification/#reusablefunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"reusable\"). string yes image Image URI. string yes files Workflow file definition. []FunctionFileDefinition no cmd Command to run in container string no size Size of virtual machine enum no scale Minimum number of instances int no A reusable function can be defined in three different sizes: \" small \"(default), \" medium \", and \" large \". These sizes control how much cpu, memory and storage a virtual machine is given for a function when their virtual machine is created. The default value for \" scale \" is 0 which means the service will be removed after a ceratin amount of time. It defines the minimum number of containers to run for this services if it is greater than 0. Size CPU Memory Storage small 1 256 MB +64 MB medium 1 512 MB +64 MB large 2 1024 MB +64 MB","title":"ReusableFunctionDefinition"},{"location":"specification/#subflowfunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"subflow\"). enum yes workflow ID of workflow within the same namespace. string yes","title":"SubflowFunctionDefinition"},{"location":"specification/#isolatedfunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"isolated\"). string yes image Image URI. string yes files Workflow file definition. []FunctionFileDefinition no cmd Command to run in container string no size Size of virtual machine enum no scale Minimum number of instances int no An isolated function can be defined in three different sizes: \" small \"(default), \" medium \", and \" large \". These sizes control how much cpu, memory and storage a virtual machine is given for a function when their virtual machine is created. The default value for \" scale \" is 0 which means the service will be removed after a ceratin amount of time. It defines the minimum number of containers to run for this services if it is greater than 0. Size CPU Memory Storage small 1 256 MB +64 MB medium 1 512 MB +64 MB large 2 1024 MB +64 MB","title":"IsolatedFunctionDefinition"},{"location":"specification/#functionfiledefinition","text":"Parameter Description Type Required key Key used to select variable. string yes scope Scope used to select variable. Defaults to 'instance', but can be 'workflow' or 'namespace'. string no as Set the filename of the file. The default is the same as the key. string no type How to treat the file. Options include 'plain', 'base64', 'tar', 'tar.gz'. string no","title":"FunctionFileDefinition"},{"location":"specification/#states","text":"","title":"States"},{"location":"specification/#common-fields","text":"Parameter Description Type Required id State unique identifier. string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no log jq command to generate data for instance-logging. string no catch Error handling. []ErrorDefinition no The id field must be unique amongst all states in the workflow, and may consist of only alphanumeric characters as well as periods, dashes, and underscores. The transform field can be a jq command string applied to the state information in order to enrich, filter, or change it. Whatever the command resolves to will completely replace the state's information. The transform will be applied immediately before the transition , so it won't change the state information before the main function of the state is performed. The transition , if provided, must be set to the id of a state within the workflow. If left unspecified, reaching this transition will end the workflow without raising an error.","title":"Common Fields"},{"location":"specification/#errordefinition","text":"Parameter Description Type Required error A glob pattern to test error codes for a match. string yes transition State to transition to next. string no The error parameter can be a glob pattern to match multiple types of errors. When an error is thrown it will be compared against each ErrorDefinition in order until it finds a match. If no matches are found the workflow will immediately abort and escalate the error to any caller, unless the retry policy is ready to take over.","title":"ErrorDefinition"},{"location":"specification/#actionstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"action\"). string yes action Action to perform. ActionDefinition yes async If workflow execution can continue without waiting for the action to return. boolean no timeout Duration to wait for action to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"ActionState"},{"location":"specification/#actiondefinition","text":"Parameter Description Type Required function Name of the referenced function. string yes input jq command to generate the input for the action. string no secrets List of secrets to temporarily add to the state data under .secrets before running the input jq command. []string no retries Retry policy. RetryDefinition no","title":"ActionDefinition"},{"location":"specification/#retrydefinition","text":"Parameter Description Type Required max_attempts Maximum number of retry attempts. int yes delay Time delay between retry attempts (ISO8601). string no multiplier Value by which the delay is multiplied after each attempt. float no codes Regex patterns to specify which error codes to catch. []string yes If a retry strategy is defined, the action will be retried on any failures that statify any of the regex codes . If the retry fails max_attempts times a direktiv.retries.exceeded error will be thrown.","title":"RetryDefinition"},{"location":"specification/#an-example-definition","text":"- id : insert-into-database type : action action : function : insert-into-database-function input : customer : jq(.customer) The Action State runs another workflow as a subflow, or a function as defined in the functions section of the workflow definition. Functions may include things such as containers or direktiv virtual-machines. The input for the action is determined by an optional jq command in the input field. If unspecified, the default command is \".\" , which duplicates the entire state data. After the action has returned, whatever the results were will be stored in the state information under return . If an error occurred, it will be automatically raised, and can be handled using catch , or ignored if the desired behaviour is to abort the workflow. If async is true , the workflow will not wait for it to return before transitioning to the next state. The action will be fire-and-forget, and considered completely detached from the calling workflow. In this case, the Action State will not set the return value.","title":"An example definition"},{"location":"specification/#consumeeventstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"consumeEvent\"). string yes event Event to consume. ConsumeEventDefinition yes timeout Duration to wait to receive event (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"ConsumeEventState"},{"location":"specification/#consumeeventdefinition","text":"Parameter Description Type Required type CloudEvent type. string yes context Key-value pairs for CloudEvent context values that must match. object no","title":"ConsumeEventDefinition"},{"location":"specification/#an-example-definition_1","text":"- id : wait-for-booking type : consumeEvent event : type : guestbooking context : source : 'bookings.*' customerId : jq(.customerId) venue : Sydney timeout : PT1H transform : jq(.customer) transition : add-booking-to-database The ConsumeEvent State is the simplest state you can use to listen for CloudEvents in the middle of a workflow (for triggering a workflow when receiving an event, see Start ). More complex event consumers include EventXor State and the EventAnd State . When a workflow reaches a ConsumeEvent State it will halt its execution until it receives a matching event, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving a matching event a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The event payload will stored at a variable with the same name as the event's type . If the payload is not valid JSON it will be base64 encoded as a string first.","title":"An example definition"},{"location":"specification/#delaystate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"delay\"). string yes duration Duration to delay (ISO8601). string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"DelayState"},{"location":"specification/#an-example-definition_2","text":"- id : sleep type : delay duration : PT1H transition : fetch-data The Delay State pauses execution of the workflow for a predefined length of time.","title":"An example definition"},{"location":"specification/#errorstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"error\"). string yes error Error code, catchable on a calling workflow. string yes message Format string to provide more context to the error. string yes args A list of jq commands to generate arguments for substitution in the message format string. []string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"ErrorState"},{"location":"specification/#an-example-definition_3","text":"- id : error-out-of-date type : error error : validation.outOfDate message : \"food item %s is out of date\" args : - jq(.item.name) The Error State allows a subflow to throw an error, catchable by the calling workflow. The first transition to an Error State anywhere within the workflow means that a waiting caller -- if one exists -- will receive that error after this subflow returns. This doesn't prevent the Error State from transitioning to other states, which might be necessary to clean up or undo actions performed by the workflow. Subsequent transitions into Error States after the first have no effect. An error consists of two parts: an error code, and an error message. The code should be a short string can can contain alphanumeric characters, periods, dashes, and underscores. It is good practice to structure error codes similar to domain names, to make them easier to handle. The message allows you to provide extra context, and can be formatted like a printf string where each entry in args will be substituted. The args must be jq commands, allowing the state to insert state information into the error message. Alternatively the message can be generated with jq commands e.g. message: This is an error jq(.myvalue) .","title":"An example definition"},{"location":"specification/#eventandstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"eventAnd\"). string yes events Events to consume. []ConsumeEventDefinition yes timeout Duration to wait to receive all events (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"EventAndState"},{"location":"specification/#an-example-definition_4","text":"- id : event-and type : eventAnd timeout : PT1H transition : done events : - event : type : purchasePaid context : source : 'purchase.*' customerId : jq(.customerId) country : Australia - event : type : purchaseSent context : source : 'purchase.*' customerId : jq(.customerId) country : Australia When a workflow reaches an EventAnd State it will halt its execution until it receives a matching event for every event in its events list, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving matches for all required events a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The event payloads will stored in variables with the same names as each event's type . If a payload is not valid JSON it will be base64 encoded as a string first.","title":"An example definition"},{"location":"specification/#eventxorstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"eventXor\"). string yes events Events to consume, and what to do based on which event was received. []EventConditionDefinition yes timeout Duration to wait to receive event (ISO8601). string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"EventXorState"},{"location":"specification/#eventconditiondefinition","text":"Parameter Description Type Required event Event to consume. ConsumeEventDefinition yes transition State to transition to if this branch is selected. string no transform jq command to transform the state's data output. string no","title":"EventConditionDefinition"},{"location":"specification/#an-example-definition_5","text":"- id : event-xor type : eventXor timeout : PT1H events : - transition : \"reservation-accept\" event : type : reservationAccept context : source : \"reservation.*\" guestName : jq(.guestName) venue : \"Compu Global HMN\" - transition : \"reservation-decline\" event : type : reservationDecline context : source : \"reservation.*\" guestName : jq(.guestName) venue : \"Compu Global HMN\" When a workflow reaches an EventXor State it will halt its execution until it receives any matching event in its events list, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving matches for any required event a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The received event payload will stored in a variable with the same name as its event type . If a payload is not valid JSON it will be base64 encoded as a string first.","title":"An example definition"},{"location":"specification/#foreachstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"foreach\"). string yes array jq command to produce an array of objects to loop through. string yes action Action to perform. ActionDefinition yes timeout Duration to wait for all actions to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no The ForeachState can be used to split up state data into an array and then perform an action on each element in parallel. The jq command provided in the array must produce an array or a direktiv.foreachInput error will be thrown. The jq command used to generate the input for the action will be applied to a single element from that array. The return values of each action will be included in an array stored at .return at the same index from which its input was generated.","title":"ForeachState"},{"location":"specification/#generateeventstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"generateEvent\"). string yes event Event to generate. GenerateEventDefinition yes transform jq command to transform the state's data output. string no delay Duration to wait before publishing event (ISO8601). string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"GenerateEventState"},{"location":"specification/#generateeventdefinition","text":"Parameter Description Type Required type CloudEvent type. string yes source CloudEvent source. string yes data A jq command to generate the data (payload) for the produced event. string no datacontenttype An RFC 2046 string specifying the payload content type. string no context Add additional event extension context attributes (key-value). object no The GenerateEvent State will produce an event that other workflows could listen for. If the optional datacontenttype is defined and set to something other than application/json , and the jq command defined in data produces a base64 encoded string, it will be decoded before being used as the event payload.","title":"GenerateEventDefinition"},{"location":"specification/#getterstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"getter\"). string yes variables Variables to fetch. []VariableGetterDefinition yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"GetterState"},{"location":"specification/#variablegetterdefinition","text":"Parameter Description Type Required key Variable name. string yes scope Variable scope (\"instance\", \"workflow\", or \"namespace\"). string yes The getter state is used to retrieve persistent data.","title":"VariableGetterDefinition"},{"location":"specification/#noopstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"noop\"). string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"NoopState"},{"location":"specification/#an-example-definition_6","text":"- id : hello type : noop transform : message : \"Hello\" transition : world The No-op State exists for when nothing more than generic state functionality is required. A common use-case would be to perform a jq operation on the state data without performing another operation.","title":"An example definition"},{"location":"specification/#parallelstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"parallel\"). string yes actions Actions to perform. []ActionDefinition yes mode Option types on how to complete branch execution: \"and\" (default), or \"or\". enum no timeout Duration to wait for all actions to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no catch Error handling. []ErrorDefinition no The Parallel State is an expansion on the Action State , used for running multiple actions in parallel. The state can operate in two different modes: and and or . In and mode all actions must return successfully before completing. In or mode the state can complete as soon as any one action returns without error. Return values from each of the actions will be stored in an array at .return in the order that each action is defined. If an action doesn't return but the state can still complete without errors any missing return values will be null in the array. If the timeout is reached before the state can transition a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . Any actions still running when the state transitions will be cancelled with \"best effort\" attempts.","title":"ParallelState"},{"location":"specification/#setterstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"setter\"). string yes variables Variables to push. []VariableSetterDefinition yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"SetterState"},{"location":"specification/#variablesetterdefinition","text":"Parameter Description Type Required key Variable name. string yes scope Variable scope (\"instance\", \"workflow\", or \"namespace\"). string yes value jq command to generate variable value. string yes mimeType MimeType to store variable value as. string no The setter state is used to store persistent data. A mimeType type can be provided to specify the type of content a variable is. If mimeType is not provided it will default to application/json . There are three mimeType's that are specifcically handled: * application/json - Default behaviour, value is treated as a json object. * text/plain - Value is treated as a plaintext string, no json marshalling is done. * application/octet-stream - Value is expected to be a base64 string and is stored as its decoded binary value. Read more about mimeTyps in the Examples .","title":"VariableSetterDefinition"},{"location":"specification/#switchstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"switch\"). string yes conditions Conditions to evaluate and determine which state to transition to next. []SwitchConditionDefinition yes defaultTransition State to transition to next if no conditions are matched. string no defaultTransform jq command to transform the state's data output. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"SwitchState"},{"location":"specification/#switchconditiondefinition","text":"Parameter Description Type Required condition jq command evaluated against state data. True if results are not empty. string yes transition State to transition to if this branch is selected. string no transform jq command to transform the state's data output. string no","title":"SwitchConditionDefinition"},{"location":"specification/#an-example-definition_7","text":"- id : decision type : switch conditions : - condition : jq(.patient.contactInfo.mobile) transition : sms transform : 'jq(. + { phone: .contact.mobile)' - condition : jq(.patient.contactInfo.landline) transition : call transform : 'jq(. + { phone: .contact.landline })' defaultTransition : email The Switch State is used to perform conditional transitions based on the current state information. A condition can be any jq command. The command will be run on the current state information and a result of anything other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a match. The list of conditions is evaluated in-order and the first match determines what happens next. If no conditions are matched the defaultTransition will be used.","title":"An example definition"},{"location":"specification/#validatestate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"validate\"). string yes subject jq command to select the subject of the schema validation. Defaults to '.' if unspecified. no string schema Name of the referenced state data schema. string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"ValidateState"},{"location":"specification/#an-example-definition_8","text":"- id : validate-input type : validate schema : type : object required : - name properties : name : type : string additionalProperties : false transition : process-request This schema is based off the following JSON Schema: { \"type\" : \"object\" , \"required\" :[ \"name\" ], \"properties\" :{ \"name\" :{ \"type\" : \"string\" } }, \"additionalProperties\" : false } The Validate State can be used to validate the structure of the state's data. The schema field takes a yaml-ified representation of a JSON Schema document.","title":"An example definition"},{"location":"specification/#timeoutdefinition","text":"Parameter Description Type Required interrupt Duration to wait before triggering a timeout error in the workflow (ISO8601). string no kill Duration to wait before killing the workflow (ISO8601). string no","title":"TimeoutDefinition"},{"location":"specification/#start","text":"","title":"Start"},{"location":"specification/#scheduledstartdefinition","text":"Parameter Description Type Required type Start type (\"scheduled\"). string yes state ID of the state to use as the start state. string no cron Cron expression to schedule workflow. string no","title":"ScheduledStartDefinition"},{"location":"specification/#eventstartdefinition","text":"Parameter Description Type Required type Start type (\"event\"). string yes state ID of the state to use as the start state. string no event Event to listen for, which can trigger the workflow. StartEventDefinition yes","title":"EventStartDefinition"},{"location":"specification/#starteventdefinition","text":"Parameter Description Type Required type CloudEvent type. string yes filters Key-value regex pairs for CloudEvent context values that must match. object no","title":"StartEventDefinition"},{"location":"specification/#eventsxorstartdefinition","text":"Parameter Description Type Required type Start type (\"eventsXor\"). string yes state ID of the state to use as the start state. string no events Event to listen for, which can trigger the workflow. []StartEventDefinition yes","title":"EventsXorStartDefinition"},{"location":"specification/#eventsandstartdefinition","text":"Parameter Description Type Required type Start type (\"eventsAnd\"). string yes state ID of the state to use as the start state. string no events Event to listen for, which can trigger the workflow. []StartEventDefinition yes lifespan Maximum duration an event can be stored before being discarded while waiting for other events (ISO8601). string no correlate Context keys that must exist on every event and have matching values to be grouped together. []string no","title":"EventsAndStartDefinition"},{"location":"environment/cli/","text":"Direktiv Sync Tool Although developing workflows with the web UI is easy, Direktiv's Sync Tool can be used to make local workflow development faster and more convenient. Installing The Direktiv Sync Tool is available for Linux, Windows, and Mac platforms and is distributed as a tar.gz file with every new release of the software. The asset can be downloaded and unpacked to get the direktiv-sync binary. Linux Mac Windows Linux Install Example curl -L https://github.com/direktiv/direktiv/releases/latest/download/direktiv_sync_amd64.tar.gz | tar -xz && \\ sudo mv direktiv-sync /usr/local/bin Setting up a Namespace Working with this tool assumes that you create a directory which is mirroring a namespace in Direktiv. This directory should be empty at start. The first thing to setup is the connectivity to Direktiv. For this a .direktiv.yaml file has to be created within this directory. This file needs the api key or token, the address of the Direktiv instance and the namespace it should use. auth-token : my-api-key-token addr : https://my-direktiv.server namespace : direktiv This folder is now the base for a namespace direktiv . The path from here is relative in Direktiv. This means the folder structure will be the same as the folder structure in Direktiv. Pushing and Executing At the moment there is one limitation working with this tool. It can not pull the namespace from Direktiv. For this functionality the Git integration has to be used. After setup there are two commands available. The push command pushes a workflow to Direktiv but does not execute it. This command works recursively e.g. direktiv-sync push . . The exec command uploads and executes the workflow. During execution the logs are printed to stdout . Examples direktiv-sync push myworkflow.yaml direktiv-sync push myfolder/ direktiv-sync exec mywf.yaml Workflow Attributes Based on naming convetion workflow attributes can be set as well. If the file starts with the characters as the workflow the Sync Tool will assume it is a workflow attribute and create it. mywf.yaml mywf.yaml.script.sh The above example will create a workflow variable script.sh for the workflow mywf.yaml . Profiles If you need to manage multiple configurations, the tool supports \"profiles\". A profile is a configuration in a list of configurations in the config file. A valid configuration file might look like this: profiles : - id : a auth-token : my-api-key-token addr : https://my-direktiv.server namespace : dev - id : b auth-token : my-api-key-token addr : https://my-direktiv.server namespace : prod The tool supports both types of configuration files, but you cannot mix and match. Either define fields within profiles or define no profiles at all. When using profiles, the default behaviour is to select the first profile defined in the list. To override this behaviour, you can use the -P / --profile flag to select one of the other profiles according to its id . So for the example above, we can push to prod with this flag --profile=b . Other Ways to Configure For most configuration settings, the tool will check for values in three places in the following order: Commandline flags. Environment variables. A configuration file. As long as the tool finds all of the values it needs, it doesn't care where it got them from. This means it's not strictly necessary to have a configuration file at all, so long as the settings are defined elsewhere. The flags are self explanatory, and otherwise available via help information ( -h / --help ). For environment variables, all settings are named the same way they appear in a configuration file, except for the following adjustments: All characters are UPPERCASE All dashes are replaced with underscores. All named are prefixed with DIREKTIV_ . So, for example, we can define an auth token with DIREKTIV_AUTH_TOKEN=my-api-key-token .","title":"Sync Tool"},{"location":"environment/cli/#direktiv-sync-tool","text":"Although developing workflows with the web UI is easy, Direktiv's Sync Tool can be used to make local workflow development faster and more convenient.","title":"Direktiv Sync Tool"},{"location":"environment/cli/#installing","text":"The Direktiv Sync Tool is available for Linux, Windows, and Mac platforms and is distributed as a tar.gz file with every new release of the software. The asset can be downloaded and unpacked to get the direktiv-sync binary. Linux Mac Windows Linux Install Example curl -L https://github.com/direktiv/direktiv/releases/latest/download/direktiv_sync_amd64.tar.gz | tar -xz && \\ sudo mv direktiv-sync /usr/local/bin","title":"Installing"},{"location":"environment/cli/#setting-up-a-namespace","text":"Working with this tool assumes that you create a directory which is mirroring a namespace in Direktiv. This directory should be empty at start. The first thing to setup is the connectivity to Direktiv. For this a .direktiv.yaml file has to be created within this directory. This file needs the api key or token, the address of the Direktiv instance and the namespace it should use. auth-token : my-api-key-token addr : https://my-direktiv.server namespace : direktiv This folder is now the base for a namespace direktiv . The path from here is relative in Direktiv. This means the folder structure will be the same as the folder structure in Direktiv.","title":"Setting up a Namespace"},{"location":"environment/cli/#pushing-and-executing","text":"At the moment there is one limitation working with this tool. It can not pull the namespace from Direktiv. For this functionality the Git integration has to be used. After setup there are two commands available. The push command pushes a workflow to Direktiv but does not execute it. This command works recursively e.g. direktiv-sync push . . The exec command uploads and executes the workflow. During execution the logs are printed to stdout . Examples direktiv-sync push myworkflow.yaml direktiv-sync push myfolder/ direktiv-sync exec mywf.yaml","title":"Pushing and Executing"},{"location":"environment/cli/#workflow-attributes","text":"Based on naming convetion workflow attributes can be set as well. If the file starts with the characters as the workflow the Sync Tool will assume it is a workflow attribute and create it. mywf.yaml mywf.yaml.script.sh The above example will create a workflow variable script.sh for the workflow mywf.yaml .","title":"Workflow Attributes"},{"location":"environment/cli/#profiles","text":"If you need to manage multiple configurations, the tool supports \"profiles\". A profile is a configuration in a list of configurations in the config file. A valid configuration file might look like this: profiles : - id : a auth-token : my-api-key-token addr : https://my-direktiv.server namespace : dev - id : b auth-token : my-api-key-token addr : https://my-direktiv.server namespace : prod The tool supports both types of configuration files, but you cannot mix and match. Either define fields within profiles or define no profiles at all. When using profiles, the default behaviour is to select the first profile defined in the list. To override this behaviour, you can use the -P / --profile flag to select one of the other profiles according to its id . So for the example above, we can push to prod with this flag --profile=b .","title":"Profiles"},{"location":"environment/cli/#other-ways-to-configure","text":"For most configuration settings, the tool will check for values in three places in the following order: Commandline flags. Environment variables. A configuration file. As long as the tool finds all of the values it needs, it doesn't care where it got them from. This means it's not strictly necessary to have a configuration file at all, so long as the settings are defined elsewhere. The flags are self explanatory, and otherwise available via help information ( -h / --help ). For environment variables, all settings are named the same way they appear in a configuration file, except for the following adjustments: All characters are UPPERCASE All dashes are replaced with underscores. All named are prefixed with DIREKTIV_ . So, for example, we can define an auth token with DIREKTIV_AUTH_TOKEN=my-api-key-token .","title":"Other Ways to Configure"},{"location":"environment/direktiv-development-environment/","text":"Development Standalone environment To improve function and workflows development it is recommended to setup a local development environment. This section explains how to setup the development environment. Details about developing custom functions is described in this section . Running direktiv Setting up a development direktiv instance on a local machine is pretty simple. Assuming docker is installed, run the folowing command: docker run --privileged -p 8080 :80 -p 31212 :31212 -d --name direktiv direktiv/direktiv-kube This command starts direktiv as container 'direktiv'. The initial boot-time will take a few minutes. The progress can be followed with: docker logs direktiv -f Once all pods reach 'running' status, direktiv is ready and the URL http://localhost:8080/api/namespaces is accessible. The database uses a persistent volume so the data stored should survive restarts with 'docker stop/start' . Running with proxy Running direktiv with a proxy configuration, the following settings can be passed as environmental variables: docker run --privileged -p 8080 :80 -p 31212 :31212 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \".default,10.0.0.0/8,172.0.0.0/8,localhost\" -ti direktiv/direktiv-kube Docker registry Direktiv pulls containers from a registry and runs them as functions. For development purposes the direktiv docker container comes with a registry installed. It is accessible on localhost:31212. To test the local repository the golang example from direktiv-apps can be used: git clone https://github.com/direktiv/direktiv-apps.git docker build direktiv-apps/examples/golang/ -t localhost:31212/myapp docker push localhost:31212/myapp # confirm upload curl http://localhost:31212/v2/_catalog To use it we need to create a namespace and a workflow. # create namespace 'test' curl -X PUT http://localhost:8080/api/namespaces/test # create the workflow file cat > helloworld.yml <<- EOF description: A simple 'action' state that sends a get request\" functions: - id: get type: reusable image: localhost:31212/myapp states: - id: getter type: action action: function: get input: name: John EOF # upload workflow curl -X PUT --data-binary @helloworld.yml \"http://localhost:8080/api/namespaces/test/tree/test?op=create-workflow\" # execute workflow (initial call will be slightly slower than subsequent calls) curl \"http://localhost:8080/api/namespaces/test/tree/test?op=wait\"","title":"Standalone Environment"},{"location":"environment/direktiv-development-environment/#development-standalone-environment","text":"To improve function and workflows development it is recommended to setup a local development environment. This section explains how to setup the development environment. Details about developing custom functions is described in this section .","title":"Development Standalone environment"},{"location":"environment/direktiv-development-environment/#running-direktiv","text":"Setting up a development direktiv instance on a local machine is pretty simple. Assuming docker is installed, run the folowing command: docker run --privileged -p 8080 :80 -p 31212 :31212 -d --name direktiv direktiv/direktiv-kube This command starts direktiv as container 'direktiv'. The initial boot-time will take a few minutes. The progress can be followed with: docker logs direktiv -f Once all pods reach 'running' status, direktiv is ready and the URL http://localhost:8080/api/namespaces is accessible. The database uses a persistent volume so the data stored should survive restarts with 'docker stop/start' .","title":"Running direktiv"},{"location":"environment/direktiv-development-environment/#running-with-proxy","text":"Running direktiv with a proxy configuration, the following settings can be passed as environmental variables: docker run --privileged -p 8080 :80 -p 31212 :31212 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \".default,10.0.0.0/8,172.0.0.0/8,localhost\" -ti direktiv/direktiv-kube","title":"Running with proxy"},{"location":"environment/direktiv-development-environment/#docker-registry","text":"Direktiv pulls containers from a registry and runs them as functions. For development purposes the direktiv docker container comes with a registry installed. It is accessible on localhost:31212. To test the local repository the golang example from direktiv-apps can be used: git clone https://github.com/direktiv/direktiv-apps.git docker build direktiv-apps/examples/golang/ -t localhost:31212/myapp docker push localhost:31212/myapp # confirm upload curl http://localhost:31212/v2/_catalog To use it we need to create a namespace and a workflow. # create namespace 'test' curl -X PUT http://localhost:8080/api/namespaces/test # create the workflow file cat > helloworld.yml <<- EOF description: A simple 'action' state that sends a get request\" functions: - id: get type: reusable image: localhost:31212/myapp states: - id: getter type: action action: function: get input: name: John EOF # upload workflow curl -X PUT --data-binary @helloworld.yml \"http://localhost:8080/api/namespaces/test/tree/test?op=create-workflow\" # execute workflow (initial call will be slightly slower than subsequent calls) curl \"http://localhost:8080/api/namespaces/test/tree/test?op=wait\"","title":"Docker registry"},{"location":"events/","text":"Events Direktiv implements HTTP Protocol Binding for CloudEvents . To trigger a cloud event just send a post request to '/api/namespaces/{namespace}/broadcast'. Binary Content Mode Th binary content mode uses headers to describe the event metadata with a \"ce-\" prefix and allows for efficient transfer and without transcoding effort. The header \"content-type\" must be set to the content-type of the body of the event. POST /api/namespaces/{namespace}/broadcast HTTP/1.1 Host: direktiv.io ce-specversion: 1.0 ce-type: com.example.event ce-id: 1234-1234-1234 ce-source: /mycontext/subcontext Content-Type: application/json; charset=utf-8 { \"hello\": \"world\" } Structured Content Mode In structured mode the whole cloudevent is in the payload. The content-type header needs to be set to \"application/cloudevents+json\". { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Batched Content Mode In batch mode multiple events can be send to direktiv. The content-type has to be \"application/cloudevents-batch+json\" and the body is a JSON array of cloud events. [ { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"C234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" }, { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"B234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } ] Authentication Use ApiKey Apply this header \"apikey\" : \"API_KEY\" Use Token Apply this header \"Direktiv-Token\" : \"ACCESS_TOKEN\"","title":"General"},{"location":"events/#events","text":"Direktiv implements HTTP Protocol Binding for CloudEvents . To trigger a cloud event just send a post request to '/api/namespaces/{namespace}/broadcast'.","title":"Events"},{"location":"events/#binary-content-mode","text":"Th binary content mode uses headers to describe the event metadata with a \"ce-\" prefix and allows for efficient transfer and without transcoding effort. The header \"content-type\" must be set to the content-type of the body of the event. POST /api/namespaces/{namespace}/broadcast HTTP/1.1 Host: direktiv.io ce-specversion: 1.0 ce-type: com.example.event ce-id: 1234-1234-1234 ce-source: /mycontext/subcontext Content-Type: application/json; charset=utf-8 { \"hello\": \"world\" }","title":"Binary Content Mode"},{"location":"events/#structured-content-mode","text":"In structured mode the whole cloudevent is in the payload. The content-type header needs to be set to \"application/cloudevents+json\". { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" }","title":"Structured Content Mode"},{"location":"events/#batched-content-mode","text":"In batch mode multiple events can be send to direktiv. The content-type has to be \"application/cloudevents-batch+json\" and the body is a JSON array of cloud events. [ { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"C234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" }, { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"B234-1234-1234\" , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } ]","title":"Batched Content Mode"},{"location":"events/#authentication","text":"","title":"Authentication"},{"location":"events/#use-apikey","text":"Apply this header \"apikey\" : \"API_KEY\"","title":"Use ApiKey"},{"location":"events/#use-token","text":"Apply this header \"Direktiv-Token\" : \"ACCESS_TOKEN\"","title":"Use Token"},{"location":"events/cloud/","text":"Cloud A simple introduction to using events provided from Google, Amazon and Azure to send to Direktiv.","title":"General"},{"location":"events/cloud/#cloud","text":"A simple introduction to using events provided from Google, Amazon and Azure to send to Direktiv.","title":"Cloud"},{"location":"events/cloud/amazon/","text":"Amazon EventBridge We're going to go through the process of setting up a rule for 'ec2' to send events to our Direktiv service. This explains how to create an api destination and transform the aws event input to cloud event format. Note: the below tutorial assumes that the user has already created the IAM role for the EventBridge API integration as described in Amazon EventBridge User Guide From the Role create above - keep the Role Arn details as it is needed in the final step. A screenshot is shown below: Create a rule aws events put-rule --name \"direktiv-rule\" --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"]}\" The following output should appear (make sure you hold onto the ARN as it is used further down to attach a target to the rule): { \"RuleArn\" : \"<RULE_ARN>\" } Create a connection After creating an Authorization token from the Direktiv interface, create the connection using the token as follow: aws events create-connection --name direktiv-connection --authorization-type API_KEY --auth-parameters \"{\\\"ApiKeyAuthParameters\\\": {\\\"ApiKeyName\\\":\\\"direktiv-token\\\", \\\"ApiKeyValue\\\":\\\"<DIREKTIV_TOKEN>\\\"}}\" Upon creating the connection the following output from the CLI should appear. { \"ConnectionArn\" : \"<CONNECTION_ARN>\" , \"ConnectionState\" : \"AUTHORIZED\" , \"CreationTime\" : \"2021-08-04T05:28:24+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:28:24+00:00\" } We will need to use the connection arn in the next command. Create an Api-Destination aws events create-api-destination --name direktiv-api --connection-arn \"<CONNECTION_ARN>\" --invocation-endpoint https://<DIREKTIV_URL>/api/namespaces/<NAMESPACE>/broadcast --http-method POST The output should resemble this: { \"ApiDestinationArn\" : \"<API_ARN>\" , \"ApiDestinationState\" : \"ACTIVE\" , \"CreationTime\" : \"2021-08-04T05:30:50+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:30:50+00:00\" } Put Targets to the AWS EventBridge Rule Adding the targets to the EventBridge rule also requires us to define an Input Path and Input Template. aws events put-targets --rule direktiv-rule --targets '[ { \"Id\": \"direktiv-api\", \"RoleArn\": \"<ROLE_ARN>\", \"Arn\": \"<API_ARN>\", \"InputTransformer\": { \"InputPathsMap\": { \"id\":\"$.id\", \"source\":\"$.source\", \"state\":\"$.detail.state\", \"subject\":\"$.source\", \"time\":\"$.time\", \"type\":\"$.detail-type\" }, \"InputTemplate\": \" {\\\"specversion\\\":\\\"1.0\\\", \\\"id\\\":<id>, \\\"source\\\":<source>, \\\"type\\\":<type>, \\\"subject\\\":<subject>, \\\"time\\\":<time>, \\\"data\\\":<aws.events.event.json>}\" } } ]' The output (if successful) below: { \"FailedEntryCount\" : 0 , \"FailedEntries\" : [] } Input Path Map Example Input Path Map captures the EventBridge event so we can easily filter into a cloud event to send to Direktiv { \"id\" : \"$.id\" , \"source\" : \"$.source\" , \"subject\" : \"$.source\" , \"time\" : \"$.time\" , \"type\" : \"$.detail-type\" } Input Template Example The Input Template allows you to spec out what you want the JSON to look like parsing the values from the input path. { \"specversion\" : \"1.0\" , \"id\" : \"<id>\" , \"source\" : \"<source>\" , \"type\" : \"<type>\" , \"subject\" : \"<subject>\" , \"time\" : \"<time>\" , \"data\" : <aws.eve nts .eve nt .jso n > } So now when you change the state of an instance on EC2 a workflow will be triggered on Direktiv if it is listening to 'aws.ec2'. For reference, when an AWS event is generated, the default event structure (for an EC2 status change as an example) is shown below: { \"version\" : \"0\" , \"id\" : \"7bf73129-1428-4cd3-a780-95db273d1602\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"123456789012\" , \"time\" : \"2015-11-11T21:29:54Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:ec2:us-east-1:123456789012:instance/i-abcd1111\" ], \"detail\" : { \"instance-id\" : \"i-abcd1111\" , \"state\" : \"pending\" } } The CloudEvent received by Direktiv after the transformation is shown below: { \"specversion\" : \"1.0\" , \"id\" : \"f694954a-c307-368c-005a-d4279473e156\" , \"source\" : \"aws.ec2\" , \"type\" : \"EC2 Instance State-change Notification\" , \"subject\" : \"aws.ec2\" , \"time\" : \"2022-05-04T01:57:06Z\" , \"data\" : { \"version\" : \"0\" , \"id\" : \"f694954a-c307-368c-005a-d4279473e156\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"338328518639\" , \"time\" : \"2022-05-04T01:57:06Z\" , \"region\" : \"ap-southeast-2\" , \"resources\" : [ \"arn:aws:ec2:ap-southeast-2:338328518639:instance/i-0cf5a83f321fbed55\" ], \"detail\" : { \"instance-id\" : \"i-0cf5a83f321fbed55\" , \"state\" : \"pending\" } } } Testing Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : \"EC2 Instance State-change Notification\" states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Amazon"},{"location":"events/cloud/amazon/#amazon-eventbridge","text":"We're going to go through the process of setting up a rule for 'ec2' to send events to our Direktiv service. This explains how to create an api destination and transform the aws event input to cloud event format. Note: the below tutorial assumes that the user has already created the IAM role for the EventBridge API integration as described in Amazon EventBridge User Guide From the Role create above - keep the Role Arn details as it is needed in the final step. A screenshot is shown below:","title":"Amazon EventBridge"},{"location":"events/cloud/amazon/#create-a-rule","text":"aws events put-rule --name \"direktiv-rule\" --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"]}\" The following output should appear (make sure you hold onto the ARN as it is used further down to attach a target to the rule): { \"RuleArn\" : \"<RULE_ARN>\" }","title":"Create a rule"},{"location":"events/cloud/amazon/#create-a-connection","text":"After creating an Authorization token from the Direktiv interface, create the connection using the token as follow: aws events create-connection --name direktiv-connection --authorization-type API_KEY --auth-parameters \"{\\\"ApiKeyAuthParameters\\\": {\\\"ApiKeyName\\\":\\\"direktiv-token\\\", \\\"ApiKeyValue\\\":\\\"<DIREKTIV_TOKEN>\\\"}}\" Upon creating the connection the following output from the CLI should appear. { \"ConnectionArn\" : \"<CONNECTION_ARN>\" , \"ConnectionState\" : \"AUTHORIZED\" , \"CreationTime\" : \"2021-08-04T05:28:24+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:28:24+00:00\" } We will need to use the connection arn in the next command.","title":"Create a connection"},{"location":"events/cloud/amazon/#create-an-api-destination","text":"aws events create-api-destination --name direktiv-api --connection-arn \"<CONNECTION_ARN>\" --invocation-endpoint https://<DIREKTIV_URL>/api/namespaces/<NAMESPACE>/broadcast --http-method POST The output should resemble this: { \"ApiDestinationArn\" : \"<API_ARN>\" , \"ApiDestinationState\" : \"ACTIVE\" , \"CreationTime\" : \"2021-08-04T05:30:50+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:30:50+00:00\" }","title":"Create an Api-Destination"},{"location":"events/cloud/amazon/#put-targets-to-the-aws-eventbridge-rule","text":"Adding the targets to the EventBridge rule also requires us to define an Input Path and Input Template. aws events put-targets --rule direktiv-rule --targets '[ { \"Id\": \"direktiv-api\", \"RoleArn\": \"<ROLE_ARN>\", \"Arn\": \"<API_ARN>\", \"InputTransformer\": { \"InputPathsMap\": { \"id\":\"$.id\", \"source\":\"$.source\", \"state\":\"$.detail.state\", \"subject\":\"$.source\", \"time\":\"$.time\", \"type\":\"$.detail-type\" }, \"InputTemplate\": \" {\\\"specversion\\\":\\\"1.0\\\", \\\"id\\\":<id>, \\\"source\\\":<source>, \\\"type\\\":<type>, \\\"subject\\\":<subject>, \\\"time\\\":<time>, \\\"data\\\":<aws.events.event.json>}\" } } ]' The output (if successful) below: { \"FailedEntryCount\" : 0 , \"FailedEntries\" : [] }","title":"Put Targets to the AWS EventBridge Rule"},{"location":"events/cloud/amazon/#input-path-map-example","text":"Input Path Map captures the EventBridge event so we can easily filter into a cloud event to send to Direktiv { \"id\" : \"$.id\" , \"source\" : \"$.source\" , \"subject\" : \"$.source\" , \"time\" : \"$.time\" , \"type\" : \"$.detail-type\" }","title":"Input Path Map Example"},{"location":"events/cloud/amazon/#input-template-example","text":"The Input Template allows you to spec out what you want the JSON to look like parsing the values from the input path. { \"specversion\" : \"1.0\" , \"id\" : \"<id>\" , \"source\" : \"<source>\" , \"type\" : \"<type>\" , \"subject\" : \"<subject>\" , \"time\" : \"<time>\" , \"data\" : <aws.eve nts .eve nt .jso n > } So now when you change the state of an instance on EC2 a workflow will be triggered on Direktiv if it is listening to 'aws.ec2'. For reference, when an AWS event is generated, the default event structure (for an EC2 status change as an example) is shown below: { \"version\" : \"0\" , \"id\" : \"7bf73129-1428-4cd3-a780-95db273d1602\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"123456789012\" , \"time\" : \"2015-11-11T21:29:54Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:ec2:us-east-1:123456789012:instance/i-abcd1111\" ], \"detail\" : { \"instance-id\" : \"i-abcd1111\" , \"state\" : \"pending\" } } The CloudEvent received by Direktiv after the transformation is shown below: { \"specversion\" : \"1.0\" , \"id\" : \"f694954a-c307-368c-005a-d4279473e156\" , \"source\" : \"aws.ec2\" , \"type\" : \"EC2 Instance State-change Notification\" , \"subject\" : \"aws.ec2\" , \"time\" : \"2022-05-04T01:57:06Z\" , \"data\" : { \"version\" : \"0\" , \"id\" : \"f694954a-c307-368c-005a-d4279473e156\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"338328518639\" , \"time\" : \"2022-05-04T01:57:06Z\" , \"region\" : \"ap-southeast-2\" , \"resources\" : [ \"arn:aws:ec2:ap-southeast-2:338328518639:instance/i-0cf5a83f321fbed55\" ], \"detail\" : { \"instance-id\" : \"i-0cf5a83f321fbed55\" , \"state\" : \"pending\" } } }","title":"Input Template Example"},{"location":"events/cloud/amazon/#testing","text":"Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : \"EC2 Instance State-change Notification\" states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/cloud/azure/","text":"Azure EventGrid Goes through the process of setting up a storage account that listens for events on upload. Being that Azure uses native cloud events we won't need to run anything apart from the initial setup. Setup To follow along you will need access to the resource group you wish to setup in. This example includes the creation of a storage account but an existing one can be used. Create a Storage Account & Container Create a storage account under a resource group az storage account create --name direktivstoragetest --resource-group trentis-direktiv-apps-test Create a container under that storage account. You can get the --account-key by doing the following az storage account keys list --account-name direktivstoragetest az storage container create --name direktiv-container --account-name direktivstorage100 --account-key ACCOUNT-KEY Create an Event Subscription webhook-request-callback sends option request Create an event subscription attached to the storage account. az eventgrid event-subscription create \\ --name direktiv-event \\ --source-resource-id = $( az storage account show --name direktivstoragetest --resource-group trentis-direktiv-apps-test --query id --output tsv ) \\ --endpoint = https://playground.direktiv.io/api/namespaces/trent/event \\ --endpoint-type = webhook --event-delivery-schema cloudeventschemav1_0 \\ --delivery-attribute-mapping Authorization Static \"Bearer ACCESS_TOKEN\" true Testing id : listen-for-azure-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : Microsoft.Storage.BlobCreated states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Azure"},{"location":"events/cloud/azure/#azure-eventgrid","text":"Goes through the process of setting up a storage account that listens for events on upload. Being that Azure uses native cloud events we won't need to run anything apart from the initial setup.","title":"Azure EventGrid"},{"location":"events/cloud/azure/#setup","text":"To follow along you will need access to the resource group you wish to setup in. This example includes the creation of a storage account but an existing one can be used.","title":"Setup"},{"location":"events/cloud/azure/#create-a-storage-account-container","text":"Create a storage account under a resource group az storage account create --name direktivstoragetest --resource-group trentis-direktiv-apps-test Create a container under that storage account. You can get the --account-key by doing the following az storage account keys list --account-name direktivstoragetest az storage container create --name direktiv-container --account-name direktivstorage100 --account-key ACCOUNT-KEY","title":"Create a Storage Account &amp; Container"},{"location":"events/cloud/azure/#create-an-event-subscription","text":"webhook-request-callback sends option request Create an event subscription attached to the storage account. az eventgrid event-subscription create \\ --name direktiv-event \\ --source-resource-id = $( az storage account show --name direktivstoragetest --resource-group trentis-direktiv-apps-test --query id --output tsv ) \\ --endpoint = https://playground.direktiv.io/api/namespaces/trent/event \\ --endpoint-type = webhook --event-delivery-schema cloudeventschemav1_0 \\ --delivery-attribute-mapping Authorization Static \"Bearer ACCESS_TOKEN\" true","title":"Create an Event Subscription"},{"location":"events/cloud/azure/#testing","text":"id : listen-for-azure-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : Microsoft.Storage.BlobCreated states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/cloud/gcp/","text":"Google Cloud EventArc To send Google Cloud Audit log events to EventArc you will need a container service running on Cloud Run. We provide you a container located at 'gcr.io/direktiv/event-arc-listener'. That container's job is to read the cloud event it receives and relays it back to a Direktiv service. Setup Setup Audit Logs to be managed Read policy file to /tmp/policy.yaml gcloud projects get-iam-policy PROJECT_ID > /tmp/policy.yaml Add the follow section above 'bindings:' auditConfigs : - auditLogConfigs : - logType : ADMIN_READ - logType : DATA_WRITE - logType : DATA_READ service : storage.googleapis.com Set the new policy gcloud projects set-iam-policy PROJECT_ID /tmp/policy.yaml Setup Configs for Gcloud to run properly gcloud config set project PROJECT_ID gcloud config set run/region us-central1 gcloud config set run/platform managed gcloud config set eventarc/location us-central1 Configure the Cloud Run Service Using Authentication Create a secret to use as the DIREKTIV_TOKEN gcloud secrets create DIREKTIV_TOKEN \\ --replication-policy = \"automatic\" Create a file that contains the ACCESS_TOKEN generated from Direktiv that has 'namespaceEvent' privilege. I chose to create the file as '/tmp/ac'. Add the secret data to the secret gcloud secrets versions add DIREKTIV_TOKEN --data-file = /tmp/ac Create a Cloud Run Service Deploy the container to your environment gcloud beta run deploy event-arc-listener --image gcr.io/direktiv/event-arc-listener \\ --update-secrets = DIREKTIV_TOKEN = DIREKTIV_TOKEN:1 \\ --set-env-vars \"DIREKTIV_NAMESPACE=trent\" \\ --set-env-vars \"DIREKTIV_ENDPOINT=https://playground.direktiv.io\" \\ --allow-unauthenticated Create a Trigger for the Cloud Run Service Create a new trigger to listen for storage events on this project. gcloud eventarc triggers create storage-upload-trigger \\ --destination-run-service = event-arc-listener \\ --destination-run-region = us-central1 \\ --event-filters = \"type=google.cloud.audit.log.v1.written\" \\ --event-filters = \"serviceName=storage.googleapis.com\" \\ --event-filters = \"methodName=storage.objects.create\" \\ --service-account = SERVICE_ACCOUNT_ADDRESS Note: Keep in mind this trigger will take 10 minutes to work Testing Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : google.cloud.audit.log.v1.written states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Google Cloud Platform"},{"location":"events/cloud/gcp/#google-cloud-eventarc","text":"To send Google Cloud Audit log events to EventArc you will need a container service running on Cloud Run. We provide you a container located at 'gcr.io/direktiv/event-arc-listener'. That container's job is to read the cloud event it receives and relays it back to a Direktiv service.","title":"Google Cloud EventArc"},{"location":"events/cloud/gcp/#setup","text":"","title":"Setup"},{"location":"events/cloud/gcp/#setup-audit-logs-to-be-managed","text":"Read policy file to /tmp/policy.yaml gcloud projects get-iam-policy PROJECT_ID > /tmp/policy.yaml Add the follow section above 'bindings:' auditConfigs : - auditLogConfigs : - logType : ADMIN_READ - logType : DATA_WRITE - logType : DATA_READ service : storage.googleapis.com Set the new policy gcloud projects set-iam-policy PROJECT_ID /tmp/policy.yaml","title":"Setup Audit Logs to be managed"},{"location":"events/cloud/gcp/#setup-configs-for-gcloud-to-run-properly","text":"gcloud config set project PROJECT_ID gcloud config set run/region us-central1 gcloud config set run/platform managed gcloud config set eventarc/location us-central1","title":"Setup Configs for Gcloud to run properly"},{"location":"events/cloud/gcp/#configure-the-cloud-run-service","text":"","title":"Configure the Cloud Run Service"},{"location":"events/cloud/gcp/#using-authentication","text":"Create a secret to use as the DIREKTIV_TOKEN gcloud secrets create DIREKTIV_TOKEN \\ --replication-policy = \"automatic\" Create a file that contains the ACCESS_TOKEN generated from Direktiv that has 'namespaceEvent' privilege. I chose to create the file as '/tmp/ac'. Add the secret data to the secret gcloud secrets versions add DIREKTIV_TOKEN --data-file = /tmp/ac","title":"Using Authentication"},{"location":"events/cloud/gcp/#create-a-cloud-run-service","text":"Deploy the container to your environment gcloud beta run deploy event-arc-listener --image gcr.io/direktiv/event-arc-listener \\ --update-secrets = DIREKTIV_TOKEN = DIREKTIV_TOKEN:1 \\ --set-env-vars \"DIREKTIV_NAMESPACE=trent\" \\ --set-env-vars \"DIREKTIV_ENDPOINT=https://playground.direktiv.io\" \\ --allow-unauthenticated","title":"Create a Cloud Run Service"},{"location":"events/cloud/gcp/#create-a-trigger-for-the-cloud-run-service","text":"Create a new trigger to listen for storage events on this project. gcloud eventarc triggers create storage-upload-trigger \\ --destination-run-service = event-arc-listener \\ --destination-run-region = us-central1 \\ --event-filters = \"type=google.cloud.audit.log.v1.written\" \\ --event-filters = \"serviceName=storage.googleapis.com\" \\ --event-filters = \"methodName=storage.objects.create\" \\ --service-account = SERVICE_ACCOUNT_ADDRESS Note: Keep in mind this trigger will take 10 minutes to work","title":"Create a Trigger for the Cloud Run Service"},{"location":"events/cloud/gcp/#testing","text":"Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : google.cloud.audit.log.v1.written states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/knative/example/","text":"Kafka, Knative and Direktiv In this example we will generate an event in Kafka, consume it in Direktiv via Knative and publish a new event back to Knative. Install Knative The knative installation is simply applying two yaml files to the cluster. Install Knative kubectl apply -f https://github.com/knative/eventing/releases/download/v0.26.1/eventing-crds.yaml kubectl apply -f https://github.com/knative/eventing/releases/download/v0.26.1/eventing-core.yaml After successful installation there are two pods runnning: kubectl get pods -n knative-eventing NAME READY STATUS RESTARTS AGE eventing-controller-7b466b585f-76fd4 1/1 Running 0 7s eventing-webhook-5c8886cb56-q2h7r 1/1 Running 0 7s Kafka Setup If there is no Kafka instance available in the environment, it is easy to setup and configure Kafka with Strimzi . This is a two step process: installing the kafka operator and creating the Kafka cluster itself. Install Operator kubectl create namespace kafka kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka Create Kafka Cluster kubectl apply -f https://strimzi.io/examples/latest/kafka/kafka-persistent-single.yaml -n kafka # wait for cluster to be ready kubectl wait kafka/my-cluster --for=condition=Ready --timeout=300s -n kafka Create Kafka Topics In this example we will consume an event from one channel and publish to a second channel. Therefore we need two new topics to subscribe and publish to. cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-direktiv-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : receiver-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF # test if topics have been created kubectl get kafkatopics.kafka.strimzi.io -n kafka Create Broker In Knative there are different broker and channel combinations available. Here we will use Kafka as a Broker. To install Kafka as Knative broker there needs to be a Kafka broker controller and the implementation itself. Install Kafka Broker Controller kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-controller.yaml kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-broker.yaml With these installed the actual broker can be created. It requires a configmap to nominate the Kafka bootstrap server(s) and the broker yaml file to create the actual broker. Kafka Broker Configuration cat <<-EOF | kubectl apply -f - --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : default.topic.partitions : \"10\" default.topic.replication.factor : \"1\" bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" EOF Kafka Broker cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing EOF An successful installation creates a broker in ready state. kubectl get brokers.eventing.knative.dev NAME URL AGE READY REASON default http://kafka-broker-ingress.knative-eventing.svc.cluster.local/default/default 3s True Kafka Source The basic Knative and Kafka setup is finished and the sources and sinks to integrate the components are the last components missing. Install Kafka Source Implementation kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/source.yaml Creating a Kafka source requires to provide the bootstrap server and the topic to subscribe to. The topic knative-direktiv-topic was created at the beginning of this example and will be the channel to ingest events into Direktiv. The sink value configures this source to send all events coming from this channel to the Kafka broker. Create Kafka Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : direktiv-kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-direktiv-topic sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF This yaml creates the source and it is ready to consume events. kubectl get kafkasources.sources.knative.dev NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE direktiv-kafka-source [\"knative-direktiv-topic\"] [\"my-cluster-kafka-bootstrap.kafka:9092\"] True 10s With this source Knative can receive events but it requires a trigger to have another system consume the event. The setup of source, broker and trigger decouples the systems involved in this architecture. The folllwing yaml creates such a trigger. Because there is a trigger filter defined this trigger consumes all events of type dev.knative.kafka.event and forwards it to Direktiv's direktiv-eventing service. The uri value specifies the target namespace in Direktiv. cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-in namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /direktiv EOF This setup will already send events to a namespace called direktiv if data arrives at the knative-direktiv-topic topic in Kafka. This can be easily tested but the direktiv namespace has to exist in Direktiv. To test it we start a pod which connects to the topic. kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:0.26.0-kafka-3.0.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic knative-direktiv-topic After running the pod add JSON into the command prompt, e.g. {} . This sends the JSON object to Kafka. Knative's broker will pick it up and executes the trigger for direktiv. The event will appear on the direktiv namespace dashboard. Direktiv Source To connect Direktiv back to Knative again we need to install direktiv-knative-source . This source listens to events generated in Direktiv and pushes them to Knative. In this example the message is pushed back to the broker. The required argument for this source is the direktiv URI within the cluster, e.g. direktiv-flow.default:3333. Direktiv Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF Kafka Sink The last task left is installing a Kafka sink and trigger to send events coming from Direktiv to the broker. Installing Kafka Sink Implementation kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-sink.yaml Creating Kafka Sink cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : direktiv-kafka-sink namespace : default spec : topic : receiver-topic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 EOF The following yaml configures the trigger for Kafka. It is important to add a filter for this trigger. In this case the trigger fires if the type of the cloudevent is myevent . Creating Kafka Trigger cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-receive namespace : default spec : broker : default filter : attributes : type : myevent subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : direktiv-kafka-sink EOF Workflow After all these components are installed and connected we need to create a workflow in the direktiv namespace. To see the full configuration we will create flow which listens to Knative events, extracts the data and sends it back to Knative and eventually Kafka. start : type : event state : tellme event : type : dev.knative.kafka.event states : - id : tellme type : generateEvent event : type : myevent source : Direktiv data : x : jq(.\"dev.knative.kafka.event\".data) A second receiver pod is needed to see the events coming from Direktiv . It listens to topic receiver-topic which was created at the beginning of this tutorial. If data is put on the knative-direktiv-topic it will appear in this receiver topic. kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.26.0-kafka-3.0.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic receiver-topic --from-beginning","title":"Kafka Example"},{"location":"events/knative/example/#kafka-knative-and-direktiv","text":"In this example we will generate an event in Kafka, consume it in Direktiv via Knative and publish a new event back to Knative.","title":"Kafka, Knative and Direktiv"},{"location":"events/knative/example/#install-knative","text":"The knative installation is simply applying two yaml files to the cluster. Install Knative kubectl apply -f https://github.com/knative/eventing/releases/download/v0.26.1/eventing-crds.yaml kubectl apply -f https://github.com/knative/eventing/releases/download/v0.26.1/eventing-core.yaml After successful installation there are two pods runnning: kubectl get pods -n knative-eventing NAME READY STATUS RESTARTS AGE eventing-controller-7b466b585f-76fd4 1/1 Running 0 7s eventing-webhook-5c8886cb56-q2h7r 1/1 Running 0 7s","title":"Install Knative"},{"location":"events/knative/example/#kafka-setup","text":"If there is no Kafka instance available in the environment, it is easy to setup and configure Kafka with Strimzi . This is a two step process: installing the kafka operator and creating the Kafka cluster itself. Install Operator kubectl create namespace kafka kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka Create Kafka Cluster kubectl apply -f https://strimzi.io/examples/latest/kafka/kafka-persistent-single.yaml -n kafka # wait for cluster to be ready kubectl wait kafka/my-cluster --for=condition=Ready --timeout=300s -n kafka","title":"Kafka Setup"},{"location":"events/knative/example/#create-kafka-topics","text":"In this example we will consume an event from one channel and publish to a second channel. Therefore we need two new topics to subscribe and publish to. cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-direktiv-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : receiver-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF # test if topics have been created kubectl get kafkatopics.kafka.strimzi.io -n kafka","title":"Create Kafka Topics"},{"location":"events/knative/example/#create-broker","text":"In Knative there are different broker and channel combinations available. Here we will use Kafka as a Broker. To install Kafka as Knative broker there needs to be a Kafka broker controller and the implementation itself. Install Kafka Broker Controller kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-controller.yaml kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-broker.yaml With these installed the actual broker can be created. It requires a configmap to nominate the Kafka bootstrap server(s) and the broker yaml file to create the actual broker. Kafka Broker Configuration cat <<-EOF | kubectl apply -f - --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : default.topic.partitions : \"10\" default.topic.replication.factor : \"1\" bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" EOF Kafka Broker cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing EOF An successful installation creates a broker in ready state. kubectl get brokers.eventing.knative.dev NAME URL AGE READY REASON default http://kafka-broker-ingress.knative-eventing.svc.cluster.local/default/default 3s True","title":"Create Broker"},{"location":"events/knative/example/#kafka-source","text":"The basic Knative and Kafka setup is finished and the sources and sinks to integrate the components are the last components missing. Install Kafka Source Implementation kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/source.yaml Creating a Kafka source requires to provide the bootstrap server and the topic to subscribe to. The topic knative-direktiv-topic was created at the beginning of this example and will be the channel to ingest events into Direktiv. The sink value configures this source to send all events coming from this channel to the Kafka broker. Create Kafka Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : direktiv-kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-direktiv-topic sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF This yaml creates the source and it is ready to consume events. kubectl get kafkasources.sources.knative.dev NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE direktiv-kafka-source [\"knative-direktiv-topic\"] [\"my-cluster-kafka-bootstrap.kafka:9092\"] True 10s With this source Knative can receive events but it requires a trigger to have another system consume the event. The setup of source, broker and trigger decouples the systems involved in this architecture. The folllwing yaml creates such a trigger. Because there is a trigger filter defined this trigger consumes all events of type dev.knative.kafka.event and forwards it to Direktiv's direktiv-eventing service. The uri value specifies the target namespace in Direktiv. cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-in namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /direktiv EOF This setup will already send events to a namespace called direktiv if data arrives at the knative-direktiv-topic topic in Kafka. This can be easily tested but the direktiv namespace has to exist in Direktiv. To test it we start a pod which connects to the topic. kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:0.26.0-kafka-3.0.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic knative-direktiv-topic After running the pod add JSON into the command prompt, e.g. {} . This sends the JSON object to Kafka. Knative's broker will pick it up and executes the trigger for direktiv. The event will appear on the direktiv namespace dashboard.","title":"Kafka Source"},{"location":"events/knative/example/#direktiv-source","text":"To connect Direktiv back to Knative again we need to install direktiv-knative-source . This source listens to events generated in Direktiv and pushes them to Knative. In this example the message is pushed back to the broker. The required argument for this source is the direktiv URI within the cluster, e.g. direktiv-flow.default:3333. Direktiv Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF","title":"Direktiv Source"},{"location":"events/knative/example/#kafka-sink","text":"The last task left is installing a Kafka sink and trigger to send events coming from Direktiv to the broker. Installing Kafka Sink Implementation kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-sink.yaml Creating Kafka Sink cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : direktiv-kafka-sink namespace : default spec : topic : receiver-topic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 EOF The following yaml configures the trigger for Kafka. It is important to add a filter for this trigger. In this case the trigger fires if the type of the cloudevent is myevent . Creating Kafka Trigger cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-receive namespace : default spec : broker : default filter : attributes : type : myevent subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : direktiv-kafka-sink EOF","title":"Kafka Sink"},{"location":"events/knative/example/#workflow","text":"After all these components are installed and connected we need to create a workflow in the direktiv namespace. To see the full configuration we will create flow which listens to Knative events, extracts the data and sends it back to Knative and eventually Kafka. start : type : event state : tellme event : type : dev.knative.kafka.event states : - id : tellme type : generateEvent event : type : myevent source : Direktiv data : x : jq(.\"dev.knative.kafka.event\".data) A second receiver pod is needed to see the events coming from Direktiv . It listens to topic receiver-topic which was created at the beginning of this tutorial. If data is put on the knative-direktiv-topic it will appear in this receiver topic. kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.26.0-kafka-3.0.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic receiver-topic --from-beginning","title":"Workflow"},{"location":"events/knative/knative/","text":"Direktiv provides a sink and a source for integration into Knative Eventing . This Kafka example provides a test configuration with Knative Eventing, Kafka and Direktiv. Sink If eventing is enabled in Direktiv's helm chart an additional service is available in the namespace called direktiv-eventing . Knative triggers can be used to subscribe to events from configured Knative sources and executes flows in Direktiv. Triggers can target namespaces with the uri parameter in the YAML configuration. It is also possible to send to all namespaces if the uri is set to '/'. Sending events to all namespaces is a costly operation and should not be used if not absolutely necessary. Knative trigger for namespace apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-namespace-trigger namespace : default spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /mynamespace Source Direktiv provides a source for integrating Direktiv events into Knative as well. To use the source eventing needs to be enabled via helm. Helm Configuration eventing : enabled : true The image to use as a source is vorteil/direktiv-knative-source which establishes a GRPC stream to Direktiv to fetch the generated events. The required arg provides the Direktiv connection URL. Direktiv Knative Source apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default","title":"Eventing"},{"location":"events/knative/knative/#sink","text":"If eventing is enabled in Direktiv's helm chart an additional service is available in the namespace called direktiv-eventing . Knative triggers can be used to subscribe to events from configured Knative sources and executes flows in Direktiv. Triggers can target namespaces with the uri parameter in the YAML configuration. It is also possible to send to all namespaces if the uri is set to '/'. Sending events to all namespaces is a costly operation and should not be used if not absolutely necessary. Knative trigger for namespace apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-namespace-trigger namespace : default spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /mynamespace","title":"Sink"},{"location":"events/knative/knative/#source","text":"Direktiv provides a source for integrating Direktiv events into Knative as well. To use the source eventing needs to be enabled via helm. Helm Configuration eventing : enabled : true The image to use as a source is vorteil/direktiv-knative-source which establishes a GRPC stream to Direktiv to fetch the generated events. The required arg provides the Direktiv connection URL. Direktiv Knative Source apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default","title":"Source"},{"location":"examples/automatically-blur-nsfw-images/","text":"Introduction We're going to be creating a workflow that takes an image via a URL and checks if it is safe for work using Googles Vision api. The response of this workflow will either be the image unaltered or blurred (if the contents are explicit). This workflow requires three functions: The imagerecognition container to determine if the image at a URL is safe for work. The blur container to fetch and apply a blur filter to the image at the URL. The request container to fetch the unaltered image at the URL. id : check-image functions : - id : check image : direktiv/imagerecognition:v1 type : reusable - id : blur image : direktiv/blur:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : \"Evaluates an image using Google Vision API\" states : # continued in next code block Google Vision First we need to define a state that fetches the image from a url input required and then it uses the Google Vision AI to determine whether it is safe for work. - id : check_image type : action action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : check input : url : jq(.image) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check_val Switch Next with the results of the Google Vision AI we'll go into a switch state to determine if we need transition to the fetch_image or blur_image state - id : check_val type : switch conditions : - condition : jq(.return.safeForWork == true) transition : fetch_image defaultTransition : blur_image Blur Image A simple state that uses the blur container to the fetch an image from a URL and apply a blur filter. - id : blur_image type : action action : function : blur input : image : jq(.image) Fetch Image A simple state that uses the request container to the fetch an image from a URL. - id : fetch_image type : action action : function : request input : url : jq(.image) method : \"GET\" transform : return : jq(.return.data) Full Example Joining every part above we end up with the following workflow. The input required for this workflow is a json field 'image' which contains a url to an image. We can either send this via rest client using 'POST' and adding a JSON body or we could type it directly into the browser like so by filling in the NAMESPACE and WORKFLOW_NAME fields. http://localhost/api/namespaces/{NAMESPACE}/workflows/{WORKFLOW_NAME}/execute?wait=true&field=.return&image=https://images2.minutemediacdn.com/image/fetch/w_736,h_485,c_fill,g_auto,f_auto/https%3A%2F%2Fundeadwalking.com%2Ffiles%2Fimage-exchange%2F2018%2F08%2Fie_58809-850x560.jpeg wait tells the request to return the result instead of an instance id field tells the request what to return image is the input im providing take the json example below { \"image\" : \"https://images2.minutemediacdn.com/image/fetch/w_736,h_485,c_fill,g_auto,f_auto/https%3A%2F%2Fundeadwalking.com%2Ffiles%2Fimage-exchange%2F2018%2F08%2Fie_58809-850x560.jpeg\" } id : check-image functions : - id : check image : direktiv/imagerecognition:v1 type : reusable - id : blur image : direktiv/blur:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : \"Evaluates an image using Google Vision API\" states : - id : check_image type : action action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : check input : url : jq(.image) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check_val - id : check_val type : switch conditions : - condition : jq(.return.safeForWork == true) transition : fetch_image defaultTransition : blur_image - id : blur_image type : action action : function : blur input : image : jq(.image) - id : fetch_image type : action action : function : request input : url : jq(.image) method : \"GET\" transform : return : jq(.return.data)","title":"Blur a NSFW Image"},{"location":"examples/automatically-blur-nsfw-images/#introduction","text":"We're going to be creating a workflow that takes an image via a URL and checks if it is safe for work using Googles Vision api. The response of this workflow will either be the image unaltered or blurred (if the contents are explicit). This workflow requires three functions: The imagerecognition container to determine if the image at a URL is safe for work. The blur container to fetch and apply a blur filter to the image at the URL. The request container to fetch the unaltered image at the URL. id : check-image functions : - id : check image : direktiv/imagerecognition:v1 type : reusable - id : blur image : direktiv/blur:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : \"Evaluates an image using Google Vision API\" states : # continued in next code block","title":"Introduction"},{"location":"examples/automatically-blur-nsfw-images/#google-vision","text":"First we need to define a state that fetches the image from a url input required and then it uses the Google Vision AI to determine whether it is safe for work. - id : check_image type : action action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : check input : url : jq(.image) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check_val","title":"Google Vision"},{"location":"examples/automatically-blur-nsfw-images/#switch","text":"Next with the results of the Google Vision AI we'll go into a switch state to determine if we need transition to the fetch_image or blur_image state - id : check_val type : switch conditions : - condition : jq(.return.safeForWork == true) transition : fetch_image defaultTransition : blur_image","title":"Switch"},{"location":"examples/automatically-blur-nsfw-images/#blur-image","text":"A simple state that uses the blur container to the fetch an image from a URL and apply a blur filter. - id : blur_image type : action action : function : blur input : image : jq(.image)","title":"Blur Image"},{"location":"examples/automatically-blur-nsfw-images/#fetch-image","text":"A simple state that uses the request container to the fetch an image from a URL. - id : fetch_image type : action action : function : request input : url : jq(.image) method : \"GET\" transform : return : jq(.return.data)","title":"Fetch Image"},{"location":"examples/automatically-blur-nsfw-images/#full-example","text":"Joining every part above we end up with the following workflow. The input required for this workflow is a json field 'image' which contains a url to an image. We can either send this via rest client using 'POST' and adding a JSON body or we could type it directly into the browser like so by filling in the NAMESPACE and WORKFLOW_NAME fields. http://localhost/api/namespaces/{NAMESPACE}/workflows/{WORKFLOW_NAME}/execute?wait=true&field=.return&image=https://images2.minutemediacdn.com/image/fetch/w_736,h_485,c_fill,g_auto,f_auto/https%3A%2F%2Fundeadwalking.com%2Ffiles%2Fimage-exchange%2F2018%2F08%2Fie_58809-850x560.jpeg wait tells the request to return the result instead of an instance id field tells the request what to return image is the input im providing take the json example below { \"image\" : \"https://images2.minutemediacdn.com/image/fetch/w_736,h_485,c_fill,g_auto,f_auto/https%3A%2F%2Fundeadwalking.com%2Ffiles%2Fimage-exchange%2F2018%2F08%2Fie_58809-850x560.jpeg\" } id : check-image functions : - id : check image : direktiv/imagerecognition:v1 type : reusable - id : blur image : direktiv/blur:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : \"Evaluates an image using Google Vision API\" states : - id : check_image type : action action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : check input : url : jq(.image) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check_val - id : check_val type : switch conditions : - condition : jq(.return.safeForWork == true) transition : fetch_image defaultTransition : blur_image - id : blur_image type : action action : function : blur input : image : jq(.image) - id : fetch_image type : action action : function : request input : url : jq(.image) method : \"GET\" transform : return : jq(.return.data)","title":"Full Example"},{"location":"examples/check-email-for-intent/","text":"Introduction In this example, we will create two workflows; one will send and email and generate a cloud event, and the other will trigger upon receiving the cloud event, check the contents of an email, use AI to discern the 'intent' of the message, and respond if the intent is deemed to be negative. Send an Email and Trigger an Event id : send-email-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : # continued in next code block Send Email This state uses the direktiv/smtp:v1 container to send an email. - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"You are very bad at doing work.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD))\" server : smtp.gmail.com port : 587 transition : sendcloudevent Trigger Event This generateEvent state sends a cloud event to a namespace. - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp Workflow Send Email id : send-email-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : - id : sendemail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"You are very bad at doing work.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp Read the Email and Check its Intent id : listen-for-email description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : smtp image : direktiv/smtp:v1 type : reusable - id : sentiment image : direktiv/google-sentiment-check:v1 type : reusable start : type : event state : read-email event : type : smtp states : # continued in next code block NOTE: This workflow will be triggered when the 'direktiv' namespace receives a cloud event of type smtp. Read Email This state takes the first message from the email 'INBOX', reads & outputs the contents of the message, and transitions to the sentiment-check state. - id : read-email type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : sentiment-check Check Intent - id : sentiment-check type : action action : function : sentiment secrets : [ \"SERVICE_ACCOUNT_KEY\" ] input : message : jq(.return.msg) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check-feeling Check Intent If the message 'intent' is determined to be negative, send an automated response to the sender. - id : check-feeling type : switch conditions : - condition : jq(.return.feeling == \"Negative\") transition : send-response Reply to Email This state uses the direktiv/smtp:v2 isolate to send an email to the sender of the negative email. - id : send-response type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"I dont appreciate your message\" message : \"\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 Workflow Listen For Email id : listen-for-email description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : smtp image : direktiv/smtp:v1 type : reusable - id : sentiment image : direktiv/google-sentiment-check:v1 type : reusable start : type : event state : read-email event : type : smtp states : - id : read-email type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : sentiment-check - id : sentiment-check type : action action : function : sentiment secrets : [ \"SERVICE_ACCOUNT_KEY\" ] input : message : jq(.return.msg) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check-feeling - id : check-feeling type : switch conditions : - condition : jq(.return.feeling == \"Negative\") transition : send-response - id : send-response type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"I dont appreciate your message\" message : \"\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 Although this particular example is quite simple, the logic used could be modified for something far more 'in-depth'. For example, it could be extended to run an 'IMAP' listener that generates a cloud event, triggering the 'intent' checker on Direktiv. Depending on the determined intent of each email received, actions such as email deletion could be performed.","title":"Check Email for Intent"},{"location":"examples/check-email-for-intent/#introduction","text":"In this example, we will create two workflows; one will send and email and generate a cloud event, and the other will trigger upon receiving the cloud event, check the contents of an email, use AI to discern the 'intent' of the message, and respond if the intent is deemed to be negative.","title":"Introduction"},{"location":"examples/check-email-for-intent/#send-an-email-and-trigger-an-event","text":"id : send-email-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : # continued in next code block","title":"Send an Email and Trigger an Event"},{"location":"examples/check-email-for-intent/#send-email","text":"This state uses the direktiv/smtp:v1 container to send an email. - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"You are very bad at doing work.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD))\" server : smtp.gmail.com port : 587 transition : sendcloudevent","title":"Send Email"},{"location":"examples/check-email-for-intent/#trigger-event","text":"This generateEvent state sends a cloud event to a namespace. - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp","title":"Trigger Event"},{"location":"examples/check-email-for-intent/#workflow-send-email","text":"id : send-email-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : - id : sendemail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"You are very bad at doing work.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp","title":"Workflow Send Email"},{"location":"examples/check-email-for-intent/#read-the-email-and-check-its-intent","text":"id : listen-for-email description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : smtp image : direktiv/smtp:v1 type : reusable - id : sentiment image : direktiv/google-sentiment-check:v1 type : reusable start : type : event state : read-email event : type : smtp states : # continued in next code block NOTE: This workflow will be triggered when the 'direktiv' namespace receives a cloud event of type smtp.","title":"Read the Email and Check its Intent"},{"location":"examples/check-email-for-intent/#read-email","text":"This state takes the first message from the email 'INBOX', reads & outputs the contents of the message, and transitions to the sentiment-check state. - id : read-email type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : sentiment-check","title":"Read Email"},{"location":"examples/check-email-for-intent/#check-intent","text":"- id : sentiment-check type : action action : function : sentiment secrets : [ \"SERVICE_ACCOUNT_KEY\" ] input : message : jq(.return.msg) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check-feeling","title":"Check Intent"},{"location":"examples/check-email-for-intent/#check-intent_1","text":"If the message 'intent' is determined to be negative, send an automated response to the sender. - id : check-feeling type : switch conditions : - condition : jq(.return.feeling == \"Negative\") transition : send-response","title":"Check Intent"},{"location":"examples/check-email-for-intent/#reply-to-email","text":"This state uses the direktiv/smtp:v2 isolate to send an email to the sender of the negative email. - id : send-response type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"I dont appreciate your message\" message : \"\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587","title":"Reply to Email"},{"location":"examples/check-email-for-intent/#workflow-listen-for-email","text":"id : listen-for-email description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : smtp image : direktiv/smtp:v1 type : reusable - id : sentiment image : direktiv/google-sentiment-check:v1 type : reusable start : type : event state : read-email event : type : smtp states : - id : read-email type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : sentiment-check - id : sentiment-check type : action action : function : sentiment secrets : [ \"SERVICE_ACCOUNT_KEY\" ] input : message : jq(.return.msg) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check-feeling - id : check-feeling type : switch conditions : - condition : jq(.return.feeling == \"Negative\") transition : send-response - id : send-response type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"I dont appreciate your message\" message : \"\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 Although this particular example is quite simple, the logic used could be modified for something far more 'in-depth'. For example, it could be extended to run an 'IMAP' listener that generates a cloud event, triggering the 'intent' checker on Direktiv. Depending on the determined intent of each email received, actions such as email deletion could be performed.","title":"Workflow Listen For Email"},{"location":"examples/create-vm-set-dns/","text":"Creating a VM and a DNS Record There are a number of pre-made 'isolate' containers designed specifically to bootstrap workflow development with direktiv. In this article, the following four isolates will be used: direktiv/aws-ec2-create:v1 Creates an instance on Amazon Web Services EC2 direktiv/awsgo:v1 Wraps the AWS CLI, enabling the use of any existing AWS CLI command in a workflow direktiv/request:v1 Sends a custom HTTP request direktiv/smtp:v1 Sends an email To keep everything clean, this workflow will actually be split up in to 1 'main' workflow and 2 'subflows' that the main workflow calls. The following 'secrets' must be configured, in order to authenticate with various services: AWS_KEY AWS access key AWS_SECRET AWS access key secret GODADDY_KEY GoDaddy API Key GODADDY_SECRET GoDaddy API Key Secret SMTP_USER The 'sender' address, used to authenticate with the SMTP server. SMTP_PASSWORD Password for the SMTP user. Workflow #1 - Main This workflow creates an AWS EC2 instance, retrieves it's public IP address, and invokes the add-dns-record and send-email subflows. The first state, set-input , is included purely as a matter of convenience, as it allows us to execute the workflow without needing to remember to provide it with an input struct. id : create-vm-with-dns description : Creates an instance on AWS EC2, add a DNS record to GoDaddy, and posts a CloudEvent on completion. functions : - id : create-vm image : direktiv/aws-ec2-create:v1 type : reusable size : medium - id : get-vm image : direktiv/awsgo:v1 type : reusable - id : add-dns-record type : subflow workflow : add-dns-record - id : send-email type : subflow workflow : send-email states : # Set data (skips need for providing an input object on each invocation) - id : set-input type : noop transform : jq(.ami = \"ami-093d266a\" | .region = \"ap-southeast-2\" | .instanceType = \"t1.micro\" | .recipient = \"john.doe@example.com\" | .subdomain = \"direktiv\" | .domain = \"mydomain.com\") transition : create-instance # Create the AWS EC2 Instance - id : create-instance log : jq(.) type : action action : function : create-vm secrets : [ 'AWS_KEY' , 'AWS_SECRET' ] input : access-key : 'jq( .secrets.AWS_KEY )' access-secret : 'jq( .secrets.AWS_SECRET )' image-id : 'jq( .ami )' region : 'jq( .region )' instance-type : 'jq( .instanceType )' transition : get-instance-ip transform : jq(.) # Query AWS for the public IP address of the instance - id : get-instance-ip log : jq(.) type : action action : function : get-vm secrets : [ 'AWS_KEY' , 'AWS_SECRET' ] input : access-key : jq( .secrets.AWS_KEY ) access-secret : jq( .secrets.AWS_SECRET ) command : - '--region' - jq( .region ) - 'ec2' - 'describe-instances' - '--filters' - 'Name=instance-state-name,Values=running' - jq( \"Name=instance-id,Values=\" + .return.Instances[0].InstanceId ) - '--query' - 'Reservations[*].Instances[*].[PublicIpAddress]' - '--output' - 'json' transform : jq(.address = .return[0][0][0]) transition : add-dns-record # Add an 'A' DNS record - id : add-dns-record type : action log : jq(.) action : function : add-dns-record input : domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) transition : send-email # Send a 'success' email - id : send-email type : action action : function : send-email input : recipient : jq(.recipient) domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) Workflow #2 - Add DNS Record The add-dns-record contains 2 states. The first, validate , simply ensures that the provided input matches the expected schema, and will cause the workflow to fail otherwise. The second state, api-request , sends a custom HTTP PATCH request to GoDaddy, instructing it to create a new DNS record pointing to the IP address of the newly created VM. id : add-dns-record description : Add an A DNS record to the specified domain on GoDaddy. functions : - id : req image : direktiv/request:v1 type : reusable states : # Validate input - id : validate type : validate log : '.' transition : api-request schema : type : object required : - domain - subdomain - address additionalProperties : false properties : domain : type : string subdomain : type : string address : type : string # Send GoDaddy API request - id : api-request type : action action : secrets : [ \"GODADDY_KEY\" , \"GODADDY_SECRET\" ] function : req input : method : \"PATCH\" url : jq( \"https://api.godaddy.com/v1/domains/\" + .domain + \"/records\" ) headers : \"Content-Type\" : \"application/json\" \"Authorization\" : jq( \"sso-key \" + .secrets.GODADDY_KEY + \":\" + .secrets.GODADDY_SECRET ) body : - data : jq( .address ) name : jq( .subdomain ) ttl : 3600 type : \"A\" Workflow #3 - Send Email This workflow is only called once the instance is successfully created and the DNS record is set. It contains only 2 states. The validate state operates in the same way as the validate state of the add-dns-record workflow. The send-email state uses the direktiv/smtp:v1 isolate to generate and send and email to the specified email address. id : send-email description : Sends an email to the specified user informing them of successful VM / DNS setup. functions : - id : send-email image : direktiv/smtp:v1 type : reusable states : # Validate input - id : validate type : validate schema : type : object required : - recipient - domain - subdomain - address additionalProperties : false properties : recipient : type : string domain : type : string subdomain : type : string address : type : string transition : send-email # Send email - id : send-email type : action action : function : send-email secrets : [ \"SMTP_USER\" , \"SMTP_PASSWORD\" ] input : to : jq( .recipient ) subject : 'Success!' message : jq( \"Instance creation successful. Created DNS record pointing \" + .subdomain + \".\" + .domain + \" to \" + .address + \"!\" ) from : jq( .secrets.SMTP_USER ) password : jq( .secrets.SMTP_PASSWORD ) server : \"smtp.gmail.com\" port : 587 Finishing up Hopefully this article has illustrated how to use pre-existing direktiv isolates to bootstrap workflow development! It should also serve as a reminder that, by making a workflow 'modular' through the use of subflows, a complicated workflow can be made to appear quite straightforward. If you're interested in seeing what other isolates already exist, check out the direktiv-apps GitHub page . To learn how to write your own custom isolates, click here","title":"Create VM and DNS Record"},{"location":"examples/create-vm-set-dns/#creating-a-vm-and-a-dns-record","text":"There are a number of pre-made 'isolate' containers designed specifically to bootstrap workflow development with direktiv. In this article, the following four isolates will be used: direktiv/aws-ec2-create:v1 Creates an instance on Amazon Web Services EC2 direktiv/awsgo:v1 Wraps the AWS CLI, enabling the use of any existing AWS CLI command in a workflow direktiv/request:v1 Sends a custom HTTP request direktiv/smtp:v1 Sends an email To keep everything clean, this workflow will actually be split up in to 1 'main' workflow and 2 'subflows' that the main workflow calls. The following 'secrets' must be configured, in order to authenticate with various services: AWS_KEY AWS access key AWS_SECRET AWS access key secret GODADDY_KEY GoDaddy API Key GODADDY_SECRET GoDaddy API Key Secret SMTP_USER The 'sender' address, used to authenticate with the SMTP server. SMTP_PASSWORD Password for the SMTP user.","title":"Creating a VM and a DNS Record"},{"location":"examples/create-vm-set-dns/#workflow-1-main","text":"This workflow creates an AWS EC2 instance, retrieves it's public IP address, and invokes the add-dns-record and send-email subflows. The first state, set-input , is included purely as a matter of convenience, as it allows us to execute the workflow without needing to remember to provide it with an input struct. id : create-vm-with-dns description : Creates an instance on AWS EC2, add a DNS record to GoDaddy, and posts a CloudEvent on completion. functions : - id : create-vm image : direktiv/aws-ec2-create:v1 type : reusable size : medium - id : get-vm image : direktiv/awsgo:v1 type : reusable - id : add-dns-record type : subflow workflow : add-dns-record - id : send-email type : subflow workflow : send-email states : # Set data (skips need for providing an input object on each invocation) - id : set-input type : noop transform : jq(.ami = \"ami-093d266a\" | .region = \"ap-southeast-2\" | .instanceType = \"t1.micro\" | .recipient = \"john.doe@example.com\" | .subdomain = \"direktiv\" | .domain = \"mydomain.com\") transition : create-instance # Create the AWS EC2 Instance - id : create-instance log : jq(.) type : action action : function : create-vm secrets : [ 'AWS_KEY' , 'AWS_SECRET' ] input : access-key : 'jq( .secrets.AWS_KEY )' access-secret : 'jq( .secrets.AWS_SECRET )' image-id : 'jq( .ami )' region : 'jq( .region )' instance-type : 'jq( .instanceType )' transition : get-instance-ip transform : jq(.) # Query AWS for the public IP address of the instance - id : get-instance-ip log : jq(.) type : action action : function : get-vm secrets : [ 'AWS_KEY' , 'AWS_SECRET' ] input : access-key : jq( .secrets.AWS_KEY ) access-secret : jq( .secrets.AWS_SECRET ) command : - '--region' - jq( .region ) - 'ec2' - 'describe-instances' - '--filters' - 'Name=instance-state-name,Values=running' - jq( \"Name=instance-id,Values=\" + .return.Instances[0].InstanceId ) - '--query' - 'Reservations[*].Instances[*].[PublicIpAddress]' - '--output' - 'json' transform : jq(.address = .return[0][0][0]) transition : add-dns-record # Add an 'A' DNS record - id : add-dns-record type : action log : jq(.) action : function : add-dns-record input : domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) transition : send-email # Send a 'success' email - id : send-email type : action action : function : send-email input : recipient : jq(.recipient) domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address)","title":"Workflow #1 - Main"},{"location":"examples/create-vm-set-dns/#workflow-2-add-dns-record","text":"The add-dns-record contains 2 states. The first, validate , simply ensures that the provided input matches the expected schema, and will cause the workflow to fail otherwise. The second state, api-request , sends a custom HTTP PATCH request to GoDaddy, instructing it to create a new DNS record pointing to the IP address of the newly created VM. id : add-dns-record description : Add an A DNS record to the specified domain on GoDaddy. functions : - id : req image : direktiv/request:v1 type : reusable states : # Validate input - id : validate type : validate log : '.' transition : api-request schema : type : object required : - domain - subdomain - address additionalProperties : false properties : domain : type : string subdomain : type : string address : type : string # Send GoDaddy API request - id : api-request type : action action : secrets : [ \"GODADDY_KEY\" , \"GODADDY_SECRET\" ] function : req input : method : \"PATCH\" url : jq( \"https://api.godaddy.com/v1/domains/\" + .domain + \"/records\" ) headers : \"Content-Type\" : \"application/json\" \"Authorization\" : jq( \"sso-key \" + .secrets.GODADDY_KEY + \":\" + .secrets.GODADDY_SECRET ) body : - data : jq( .address ) name : jq( .subdomain ) ttl : 3600 type : \"A\"","title":"Workflow #2 - Add DNS Record"},{"location":"examples/create-vm-set-dns/#workflow-3-send-email","text":"This workflow is only called once the instance is successfully created and the DNS record is set. It contains only 2 states. The validate state operates in the same way as the validate state of the add-dns-record workflow. The send-email state uses the direktiv/smtp:v1 isolate to generate and send and email to the specified email address. id : send-email description : Sends an email to the specified user informing them of successful VM / DNS setup. functions : - id : send-email image : direktiv/smtp:v1 type : reusable states : # Validate input - id : validate type : validate schema : type : object required : - recipient - domain - subdomain - address additionalProperties : false properties : recipient : type : string domain : type : string subdomain : type : string address : type : string transition : send-email # Send email - id : send-email type : action action : function : send-email secrets : [ \"SMTP_USER\" , \"SMTP_PASSWORD\" ] input : to : jq( .recipient ) subject : 'Success!' message : jq( \"Instance creation successful. Created DNS record pointing \" + .subdomain + \".\" + .domain + \" to \" + .address + \"!\" ) from : jq( .secrets.SMTP_USER ) password : jq( .secrets.SMTP_PASSWORD ) server : \"smtp.gmail.com\" port : 587","title":"Workflow #3 - Send Email"},{"location":"examples/create-vm-set-dns/#finishing-up","text":"Hopefully this article has illustrated how to use pre-existing direktiv isolates to bootstrap workflow development! It should also serve as a reminder that, by making a workflow 'modular' through the use of subflows, a complicated workflow can be made to appear quite straightforward. If you're interested in seeing what other isolates already exist, check out the direktiv-apps GitHub page . To learn how to write your own custom isolates, click here","title":"Finishing up"},{"location":"examples/event-based-greeting/","text":"Event-based Greeting Example This example demonstrates a workflow that waits for a greetingcloudevent event. When the event is received, a state will be triggered using the data provided by the event. The generate-greeting workflow generates the greetingcloudevent that the eventbased-greeting workflow is waiting for. Event Listener Workflow YAML id : eventbased-greeting functions : - id : greeter image : direktiv/greeting:v1 type : reusable start : type : event state : greeter event : type : greetingcloudevent description : \"A simple action that greets you\" states : - id : greeter type : action action : function : greeter input : jq(.greetingcloudevent) transform : 'jq({ \"greeting\": .return.greeting })' GenerateGreeting Workflow YAML id : generate-greeting description : \"Generate greeting event\" states : - id : gen type : generateEvent event : type : greetingcloudevent source : Direktiv data : name : \"Trent\"","title":"Event-based Greeting (StartEvent)"},{"location":"examples/event-based-greeting/#event-based-greeting-example","text":"This example demonstrates a workflow that waits for a greetingcloudevent event. When the event is received, a state will be triggered using the data provided by the event. The generate-greeting workflow generates the greetingcloudevent that the eventbased-greeting workflow is waiting for.","title":"Event-based Greeting Example"},{"location":"examples/event-based-greeting/#event-listener-workflow-yaml","text":"id : eventbased-greeting functions : - id : greeter image : direktiv/greeting:v1 type : reusable start : type : event state : greeter event : type : greetingcloudevent description : \"A simple action that greets you\" states : - id : greeter type : action action : function : greeter input : jq(.greetingcloudevent) transform : 'jq({ \"greeting\": .return.greeting })'","title":"Event Listener Workflow YAML"},{"location":"examples/event-based-greeting/#generategreeting-workflow-yaml","text":"id : generate-greeting description : \"Generate greeting event\" states : - id : gen type : generateEvent event : type : greetingcloudevent source : Direktiv data : name : \"Trent\"","title":"GenerateGreeting Workflow YAML"},{"location":"examples/event-based-transition/","text":"Check Credit Score This example demonstrates the use of a switch state in an event-based workflow. The state waits for the arrival of a checkcredit event, and conditionally 'approves' or 'rejects' a hypothetical loan request based on data included in the checkcredit event using a state. check-credit Workflow YAML id : check-credit start : type : event state : check-credit event : type : checkcredit states : - id : check-credit type : switch conditions : - condition : jq(.checkcredit.data.value > 500) transition : approve-loan defaultTransition : reject-loan - id : reject-loan type : noop transform : 'jq({ \"msg\": \"You have been rejected for this loan\" })' - id : approve-loan type : noop transform : 'jq({ \"msg\": \"You have been approved for this loan\" })' gen-credit Workflow YAML id : generate-credit description : \"Generate credit score event\" states : - id : gen type : generateEvent event : type : checkcredit source : Direktiv data : value : 501","title":"Check Credit Score (ConsumeEvent)"},{"location":"examples/event-based-transition/#check-credit-score","text":"This example demonstrates the use of a switch state in an event-based workflow. The state waits for the arrival of a checkcredit event, and conditionally 'approves' or 'rejects' a hypothetical loan request based on data included in the checkcredit event using a state.","title":"Check Credit Score"},{"location":"examples/event-based-transition/#check-credit-workflow-yaml","text":"id : check-credit start : type : event state : check-credit event : type : checkcredit states : - id : check-credit type : switch conditions : - condition : jq(.checkcredit.data.value > 500) transition : approve-loan defaultTransition : reject-loan - id : reject-loan type : noop transform : 'jq({ \"msg\": \"You have been rejected for this loan\" })' - id : approve-loan type : noop transform : 'jq({ \"msg\": \"You have been approved for this loan\" })'","title":"check-credit Workflow YAML"},{"location":"examples/event-based-transition/#gen-credit-workflow-yaml","text":"id : generate-credit description : \"Generate credit score event\" states : - id : gen type : generateEvent event : type : checkcredit source : Direktiv data : value : 501","title":"gen-credit Workflow YAML"},{"location":"examples/git-clone-go-build/","text":"Introduction This article seeks to demonstrate how Direktiv workflows can be used to clone a git repository, build a binary from the code contained within the repository, and upload it to Amazon S3. The following snippet is the start of the workflow definition, and details each of the functions that will be required within the workflow. id : build-go-binary functions : - id : go image : direktiv/go:v1 type : reusable files : - key : helloworld scope : instance type : tar.gz - id : git image : direktiv/git:v1 type : reusable - id : upload image : direktiv/amazon-upload:v1 type : reusable files : - key : helloworldserver scope : instance description : \"Clones a repository and builds a Go server.\" states : # continued in the next code block The git function will clone the entire repository and save it as an instance-scope variable called helloworld, which is then referenced by the go function (note that the go function definition references the helloworld variable in its files section). The upload function takes the output from the go function and uploads it to Amazon S3. Git Clone For the purposes of this demonstration, I've created a git repository that provides the code required to build the Go binary. The contents of the repository will be saved to an instance variable, to be accessed by subsequent functions/states. - id : clone-repo type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/helloworld.git $out/instance/helloworld\" ] transition : build-server Go Build This state runs the go isolate, which is currently only capable of running go build commands. Additionally functionality may be added to this isolate in the future. - id : build-server type : action action : function : go input : args : [ \"build\" , \"-o\" , \"helloworldserver\" ] execution-folder : helloworld variable : helloworldserver variable-type : instance transition : upload-binary Upload to S3 This container reads the contents of the helloworldserver instance-scope variable and uploads it to Amazon S3. - id : upload-binary type : action action : function : upload secrets : [ \"AWS_ACCESS_KEY\" , \"AWS_SECRET_KEY\" ] input : filename : helloworldserver bucket : direktiv region : us-east-1 upload-name : helloworldserver key : jq(.secrets.AWS_ACCESS_KEY) secret : jq(.secrets.AWS_SECRET_KEY) Final Workflow Putting all of the pieces together; this workflow clones a git repository, builds a go binary, and uploads the results to Amazon S3: id : build-go-binary functions : - id : go image : direktiv/go:v1 type : reusable files : - key : helloworld scope : instance type : tar.gz - id : git image : direktiv/git:v1 type : reusable - id : upload image : direktiv/amazon-upload:v1 type : reusable files : - key : helloworldserver scope : instance description : \"Clones a repository and builds a Go server.\" states : - id : clone-repo type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/helloworld.git $out/instance/helloworld\" ] transition : build-server - id : build-server type : action action : function : go input : args : [ \"build\" , \"-o\" , \"helloworldserver\" ] execution-folder : helloworld variable : helloworldserver variable-type : instance transition : upload-binary - id : upload-binary type : action action : function : upload secrets : [ \"AWS_ACCESS_KEY\" , \"AWS_SECRET_KEY\" ] input : filename : helloworldserver bucket : direktiv region : us-east-1 upload-name : helloworldserver key : \"jq(.secrets.AWS_ACCESS_KEY)\" secret : \"jq(.secrets.AWS_SECRET_KEY)\"","title":"Git Clone and Go Build"},{"location":"examples/git-clone-go-build/#introduction","text":"This article seeks to demonstrate how Direktiv workflows can be used to clone a git repository, build a binary from the code contained within the repository, and upload it to Amazon S3. The following snippet is the start of the workflow definition, and details each of the functions that will be required within the workflow. id : build-go-binary functions : - id : go image : direktiv/go:v1 type : reusable files : - key : helloworld scope : instance type : tar.gz - id : git image : direktiv/git:v1 type : reusable - id : upload image : direktiv/amazon-upload:v1 type : reusable files : - key : helloworldserver scope : instance description : \"Clones a repository and builds a Go server.\" states : # continued in the next code block The git function will clone the entire repository and save it as an instance-scope variable called helloworld, which is then referenced by the go function (note that the go function definition references the helloworld variable in its files section). The upload function takes the output from the go function and uploads it to Amazon S3.","title":"Introduction"},{"location":"examples/git-clone-go-build/#git-clone","text":"For the purposes of this demonstration, I've created a git repository that provides the code required to build the Go binary. The contents of the repository will be saved to an instance variable, to be accessed by subsequent functions/states. - id : clone-repo type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/helloworld.git $out/instance/helloworld\" ] transition : build-server","title":"Git Clone"},{"location":"examples/git-clone-go-build/#go-build","text":"This state runs the go isolate, which is currently only capable of running go build commands. Additionally functionality may be added to this isolate in the future. - id : build-server type : action action : function : go input : args : [ \"build\" , \"-o\" , \"helloworldserver\" ] execution-folder : helloworld variable : helloworldserver variable-type : instance transition : upload-binary","title":"Go Build"},{"location":"examples/git-clone-go-build/#upload-to-s3","text":"This container reads the contents of the helloworldserver instance-scope variable and uploads it to Amazon S3. - id : upload-binary type : action action : function : upload secrets : [ \"AWS_ACCESS_KEY\" , \"AWS_SECRET_KEY\" ] input : filename : helloworldserver bucket : direktiv region : us-east-1 upload-name : helloworldserver key : jq(.secrets.AWS_ACCESS_KEY) secret : jq(.secrets.AWS_SECRET_KEY)","title":"Upload to S3"},{"location":"examples/git-clone-go-build/#final-workflow","text":"Putting all of the pieces together; this workflow clones a git repository, builds a go binary, and uploads the results to Amazon S3: id : build-go-binary functions : - id : go image : direktiv/go:v1 type : reusable files : - key : helloworld scope : instance type : tar.gz - id : git image : direktiv/git:v1 type : reusable - id : upload image : direktiv/amazon-upload:v1 type : reusable files : - key : helloworldserver scope : instance description : \"Clones a repository and builds a Go server.\" states : - id : clone-repo type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/helloworld.git $out/instance/helloworld\" ] transition : build-server - id : build-server type : action action : function : go input : args : [ \"build\" , \"-o\" , \"helloworldserver\" ] execution-folder : helloworld variable : helloworldserver variable-type : instance transition : upload-binary - id : upload-binary type : action action : function : upload secrets : [ \"AWS_ACCESS_KEY\" , \"AWS_SECRET_KEY\" ] input : filename : helloworldserver bucket : direktiv region : us-east-1 upload-name : helloworldserver key : \"jq(.secrets.AWS_ACCESS_KEY)\" secret : \"jq(.secrets.AWS_SECRET_KEY)\"","title":"Final Workflow"},{"location":"examples/greeting/","text":"Greeting Example This simple example workflow uses a single action state to call the direktiv/greeting action, which 'greets' the user specified in the \"name\" field of the input provided to the workflow. Workflow YAML id : greeting functions : - id : greeter image : direktiv/greeting:v1 type : reusable description : \"A simple action that greets you\" states : - id : greeter type : action action : function : greeter input : jq(.) transform : 'jq({ \"greeting\": .return.greeting })' Input { \"name\" : \"Trent\" } Output The results of this action will contain a greeting addressed to the provided name. { \"return\" : { \"greeting\" : \"Welcome to Direktiv, Trent!\" } }","title":"Greeting (ActionState)"},{"location":"examples/greeting/#greeting-example","text":"This simple example workflow uses a single action state to call the direktiv/greeting action, which 'greets' the user specified in the \"name\" field of the input provided to the workflow.","title":"Greeting Example"},{"location":"examples/greeting/#workflow-yaml","text":"id : greeting functions : - id : greeter image : direktiv/greeting:v1 type : reusable description : \"A simple action that greets you\" states : - id : greeter type : action action : function : greeter input : jq(.) transform : 'jq({ \"greeting\": .return.greeting })'","title":"Workflow YAML"},{"location":"examples/greeting/#input","text":"{ \"name\" : \"Trent\" }","title":"Input"},{"location":"examples/greeting/#output","text":"The results of this action will contain a greeting addressed to the provided name. { \"return\" : { \"greeting\" : \"Welcome to Direktiv, Trent!\" } }","title":"Output"},{"location":"examples/logging/","text":"Logging Though Direktiv provides instance logs viewable on the instance's information page, it won't keep them forever. Sending logs to a third-party logging service can be useful for taking control of your data. Here's how we can do it. The 'log' Parameter Every state in a workflow definition supports an optional log parameter. If this parameter is defined, Direktiv will evaluate it as a jq query against the instance data before executing the state's logic, and send the results to the instance logs. CloudEvents From Logs Workflows can be configured to generate CloudEvents on their namespace anytime the log parameter produces data. Look for a field called \"Log To Event\" on the workflow definition (YAML) UI page. If this field is set to anything other than an empty string CloudEvents will be generated with an additional extension logger set to the value saved here. Using this it's easy to capture instance logs and do whatever you want with them. Create another workflow that is triggered by matching CloudEvents, then send them to your own Logstash server, for example. id : helloworld states : - id : hello type : noop transform : 'jq({ result: \"Hello World!\" })' log : '\"Hello, logger!\"' This workflow, configured to Log To Event to mylog , will produce a CloudEvent like the following: { \"specversion\" : \"1.0\" , \"type\" : \"direktiv.instanceLog\" , \"source\" : \"4e455e04-dfcc-4d7d-90b3-37e00988335d\" , \"id\" : \"a9a07797-fe2a-490e-ba61-61b86e61d6c0\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"logger\" : \"mylog\" , \"datacontenttype\" : \"text/json\" , \"data\" : \"Hello, logger!\" } Google Cloud Platform Stackdriver Example This workflow listens for the logger gcpLogger and sends it to gcp: id : gcp-logger functions : - id : send-log image : direktiv/gcplog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : gcpLogger states : - id : log type : action action : function : send-log secrets : [ GCP_SERVICEACCOUNTKEY ] input : serviceAccountKey : jq(.secrets.GCP_SERVICEACCOUNTKEY) \"project-id\" : \"direktiv\" \"log-name\" : \"direktiv-log\" message : jq(.\"direktiv.instanceLog\") AWS Cloudwatch Example This workflow listens for the logger awsLogger and sends it to aws: id : aws-logger functions : - id : send-log image : direktiv/awslog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : awsLogger states : - id : log type : action action : function : send-log secrets : [ AWS_KEY , AWS_SECRET ] input : key : jq(.secrets.AWS_KEY) secret : jq(.secrets.AWS_SECRET) region : \"us-east-2\" \"log-group\" : \"direktiv\" \"log-stream\" : \"direktiv\" message : jq(.\"direktiv.instanceLog\") Azure Log Analytics Example This workflow listens for the logger azureLogger and sends it to azure: id : azure-logger functions : - id : send-log image : direktiv/azlog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : azure-logger states : - id : log type : action action : function : send-log secrets : [ AZURE_WORKSPACE_ID , AZURE_WORKSPACE_KEY ] input : \"workspace-id\" : jq(.secrets.AZURE_WORKSPACE_ID) key : jq(.secrets.AZURE_WORKSPACE_KEY) type : \"direktiv-log\" message : jq(.\"direktiv.instanceLog\") Other Providers These three cloud examples serve to demonstrate how this all works, but they're not the only options. Following this pattern you can log any way you like.","title":"Logging"},{"location":"examples/logging/#logging","text":"Though Direktiv provides instance logs viewable on the instance's information page, it won't keep them forever. Sending logs to a third-party logging service can be useful for taking control of your data. Here's how we can do it.","title":"Logging"},{"location":"examples/logging/#the-log-parameter","text":"Every state in a workflow definition supports an optional log parameter. If this parameter is defined, Direktiv will evaluate it as a jq query against the instance data before executing the state's logic, and send the results to the instance logs.","title":"The 'log' Parameter"},{"location":"examples/logging/#cloudevents-from-logs","text":"Workflows can be configured to generate CloudEvents on their namespace anytime the log parameter produces data. Look for a field called \"Log To Event\" on the workflow definition (YAML) UI page. If this field is set to anything other than an empty string CloudEvents will be generated with an additional extension logger set to the value saved here. Using this it's easy to capture instance logs and do whatever you want with them. Create another workflow that is triggered by matching CloudEvents, then send them to your own Logstash server, for example. id : helloworld states : - id : hello type : noop transform : 'jq({ result: \"Hello World!\" })' log : '\"Hello, logger!\"' This workflow, configured to Log To Event to mylog , will produce a CloudEvent like the following: { \"specversion\" : \"1.0\" , \"type\" : \"direktiv.instanceLog\" , \"source\" : \"4e455e04-dfcc-4d7d-90b3-37e00988335d\" , \"id\" : \"a9a07797-fe2a-490e-ba61-61b86e61d6c0\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"logger\" : \"mylog\" , \"datacontenttype\" : \"text/json\" , \"data\" : \"Hello, logger!\" }","title":"CloudEvents From Logs"},{"location":"examples/logging/#google-cloud-platform-stackdriver-example","text":"This workflow listens for the logger gcpLogger and sends it to gcp: id : gcp-logger functions : - id : send-log image : direktiv/gcplog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : gcpLogger states : - id : log type : action action : function : send-log secrets : [ GCP_SERVICEACCOUNTKEY ] input : serviceAccountKey : jq(.secrets.GCP_SERVICEACCOUNTKEY) \"project-id\" : \"direktiv\" \"log-name\" : \"direktiv-log\" message : jq(.\"direktiv.instanceLog\")","title":"Google Cloud Platform Stackdriver Example"},{"location":"examples/logging/#aws-cloudwatch-example","text":"This workflow listens for the logger awsLogger and sends it to aws: id : aws-logger functions : - id : send-log image : direktiv/awslog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : awsLogger states : - id : log type : action action : function : send-log secrets : [ AWS_KEY , AWS_SECRET ] input : key : jq(.secrets.AWS_KEY) secret : jq(.secrets.AWS_SECRET) region : \"us-east-2\" \"log-group\" : \"direktiv\" \"log-stream\" : \"direktiv\" message : jq(.\"direktiv.instanceLog\")","title":"AWS Cloudwatch Example"},{"location":"examples/logging/#azure-log-analytics-example","text":"This workflow listens for the logger azureLogger and sends it to azure: id : azure-logger functions : - id : send-log image : direktiv/azlog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : azure-logger states : - id : log type : action action : function : send-log secrets : [ AZURE_WORKSPACE_ID , AZURE_WORKSPACE_KEY ] input : \"workspace-id\" : jq(.secrets.AZURE_WORKSPACE_ID) key : jq(.secrets.AZURE_WORKSPACE_KEY) type : \"direktiv-log\" message : jq(.\"direktiv.instanceLog\")","title":"Azure Log Analytics Example"},{"location":"examples/logging/#other-providers","text":"These three cloud examples serve to demonstrate how this all works, but they're not the only options. Following this pattern you can log any way you like.","title":"Other Providers"},{"location":"examples/md-translation/","text":"Introduction Today we will be creating a workflow that takes a string of markdown and convert it to Spanish and German. It will showcase how to use a foreach state to run the workflow. First we will need the 'google-translator' container to convert what is being passed to a different language. id : translate-md description : Translates a string into different languages functions : - id : translate image : direktiv/google-translator:v1 type : reusable states : # continued in next code block Google Translate Next we'll define a state that gets passed an array of strings. Where we pass each element in the string array as an object with the property 'id' so JQ can interpret it. - id : translate-markdown type : foreach array : \"jq([.langs[] | {id: .}])\" action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : translate input : serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) target-language : jq(.id) message : \"# Hello\\n\\n ## World! \\n\\n This is a test message that will get converted to a different language.\" Full Example Joining every part above we end up with the following workflow which takes the input as below. { \"langs\" : [ \"es\" , \"de\" ] } For a reference to what you can translate check out this page id : translate-md description : Translates a string into different languages functions : - id : translate image : direktiv/google-translator:v1 type : reusable states : - id : translate-markdown type : foreach array : \"jq([.langs[] | {id: .}])\" action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : translate input : serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) target-language : jq(.id) message : \"# Hello\\n\\n ## World! \\n\\n This is a test message that will get converted to a different language.\"","title":"Convert Markdown to Multiple Languages"},{"location":"examples/md-translation/#introduction","text":"Today we will be creating a workflow that takes a string of markdown and convert it to Spanish and German. It will showcase how to use a foreach state to run the workflow. First we will need the 'google-translator' container to convert what is being passed to a different language. id : translate-md description : Translates a string into different languages functions : - id : translate image : direktiv/google-translator:v1 type : reusable states : # continued in next code block","title":"Introduction"},{"location":"examples/md-translation/#google-translate","text":"Next we'll define a state that gets passed an array of strings. Where we pass each element in the string array as an object with the property 'id' so JQ can interpret it. - id : translate-markdown type : foreach array : \"jq([.langs[] | {id: .}])\" action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : translate input : serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) target-language : jq(.id) message : \"# Hello\\n\\n ## World! \\n\\n This is a test message that will get converted to a different language.\"","title":"Google Translate"},{"location":"examples/md-translation/#full-example","text":"Joining every part above we end up with the following workflow which takes the input as below. { \"langs\" : [ \"es\" , \"de\" ] } For a reference to what you can translate check out this page id : translate-md description : Translates a string into different languages functions : - id : translate image : direktiv/google-translator:v1 type : reusable states : - id : translate-markdown type : foreach array : \"jq([.langs[] | {id: .}])\" action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : translate input : serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) target-language : jq(.id) message : \"# Hello\\n\\n ## World! \\n\\n This is a test message that will get converted to a different language.\"","title":"Full Example"},{"location":"examples/non-json-input/","text":"Handling non-JSON input data Workflows can be invoked with input data that will be available as instance data. The format of the data being provided as input data dictates how it will be available as instance data. JSON Input Data If the input is provided as a JSON object, it will be unchanged when converted to instance data. Example Input Data { \"key\" : \"value\" } Instance Data { \"key\" : \"value\" } Non-JSON Input Data If the input is provided in a format that is not JSON, it will be base64 encoded into a string and stored as the value for the \"input\" key of the resulting JSON object. This will happen if, for example, the input data provided is a binary file. Example Input Data Hello , world! Instance Data { \"input\" : \"SGVsbG8sIHdvcmxkIQ==\" }","title":"Handling Non-JSON Input"},{"location":"examples/non-json-input/#handling-non-json-input-data","text":"Workflows can be invoked with input data that will be available as instance data. The format of the data being provided as input data dictates how it will be available as instance data.","title":"Handling non-JSON input data"},{"location":"examples/non-json-input/#json-input-data","text":"If the input is provided as a JSON object, it will be unchanged when converted to instance data.","title":"JSON Input Data"},{"location":"examples/non-json-input/#example","text":"","title":"Example"},{"location":"examples/non-json-input/#input-data","text":"{ \"key\" : \"value\" }","title":"Input Data"},{"location":"examples/non-json-input/#instance-data","text":"{ \"key\" : \"value\" }","title":"Instance Data"},{"location":"examples/non-json-input/#non-json-input-data","text":"If the input is provided in a format that is not JSON, it will be base64 encoded into a string and stored as the value for the \"input\" key of the resulting JSON object. This will happen if, for example, the input data provided is a binary file.","title":"Non-JSON Input Data"},{"location":"examples/non-json-input/#example_1","text":"","title":"Example"},{"location":"examples/non-json-input/#input-data_1","text":"Hello , world!","title":"Input Data"},{"location":"examples/non-json-input/#instance-data_1","text":"{ \"input\" : \"SGVsbG8sIHdvcmxkIQ==\" }","title":"Instance Data"},{"location":"examples/parallel-execution/","text":"Parallel Execution and Wait Example This example demonstrates the use of parallel subflows that must all complete before the run state will succeed. A hypothetical scenario where this approach may be used could involve a CI/CD process for which 3 different binaries are built (one each on Windows, Linux, and Mac) before creating a new product release. The run workflow will wait until all three subflows have received an event before proceeding. waiting Workflow YAML id : waiting functions : - id : wait-for-windows type : subflow workflow : wait-for-windows - id : wait-for-linux type : subflow workflow : wait-for-linux - id : wait-for-mac type : subflow workflow : wait-for-mac states : - id : run type : parallel actions : - function : wait-for-windows - function : wait-for-linux - function : wait-for-mac mode : and wait-for Workflow YAML Replace {OS} with windows , mac , and linux , to create the 3 subflows referenced by the run state. id : wait-for-{OS} states : - id : wait-for-event type : consumeEvent event : type : gen-event-{OS} generateEvent Workflow YAML Replace {OS} with windows , mac and linux to create workflows that will generate the events that the previous three subflows are waiting to receive. id : send-event-for-{OS} states : - id : send-event type : generateEvent event : type : gen-event-{OS} source : direktiv This example defines 7 workflows: * waiting * wait-for-linux * wait-for-windows * wait-for-mac * send-event-for-linux * send-event-for-windows * send-event-for-mac Executing the waiting workflow will begin the three wait-for-{OS} workflows. The waiting instance will not continue past its run state until all three send-event-for-{OS} workflows are executed.","title":"Parallel Execution (Parallel)"},{"location":"examples/parallel-execution/#parallel-execution-and-wait-example","text":"This example demonstrates the use of parallel subflows that must all complete before the run state will succeed. A hypothetical scenario where this approach may be used could involve a CI/CD process for which 3 different binaries are built (one each on Windows, Linux, and Mac) before creating a new product release. The run workflow will wait until all three subflows have received an event before proceeding.","title":"Parallel Execution and Wait Example"},{"location":"examples/parallel-execution/#waiting-workflow-yaml","text":"id : waiting functions : - id : wait-for-windows type : subflow workflow : wait-for-windows - id : wait-for-linux type : subflow workflow : wait-for-linux - id : wait-for-mac type : subflow workflow : wait-for-mac states : - id : run type : parallel actions : - function : wait-for-windows - function : wait-for-linux - function : wait-for-mac mode : and","title":"waiting Workflow YAML"},{"location":"examples/parallel-execution/#wait-for-workflow-yaml","text":"Replace {OS} with windows , mac , and linux , to create the 3 subflows referenced by the run state. id : wait-for-{OS} states : - id : wait-for-event type : consumeEvent event : type : gen-event-{OS}","title":"wait-for Workflow YAML"},{"location":"examples/parallel-execution/#generateevent-workflow-yaml","text":"Replace {OS} with windows , mac and linux to create workflows that will generate the events that the previous three subflows are waiting to receive. id : send-event-for-{OS} states : - id : send-event type : generateEvent event : type : gen-event-{OS} source : direktiv This example defines 7 workflows: * waiting * wait-for-linux * wait-for-windows * wait-for-mac * send-event-for-linux * send-event-for-windows * send-event-for-mac Executing the waiting workflow will begin the three wait-for-{OS} workflows. The waiting instance will not continue past its run state until all three send-event-for-{OS} workflows are executed.","title":"generateEvent Workflow YAML"},{"location":"examples/regex-check-for-mobile/","text":"Introduction We're going to be creating two workflows. Send an email and Trigger a cloud event Read the email and use regex to workout the mobile number provided. Which will then send an SMS to the number saying we are out of the office at the moment. Send Email and Trigger Event To execute this workflow we need to define some functions the following are defined. smtp sends an email request send a http request id : send-mobile-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : This workflow sends an email and triggers an event. states : # continued in next code block Send Email This action state uses the smtp container to send an email. For example purposes you will notice that I am sendin the email to myself. - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"Hello my name is Trent Hilliam please msg me on +INSERT_MOBILE_NUMBER.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent Trigger Event This generateEvent state sends a cloud event to a namespace. - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp-mobile Workflow Send Email Full Example id : send-mobile-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"Hello my name is Trent Hilliam please msg me on +61430545789.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp-mobile Read Mail and Send SMS To execute this workflow we need to define some functions the following are defined. imap reads the first email message received regex takes a regex string and a string and returns the values the regex matches twilio sends an sms or an email id : listen-for-email-mobile description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : regex image : direktiv/regex:v1 type : reusable - id : twilio image : direktiv/twilio:v1 type : reusable start : type : event state : read-mail event : type : smtp-mobile states : # continued in next code block Read Email This takes the first message from your \"INBOX\" email and reads the body and outputs it. - id : read-mail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : regex-check Check Regex The following state uses the regex container we're provided the regex of '+[0-9]{1,2}[0-9]{9}' and it should return the results from the msg being provided. - id : regex-check type : action action : function : regex input : msg : jq(.return.msg) regex : \"\\\\+[0-9]{1,2}[0-9]{9}\" transition : send-sms transform : number : jq(.return.results[0]) Send SMS This uses a 'twilio' container to send a message to the emailee. - id : send-sms type : action action : secrets : [ \"TWILIO_SID\" , \"TWILIO_TOKEN\" , \"TWILIO_PROVIDED_NUMBER\" ] function : twilio input : typeof : sms sid : jq(.secrets.TWILIO_SID) token : jq(.secrets.TWILIO_TOKEN) message : \"Hey you just emailed but I am currently out of office.\" from : jq(.secrets.TWILIO_PROVIDED_NUMBER) to : jq(.number) Workflow Read Email and Send SMS id : listen-for-email-mobile description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : regex image : direktiv/regex:v1 type : reusable - id : twilio image : direktiv/twilio:v1 type : reusable start : type : event state : read-mail event : type : smtp-mobile states : - id : read-mail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : regex-check - id : regex-check type : action action : function : regex input : msg : jq(.return.msg) regex : \"\\\\+[0-9]{1,2}[0-9]{9}\" transition : send-sms transform : number : jq(.return.results[0]) - id : send-sms type : action action : secrets : [ \"TWILIO_SID\" , \"TWILIO_TOKEN\" , \"TWILIO_PROVIDED_NUMBER\" ] function : twilio input : typeof : sms sid : jq(.secrets.TWILIO_SID) token : jq(.secrets.TWILIO_TOKEN) message : \"Hey you just emailed but I am currently out of office.\" from : jq(.secrets.TWILIO_PROVIDED_NUMBER) to : jq(.number)","title":"Check Email for Mobile Number"},{"location":"examples/regex-check-for-mobile/#introduction","text":"We're going to be creating two workflows. Send an email and Trigger a cloud event Read the email and use regex to workout the mobile number provided. Which will then send an SMS to the number saying we are out of the office at the moment.","title":"Introduction"},{"location":"examples/regex-check-for-mobile/#send-email-and-trigger-event","text":"To execute this workflow we need to define some functions the following are defined. smtp sends an email request send a http request id : send-mobile-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : This workflow sends an email and triggers an event. states : # continued in next code block","title":"Send Email and Trigger Event"},{"location":"examples/regex-check-for-mobile/#send-email","text":"This action state uses the smtp container to send an email. For example purposes you will notice that I am sendin the email to myself. - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"Hello my name is Trent Hilliam please msg me on +INSERT_MOBILE_NUMBER.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent","title":"Send Email"},{"location":"examples/regex-check-for-mobile/#trigger-event","text":"This generateEvent state sends a cloud event to a namespace. - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp-mobile","title":"Trigger Event"},{"location":"examples/regex-check-for-mobile/#workflow-send-email-full-example","text":"id : send-mobile-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"Hello my name is Trent Hilliam please msg me on +61430545789.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp-mobile","title":"Workflow Send Email Full Example"},{"location":"examples/regex-check-for-mobile/#read-mail-and-send-sms","text":"To execute this workflow we need to define some functions the following are defined. imap reads the first email message received regex takes a regex string and a string and returns the values the regex matches twilio sends an sms or an email id : listen-for-email-mobile description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : regex image : direktiv/regex:v1 type : reusable - id : twilio image : direktiv/twilio:v1 type : reusable start : type : event state : read-mail event : type : smtp-mobile states : # continued in next code block","title":"Read Mail and Send SMS"},{"location":"examples/regex-check-for-mobile/#read-email","text":"This takes the first message from your \"INBOX\" email and reads the body and outputs it. - id : read-mail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : regex-check","title":"Read Email"},{"location":"examples/regex-check-for-mobile/#check-regex","text":"The following state uses the regex container we're provided the regex of '+[0-9]{1,2}[0-9]{9}' and it should return the results from the msg being provided. - id : regex-check type : action action : function : regex input : msg : jq(.return.msg) regex : \"\\\\+[0-9]{1,2}[0-9]{9}\" transition : send-sms transform : number : jq(.return.results[0])","title":"Check Regex"},{"location":"examples/regex-check-for-mobile/#send-sms","text":"This uses a 'twilio' container to send a message to the emailee. - id : send-sms type : action action : secrets : [ \"TWILIO_SID\" , \"TWILIO_TOKEN\" , \"TWILIO_PROVIDED_NUMBER\" ] function : twilio input : typeof : sms sid : jq(.secrets.TWILIO_SID) token : jq(.secrets.TWILIO_TOKEN) message : \"Hey you just emailed but I am currently out of office.\" from : jq(.secrets.TWILIO_PROVIDED_NUMBER) to : jq(.number)","title":"Send SMS"},{"location":"examples/regex-check-for-mobile/#workflow-read-email-and-send-sms","text":"id : listen-for-email-mobile description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : regex image : direktiv/regex:v1 type : reusable - id : twilio image : direktiv/twilio:v1 type : reusable start : type : event state : read-mail event : type : smtp-mobile states : - id : read-mail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : regex-check - id : regex-check type : action action : function : regex input : msg : jq(.return.msg) regex : \"\\\\+[0-9]{1,2}[0-9]{9}\" transition : send-sms transform : number : jq(.return.results[0]) - id : send-sms type : action action : secrets : [ \"TWILIO_SID\" , \"TWILIO_TOKEN\" , \"TWILIO_PROVIDED_NUMBER\" ] function : twilio input : typeof : sms sid : jq(.secrets.TWILIO_SID) token : jq(.secrets.TWILIO_TOKEN) message : \"Hey you just emailed but I am currently out of office.\" from : jq(.secrets.TWILIO_PROVIDED_NUMBER) to : jq(.number)","title":"Workflow Read Email and Send SMS"},{"location":"examples/respond-to-events/","text":"Reacting to (\"consuming\") Cloud Events Workflows can be triggered in a number of ways; by default they must be manually triggered, but with the correct configuration a workflow will start each time a cloud event reaches the parent namespace that satisfies the constraints detailed in the workflow definition. To demonstrate this, let's modify the 'main' workflow from this article , removing the send-email state and replacing it with a state that will generate an event. ... # Add an 'A' DNS record - id : add-dns-record type : action log : jq(.) action : function : add-dns-record input : domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) transition : generate-event # Send a custom event that will trigger the next workflow - id : generate-event type : generateEvent event : type : example.vm.created source : exampleWorkflow data : address : jq(.address) host : \"jq(.subdomain).jq(.domain)\" recipient : jq(.recipient) domain : jq(.domain) subdomain : jq(.subdomain) region : jq(.region) Now that the main workflow will generate a cloud event on completion, we require a workflow that will react to it. This workflow will extract information from the body of the caught event and submit a new record to a 'FreshService' service before sending the 'success' email to the specified user. id : consume-new-vm-event # Start workflow when correct event occurs start : type : event event : type : example.vm.created filters : source : exampleWorkflow functions : - id : query-fresh-service-cmdb image : direktiv/request:v1 type : reusable - id : send-email type : subflow workflow : send-email states : # Submit new record to FreshService - id : push-instance-2-cmdb type : action action : function : query-fresh-service-cmdb input : url : \"https://direktiv.freshservice.com/cmdb/items.json\" method : \"POST\" body : cmdb_config_item : name : jq(.\"example.vm.created\".host) ci_type_id : \"75000270995\" level_field_attributes : aws_region_75000270981 : jq(.\"example.vm.created\".region) availability_zone_75000270981 : jq(.\"example.vm.created\".region) instance_id_75000270995 : jq(.\"example.vm.created\".host) public_ip_75000270995 : jq(.\"example.vm.created\".address) public_dns_75000270995 : jq(.\"example.vm.created\".host) instance_state_75000270995 : \"created\" headers : \"Content-Type\" : \"application/json\" Authorization : \"Basic <EXAMPLE_AUTHORISATION>\" transform : jq(.msg = .return.item.config_item | del(.return)) transition : send-email # Send a 'success' email - id : send-email log : jq(.) type : action action : function : send-email input : recipient : jq(.\"example.vm.created\".recipient) domain : jq(.\"example.vm.created\".domain) subdomain : jq(.\"example.vm.created\".subdomain) address : jq(.\"example.vm.created\".address) Note: The value of the type and source fields defined in the start configuration of this workflow must match the corresponding fields of an incoming cloud event. After making these changes, trigger the main workflow ( create-vm-with-dns ). When complete, an instance will be created for the consume-new-vm-event workflow. All of the data provided to the data field of the generated event is accessible to the receiving workflow, as children of the example.vm.created field (the name of the field corresponds to the event type ).","title":"Reacting to Consuming Cloud Events"},{"location":"examples/respond-to-events/#reacting-to-consuming-cloud-events","text":"Workflows can be triggered in a number of ways; by default they must be manually triggered, but with the correct configuration a workflow will start each time a cloud event reaches the parent namespace that satisfies the constraints detailed in the workflow definition. To demonstrate this, let's modify the 'main' workflow from this article , removing the send-email state and replacing it with a state that will generate an event. ... # Add an 'A' DNS record - id : add-dns-record type : action log : jq(.) action : function : add-dns-record input : domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) transition : generate-event # Send a custom event that will trigger the next workflow - id : generate-event type : generateEvent event : type : example.vm.created source : exampleWorkflow data : address : jq(.address) host : \"jq(.subdomain).jq(.domain)\" recipient : jq(.recipient) domain : jq(.domain) subdomain : jq(.subdomain) region : jq(.region) Now that the main workflow will generate a cloud event on completion, we require a workflow that will react to it. This workflow will extract information from the body of the caught event and submit a new record to a 'FreshService' service before sending the 'success' email to the specified user. id : consume-new-vm-event # Start workflow when correct event occurs start : type : event event : type : example.vm.created filters : source : exampleWorkflow functions : - id : query-fresh-service-cmdb image : direktiv/request:v1 type : reusable - id : send-email type : subflow workflow : send-email states : # Submit new record to FreshService - id : push-instance-2-cmdb type : action action : function : query-fresh-service-cmdb input : url : \"https://direktiv.freshservice.com/cmdb/items.json\" method : \"POST\" body : cmdb_config_item : name : jq(.\"example.vm.created\".host) ci_type_id : \"75000270995\" level_field_attributes : aws_region_75000270981 : jq(.\"example.vm.created\".region) availability_zone_75000270981 : jq(.\"example.vm.created\".region) instance_id_75000270995 : jq(.\"example.vm.created\".host) public_ip_75000270995 : jq(.\"example.vm.created\".address) public_dns_75000270995 : jq(.\"example.vm.created\".host) instance_state_75000270995 : \"created\" headers : \"Content-Type\" : \"application/json\" Authorization : \"Basic <EXAMPLE_AUTHORISATION>\" transform : jq(.msg = .return.item.config_item | del(.return)) transition : send-email # Send a 'success' email - id : send-email log : jq(.) type : action action : function : send-email input : recipient : jq(.\"example.vm.created\".recipient) domain : jq(.\"example.vm.created\".domain) subdomain : jq(.\"example.vm.created\".subdomain) address : jq(.\"example.vm.created\".address) Note: The value of the type and source fields defined in the start configuration of this workflow must match the corresponding fields of an incoming cloud event. After making these changes, trigger the main workflow ( create-vm-with-dns ). When complete, an instance will be created for the consume-new-vm-event workflow. All of the data provided to the data field of the generated event is accessible to the receiving workflow, as children of the example.vm.created field (the name of the field corresponds to the event type ).","title":"Reacting to (\"consuming\") Cloud Events"},{"location":"examples/scheduled-example/","text":"Scheduled Cronjob Workflow This example demonstrates a workflow that triggers every minute via a cron setup. You can setup the cron to trigger at certain periods of time via the start field. * * * * * means it will trigger each minute not every minute. Scheduled Cronjob Workflow YAML start : type : scheduled cron : '* * * * *' description : A simple 'no-op' state that returns 'Hello world!' states : - id : helloworld type : noop transform : result : Hello world!","title":"Scheduling (ScheduledStartType)"},{"location":"examples/scheduled-example/#scheduled-cronjob-workflow","text":"This example demonstrates a workflow that triggers every minute via a cron setup. You can setup the cron to trigger at certain periods of time via the start field. * * * * * means it will trigger each minute not every minute.","title":"Scheduled Cronjob Workflow"},{"location":"examples/scheduled-example/#scheduled-cronjob-workflow-yaml","text":"start : type : scheduled cron : '* * * * *' description : A simple 'no-op' state that returns 'Hello world!' states : - id : helloworld type : noop transform : result : Hello world!","title":"Scheduled Cronjob Workflow YAML"},{"location":"examples/solving-math-expressions/","text":"Solving Math Expressions Example This example shows how we can iterate over data using the ForEach state. Which executes an action that solves a math expression. The workflow data input are the expressions you want to solve as a string array. The example demonstrates the use of an action isolate to solve a number of mathematical expressions using a foreach state. For each expression in the input array, the isolate will be run once. Solver Workflow YAML id : solver description : \"Solves a string array of expressions\" functions : - id : solve-math-expression image : direktiv/solve:v1 type : reusable states : - id : solve type : foreach array : 'jq([.expressions[] | { expression: . }])' action : function : solve-math-expression input : 'jq({ x: .expression })' transform : 'jq({ solved: .return })' Input { \"expressions\" : [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] } Output The results of this foreach loop will be a json array of strings that have the solved answers. { \"solved\" : [ \"14\" , \"1\" , \"300\" , \"100\" ] } Note: The array for a foreach state must be passed as an array of objects. This is why to iterate over the expressions string array, we must pipe it and construct a new array of objects using [.expressions[] | { expression: . }] . jq: .expressions [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] jq: [.expressions[] | { expression: . }] [ { \"expression\" : \"4+10\" }, { \"expression\" : \"15-14\" }, { \"expression\" : \"100*3\" }, { \"expression\" : \"200/2\" } ]","title":"Solving Math Expressions (Foreach)"},{"location":"examples/solving-math-expressions/#solving-math-expressions-example","text":"This example shows how we can iterate over data using the ForEach state. Which executes an action that solves a math expression. The workflow data input are the expressions you want to solve as a string array. The example demonstrates the use of an action isolate to solve a number of mathematical expressions using a foreach state. For each expression in the input array, the isolate will be run once.","title":"Solving Math Expressions Example"},{"location":"examples/solving-math-expressions/#solver-workflow-yaml","text":"id : solver description : \"Solves a string array of expressions\" functions : - id : solve-math-expression image : direktiv/solve:v1 type : reusable states : - id : solve type : foreach array : 'jq([.expressions[] | { expression: . }])' action : function : solve-math-expression input : 'jq({ x: .expression })' transform : 'jq({ solved: .return })'","title":"Solver Workflow YAML"},{"location":"examples/solving-math-expressions/#input","text":"{ \"expressions\" : [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] }","title":"Input"},{"location":"examples/solving-math-expressions/#output","text":"The results of this foreach loop will be a json array of strings that have the solved answers. { \"solved\" : [ \"14\" , \"1\" , \"300\" , \"100\" ] } Note: The array for a foreach state must be passed as an array of objects. This is why to iterate over the expressions string array, we must pipe it and construct a new array of objects using [.expressions[] | { expression: . }] .","title":"Output"},{"location":"examples/solving-math-expressions/#jq-expressions","text":"[ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ]","title":"jq: .expressions"},{"location":"examples/solving-math-expressions/#jq-expressions-expression","text":"[ { \"expression\" : \"4+10\" }, { \"expression\" : \"15-14\" }, { \"expression\" : \"100*3\" }, { \"expression\" : \"200/2\" } ]","title":"jq: [.expressions[] | { expression: . }]"},{"location":"examples/start-event-and-example/","text":"Start EventAnd Workflow This example demonstrates a workflow that triggers whenever the cloud event type greeting and now was received. Start EventAnd Workflow YAML start : type : eventsAnd events : - type : greeting - type : now state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use the following two workflows to trigger the above workflow. Generate Greeting Event Workflow YAML description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv Generate Now Event Workflow YAML description : A simple 'generateEvent' state that triggers a now listener. states : - id : generate type : generateEvent event : type : now source : direktiv","title":"EventAnd Listener (EventAndStartType)"},{"location":"examples/start-event-and-example/#start-eventand-workflow","text":"This example demonstrates a workflow that triggers whenever the cloud event type greeting and now was received.","title":"Start EventAnd Workflow"},{"location":"examples/start-event-and-example/#start-eventand-workflow-yaml","text":"start : type : eventsAnd events : - type : greeting - type : now state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use the following two workflows to trigger the above workflow.","title":"Start EventAnd Workflow YAML"},{"location":"examples/start-event-and-example/#generate-greeting-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv","title":"Generate Greeting Event Workflow YAML"},{"location":"examples/start-event-and-example/#generate-now-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a now listener. states : - id : generate type : generateEvent event : type : now source : direktiv","title":"Generate Now Event Workflow YAML"},{"location":"examples/start-event-xor-example/","text":"Start EventXor Workflow This example demonstrates a workflow that triggers whenever the cloud event type greeting or now was received. Start EventsXor Workflow YAML start : type : eventsXor events : - type : greeting - type : now state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use one of the following workflows to trigger the above workflow. Generate Greeting Event Workflow YAML description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv Generate Now Event Workflow YAML description : A simple 'generateEvent' state that triggers a now listener. states : - id : generate type : generateEvent event : type : now source : direktiv","title":"EventXor Listener (EventXorStartType)"},{"location":"examples/start-event-xor-example/#start-eventxor-workflow","text":"This example demonstrates a workflow that triggers whenever the cloud event type greeting or now was received.","title":"Start EventXor Workflow"},{"location":"examples/start-event-xor-example/#start-eventsxor-workflow-yaml","text":"start : type : eventsXor events : - type : greeting - type : now state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use one of the following workflows to trigger the above workflow.","title":"Start EventsXor Workflow YAML"},{"location":"examples/start-event-xor-example/#generate-greeting-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv","title":"Generate Greeting Event Workflow YAML"},{"location":"examples/start-event-xor-example/#generate-now-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a now listener. states : - id : generate type : generateEvent event : type : now source : direktiv","title":"Generate Now Event Workflow YAML"},{"location":"examples/start-example/","text":"Start Event Workflow This example demonstrates a workflow that triggers whenever the cloud event type greeting was received. Start Event Workflow YAML start : type : event event : type : greeting state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use the following workflow to generate the event yourself. Generate Greeting Event Workflow YAML description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv","title":"Event Listener (EventStartType)"},{"location":"examples/start-example/#start-event-workflow","text":"This example demonstrates a workflow that triggers whenever the cloud event type greeting was received.","title":"Start Event Workflow"},{"location":"examples/start-example/#start-event-workflow-yaml","text":"start : type : event event : type : greeting state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use the following workflow to generate the event yourself.","title":"Start Event Workflow YAML"},{"location":"examples/start-example/#generate-greeting-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv","title":"Generate Greeting Event Workflow YAML"},{"location":"examples/terraform-and-ansible/","text":"Using Terraform & Ansible in a Workflow Terraform and Ansible are both widely used technologies in the area of automating infrastructure deployments, configurations, and more. At first glance, it might seem daunting to incorporate these technologies into a Direktiv workflow; but it's actually fairly straightforward. This article seeks to explain how to structure potentially 'complex' workflows in order to support technologies that may require data that persists across multiple workflow executions. The Workflow This workflow consists of only 2 states. The first state runs the terraform isolate, and requires access to a variable that contains the contents of a main.tf file. This main.tf file includes all of the configuration and authorisation details required to create a new virtual machine on Google Cloud Platform. The state is configured to assign the public IP address of the resulting virtual machine as .addr The second state uses the ansible isolate, and requires access to two different variables: playbook.yml Ansible playbook that will be executed to connect to the remote machine and print a 'Hello, world!' message. pk.pem Private key included in the main.tf variable of the previous state, used by Ansible to securely connect to the remote virtual machine. Examples of each variable are included at the end of this article. # This workflow uses Terraform to create an instance on # Google Cloud Platform, and connects to it with Ansible. id : terraform-and-ansible description : \"This workflow uses Terraform to create an instance on Google Cloud Platform, and connects to it with Ansible.\" functions : # Calls the standard 'terraform' isolate, providing # the `main.tf` file from an existing workflow variable. - id : terraform image : direktiv/terraform:v1 files : - key : main.tf scope : workflow type : plain # Runs ansible - id : ansible image : direktiv/ansible:v1 files : - key : playbook.yml scope : workflow type : plain - key : pk.pem scope : workflow states : # Create a GCP instance using Terraform - id : run-terraform type : action action : secrets : [ \"GCP_PROJECT\" ] function : terraform input : action : \"apply\" \"args-on-init\" : - \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" variables : \"state-name\" : \"terraform-gcp-instance\" project_id : jq(.secrets.GCP_PROJECT) transform : jq(.addr = .return.output[\"ip-address\"].value | del(.return)) transition : run-ansible # Use Ansible to connect to the instance using provided private key file - id : run-ansible type : action action : function : ansible input : playbook : playbook.yml privateKey : pk.pem args : - \"-i\" - \"jq(.addr),\" Variables main.tf The main.tf file used by the terraform isolate ensures the creation of a Virtual Machine on Google Cloud Platform with a startup script that will save a private/public key pair to the system and ensure that it is authorised for access to the machine via SSH. To change the name of the resulting virtual machine, modify the value of the name field. terraform { backend \"http\" {} } provider \"google\" { credentials = var.service_account_key } resource \"google_compute_instance\" \"default\" { project = var.project_id name = \"direktiv-terraform-ansible\" machine_type = \"n1-standard-1\" zone = \"australia-southeast1-a\" tags = [ \"direktiv\", \"direktiv\" ] boot_disk { initialize_params { image = \"ubuntu-os-cloud/ubuntu-2004-lts\" } } network_interface { network = \"default\" access_config { // Ephemeral IP } } metadata_startup_script = << EOT echo <base64-encoded contents of a pem privat key> > /pk.b64 base64 -d /pk.b64 > /pk.pem echo <base64-encoded contents of an rsa public key> > /ssh.b64 base64 -d /ssh.b64 > /ssh.key chmod 600 /pk.pem eval `ssh-agent -s` ssh-add /pk.pem cat /ssh.key >> ~/.ssh/authorized_keys EOT } variable \"service_account_key\" { description = \"the entire contents of a service account key\" default = << EOT { \"type\": \"service_account\", \"project_id\": \"*****\", \"private_key_id\": \"*****\", \"private_key\": \"*****\", \"client_email\": \"*****\", \"client_id\": \"*****\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://oauth2.googleapis.com/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/*****\" } EOT } variable \"project_id\" { description = \"project_id to spawn the virtual machine on\" } output \"ip-address\" { value = google_compute_instance.default.network_interface[0].access_config[0].nat_ip } playbook.yml This is an incredibly basic Ansible playbook. After connecting to the remote machine, it simply prints Hello, world! and returns. --- - hosts: all name: helloworld playbook gather_facts: yes remote_user: root connection: ssh tasks: - name: run helloworld logic ansible.builtin.debug: msg: - \"Hello, world!\" pk.pem This is a PEM formatted private key file, provided to both the created virtual machine (via main.tf ) and the ansible isolate. It provides a way for the ansible isolate to securely connect to the virtual machine via SSH.","title":"Terraform or Ansible"},{"location":"examples/terraform-and-ansible/#using-terraform-ansible-in-a-workflow","text":"Terraform and Ansible are both widely used technologies in the area of automating infrastructure deployments, configurations, and more. At first glance, it might seem daunting to incorporate these technologies into a Direktiv workflow; but it's actually fairly straightforward. This article seeks to explain how to structure potentially 'complex' workflows in order to support technologies that may require data that persists across multiple workflow executions.","title":"Using Terraform &amp; Ansible in a Workflow"},{"location":"examples/terraform-and-ansible/#the-workflow","text":"This workflow consists of only 2 states. The first state runs the terraform isolate, and requires access to a variable that contains the contents of a main.tf file. This main.tf file includes all of the configuration and authorisation details required to create a new virtual machine on Google Cloud Platform. The state is configured to assign the public IP address of the resulting virtual machine as .addr The second state uses the ansible isolate, and requires access to two different variables: playbook.yml Ansible playbook that will be executed to connect to the remote machine and print a 'Hello, world!' message. pk.pem Private key included in the main.tf variable of the previous state, used by Ansible to securely connect to the remote virtual machine. Examples of each variable are included at the end of this article. # This workflow uses Terraform to create an instance on # Google Cloud Platform, and connects to it with Ansible. id : terraform-and-ansible description : \"This workflow uses Terraform to create an instance on Google Cloud Platform, and connects to it with Ansible.\" functions : # Calls the standard 'terraform' isolate, providing # the `main.tf` file from an existing workflow variable. - id : terraform image : direktiv/terraform:v1 files : - key : main.tf scope : workflow type : plain # Runs ansible - id : ansible image : direktiv/ansible:v1 files : - key : playbook.yml scope : workflow type : plain - key : pk.pem scope : workflow states : # Create a GCP instance using Terraform - id : run-terraform type : action action : secrets : [ \"GCP_PROJECT\" ] function : terraform input : action : \"apply\" \"args-on-init\" : - \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" variables : \"state-name\" : \"terraform-gcp-instance\" project_id : jq(.secrets.GCP_PROJECT) transform : jq(.addr = .return.output[\"ip-address\"].value | del(.return)) transition : run-ansible # Use Ansible to connect to the instance using provided private key file - id : run-ansible type : action action : function : ansible input : playbook : playbook.yml privateKey : pk.pem args : - \"-i\" - \"jq(.addr),\"","title":"The Workflow"},{"location":"examples/terraform-and-ansible/#variables","text":"","title":"Variables"},{"location":"examples/terraform-and-ansible/#maintf","text":"The main.tf file used by the terraform isolate ensures the creation of a Virtual Machine on Google Cloud Platform with a startup script that will save a private/public key pair to the system and ensure that it is authorised for access to the machine via SSH. To change the name of the resulting virtual machine, modify the value of the name field. terraform { backend \"http\" {} } provider \"google\" { credentials = var.service_account_key } resource \"google_compute_instance\" \"default\" { project = var.project_id name = \"direktiv-terraform-ansible\" machine_type = \"n1-standard-1\" zone = \"australia-southeast1-a\" tags = [ \"direktiv\", \"direktiv\" ] boot_disk { initialize_params { image = \"ubuntu-os-cloud/ubuntu-2004-lts\" } } network_interface { network = \"default\" access_config { // Ephemeral IP } } metadata_startup_script = << EOT echo <base64-encoded contents of a pem privat key> > /pk.b64 base64 -d /pk.b64 > /pk.pem echo <base64-encoded contents of an rsa public key> > /ssh.b64 base64 -d /ssh.b64 > /ssh.key chmod 600 /pk.pem eval `ssh-agent -s` ssh-add /pk.pem cat /ssh.key >> ~/.ssh/authorized_keys EOT } variable \"service_account_key\" { description = \"the entire contents of a service account key\" default = << EOT { \"type\": \"service_account\", \"project_id\": \"*****\", \"private_key_id\": \"*****\", \"private_key\": \"*****\", \"client_email\": \"*****\", \"client_id\": \"*****\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://oauth2.googleapis.com/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/*****\" } EOT } variable \"project_id\" { description = \"project_id to spawn the virtual machine on\" } output \"ip-address\" { value = google_compute_instance.default.network_interface[0].access_config[0].nat_ip }","title":"main.tf"},{"location":"examples/terraform-and-ansible/#playbookyml","text":"This is an incredibly basic Ansible playbook. After connecting to the remote machine, it simply prints Hello, world! and returns. --- - hosts: all name: helloworld playbook gather_facts: yes remote_user: root connection: ssh tasks: - name: run helloworld logic ansible.builtin.debug: msg: - \"Hello, world!\"","title":"playbook.yml"},{"location":"examples/terraform-and-ansible/#pkpem","text":"This is a PEM formatted private key file, provided to both the created virtual machine (via main.tf ) and the ansible isolate. It provides a way for the ansible isolate to securely connect to the virtual machine via SSH.","title":"pk.pem"},{"location":"examples/using-terraform-plugin/","text":"Introduction This example will detail how to user Direktiv with Terraform to create a virtual machine. To do this, we will use examples listed on the public git repository . After creating the virtual machine, a message will be sent to a Discord webhook, resulting in the message being posted to a Discord server text channel. id : spawn-instance functions : - id : git image : direktiv/git:v1 type : reusable - id : discordmsg image : direktiv/discordmsg:v1 type : reusable - id : tfrun image : direktiv/terraform:v1 type : reusable files : - key : terraform-examples scope : instance type : tar.gz description : Clones a repository and uses terraform to deploy an instance states : # continued in next code block NOTE: The files attribute is empty and will be populated using the git state. Git The following state fetches the repository and clones it into an instance variable to be used in the terraform container. - id : cloning type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/terraform-examples.git $out/instance/terraform-examples\" ] transition : deploy_{CLOUD} log : . Terraform The files terraform need are provided from the git clone state that happened before which saves the variable as tar.gz file. When it is imported into the terraform container it ends up being a folder on the temp directory. The execution-folder in the input directs terraform to where the apply will be executed from. NOTE: Terraform container has its own http backend located at 'http://localhost:8001/{state-name}'. If provided args-on-init and state-name we will write the tfstate to a workflow variable. Azure The only secrets required to run this workflow with Azure are: subscription_id client_id client_secret tenant_id - id : deploy_azure type : action log : . action : secrets : [ \"CLIENT_ID\" , \"TENANT_ID\" , \"SUBSCRIPTION_ID\" , \"CLIENT_SECRET\" ] function : tfrun input : action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-azure-instance\" ] execution-folder : terraform-examples/azure variables : state-name : terraform-azure-instance subscription_id : jq(.secrets.SUBSCRIPTION_ID) client_id : jq(.secrets.CLIENT_ID) client_secret : jq(.secrets.CLIENT_SECRET) tenant_id : jq(.secrets.TENANT_ID) transform : ip : jq(.ip) transition : send_message Google Cloud Platform The only secrets required to run this workflow with Google Cloud Platform are: service_account_key (plain contents of a service account key) project_id - id : deploy_gcp type : action log : . action : secrets : [ \"SERVICE_ACCOUNT_KEY\" , \"PROJECT_ID\" ] function : tfrun input : execution-folder : terraform-examples/google action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" ] variables : state-name : terraform-gcp-instance project_id : jq(.secrets.PROJECT_ID) service_account_key : jq(.secrets.SERVICE_ACCOUNT_KEY) transform : ip : jq(.ip) transition : send_message Amazon Web Services The only secrets required to run this workflow with Amazon Web Services are: access_key secret_key - id : deploy_amazon type : action log : . action : secrets : [ \"AMAZON_KEY\" , \"AMAZON_SECRET\" ] function : tfrun input : execution-folder : terraform-examples/amazon action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-amazon-instance\" ] variables : region : us-east-2 state-name : terraform-amazon-instance amazon_key : jq(.secrets.AMAZON_KEY) amazon_secret : jq(.secrets.AMAZON_SECRET) transition : send_message transform : ip : jq(.ip) NOTE: Each terraform state uses the transform field to pluck the IP address of the created virtual machine, to be sent to the Discord webhook. Discord Message A simple action that sends a request to a Discord Webhook to post a message. - id : send_message type : action action : secrets : [ \"WEBHOOK_URL\" ] function : discordmsg input : tts : false url : jq(.secrets.WEBHOOK_URL) message : The ip address of your machine is jq(.ip). Full Example Let's bring all of the states together to create a workflow that creates a virtual machine on Google Cloud Platform, Amazon Web Services, and Azure. The following input is required: { \"action\" : \"'apply' or 'destroy'\" } We've also included a new switch state to facilitate not sending anything to the Discord webhook on destroy actions. id : spawn-instance functions : - id : git image : direktiv/git:v1 type : reusable - id : discordmsg image : direktiv/discordmsg:v1 type : reusable - id : tfrun image : direktiv/terraform:v1 type : reusable files : - key : terraform-examples scope : instance type : tar.gz description : Clones a repository and uses terraform to deploy an instance states : - id : cloning type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/terraform-examples.git $out/instance/terraform-examples\" ] transition : deploy_azure log : . - id : deploy_azure type : action log : . action : secrets : [ \"CLIENT_ID\" , \"TENANT_ID\" , \"SUBSCRIPTION_ID\" , \"CLIENT_SECRET\" ] function : tfrun input : action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-azure-instance\" ] execution-folder : terraform-examples/azure variables : state-name : terraform-azure-instance subscription_id : jq(.secrets.SUBSCRIPTION_ID) client_id : jq(.secrets.CLIENT_ID) client_secret : jq(.secrets.CLIENT_SECRET) tenant_id : jq(.secrets.TENANT_ID) transform : action : jq(.action) azure_ip : jq(.return.output.ip_address.value) transition : deploy_gcp - id : deploy_gcp type : action log : . action : secrets : [ \"SERVICE_ACCOUNT_KEY\" , \"PROJECT_ID\" ] function : tfrun input : execution-folder : terraform-examples/google action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" ] variables : state-name : terraform-gcp-instance project_id : jq(.secrets.PROJECT_ID) service_account_key : jq(.secrets.SERVICE_ACCOUNT_KEY) transform : action : jq(.action) azure_ip : jq(.azure_ip) google_ip : jq(.return.output.\"ip-address\".value) transition : deploy_amazon - id : deploy_amazon type : action log : . action : secrets : [ \"AMAZON_KEY\" , \"AMAZON_SECRET\" ] function : tfrun input : execution-folder : terraform-examples/amazon action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-amazon-instance\" ] variables : region : us-east-2 state-name : terraform-amazon-instance amazon_key : jq(.secrets.AMAZON_KEY) amazon_secret : jq(.secrets.AMAZON_SECRET) transform : action : jq(.action) azure_ip : jq(.azure_ip) google_ip : jq(.google_ip) amazon_ip : jq(.return.output.\"ip-address\".value) transition : check_apply_or_destroy - id : check_apply_or_destroy type : switch conditions : - condition : jq(.action == \"apply\") transition : send_message - id : send_message type : action log : . action : secrets : [ \"WEBHOOK_URL\" ] function : discordmsg input : tts : false url : jq(.secrets.WEBHOOK_URL) message : The ip address of your Azure machine is jq(.azure_ip). The ip address of your Google machine is jq(.google_ip). The ip address of your Amazon machine is jq(.amazon_ip).","title":"Running Terraform Scripts"},{"location":"examples/using-terraform-plugin/#introduction","text":"This example will detail how to user Direktiv with Terraform to create a virtual machine. To do this, we will use examples listed on the public git repository . After creating the virtual machine, a message will be sent to a Discord webhook, resulting in the message being posted to a Discord server text channel. id : spawn-instance functions : - id : git image : direktiv/git:v1 type : reusable - id : discordmsg image : direktiv/discordmsg:v1 type : reusable - id : tfrun image : direktiv/terraform:v1 type : reusable files : - key : terraform-examples scope : instance type : tar.gz description : Clones a repository and uses terraform to deploy an instance states : # continued in next code block NOTE: The files attribute is empty and will be populated using the git state.","title":"Introduction"},{"location":"examples/using-terraform-plugin/#git","text":"The following state fetches the repository and clones it into an instance variable to be used in the terraform container. - id : cloning type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/terraform-examples.git $out/instance/terraform-examples\" ] transition : deploy_{CLOUD} log : .","title":"Git"},{"location":"examples/using-terraform-plugin/#terraform","text":"The files terraform need are provided from the git clone state that happened before which saves the variable as tar.gz file. When it is imported into the terraform container it ends up being a folder on the temp directory. The execution-folder in the input directs terraform to where the apply will be executed from. NOTE: Terraform container has its own http backend located at 'http://localhost:8001/{state-name}'. If provided args-on-init and state-name we will write the tfstate to a workflow variable.","title":"Terraform"},{"location":"examples/using-terraform-plugin/#azure","text":"The only secrets required to run this workflow with Azure are: subscription_id client_id client_secret tenant_id - id : deploy_azure type : action log : . action : secrets : [ \"CLIENT_ID\" , \"TENANT_ID\" , \"SUBSCRIPTION_ID\" , \"CLIENT_SECRET\" ] function : tfrun input : action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-azure-instance\" ] execution-folder : terraform-examples/azure variables : state-name : terraform-azure-instance subscription_id : jq(.secrets.SUBSCRIPTION_ID) client_id : jq(.secrets.CLIENT_ID) client_secret : jq(.secrets.CLIENT_SECRET) tenant_id : jq(.secrets.TENANT_ID) transform : ip : jq(.ip) transition : send_message","title":"Azure"},{"location":"examples/using-terraform-plugin/#google-cloud-platform","text":"The only secrets required to run this workflow with Google Cloud Platform are: service_account_key (plain contents of a service account key) project_id - id : deploy_gcp type : action log : . action : secrets : [ \"SERVICE_ACCOUNT_KEY\" , \"PROJECT_ID\" ] function : tfrun input : execution-folder : terraform-examples/google action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" ] variables : state-name : terraform-gcp-instance project_id : jq(.secrets.PROJECT_ID) service_account_key : jq(.secrets.SERVICE_ACCOUNT_KEY) transform : ip : jq(.ip) transition : send_message","title":"Google Cloud Platform"},{"location":"examples/using-terraform-plugin/#amazon-web-services","text":"The only secrets required to run this workflow with Amazon Web Services are: access_key secret_key - id : deploy_amazon type : action log : . action : secrets : [ \"AMAZON_KEY\" , \"AMAZON_SECRET\" ] function : tfrun input : execution-folder : terraform-examples/amazon action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-amazon-instance\" ] variables : region : us-east-2 state-name : terraform-amazon-instance amazon_key : jq(.secrets.AMAZON_KEY) amazon_secret : jq(.secrets.AMAZON_SECRET) transition : send_message transform : ip : jq(.ip) NOTE: Each terraform state uses the transform field to pluck the IP address of the created virtual machine, to be sent to the Discord webhook.","title":"Amazon Web Services"},{"location":"examples/using-terraform-plugin/#discord-message","text":"A simple action that sends a request to a Discord Webhook to post a message. - id : send_message type : action action : secrets : [ \"WEBHOOK_URL\" ] function : discordmsg input : tts : false url : jq(.secrets.WEBHOOK_URL) message : The ip address of your machine is jq(.ip).","title":"Discord Message"},{"location":"examples/using-terraform-plugin/#full-example","text":"Let's bring all of the states together to create a workflow that creates a virtual machine on Google Cloud Platform, Amazon Web Services, and Azure. The following input is required: { \"action\" : \"'apply' or 'destroy'\" } We've also included a new switch state to facilitate not sending anything to the Discord webhook on destroy actions. id : spawn-instance functions : - id : git image : direktiv/git:v1 type : reusable - id : discordmsg image : direktiv/discordmsg:v1 type : reusable - id : tfrun image : direktiv/terraform:v1 type : reusable files : - key : terraform-examples scope : instance type : tar.gz description : Clones a repository and uses terraform to deploy an instance states : - id : cloning type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/terraform-examples.git $out/instance/terraform-examples\" ] transition : deploy_azure log : . - id : deploy_azure type : action log : . action : secrets : [ \"CLIENT_ID\" , \"TENANT_ID\" , \"SUBSCRIPTION_ID\" , \"CLIENT_SECRET\" ] function : tfrun input : action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-azure-instance\" ] execution-folder : terraform-examples/azure variables : state-name : terraform-azure-instance subscription_id : jq(.secrets.SUBSCRIPTION_ID) client_id : jq(.secrets.CLIENT_ID) client_secret : jq(.secrets.CLIENT_SECRET) tenant_id : jq(.secrets.TENANT_ID) transform : action : jq(.action) azure_ip : jq(.return.output.ip_address.value) transition : deploy_gcp - id : deploy_gcp type : action log : . action : secrets : [ \"SERVICE_ACCOUNT_KEY\" , \"PROJECT_ID\" ] function : tfrun input : execution-folder : terraform-examples/google action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" ] variables : state-name : terraform-gcp-instance project_id : jq(.secrets.PROJECT_ID) service_account_key : jq(.secrets.SERVICE_ACCOUNT_KEY) transform : action : jq(.action) azure_ip : jq(.azure_ip) google_ip : jq(.return.output.\"ip-address\".value) transition : deploy_amazon - id : deploy_amazon type : action log : . action : secrets : [ \"AMAZON_KEY\" , \"AMAZON_SECRET\" ] function : tfrun input : execution-folder : terraform-examples/amazon action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-amazon-instance\" ] variables : region : us-east-2 state-name : terraform-amazon-instance amazon_key : jq(.secrets.AMAZON_KEY) amazon_secret : jq(.secrets.AMAZON_SECRET) transform : action : jq(.action) azure_ip : jq(.azure_ip) google_ip : jq(.google_ip) amazon_ip : jq(.return.output.\"ip-address\".value) transition : check_apply_or_destroy - id : check_apply_or_destroy type : switch conditions : - condition : jq(.action == \"apply\") transition : send_message - id : send_message type : action log : . action : secrets : [ \"WEBHOOK_URL\" ] function : discordmsg input : tts : false url : jq(.secrets.WEBHOOK_URL) message : The ip address of your Azure machine is jq(.azure_ip). The ip address of your Google machine is jq(.google_ip). The ip address of your Amazon machine is jq(.amazon_ip).","title":"Full Example"},{"location":"examples/variable-mime-types/","text":"Variable Mime Type Example All variables have an associated mime type to distinguish the content type of its value. This example will show two examples, and the special behaviour that happens when mimeType is text/plain or application/octet-stream . Example 1: Storing a string as a raw plaintext variable. By default (mimeType=application/json) all variables are treated as JSON values. So this means even if you store a string in a variable, it's value is stored with quotes wrapped around it. Workflow - Example JSON String Var states : - id : set-var type : setter variables : - key : StringVar scope : workflow value : | hello world Variable - StringVar Value \"hello\\nworld\" There are certain scenarios where you would not want to store the variable with its quotes. To do this all need to do is simply set the mimeType to text/plain or text/plain; charset=utf-8 . This will store the variable as a raw string without quotes. Workflow - Example Plaintext String Var states : - id : set-var type : setter variables : - key : StringVar scope : workflow mimeType : 'text/plain' value : | hello world Variable - StringVar Value hello world Example 2: Auto Decoding Base64 string Another special behaviour is that it's also possible to auto decode a base64 string by setting the mimeType to application/octet-stream . Workflow - Example Auto Decode Base64 String states : - id : set-var type : setter variables : - key : MessageVar scope : workflow value : 'aGVsbG8gZnJvbSBkaXJla3Rpdg==' mimeType : 'application/octet-stream' Variable - MessageVar Value hello from direktiv These are the only two mime types with special behaviour. Any other mimeType will be treated internally by the default JSON behaviour. The default value for mimeType is application/json","title":"Variable Mimetypes"},{"location":"examples/variable-mime-types/#variable-mime-type-example","text":"All variables have an associated mime type to distinguish the content type of its value. This example will show two examples, and the special behaviour that happens when mimeType is text/plain or application/octet-stream .","title":"Variable Mime Type Example"},{"location":"examples/variable-mime-types/#example-1-storing-a-string-as-a-raw-plaintext-variable","text":"By default (mimeType=application/json) all variables are treated as JSON values. So this means even if you store a string in a variable, it's value is stored with quotes wrapped around it.","title":"Example 1: Storing a string as a raw plaintext variable."},{"location":"examples/variable-mime-types/#workflow-example-json-string-var","text":"states : - id : set-var type : setter variables : - key : StringVar scope : workflow value : | hello world","title":"Workflow - Example JSON String Var"},{"location":"examples/variable-mime-types/#variable-stringvar-value","text":"\"hello\\nworld\" There are certain scenarios where you would not want to store the variable with its quotes. To do this all need to do is simply set the mimeType to text/plain or text/plain; charset=utf-8 . This will store the variable as a raw string without quotes.","title":"Variable - StringVar Value"},{"location":"examples/variable-mime-types/#workflow-example-plaintext-string-var","text":"states : - id : set-var type : setter variables : - key : StringVar scope : workflow mimeType : 'text/plain' value : | hello world","title":"Workflow - Example Plaintext String Var"},{"location":"examples/variable-mime-types/#variable-stringvar-value_1","text":"hello world","title":"Variable - StringVar Value"},{"location":"examples/variable-mime-types/#example-2-auto-decoding-base64-string","text":"Another special behaviour is that it's also possible to auto decode a base64 string by setting the mimeType to application/octet-stream .","title":"Example 2: Auto Decoding Base64 string"},{"location":"examples/variable-mime-types/#workflow-example-auto-decode-base64-string","text":"states : - id : set-var type : setter variables : - key : MessageVar scope : workflow value : 'aGVsbG8gZnJvbSBkaXJla3Rpdg==' mimeType : 'application/octet-stream'","title":"Workflow - Example Auto Decode Base64 String"},{"location":"examples/variable-mime-types/#variable-messagevar-value","text":"hello from direktiv These are the only two mime types with special behaviour. Any other mimeType will be treated internally by the default JSON behaviour. The default value for mimeType is application/json","title":"Variable - MessageVar Value"},{"location":"examples/website-change/","text":"Check if Website has changed Example This example demonstrates a use case for a hypothetical scenario when you need to periodically check if a website has changed. It makes use of workflow variables to keep persistent data to test against. Most websites would likely have minor changes if you were to just fetch their HTML. These changes are usually stuff like time and session properties generated on each request. We don't want to categorize these changes as true as these are false positives. Luckily many websites have one or both header values: Etag and Last-Modified . If either one of these headers changes it is safe to assume that the website has been updated. Note: Although many websites have adopted this standard, there are still websites that have not. These websites are not supported. This example is fairly simple and can be broken up into four steps: 1. Fetch the current headers of the target website. 2. Get the saved headers variables from the previous execution of this workflow. 3. Compare the current and previous headers to determine if the target website has changed. 4. Save current headers to workflow variables. Below we'll explain each step in more detail. For this example we've chosen https://docs.direktiv.io as the target website. Fetch Website Headers Here we are performing a HTTP POST request on the target website using our direktiv/request:v1 direktiv app as a function. Since we only care about the Last-Modified and Etag headers we then extract those values and store them in the lastModified and etag properties using the transform field in the fetch-site-headers state. Now that we have fetched the current header values required, the state will transition to the get-old-headers state. id : check-website-change description : \"A simple workflow that fetches current headers from a website and compares them to the previously stored headers to determine if it has changed.\" functions : - id : get image : direktiv/request:v1 type : reusable states : - id : fetch-site-headers type : action transform : 'jq({lastModified: .return.headers.[\"Last-Modified\"][0], etag: .return.headers.[\"Etag\"][0]})' transition : get-old-headers action : function : get input : method : \"HEAD\" url : \"https://docs.direktiv.io\" Get Old Headers Direktiv has variables scoped to namespaces, workflows and instance. In this state ( get-old-headers ) we'll get the variables lastModified and etag in the workflow scope. These variables are set to whatever the header values were last time when this workflow was executed. If this is the first time this workflow is executed these value will be null . This is fine, but it will cause the results to be siteChanged: true because the current headers can never equal null . - id : get-old-headers type : getter transition : check-site variables : - key : lastModified scope : workflow - key : etag scope : workflow Compare Values Now that we have both the current and previous header values we can make a comparison and check whether the website has changed using the switch state check-site . The switch state below has three possible conditions. The first condition is used for validation and will transition to the error state unsupported-site if neither etag or lastModified was fetched from the current headers. The last two are to check if either of the etag or lastModified values have changed between the previous and current headers. If either one of these headers has changed it means that the website has changed and the property siteChanged is set to true. If none of these conditions are satisfied, the siteChanged property is set to false because we can assume that no errors/changes have occurred. - id : check-site type : switch defaultTransition : save-values defaultTransform : 'jq(. += {siteChanged: false})' conditions : - condition : 'jq(.etag == null and .lastModified == null)' transition : unsupported-site - condition : 'jq(.etag != .var.etag)' transition : save-values transform : 'jq(. += {siteChanged: true})' - condition : 'jq(.lastModified != .var.lastModified)' transition : save-values transform : 'jq(. += {siteChanged: true})' - id : unsupported-site type : error error : unsupported.site message : \"https://docs.direktiv.io is not supported: site must respond with atleast one of these headers: ['Etag', 'Last-Modified']\" Save current values Finally we save the current headers to the lastModified and etag workflow variables, so next time this workflow is executed they can be retrieved in the get-old-headers state. - id : save-values type : setter variables : - key : lastModified scope : workflow value : 'jq(.lastModified)' - key : etag scope : workflow value : 'jq(.etag)' Sample Output Note: the getter state will place variables into the var property. So the var.etag and var.lastModified values are the old headers. { \"etag\" : \"\\\"60d55d9b-54b1\\\"\" , \"lastModified\" : \"Fri, 25 Jun 2021 04:37:47 GMT\" , \"siteChanged\" : false , \"var\" : { \"etag\" : \"\\\"60d55d9b-54b1\\\"\" , \"lastModified\" : \"Fri, 25 Jun 2021 04:37:47 GMT\" } } Extra - Converting to a Cron Job This workflow can currently run as is, and be manually executed. However this example is more than likely to be used as a cron job . To convert this workflow all you need to do is add the start block to the top of the workflow. Below is an example that, if added to the workflow, will run this workflow every once every two hours. start : type : scheduled cron : \"0 */2 * * *\" Full Workflow id : a-cron-example description : A simple 'action' state that sends a get request\" functions : - id : get image : direktiv/request:v1 type : reusable states : - id : fetch-site-headers type : action transform : 'jq({lastModified: .return.headers.[\"Last-Modified\"][0], etag: .return.headers.[\"Etag\"][0]})' transition : get-old-headers action : function : get input : method : \"HEAD\" url : \"https://docs.direktiv.io\" - id : get-old-headers type : getter transition : check-site variables : - key : lastModified scope : workflow - key : etag scope : workflow - id : check-site type : switch defaultTransition : save-values defaultTransform : 'jq(. += {siteChanged: false})' conditions : - condition : 'jq(.etag == null and .lastModified == null)' transition : unsupported-site - condition : 'jq(.etag != .var.etag)' transition : save-values transform : 'jq(. += {siteChanged: true})' - condition : 'jq(.lastModified != .var.lastModified)' transition : save-values transform : 'jq(. += {siteChanged: true})' - id : unsupported-site type : error error : unsupported.site message : \"https://docs.direktiv.io is not supported: site must respond with atleast one of these headers: ['Etag', 'Last-Modified']\" - id : save-values type : setter variables : - key : lastModified scope : workflow value : 'jq(.lastModified)' - key : etag scope : workflow value : 'jq(.etag)'","title":"Check for Website Change"},{"location":"examples/website-change/#check-if-website-has-changed-example","text":"This example demonstrates a use case for a hypothetical scenario when you need to periodically check if a website has changed. It makes use of workflow variables to keep persistent data to test against. Most websites would likely have minor changes if you were to just fetch their HTML. These changes are usually stuff like time and session properties generated on each request. We don't want to categorize these changes as true as these are false positives. Luckily many websites have one or both header values: Etag and Last-Modified . If either one of these headers changes it is safe to assume that the website has been updated. Note: Although many websites have adopted this standard, there are still websites that have not. These websites are not supported. This example is fairly simple and can be broken up into four steps: 1. Fetch the current headers of the target website. 2. Get the saved headers variables from the previous execution of this workflow. 3. Compare the current and previous headers to determine if the target website has changed. 4. Save current headers to workflow variables. Below we'll explain each step in more detail. For this example we've chosen https://docs.direktiv.io as the target website.","title":"Check if Website has changed Example"},{"location":"examples/website-change/#fetch-website-headers","text":"Here we are performing a HTTP POST request on the target website using our direktiv/request:v1 direktiv app as a function. Since we only care about the Last-Modified and Etag headers we then extract those values and store them in the lastModified and etag properties using the transform field in the fetch-site-headers state. Now that we have fetched the current header values required, the state will transition to the get-old-headers state. id : check-website-change description : \"A simple workflow that fetches current headers from a website and compares them to the previously stored headers to determine if it has changed.\" functions : - id : get image : direktiv/request:v1 type : reusable states : - id : fetch-site-headers type : action transform : 'jq({lastModified: .return.headers.[\"Last-Modified\"][0], etag: .return.headers.[\"Etag\"][0]})' transition : get-old-headers action : function : get input : method : \"HEAD\" url : \"https://docs.direktiv.io\"","title":"Fetch Website Headers"},{"location":"examples/website-change/#get-old-headers","text":"Direktiv has variables scoped to namespaces, workflows and instance. In this state ( get-old-headers ) we'll get the variables lastModified and etag in the workflow scope. These variables are set to whatever the header values were last time when this workflow was executed. If this is the first time this workflow is executed these value will be null . This is fine, but it will cause the results to be siteChanged: true because the current headers can never equal null . - id : get-old-headers type : getter transition : check-site variables : - key : lastModified scope : workflow - key : etag scope : workflow","title":"Get Old Headers"},{"location":"examples/website-change/#compare-values","text":"Now that we have both the current and previous header values we can make a comparison and check whether the website has changed using the switch state check-site . The switch state below has three possible conditions. The first condition is used for validation and will transition to the error state unsupported-site if neither etag or lastModified was fetched from the current headers. The last two are to check if either of the etag or lastModified values have changed between the previous and current headers. If either one of these headers has changed it means that the website has changed and the property siteChanged is set to true. If none of these conditions are satisfied, the siteChanged property is set to false because we can assume that no errors/changes have occurred. - id : check-site type : switch defaultTransition : save-values defaultTransform : 'jq(. += {siteChanged: false})' conditions : - condition : 'jq(.etag == null and .lastModified == null)' transition : unsupported-site - condition : 'jq(.etag != .var.etag)' transition : save-values transform : 'jq(. += {siteChanged: true})' - condition : 'jq(.lastModified != .var.lastModified)' transition : save-values transform : 'jq(. += {siteChanged: true})' - id : unsupported-site type : error error : unsupported.site message : \"https://docs.direktiv.io is not supported: site must respond with atleast one of these headers: ['Etag', 'Last-Modified']\"","title":"Compare Values"},{"location":"examples/website-change/#save-current-values","text":"Finally we save the current headers to the lastModified and etag workflow variables, so next time this workflow is executed they can be retrieved in the get-old-headers state. - id : save-values type : setter variables : - key : lastModified scope : workflow value : 'jq(.lastModified)' - key : etag scope : workflow value : 'jq(.etag)'","title":"Save current values"},{"location":"examples/website-change/#sample-output","text":"Note: the getter state will place variables into the var property. So the var.etag and var.lastModified values are the old headers. { \"etag\" : \"\\\"60d55d9b-54b1\\\"\" , \"lastModified\" : \"Fri, 25 Jun 2021 04:37:47 GMT\" , \"siteChanged\" : false , \"var\" : { \"etag\" : \"\\\"60d55d9b-54b1\\\"\" , \"lastModified\" : \"Fri, 25 Jun 2021 04:37:47 GMT\" } }","title":"Sample Output"},{"location":"examples/website-change/#extra-converting-to-a-cron-job","text":"This workflow can currently run as is, and be manually executed. However this example is more than likely to be used as a cron job . To convert this workflow all you need to do is add the start block to the top of the workflow. Below is an example that, if added to the workflow, will run this workflow every once every two hours. start : type : scheduled cron : \"0 */2 * * *\"","title":"Extra - Converting to a Cron Job"},{"location":"examples/website-change/#full-workflow","text":"id : a-cron-example description : A simple 'action' state that sends a get request\" functions : - id : get image : direktiv/request:v1 type : reusable states : - id : fetch-site-headers type : action transform : 'jq({lastModified: .return.headers.[\"Last-Modified\"][0], etag: .return.headers.[\"Etag\"][0]})' transition : get-old-headers action : function : get input : method : \"HEAD\" url : \"https://docs.direktiv.io\" - id : get-old-headers type : getter transition : check-site variables : - key : lastModified scope : workflow - key : etag scope : workflow - id : check-site type : switch defaultTransition : save-values defaultTransform : 'jq(. += {siteChanged: false})' conditions : - condition : 'jq(.etag == null and .lastModified == null)' transition : unsupported-site - condition : 'jq(.etag != .var.etag)' transition : save-values transform : 'jq(. += {siteChanged: true})' - condition : 'jq(.lastModified != .var.lastModified)' transition : save-values transform : 'jq(. += {siteChanged: true})' - id : unsupported-site type : error error : unsupported.site message : \"https://docs.direktiv.io is not supported: site must respond with atleast one of these headers: ['Etag', 'Last-Modified']\" - id : save-values type : setter variables : - key : lastModified scope : workflow value : 'jq(.lastModified)' - key : etag scope : workflow value : 'jq(.etag)'","title":"Full Workflow"},{"location":"getting_started/conditional-transitions/","text":"Conditional Transitions Oftentimes a workflow needs to be a little bit smarter than an immutable sequence of states. That's when you might need conditional transitions. In this article you'll learn about instance input data, the Switch State, and how to define loops. Demo id : multiposter functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : ifelse type : switch conditions : - condition : jq(.names) transition : poster - id : poster type : action action : function : httprequest input : '{ \"method\": \"POST\", \"url\": \"https://jsonplaceholder.typicode.com/posts\", \"body\": { \"name\": .names[0] } }' transform : jq(del(.names[0])) transition : ifelse Input input : names : - \"Alan\" - \"Jon\" - \"Trent\" Output { \"names\" : [], \"return\" : { \"id\" : 101 } } Instance Input Workflows can be invoked with input data that will be available as instance data. There are a few ifs-and-buts that apply to input data, but it's not that complicated. Input JSON Object If the input is a JSON object it will become the instance data. Input Data { \"key\" : \"value\" } Instance Data { \"key\" : \"value\" } Input JSON Non-Object If the input is valid JSON but not an object it will be stored under \"input\" . Input Data [ 1 , 2 , 3 ] Instance Data { \"input\" : [ 1 , 2 , 3 ] } Input Non-JSON Finally, if the input is not valid JSON it will be base64 encoded into a string and then stored under \"input\" . Hello, world! Instance Data { \"input\" : \"SGVsbG8sIHdvcmxkIQ==\" } Switch State The Switch State can make decisions about where to transition to next based on the instance data by evaluating a number of jq expressions and checking the results. Here's an example switch state definition: - id : ifelse type : switch conditions : - condition : 'jq(.person.age > 18)' transition : accept #transform: - condition : 'jq(.person.age != nil)' transition : reject #transform: defaultTransition : failure #defaultTransform: Each of the conditions will be evaluated in the order it appears by running the jq command in condition . Any result other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a successful match. If no conditions match the default transition will be used. In the demo example the switch state will transition to poster until the list of names is empty, at which point the workflow will end. Other Conditional Transitions The Switch State is not the only way to do conditional transitions. The EventsXor state also transitions conditionally based on which CloudEvent was received. All states can also define handlers for catching various types of errors. Both of these will be discussed in a later article. Loops By transitioning to a state that has already happened it's possible to create loops in workflow instances. In this demo we've got a type of range loop, iterating over the contents of an array. Direktiv sets limits for the number of transitions an instance can make in order to protect itself from infinitely-looping workflows. Consider some of the following alternatives if you want to have a loop: Foreach State For range loops like the one in this demo there's another state called a Foreach State that simplifies the logic and splits up a data set to run many actions in parallel without doing lots of transitions. A fuller explanation of the Foreach State will be discussed in another article , but here's an equivalent workflow definition to the demo if you're curious: id : multiposter functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : poster type : foreach array : 'jq([.names[] | { name: . }])' action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/posts\" body : name : 'jq(.)' transform : 'jq(del(.names) | .names = [])' Retries One obvious use for loops is to retry some logic if an error occurs, but there's no need to design looping workflow because Direktiv has configurable error catching & retrying available on every action-based state. This will be discussed in a later article. Isolates For large data sets or logic that could needs to loop many times it's generally better to custom-write an Isolate function that performs all of the computation. Writing custom functions is discussed in another article.","title":"Conditional Transitions"},{"location":"getting_started/conditional-transitions/#conditional-transitions","text":"Oftentimes a workflow needs to be a little bit smarter than an immutable sequence of states. That's when you might need conditional transitions. In this article you'll learn about instance input data, the Switch State, and how to define loops.","title":"Conditional Transitions"},{"location":"getting_started/conditional-transitions/#demo","text":"id : multiposter functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : ifelse type : switch conditions : - condition : jq(.names) transition : poster - id : poster type : action action : function : httprequest input : '{ \"method\": \"POST\", \"url\": \"https://jsonplaceholder.typicode.com/posts\", \"body\": { \"name\": .names[0] } }' transform : jq(del(.names[0])) transition : ifelse","title":"Demo"},{"location":"getting_started/conditional-transitions/#input","text":"input : names : - \"Alan\" - \"Jon\" - \"Trent\"","title":"Input"},{"location":"getting_started/conditional-transitions/#output","text":"{ \"names\" : [], \"return\" : { \"id\" : 101 } }","title":"Output"},{"location":"getting_started/conditional-transitions/#instance-input","text":"Workflows can be invoked with input data that will be available as instance data. There are a few ifs-and-buts that apply to input data, but it's not that complicated.","title":"Instance Input"},{"location":"getting_started/conditional-transitions/#input-json-object","text":"If the input is a JSON object it will become the instance data. Input Data { \"key\" : \"value\" } Instance Data { \"key\" : \"value\" }","title":"Input JSON Object"},{"location":"getting_started/conditional-transitions/#input-json-non-object","text":"If the input is valid JSON but not an object it will be stored under \"input\" . Input Data [ 1 , 2 , 3 ] Instance Data { \"input\" : [ 1 , 2 , 3 ] }","title":"Input JSON Non-Object"},{"location":"getting_started/conditional-transitions/#input-non-json","text":"Finally, if the input is not valid JSON it will be base64 encoded into a string and then stored under \"input\" . Hello, world! Instance Data { \"input\" : \"SGVsbG8sIHdvcmxkIQ==\" }","title":"Input Non-JSON"},{"location":"getting_started/conditional-transitions/#switch-state","text":"The Switch State can make decisions about where to transition to next based on the instance data by evaluating a number of jq expressions and checking the results. Here's an example switch state definition: - id : ifelse type : switch conditions : - condition : 'jq(.person.age > 18)' transition : accept #transform: - condition : 'jq(.person.age != nil)' transition : reject #transform: defaultTransition : failure #defaultTransform: Each of the conditions will be evaluated in the order it appears by running the jq command in condition . Any result other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a successful match. If no conditions match the default transition will be used. In the demo example the switch state will transition to poster until the list of names is empty, at which point the workflow will end.","title":"Switch State"},{"location":"getting_started/conditional-transitions/#other-conditional-transitions","text":"The Switch State is not the only way to do conditional transitions. The EventsXor state also transitions conditionally based on which CloudEvent was received. All states can also define handlers for catching various types of errors. Both of these will be discussed in a later article.","title":"Other Conditional Transitions"},{"location":"getting_started/conditional-transitions/#loops","text":"By transitioning to a state that has already happened it's possible to create loops in workflow instances. In this demo we've got a type of range loop, iterating over the contents of an array. Direktiv sets limits for the number of transitions an instance can make in order to protect itself from infinitely-looping workflows. Consider some of the following alternatives if you want to have a loop:","title":"Loops"},{"location":"getting_started/conditional-transitions/#foreach-state","text":"For range loops like the one in this demo there's another state called a Foreach State that simplifies the logic and splits up a data set to run many actions in parallel without doing lots of transitions. A fuller explanation of the Foreach State will be discussed in another article , but here's an equivalent workflow definition to the demo if you're curious: id : multiposter functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : poster type : foreach array : 'jq([.names[] | { name: . }])' action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/posts\" body : name : 'jq(.)' transform : 'jq(del(.names) | .names = [])'","title":"Foreach State"},{"location":"getting_started/conditional-transitions/#retries","text":"One obvious use for loops is to retry some logic if an error occurs, but there's no need to design looping workflow because Direktiv has configurable error catching & retrying available on every action-based state. This will be discussed in a later article.","title":"Retries"},{"location":"getting_started/conditional-transitions/#isolates","text":"For large data sets or logic that could needs to loop many times it's generally better to custom-write an Isolate function that performs all of the computation. Writing custom functions is discussed in another article.","title":"Isolates"},{"location":"getting_started/error-handling/","text":"Error Handling Handling errors can be an important part of a workflow. In this article you'll learn about timeouts, how to catch errors, retries, and recovery. Demo id : unreliable start : type : scheduled cron : \"0 * * * *\" functions : - id : select image : direktiv/select:v1 type : reusable - id : insert image : direktiv/insert:v1 type : reusable - id : delete image : direktiv/delete:v1 type : reusable - id : cruncher image : direktiv/cruncher:v1 type : reusable - id : notify image : direktiv/notifier:v1 type : reusable states : - id : select-rows type : action action : function : select retries : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] transform : 'jq(.return)' transition : crunch-numbers catch : - error : \"*\" - id : crunch-numbers type : action action : function : cruncher transform : 'jq(.return)' transition : store-some-results - id : store-some-results type : action action : function : insert input : 'jq(.someResults)' transition : store-other-results catch : - error : \"*\" transition : report-failure - id : store-other-results type : action action : function : insert input : 'jq(.otherResults)' catch : - error : \"*\" transition : revert-store-some-results - id : revert-store-some-results type : action action : function : delete input : 'jq(.someResults)' transition : report-failure - id : report-failure type : action action : function : notifier In this demo the select , delete , and insert functions are hypothetical containers that interact with a database, and the crunch function is a hypothetical container that produces some output from an input. The notifier function is a stand-in for whatever method you want to report an error: email, SMS, etc. This demo simulates some sort of database transaction through a workflow in order to demonstrate error handling. In reality you should write a custom Isolate to actually perform a real database transaction if possible. Catchable Errors Errors that occur during instance execution usually are considered \"catchable\". Any workflow state may optionally define error catchers, and if a catchable error is raised Direktiv will check to see if any catchers can handle it. Errors have a \"code\", which is a string formatted in a style similar to a domain name. Error catchers can explicitly catch a single error code or they can use * wildcards in their error codes to catch ranges of errors. Setting the error catcher to just \" * \" means it will handle any error, so long as no catcher defined higher up in the list has already caught it. If no catcher is able to handle an error, the workflow will fail immediately. Uncatchable Errors Rarely, some errors are considered \"uncatchable\", but generally an uncatchable error becomes catchable if escalated to a calling workflow. One example of this is the error triggered by Direktiv if a workflow fails to complete within its maximum timeout. If a workflow fails to complete within its maximum timeout it will not be given an opportunity to catch the error and continue running. But if that workflow is running as a subflow its parent workflow will be able to detect and handle that error. Retries Action definitions may optionally define a retry strategy. If a retry strategy is defined the catcher's transition won't be used and no error will be escalated for retryable errors until all retries have failed. A retry strategy might look like the following: retry : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] In this example you can see that a maximum number of attempts is defined, alongside an initial delay between attempts and a multiplication factor to apply to the delay between subsequent attempts. Recovery Workflows sometimes perform actions which may need to be reverted or undone if the workflow as a whole cannot complete successfully. Solving these problems requires careful use of error catchers and transitions.","title":"Error Handling"},{"location":"getting_started/error-handling/#error-handling","text":"Handling errors can be an important part of a workflow. In this article you'll learn about timeouts, how to catch errors, retries, and recovery.","title":"Error Handling"},{"location":"getting_started/error-handling/#demo","text":"id : unreliable start : type : scheduled cron : \"0 * * * *\" functions : - id : select image : direktiv/select:v1 type : reusable - id : insert image : direktiv/insert:v1 type : reusable - id : delete image : direktiv/delete:v1 type : reusable - id : cruncher image : direktiv/cruncher:v1 type : reusable - id : notify image : direktiv/notifier:v1 type : reusable states : - id : select-rows type : action action : function : select retries : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] transform : 'jq(.return)' transition : crunch-numbers catch : - error : \"*\" - id : crunch-numbers type : action action : function : cruncher transform : 'jq(.return)' transition : store-some-results - id : store-some-results type : action action : function : insert input : 'jq(.someResults)' transition : store-other-results catch : - error : \"*\" transition : report-failure - id : store-other-results type : action action : function : insert input : 'jq(.otherResults)' catch : - error : \"*\" transition : revert-store-some-results - id : revert-store-some-results type : action action : function : delete input : 'jq(.someResults)' transition : report-failure - id : report-failure type : action action : function : notifier In this demo the select , delete , and insert functions are hypothetical containers that interact with a database, and the crunch function is a hypothetical container that produces some output from an input. The notifier function is a stand-in for whatever method you want to report an error: email, SMS, etc. This demo simulates some sort of database transaction through a workflow in order to demonstrate error handling. In reality you should write a custom Isolate to actually perform a real database transaction if possible.","title":"Demo"},{"location":"getting_started/error-handling/#catchable-errors","text":"Errors that occur during instance execution usually are considered \"catchable\". Any workflow state may optionally define error catchers, and if a catchable error is raised Direktiv will check to see if any catchers can handle it. Errors have a \"code\", which is a string formatted in a style similar to a domain name. Error catchers can explicitly catch a single error code or they can use * wildcards in their error codes to catch ranges of errors. Setting the error catcher to just \" * \" means it will handle any error, so long as no catcher defined higher up in the list has already caught it. If no catcher is able to handle an error, the workflow will fail immediately.","title":"Catchable Errors"},{"location":"getting_started/error-handling/#uncatchable-errors","text":"Rarely, some errors are considered \"uncatchable\", but generally an uncatchable error becomes catchable if escalated to a calling workflow. One example of this is the error triggered by Direktiv if a workflow fails to complete within its maximum timeout. If a workflow fails to complete within its maximum timeout it will not be given an opportunity to catch the error and continue running. But if that workflow is running as a subflow its parent workflow will be able to detect and handle that error.","title":"Uncatchable Errors"},{"location":"getting_started/error-handling/#retries","text":"Action definitions may optionally define a retry strategy. If a retry strategy is defined the catcher's transition won't be used and no error will be escalated for retryable errors until all retries have failed. A retry strategy might look like the following: retry : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] In this example you can see that a maximum number of attempts is defined, alongside an initial delay between attempts and a multiplication factor to apply to the delay between subsequent attempts.","title":"Retries"},{"location":"getting_started/error-handling/#recovery","text":"Workflows sometimes perform actions which may need to be reverted or undone if the workflow as a whole cannot complete successfully. Solving these problems requires careful use of error catchers and transitions.","title":"Recovery"},{"location":"getting_started/events/","text":"Events Direktiv has built-in support for CloudEvents, which can be a great way to interact with workflows. In this article you'll learn about events. Demo id : notifier start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : notify type : action action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.\"com.github.pull.create\")' CloudEvents CloudEvents are specification for describing event data in a common way. They're JSON objects with a number of required fields, some optional fields, and a payload. Here's an example CloudEvent: { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } CloudEvents can be sent via the API to a namespace, to be handled by any number of interested receivers on that namespace. Start Types The most common use for events in Direktiv is to have external services generate CloudEvents and send them to Direktiv to trigger your workflows. But to make your workflows trigger on an event you need to register the workflow's interest in the event by adding the appropriate start type to your workflow definition: start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" In this example a new instance will be created from our workflow whenever a cloudevent is received that has the matching type and source values. Two other event-based start types exist in Direktiv: the eventsXor , and the eventsAnd . The eventsXor registers an interest in multiple events and will trigger a new instance as soon as any one of them is received. The eventsAnd also registers an interest in multiple events, but will only trigger once all have been received. Event Payloads Whenever an event is received its payload will be added to the instance data under a field with the same name as the event \"type\". This allows for a uniform approach to accepting events that supports single events, eventsXor, and eventsAnd. The payload itself consists of the full cloudevent including attributes, extension context attributes and data. Instances Waiting for Events Triggering workflows is not the only thing you can do with events. Workflows can be constructed to run some logic and then wait for an event before proceeding. Like the event-based start types, there are three event consuming states: consumeEvent , eventsXor , and eventsAnd . Here's an example of what a ConsumeEvent State could look like: - id : wait-event type : consumeEvent event : type : com.github.pull.create context : source : \"https://github.com/cloudevents/spec/pull\" repository : 'jq(.repo)' timeout : PT5M transform : 'jq(.\"com.github.pull.create\")' transition : next-state Timeouts It's rarely a good idea to leave a workflow waiting indefinitely. Direktiv allows you to define timeouts in ISO8601 format when waiting on an event. If the state is not ready to proceed before the timeout has elapsed an error will be thrown. It's possible to catch this error, but that's for a later article. The timeout field is not required, but Direktiv caps the maximum timeout whether specified or not to prevent workflows from living forever. Context Similar to how the event-based start types have a filters field, event-consuming states have a context field. Like filters, the context field can restrict which events are considered matches by requiring an exact match on a CloudEvent context field. Unlike filters, context values can be determined dynamically based on instance data. GenerateEvent State Workflows can generate events for their namespace without relying on an Isolate using the GenerateEvent State. The fields for this state are fairly self-explanatory. Here's an example: - id : gen-event type : generateEvent event : type : \"my.custom.event\" source : \"direktiv\" data : 'jq(.)' datacontenttype : \"application/json\" If the jq command that populates the data field outputs a plain base64 encoded string and the datacontenttype field is set to anything other than application/json Direktiv will decode the string before sending the event.","title":"Events"},{"location":"getting_started/events/#events","text":"Direktiv has built-in support for CloudEvents, which can be a great way to interact with workflows. In this article you'll learn about events.","title":"Events"},{"location":"getting_started/events/#demo","text":"id : notifier start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : notify type : action action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.\"com.github.pull.create\")'","title":"Demo"},{"location":"getting_started/events/#cloudevents","text":"CloudEvents are specification for describing event data in a common way. They're JSON objects with a number of required fields, some optional fields, and a payload. Here's an example CloudEvent: { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } CloudEvents can be sent via the API to a namespace, to be handled by any number of interested receivers on that namespace.","title":"CloudEvents"},{"location":"getting_started/events/#start-types","text":"The most common use for events in Direktiv is to have external services generate CloudEvents and send them to Direktiv to trigger your workflows. But to make your workflows trigger on an event you need to register the workflow's interest in the event by adding the appropriate start type to your workflow definition: start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" In this example a new instance will be created from our workflow whenever a cloudevent is received that has the matching type and source values. Two other event-based start types exist in Direktiv: the eventsXor , and the eventsAnd . The eventsXor registers an interest in multiple events and will trigger a new instance as soon as any one of them is received. The eventsAnd also registers an interest in multiple events, but will only trigger once all have been received.","title":"Start Types"},{"location":"getting_started/events/#event-payloads","text":"Whenever an event is received its payload will be added to the instance data under a field with the same name as the event \"type\". This allows for a uniform approach to accepting events that supports single events, eventsXor, and eventsAnd. The payload itself consists of the full cloudevent including attributes, extension context attributes and data.","title":"Event Payloads"},{"location":"getting_started/events/#instances-waiting-for-events","text":"Triggering workflows is not the only thing you can do with events. Workflows can be constructed to run some logic and then wait for an event before proceeding. Like the event-based start types, there are three event consuming states: consumeEvent , eventsXor , and eventsAnd . Here's an example of what a ConsumeEvent State could look like: - id : wait-event type : consumeEvent event : type : com.github.pull.create context : source : \"https://github.com/cloudevents/spec/pull\" repository : 'jq(.repo)' timeout : PT5M transform : 'jq(.\"com.github.pull.create\")' transition : next-state","title":"Instances Waiting for Events"},{"location":"getting_started/events/#timeouts","text":"It's rarely a good idea to leave a workflow waiting indefinitely. Direktiv allows you to define timeouts in ISO8601 format when waiting on an event. If the state is not ready to proceed before the timeout has elapsed an error will be thrown. It's possible to catch this error, but that's for a later article. The timeout field is not required, but Direktiv caps the maximum timeout whether specified or not to prevent workflows from living forever.","title":"Timeouts"},{"location":"getting_started/events/#context","text":"Similar to how the event-based start types have a filters field, event-consuming states have a context field. Like filters, the context field can restrict which events are considered matches by requiring an exact match on a CloudEvent context field. Unlike filters, context values can be determined dynamically based on instance data.","title":"Context"},{"location":"getting_started/events/#generateevent-state","text":"Workflows can generate events for their namespace without relying on an Isolate using the GenerateEvent State. The fields for this state are fairly self-explanatory. Here's an example: - id : gen-event type : generateEvent event : type : \"my.custom.event\" source : \"direktiv\" data : 'jq(.)' datacontenttype : \"application/json\" If the jq command that populates the data field outputs a plain base64 encoded string and the datacontenttype field is set to anything other than application/json Direktiv will decode the string before sending the event.","title":"GenerateEvent State"},{"location":"getting_started/functions-intro/","text":"Introduction to Functions Workflows wouldn't be very powerful if they were limited to just the predefined states. That's why Direktiv can run \"functions\" which are basically serverless containers (or even a separate workflow, referred to as a subflow ). In this article you'll learn about the Action State and get an introduction to functions. Demo id : httpget functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" This workflow will use the Docker container at https://hub.docker.com/r/direktiv/request to perform a GET request and return the results to the instance data. Not just any Docker container will work as a function, but it isn't difficult to make one compatible. We'll discuss that later. Run this workflow. Leave the Workflow Input empty for now. You should see something like the following: Input {} Output { \"return\" : { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"completed\" : false } } The JSON structure under \"return\" is the object returned by the GET request. What is a function? 'Function' is just a term we use when we run a serverless container. Direktiv grabs a Docker container from an available Docker Registry: hub.docker.com unless custom registries are defined (more on that later). It then runs this container as a \"function\". If the container handles input and output according to our Function requirements it can do just about anything (more on our Function requirements later as well). Function Definitions functions : - id : httprequest image : direktiv/request:v1 type : reusable To use a Function it must first be defined at the top of the workflow definition. Each function definition needs an identifier that must be unique within the workflow definition. For functions of type: reusable , the image field must always be provided, pointing to the desired container image. Action State - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Like all other states, the Action State requires an id and type field identifying it as such. But the great thing about the Action State is its ability to run user-made logic in the form of \"Functions\". The function field must reference one of the functions defined in the workflow definition. In this example we're using direktiv/request , which is a simple container that performs a HTTP request and returns the results. We use a jq command specified in the input field to generate the input for the Function. Once the Function has completed its task in the Action State the results are stored in the instance data under the \"return\" field.","title":"Introduction to Functions"},{"location":"getting_started/functions-intro/#introduction-to-functions","text":"Workflows wouldn't be very powerful if they were limited to just the predefined states. That's why Direktiv can run \"functions\" which are basically serverless containers (or even a separate workflow, referred to as a subflow ). In this article you'll learn about the Action State and get an introduction to functions.","title":"Introduction to Functions"},{"location":"getting_started/functions-intro/#demo","text":"id : httpget functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" This workflow will use the Docker container at https://hub.docker.com/r/direktiv/request to perform a GET request and return the results to the instance data. Not just any Docker container will work as a function, but it isn't difficult to make one compatible. We'll discuss that later. Run this workflow. Leave the Workflow Input empty for now. You should see something like the following:","title":"Demo"},{"location":"getting_started/functions-intro/#input","text":"{}","title":"Input"},{"location":"getting_started/functions-intro/#output","text":"{ \"return\" : { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"completed\" : false } } The JSON structure under \"return\" is the object returned by the GET request.","title":"Output"},{"location":"getting_started/functions-intro/#what-is-a-function","text":"'Function' is just a term we use when we run a serverless container. Direktiv grabs a Docker container from an available Docker Registry: hub.docker.com unless custom registries are defined (more on that later). It then runs this container as a \"function\". If the container handles input and output according to our Function requirements it can do just about anything (more on our Function requirements later as well).","title":"What is a function?"},{"location":"getting_started/functions-intro/#function-definitions","text":"functions : - id : httprequest image : direktiv/request:v1 type : reusable To use a Function it must first be defined at the top of the workflow definition. Each function definition needs an identifier that must be unique within the workflow definition. For functions of type: reusable , the image field must always be provided, pointing to the desired container image.","title":"Function Definitions"},{"location":"getting_started/functions-intro/#action-state","text":"- id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Like all other states, the Action State requires an id and type field identifying it as such. But the great thing about the Action State is its ability to run user-made logic in the form of \"Functions\". The function field must reference one of the functions defined in the workflow definition. In this example we're using direktiv/request , which is a simple container that performs a HTTP request and returns the results. We use a jq command specified in the input field to generate the input for the Function. Once the Function has completed its task in the Action State the results are stored in the instance data under the \"return\" field.","title":"Action State"},{"location":"getting_started/helloworld/","text":"Hello, World! Every workflow begins with a YAML-based \"Workflow Definition\". In this article you'll learn the first basics of the workflow definition. Demo id : helloworld states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' Run this workflow. Leave the Workflow Input empty for now. You should see something like the following: Output { \"msg\" : \"Hello, world!\" } Workflow Definition Now that we've seen it in action, let's go through each of the lines in the Workflow Definition and understand what they mean. Workflow ID id : helloworld Every workflow definition needs to specify a workflow identifier at the top of the document. The identifier is used by UIs and other workflows as a reference to this workflow. This identifier must be unique within the namespace (more on namespaces later). States states : A workflow just wouldn't be a workflow without states to actually do something. Every workflow must have at least one state. State ID - id : hello Like the workflow itself, every state has to have its own identifier. The state identifier is used in logging and to define transitions, which will come up in a later example when we define more than one state. A state identifier must be unique within the workflow definition. State Type type : noop There are many types of state that do all sorts of different things. We'll go over the possible states later, but for this example we're using the noop state. The noop state (short for \"no-operation\") does nothing other than log Instance Data. Transform Command transform : 'jq({ msg: \"Hello, world!\" })' Any state may optionally define a \"transform\", and it's used here to generate the classic \"Hello, World!\" message. Transform applies a jq command to the instance data and replaces the instance data with the results. We'll go into more detail about transforms later.","title":"Helloworld"},{"location":"getting_started/helloworld/#hello-world","text":"Every workflow begins with a YAML-based \"Workflow Definition\". In this article you'll learn the first basics of the workflow definition.","title":"Hello, World!"},{"location":"getting_started/helloworld/#demo","text":"id : helloworld states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' Run this workflow. Leave the Workflow Input empty for now. You should see something like the following:","title":"Demo"},{"location":"getting_started/helloworld/#output","text":"{ \"msg\" : \"Hello, world!\" }","title":"Output"},{"location":"getting_started/helloworld/#workflow-definition","text":"Now that we've seen it in action, let's go through each of the lines in the Workflow Definition and understand what they mean.","title":"Workflow Definition"},{"location":"getting_started/helloworld/#workflow-id","text":"id : helloworld Every workflow definition needs to specify a workflow identifier at the top of the document. The identifier is used by UIs and other workflows as a reference to this workflow. This identifier must be unique within the namespace (more on namespaces later).","title":"Workflow ID"},{"location":"getting_started/helloworld/#states","text":"states : A workflow just wouldn't be a workflow without states to actually do something. Every workflow must have at least one state.","title":"States"},{"location":"getting_started/helloworld/#state-id","text":"- id : hello Like the workflow itself, every state has to have its own identifier. The state identifier is used in logging and to define transitions, which will come up in a later example when we define more than one state. A state identifier must be unique within the workflow definition.","title":"State ID"},{"location":"getting_started/helloworld/#state-type","text":"type : noop There are many types of state that do all sorts of different things. We'll go over the possible states later, but for this example we're using the noop state. The noop state (short for \"no-operation\") does nothing other than log Instance Data.","title":"State Type"},{"location":"getting_started/helloworld/#transform-command","text":"transform : 'jq({ msg: \"Hello, world!\" })' Any state may optionally define a \"transform\", and it's used here to generate the classic \"Hello, World!\" message. Transform applies a jq command to the instance data and replaces the instance data with the results. We'll go into more detail about transforms later.","title":"Transform Command"},{"location":"getting_started/isolated-functions/","text":"Isolated Functions An isolated function is written for a specific purpose. It requires an input, presents an output, and is capable of returning error information in the event that anything goes worong - not unlike regular functions. An isolated function is booted up, performs its task, and then immediately terminates. Compared to other function types this is a simple and straight-forward design, but it comes at a cost to performance and latency. Because an isolated function is not a server, it does not receive or return input data via web requests. Instead, specific filepaths are used for input, output, errors, and to tell Direktiv that the function has completed. Input Your function's input data can be found at /direktiv-data/input.json . Logging Write logs to /direktiv-data/out.log . You should probably create this file on startup, even if you don't log anything. Logs should be written to this file. They will be read in line-by-line by Direktiv. Responding If the function executes correctly you must write the results to /direktiv-data/output.json . Standard rules apply. This will usually be JSON, but can be something else, which will be base64 encoded by direktiv. Errors If you want to report that something has gone wrong, write a JSON object to /direktiv-data/error.json with the code and message keys. Same rules as other functions: any provided code makes the error catchable, otherwise it's uncatchable. Example: { \"code\": \"badInput\", \"message\": \"Should be a bool, not an int.\" } Finishing It is critical that you create a file called /direktiv-data/done when you're finished. You don't need to write anything into it, but from the moment it's created you're in a race to the finish. The sidecar will kick in and report the results and terminate the container. You should either report an Error or Respond with results before creating this file. After creating this file, you should exit with exit status zero.","title":"Isolated Functions"},{"location":"getting_started/isolated-functions/#isolated-functions","text":"An isolated function is written for a specific purpose. It requires an input, presents an output, and is capable of returning error information in the event that anything goes worong - not unlike regular functions. An isolated function is booted up, performs its task, and then immediately terminates. Compared to other function types this is a simple and straight-forward design, but it comes at a cost to performance and latency. Because an isolated function is not a server, it does not receive or return input data via web requests. Instead, specific filepaths are used for input, output, errors, and to tell Direktiv that the function has completed.","title":"Isolated Functions"},{"location":"getting_started/isolated-functions/#input","text":"Your function's input data can be found at /direktiv-data/input.json .","title":"Input"},{"location":"getting_started/isolated-functions/#logging","text":"Write logs to /direktiv-data/out.log . You should probably create this file on startup, even if you don't log anything. Logs should be written to this file. They will be read in line-by-line by Direktiv.","title":"Logging"},{"location":"getting_started/isolated-functions/#responding","text":"If the function executes correctly you must write the results to /direktiv-data/output.json . Standard rules apply. This will usually be JSON, but can be something else, which will be base64 encoded by direktiv.","title":"Responding"},{"location":"getting_started/isolated-functions/#errors","text":"If you want to report that something has gone wrong, write a JSON object to /direktiv-data/error.json with the code and message keys. Same rules as other functions: any provided code makes the error catchable, otherwise it's uncatchable. Example: { \"code\": \"badInput\", \"message\": \"Should be a bool, not an int.\" }","title":"Errors"},{"location":"getting_started/isolated-functions/#finishing","text":"It is critical that you create a file called /direktiv-data/done when you're finished. You don't need to write anything into it, but from the moment it's created you're in a race to the finish. The sidecar will kick in and report the results and terminate the container. You should either report an Error or Respond with results before creating this file. After creating this file, you should exit with exit status zero.","title":"Finishing"},{"location":"getting_started/making-functions/","text":"Making Custom Functions Need to do something more than what's supported by Direktiv? You can do just about anything by making your own custom functions to run as Functions. To make things easier Direktiv makes its Functions automatically from Docker images, but you can't just run any Docker image. In this article you'll learn how to make a Direktiv-compatible Docker image. Functions Direktiv functions are only meant to be run for short lengths to time. They're meant to take some input, do something, optionally generate some output, and then return. To enforce this Direktiv will pull the plug on any instance that runs too long, and this timeout is separate from any timeouts defined in a workflow definition. In general each function is one http request from start to end. Input Data The input data comes as post request data for each call to this function. The format is usually JSON but depending on your workflow it can be any data. It is the function-author's responsibility to load this data and extract useful information from it. Input validation is optional, but encouraged. Output Data Data generated by the function should be returned as a response to that request, also usually as JSON. Reporting Errors If something goes wrong a function can report an error to the calling workflow instance by adding HTTP headers to the response. If these headers are populated the execution of the function will be considered a failure regardless of what's stored in response data. The headers to report errors are: Direktiv-ErrorCode and Direktiv-ErrorMessage . If an error message is defined without defining an error code the calling workflow instance will be marked as \"crashed\" without exposing any helpful information, so it's important to always define both. Errors raised by functions are always 'catchable' by their error codes. Here are sample error headers: \"Direktiv-ErrorCode\": \"myapp.input\", \"Direktiv-ErrorMessage\": \"Missing 'customerId' property in JSON input.\" Example Have a look at the source code for one of the functions we've used a lot in these articles: https://github.com/direktiv/direktiv-apps/blob/master/request/main.go FAQs & Other Information Function Logs Logging for functions is a simple HTTP POST or GET request to the address: http://localhost:8889/log?aid=$ACTIONID. If POST is used the body of the request is getting logged for GET requests add a log request parameter. The important parameter is $ACTIONID. Each requests gets an action id which identifies the workflow instance. This parameter has to be passed back to attach the log to the instance. This information is passed in as in the initial request ( Direktiv-ActionID ). Networking Direktiv configures the network so that functions can reach out to the internet and receive responses. In general functions have no externally routable IP address. Internal networking policies will be implemented in the next releases.","title":"Making Custom Functions"},{"location":"getting_started/making-functions/#making-custom-functions","text":"Need to do something more than what's supported by Direktiv? You can do just about anything by making your own custom functions to run as Functions. To make things easier Direktiv makes its Functions automatically from Docker images, but you can't just run any Docker image. In this article you'll learn how to make a Direktiv-compatible Docker image.","title":"Making Custom Functions"},{"location":"getting_started/making-functions/#functions","text":"Direktiv functions are only meant to be run for short lengths to time. They're meant to take some input, do something, optionally generate some output, and then return. To enforce this Direktiv will pull the plug on any instance that runs too long, and this timeout is separate from any timeouts defined in a workflow definition. In general each function is one http request from start to end.","title":"Functions"},{"location":"getting_started/making-functions/#input-data","text":"The input data comes as post request data for each call to this function. The format is usually JSON but depending on your workflow it can be any data. It is the function-author's responsibility to load this data and extract useful information from it. Input validation is optional, but encouraged.","title":"Input Data"},{"location":"getting_started/making-functions/#output-data","text":"Data generated by the function should be returned as a response to that request, also usually as JSON.","title":"Output Data"},{"location":"getting_started/making-functions/#reporting-errors","text":"If something goes wrong a function can report an error to the calling workflow instance by adding HTTP headers to the response. If these headers are populated the execution of the function will be considered a failure regardless of what's stored in response data. The headers to report errors are: Direktiv-ErrorCode and Direktiv-ErrorMessage . If an error message is defined without defining an error code the calling workflow instance will be marked as \"crashed\" without exposing any helpful information, so it's important to always define both. Errors raised by functions are always 'catchable' by their error codes. Here are sample error headers: \"Direktiv-ErrorCode\": \"myapp.input\", \"Direktiv-ErrorMessage\": \"Missing 'customerId' property in JSON input.\"","title":"Reporting Errors"},{"location":"getting_started/making-functions/#example","text":"Have a look at the source code for one of the functions we've used a lot in these articles: https://github.com/direktiv/direktiv-apps/blob/master/request/main.go","title":"Example"},{"location":"getting_started/making-functions/#faqs-other-information","text":"","title":"FAQs &amp; Other Information"},{"location":"getting_started/making-functions/#function-logs","text":"Logging for functions is a simple HTTP POST or GET request to the address: http://localhost:8889/log?aid=$ACTIONID. If POST is used the body of the request is getting logged for GET requests add a log request parameter. The important parameter is $ACTIONID. Each requests gets an action id which identifies the workflow instance. This parameter has to be passed back to attach the log to the instance. This information is passed in as in the initial request ( Direktiv-ActionID ).","title":"Function Logs"},{"location":"getting_started/making-functions/#networking","text":"Direktiv configures the network so that functions can reach out to the internet and receive responses. In general functions have no externally routable IP address. Internal networking policies will be implemented in the next releases.","title":"Networking"},{"location":"getting_started/persistent-data/","text":"Persistent Data Direktiv supports storing and retrieving data that is persisted beyond the scope of a single state or workflow instance. In this article you'll learn about the different scopes and the states that can interact with them. Demo id : counter states : - id : a type : getter variables : - key : x scope : workflow transform : 'jq(.var.x += 1)' transition : b - id : b type : setter variables : - key : x scope : workflow value : 'jq(.var.x)' This demo increments a counter each time the workflow is executed. Scopes There are three scopes for storing persistent data: instance , workflow , and namespace . Data stored in the instance scope only exists for the duration of the running workflow instance. Data stored in the workflow scope exists until the workflow definition is deleted, and is accessible to all instances of that workflow. Data stored in the namespace scope exists until the namespace itself is deleted, and is accessible to all instances of all workflows originating on that namespace. Setter State The Setter State can be used to store any number of variables. Each variable must be explicitly scoped, and the value stored for a variable is generated by the output of a jq query. - id : a type : setter variables : - key : MyVar scope : namespace value : 'jq(.x)' The only way to delete a stored value is to set it to null . Getter State The Getter State is used to retrieve any number of variables in persistent storage. Each variable must be explicitly scoped, and the value retrieved will be stored under .var.KEY where KEY is the variable's name. - id : a type : getter variables : - key : x scope : namespace A key doesn't need to exist in storage to return successfully, but the value returned will be null if it doesn't exist. Concurrency Direktiv makes no effort to guarantee any thread-safety on persistent data. Multiple instances that interact with the same variable may have inconsistent results. Getting & Setting from Functions Getting Accessing persistent data from within a function is a fairly straightforward process. The request that the custom function receives from Direktiv contains a header 'Direktiv-TempDir', which contains all of the variables specified in the function definition. The as , key , scope , and type fields can all play a role in the placement and naming of files within this directory: key The key used to select a variable from within the workflow definition. If no as field is provided, the file on a custom function will correspond to the value of key . scope Which scope to get the variable from: instance , workflow , or namespace . Defaults to instance if omitted. as An optional field used to set the name of the file as it appears on the isolate. type plain The variable data inside of the file will be written 'as-is'. base64 If the variable is stored as base64-encoded data, it will be decoded before being written to the file system. tar If the variable is a valid tar archive, a directory will be created instead of a file, with the contents of the tar archive populating it. tar.gz Similar to tar , this will result in a populated directory being created from a valid .tar.gz file. For example, given the following state definition, a directory named 'myFiles' should exist within the directory specified by the Direktiv-TempDir header. Assuming that this header has a value of /mnt/shared/example , the following structure would be expected: - id : get image : localhost:5000/iv-getter:v1 files : - key : \"myFiles\" scope : instance type : tar /mnt/shared/example/ \u2514\u2500\u2500 myFiles \u2514\u2500\u2500 file-1 \u2514\u2500\u2500 file-2 \u2514\u2500\u2500 file-3 Setting From within an isolate running on Direktiv, variables can be set by sending a POST request: POST http://localhost:8889/var?aid=<EXAMPLE>&scope=instance&key=myFiles Body: <VARIABLE DATA> query parameters aid The action ID, found from the Direktiv-ActionID header of the request being served by the isolate. scope The scope for which the variable is set ( namespace , workflow , or instance ) key The key used by subsequent actions to access the variable. An alternative approach is to write files into certain directories. The direktiv sidecar will store those files as variables. There are three different folders for the three different scopes. For the above example they would be: /mnt/shared/example/out/instance /mnt/shared/example/out/workflow /mnt/shared/example/out/namespace Files under these folders will be stored with their names under the scope of the folder. Diretories will be stored as tar.gz files.","title":"Persistent Data"},{"location":"getting_started/persistent-data/#persistent-data","text":"Direktiv supports storing and retrieving data that is persisted beyond the scope of a single state or workflow instance. In this article you'll learn about the different scopes and the states that can interact with them.","title":"Persistent Data"},{"location":"getting_started/persistent-data/#demo","text":"id : counter states : - id : a type : getter variables : - key : x scope : workflow transform : 'jq(.var.x += 1)' transition : b - id : b type : setter variables : - key : x scope : workflow value : 'jq(.var.x)' This demo increments a counter each time the workflow is executed.","title":"Demo"},{"location":"getting_started/persistent-data/#scopes","text":"There are three scopes for storing persistent data: instance , workflow , and namespace . Data stored in the instance scope only exists for the duration of the running workflow instance. Data stored in the workflow scope exists until the workflow definition is deleted, and is accessible to all instances of that workflow. Data stored in the namespace scope exists until the namespace itself is deleted, and is accessible to all instances of all workflows originating on that namespace.","title":"Scopes"},{"location":"getting_started/persistent-data/#setter-state","text":"The Setter State can be used to store any number of variables. Each variable must be explicitly scoped, and the value stored for a variable is generated by the output of a jq query. - id : a type : setter variables : - key : MyVar scope : namespace value : 'jq(.x)' The only way to delete a stored value is to set it to null .","title":"Setter State"},{"location":"getting_started/persistent-data/#getter-state","text":"The Getter State is used to retrieve any number of variables in persistent storage. Each variable must be explicitly scoped, and the value retrieved will be stored under .var.KEY where KEY is the variable's name. - id : a type : getter variables : - key : x scope : namespace A key doesn't need to exist in storage to return successfully, but the value returned will be null if it doesn't exist.","title":"Getter State"},{"location":"getting_started/persistent-data/#concurrency","text":"Direktiv makes no effort to guarantee any thread-safety on persistent data. Multiple instances that interact with the same variable may have inconsistent results.","title":"Concurrency"},{"location":"getting_started/persistent-data/#getting-setting-from-functions","text":"","title":"Getting &amp; Setting from Functions"},{"location":"getting_started/persistent-data/#getting","text":"Accessing persistent data from within a function is a fairly straightforward process. The request that the custom function receives from Direktiv contains a header 'Direktiv-TempDir', which contains all of the variables specified in the function definition. The as , key , scope , and type fields can all play a role in the placement and naming of files within this directory: key The key used to select a variable from within the workflow definition. If no as field is provided, the file on a custom function will correspond to the value of key . scope Which scope to get the variable from: instance , workflow , or namespace . Defaults to instance if omitted. as An optional field used to set the name of the file as it appears on the isolate. type plain The variable data inside of the file will be written 'as-is'. base64 If the variable is stored as base64-encoded data, it will be decoded before being written to the file system. tar If the variable is a valid tar archive, a directory will be created instead of a file, with the contents of the tar archive populating it. tar.gz Similar to tar , this will result in a populated directory being created from a valid .tar.gz file. For example, given the following state definition, a directory named 'myFiles' should exist within the directory specified by the Direktiv-TempDir header. Assuming that this header has a value of /mnt/shared/example , the following structure would be expected: - id : get image : localhost:5000/iv-getter:v1 files : - key : \"myFiles\" scope : instance type : tar /mnt/shared/example/ \u2514\u2500\u2500 myFiles \u2514\u2500\u2500 file-1 \u2514\u2500\u2500 file-2 \u2514\u2500\u2500 file-3","title":"Getting"},{"location":"getting_started/persistent-data/#setting","text":"From within an isolate running on Direktiv, variables can be set by sending a POST request: POST http://localhost:8889/var?aid=<EXAMPLE>&scope=instance&key=myFiles Body: <VARIABLE DATA> query parameters aid The action ID, found from the Direktiv-ActionID header of the request being served by the isolate. scope The scope for which the variable is set ( namespace , workflow , or instance ) key The key used by subsequent actions to access the variable. An alternative approach is to write files into certain directories. The direktiv sidecar will store those files as variables. There are three different folders for the three different scopes. For the above example they would be: /mnt/shared/example/out/instance /mnt/shared/example/out/workflow /mnt/shared/example/out/namespace Files under these folders will be stored with their names under the scope of the folder. Diretories will be stored as tar.gz files.","title":"Setting"},{"location":"getting_started/scheduling/","text":"Scheduling Sometimes you want a workflow to run periodically. Direktiv supports scheduling based on \"cron\", and in this article you'll see how that's done. Demo id : scraper start : type : scheduled cron : \"0 */2 * * *\" functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" transform : 'jq(.return)' transition : logger - id : logger type : noop log : 'jg(.)' Start Types Workflow definitions can have one of many different start types. Up until now you've left the start section out entirely, which causes it to default , which is appropriate for a direct-invoke/subflow workflow. Now we can have a look at scheduled workflows. start : type : scheduled cron : \"0 */2 * * *\" There's not much to see here. Add the start section, set type to scheduled , and define a valid cron string and away you go! Direktiv prevents scheduled workflows from being directly invoked or used as a subflow, which is why this demo doesn't specify any input data. Just configure the workflow and check the logs over time to see the scheduled workflow in action. Active/Inactive Workflows Every workflow definition can be considered \"active\" or \"inactive\". Being \"active\" doesn't mean that there's an instance running right now, it means that Direktiv will allow instances to be created from it. This setting is part of the API, not a part of the workflow definition. With scheduled workflows we can finally see why this setting could be useful: you can toggle the schedule on and off without modifying the workflow definition itself. Cron Cron is a time-based job scheduler in Unix-like operating systems. Direktiv doesn't run cron, but it does borrow their syntax and expressions for scheduling. In the example above our cron expression is \" 0 */2 * * * \". This tells Direktiv to run the workflow once every two hours. There are many great resources online to help you create your own custom cron expressions.","title":"Scheduling"},{"location":"getting_started/scheduling/#scheduling","text":"Sometimes you want a workflow to run periodically. Direktiv supports scheduling based on \"cron\", and in this article you'll see how that's done.","title":"Scheduling"},{"location":"getting_started/scheduling/#demo","text":"id : scraper start : type : scheduled cron : \"0 */2 * * *\" functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" transform : 'jq(.return)' transition : logger - id : logger type : noop log : 'jg(.)'","title":"Demo"},{"location":"getting_started/scheduling/#start-types","text":"Workflow definitions can have one of many different start types. Up until now you've left the start section out entirely, which causes it to default , which is appropriate for a direct-invoke/subflow workflow. Now we can have a look at scheduled workflows. start : type : scheduled cron : \"0 */2 * * *\" There's not much to see here. Add the start section, set type to scheduled , and define a valid cron string and away you go! Direktiv prevents scheduled workflows from being directly invoked or used as a subflow, which is why this demo doesn't specify any input data. Just configure the workflow and check the logs over time to see the scheduled workflow in action.","title":"Start Types"},{"location":"getting_started/scheduling/#activeinactive-workflows","text":"Every workflow definition can be considered \"active\" or \"inactive\". Being \"active\" doesn't mean that there's an instance running right now, it means that Direktiv will allow instances to be created from it. This setting is part of the API, not a part of the workflow definition. With scheduled workflows we can finally see why this setting could be useful: you can toggle the schedule on and off without modifying the workflow definition itself.","title":"Active/Inactive Workflows"},{"location":"getting_started/scheduling/#cron","text":"Cron is a time-based job scheduler in Unix-like operating systems. Direktiv doesn't run cron, but it does borrow their syntax and expressions for scheduling. In the example above our cron expression is \" 0 */2 * * * \". This tells Direktiv to run the workflow once every two hours. There are many great resources online to help you create your own custom cron expressions.","title":"Cron"},{"location":"getting_started/secrets-registries/","text":"Secrets & Registries Many workflows require sensitive information such as passwords or authentication tokens to access third-party APIs. In this article you'll learn the best way to handle sensitive data such as this so that you don't need to store them as plaintext in workflow definitions. You'll also learn how to source Docker containers from private repositories. Demo id : httpget functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : secrets : [ \"secretToken\" ] function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" headers : \"Content-type\" : \"application/json; charset=UTF-8\" \"Authorization\" : \"bearer jq(.secrets.secretToken)\" This workflow will use a private Docker container marketplace.gcr.io to perform a GET request and return the results to the instance data. Registries Direktiv can store authentication information for a Docker repository on a namespace-by-namespace basis. Creating secrets can be done via the Direktiv API or web interface in the settings page. With the relevant registry defined, functions referencing containers on that registry become accessible. For example, if a registry was created via the api with the following curl command: curl -X 'POST' \\ 'URL/api/functions/registries/namespaces/NAMESPACE' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data\": \"admin:8QwFLg%D$qg*\", \"reg\": \"https://index.docker.io\" }' This registry would be used automatically by Direktiv when running the workflow in the demo. Registry Types There are 3 types of registries that encompass different function scopes: Namespace Registries Namespace registries are applied to all services and functions created in the same namespace. Global Registries Global registries are applied to all services and functions irrelevant to the namespace. These registries are also used by global services. Global Private Registries Global private registries are only used by global services. Example Google Artifact Registry To use the Google Artifact Registry a service account with a key iis required. How to create a service account and generate a key is documented here . The keed needs to be in base64 format. On linux it can be converted with the following command: base64 -w 0 mykey-da8c8b573601.json > base64google.json Please make sure that there are no line wraps in the base64 file. For base64 encoded files the username is _json_key_base64 . Example details for this registry would be something like the following: Key Value URL https://us-central1-docker.pkg.dev Username _json_key_base64 Password ewogICJ0eXBlIjo...WlJkMWhqK1RRRF Note: If a registry is created after a service, the service will need to be recreated to use the latest registry. Secrets Similar to how registry tokens are stored, arbitrary secrets can also be stored. That includes passwords, API tokens, certificates, or anything else. Secrets are stored on a namespace-by-namespace basis as key-value pairs. Secreats can be defined with the Direktiv API or web interface. Wherever actions appear in workflow definitions there's always an optional secrets field. For every secret named in this field, Direktiv will find and decrypt the relevant secret from your namespace and add it to the data from which the action input is generated just before running the jq command that generates that logic. This means your jq commands can reference your secret and place it wherever it needs to be. Direktiv discards the secret-enriched data after generating the action input, so the secrets won't naturally appear in your instance output or logs. But once Direktiv passes that data to your action it has no control over how it's used. It's up to you to ensure your action doesn't log sensitive information and doesn't send sensitive information where it shouldn't go. IMPORTANT: Be especially wary of subflows. Try to avoid passing secrets to subflows if you can, subflows can reference secrets the same way as their parents after all. Remember, your secret-enriched data will become the input for a subflow, which means it will be logged. It's also stored in that subflow's instance data and could be passed around automatically if you're not careful. If your subflow doesn't strip secrets out before it terminates those secrets could also end up in the caller's return object. Security Registry tokens and secrets are stored individually encrypted within Direktiv's database. Each namespace gets its own unique encryption keys, and the decryption key is stored in a different database. For the online Direktiv, these two databases are on different machines and are firewalled apart from one another, and all internal traffic is encrypted. These measures minimize the risk of damaging data breaches, but we still recommend using tokens rather than passwords wherever possible.","title":"Secrets & Registries"},{"location":"getting_started/secrets-registries/#secrets-registries","text":"Many workflows require sensitive information such as passwords or authentication tokens to access third-party APIs. In this article you'll learn the best way to handle sensitive data such as this so that you don't need to store them as plaintext in workflow definitions. You'll also learn how to source Docker containers from private repositories.","title":"Secrets &amp; Registries"},{"location":"getting_started/secrets-registries/#demo","text":"id : httpget functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : secrets : [ \"secretToken\" ] function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" headers : \"Content-type\" : \"application/json; charset=UTF-8\" \"Authorization\" : \"bearer jq(.secrets.secretToken)\" This workflow will use a private Docker container marketplace.gcr.io to perform a GET request and return the results to the instance data.","title":"Demo"},{"location":"getting_started/secrets-registries/#registries","text":"Direktiv can store authentication information for a Docker repository on a namespace-by-namespace basis. Creating secrets can be done via the Direktiv API or web interface in the settings page. With the relevant registry defined, functions referencing containers on that registry become accessible. For example, if a registry was created via the api with the following curl command: curl -X 'POST' \\ 'URL/api/functions/registries/namespaces/NAMESPACE' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data\": \"admin:8QwFLg%D$qg*\", \"reg\": \"https://index.docker.io\" }' This registry would be used automatically by Direktiv when running the workflow in the demo.","title":"Registries"},{"location":"getting_started/secrets-registries/#registry-types","text":"There are 3 types of registries that encompass different function scopes:","title":"Registry Types"},{"location":"getting_started/secrets-registries/#namespace-registries","text":"Namespace registries are applied to all services and functions created in the same namespace.","title":"Namespace Registries"},{"location":"getting_started/secrets-registries/#global-registries","text":"Global registries are applied to all services and functions irrelevant to the namespace. These registries are also used by global services.","title":"Global Registries"},{"location":"getting_started/secrets-registries/#global-private-registries","text":"Global private registries are only used by global services.","title":"Global Private Registries"},{"location":"getting_started/secrets-registries/#example-google-artifact-registry","text":"To use the Google Artifact Registry a service account with a key iis required. How to create a service account and generate a key is documented here . The keed needs to be in base64 format. On linux it can be converted with the following command: base64 -w 0 mykey-da8c8b573601.json > base64google.json Please make sure that there are no line wraps in the base64 file. For base64 encoded files the username is _json_key_base64 . Example details for this registry would be something like the following: Key Value URL https://us-central1-docker.pkg.dev Username _json_key_base64 Password ewogICJ0eXBlIjo...WlJkMWhqK1RRRF Note: If a registry is created after a service, the service will need to be recreated to use the latest registry.","title":"Example Google Artifact Registry"},{"location":"getting_started/secrets-registries/#secrets","text":"Similar to how registry tokens are stored, arbitrary secrets can also be stored. That includes passwords, API tokens, certificates, or anything else. Secrets are stored on a namespace-by-namespace basis as key-value pairs. Secreats can be defined with the Direktiv API or web interface. Wherever actions appear in workflow definitions there's always an optional secrets field. For every secret named in this field, Direktiv will find and decrypt the relevant secret from your namespace and add it to the data from which the action input is generated just before running the jq command that generates that logic. This means your jq commands can reference your secret and place it wherever it needs to be. Direktiv discards the secret-enriched data after generating the action input, so the secrets won't naturally appear in your instance output or logs. But once Direktiv passes that data to your action it has no control over how it's used. It's up to you to ensure your action doesn't log sensitive information and doesn't send sensitive information where it shouldn't go. IMPORTANT: Be especially wary of subflows. Try to avoid passing secrets to subflows if you can, subflows can reference secrets the same way as their parents after all. Remember, your secret-enriched data will become the input for a subflow, which means it will be logged. It's also stored in that subflow's instance data and could be passed around automatically if you're not careful. If your subflow doesn't strip secrets out before it terminates those secrets could also end up in the caller's return object.","title":"Secrets"},{"location":"getting_started/secrets-registries/#security","text":"Registry tokens and secrets are stored individually encrypted within Direktiv's database. Each namespace gets its own unique encryption keys, and the decryption key is stored in a different database. For the online Direktiv, these two databases are on different machines and are firewalled apart from one another, and all internal traffic is encrypted. These measures minimize the risk of damaging data breaches, but we still recommend using tokens rather than passwords wherever possible.","title":"Security"},{"location":"getting_started/subflows/","text":"Subflows Just like scripting or programming, with Direktiv it's possible to organize your logic into reusable modules. Anytime a workflow is invoked by another we call it a subflow. In this article you'll learn about namespaces, subflows, and instance data validation. Demo For a subflow demonstration we need to define multiple workflows. 1st Workflow Definition id : notifier functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify - id : notify type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.input)' transition : check-results - id : check-results type : switch conditions : - condition : 'jq(.warnings)' transition : throw - id : throw type : error error : notification.lint message : 'lint errors: %s' args : - 'jq(.warnings)' 2nd Workflow Definition id : worker functions : - id : httprequest image : direktiv/request:v1 type : reusable - id : notifier type : subflow workflow : notifier states : - id : do type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.input)' transition : notify transform : 'jq(del(.return) | .contact = \"Alan\")' - id : notify type : action action : function : notifier input : 'jq({ contact: .contact, payload: .payload })' Input input : contact : \"1\" payload : \"2\" Output { \"contact\" : \"Alan\" , \"payload\" : \"2\" , \"return\" : { \"contact\" : \"Alan\" , \"payload\" : \"2\" , \"return\" : { \"completed\" : false , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"userId\" : 1 } } } Namespaces Before learning about subflows you'll need to know what a \"Namespace\" is. Direktiv organizes everything into namespaces. Think of them a bit like a folder. All of your workflow definitions exist within a namespace, and any instances spawned from those definitions exist within that namespace as well. Any secrets or registries you've set up apply only within the namespace (more on these in a later article). Some limitations are applied on a namespace-level. And frontends may piggyback on the namespaces to handle permissions and multi-tennancy. Workflow identifiers are unique within a namespace, which allows them to be referenced as subflows. Subflows Anywhere an \"Action\" appears in a workflow definition either an Isolate or a Subflow can be run. Here's an example of what a subflow call could look like: id : httpget functions : - id : myworkflow type : subflow workflow : myworkflow states : - id : getter type : action action : function : myworkflow input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Nothing special needs to be done when writing a workflow definition that's intended for use as a subflow. Input is treated exactly the same way as if the workflow was directly invoked with the API, and output is merged into the caller's instance data exactly the same way as if the subflow was an Isolate. Validate State So far we've never demonstrated any way to validate external inputs. Validation is optional, but it can be important for preventing bugs in your workflows caused by unexpected data. Unexpected data could occur in many ways: bad workflow input, bad events, bad Isolate results, or mistakes in your own transforms. Using the Validate State Direktiv can detect these issues and throw an error instead of proceeding. From the example in the demo, here's what a Validate State definition might look like: - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify Unlike any other state, the fields for a Validate State are not fixed. Direktiv takes everything under the schema field and converts it into an equivalent JSON Schema , which it then uses to validate the instance data. Converting JSON Schema from JSON to YAML is straight-forward. Here's the equivalent JSON for the YAML schema in the example above: { \"type\" : \"object\" , \"required\" : [ \"contact\" , \"payload\" ], \"additionalProperties\" : false , \"properties\" : { \"contact\" : { \"type\" : \"string\" }, \"payload\" : { \"type\" : \"string\" } } } There are also many tools online that can convert JSON to YAML for you. If the instance data fails its validation Direktiv will throw a direktiv.schema.failed error, which will terminate the workflow unless an appropriate error catcher is defined (more on error handling in a later article). Error State Speaking of errors, there's another new state in this demo example: the Error State. The Error State is only really useful in the context of subflows. - id : throw type : error error : notification.lint message : 'lint errors: %s' args : - jq(.warnings) If an instance executes an Error State it will store the custom-defined error and mark the instance as failed after the instance terminates. The Error State has an optional transition field just like every other state, which might surprise you. That's because the Error State won't actually cause the workflow to terminate like you might expect. This is to allow the workflow to perform any cleanup, rollback, or recovery logic. The error will still be reported when the instance does finally finish.","title":"Subflows"},{"location":"getting_started/subflows/#subflows","text":"Just like scripting or programming, with Direktiv it's possible to organize your logic into reusable modules. Anytime a workflow is invoked by another we call it a subflow. In this article you'll learn about namespaces, subflows, and instance data validation.","title":"Subflows"},{"location":"getting_started/subflows/#demo","text":"For a subflow demonstration we need to define multiple workflows.","title":"Demo"},{"location":"getting_started/subflows/#1st-workflow-definition","text":"id : notifier functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify - id : notify type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.input)' transition : check-results - id : check-results type : switch conditions : - condition : 'jq(.warnings)' transition : throw - id : throw type : error error : notification.lint message : 'lint errors: %s' args : - 'jq(.warnings)'","title":"1st Workflow Definition"},{"location":"getting_started/subflows/#2nd-workflow-definition","text":"id : worker functions : - id : httprequest image : direktiv/request:v1 type : reusable - id : notifier type : subflow workflow : notifier states : - id : do type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.input)' transition : notify transform : 'jq(del(.return) | .contact = \"Alan\")' - id : notify type : action action : function : notifier input : 'jq({ contact: .contact, payload: .payload })'","title":"2nd Workflow Definition"},{"location":"getting_started/subflows/#input","text":"input : contact : \"1\" payload : \"2\"","title":"Input"},{"location":"getting_started/subflows/#output","text":"{ \"contact\" : \"Alan\" , \"payload\" : \"2\" , \"return\" : { \"contact\" : \"Alan\" , \"payload\" : \"2\" , \"return\" : { \"completed\" : false , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"userId\" : 1 } } }","title":"Output"},{"location":"getting_started/subflows/#namespaces","text":"Before learning about subflows you'll need to know what a \"Namespace\" is. Direktiv organizes everything into namespaces. Think of them a bit like a folder. All of your workflow definitions exist within a namespace, and any instances spawned from those definitions exist within that namespace as well. Any secrets or registries you've set up apply only within the namespace (more on these in a later article). Some limitations are applied on a namespace-level. And frontends may piggyback on the namespaces to handle permissions and multi-tennancy. Workflow identifiers are unique within a namespace, which allows them to be referenced as subflows.","title":"Namespaces"},{"location":"getting_started/subflows/#subflows_1","text":"Anywhere an \"Action\" appears in a workflow definition either an Isolate or a Subflow can be run. Here's an example of what a subflow call could look like: id : httpget functions : - id : myworkflow type : subflow workflow : myworkflow states : - id : getter type : action action : function : myworkflow input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Nothing special needs to be done when writing a workflow definition that's intended for use as a subflow. Input is treated exactly the same way as if the workflow was directly invoked with the API, and output is merged into the caller's instance data exactly the same way as if the subflow was an Isolate.","title":"Subflows"},{"location":"getting_started/subflows/#validate-state","text":"So far we've never demonstrated any way to validate external inputs. Validation is optional, but it can be important for preventing bugs in your workflows caused by unexpected data. Unexpected data could occur in many ways: bad workflow input, bad events, bad Isolate results, or mistakes in your own transforms. Using the Validate State Direktiv can detect these issues and throw an error instead of proceeding. From the example in the demo, here's what a Validate State definition might look like: - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify Unlike any other state, the fields for a Validate State are not fixed. Direktiv takes everything under the schema field and converts it into an equivalent JSON Schema , which it then uses to validate the instance data. Converting JSON Schema from JSON to YAML is straight-forward. Here's the equivalent JSON for the YAML schema in the example above: { \"type\" : \"object\" , \"required\" : [ \"contact\" , \"payload\" ], \"additionalProperties\" : false , \"properties\" : { \"contact\" : { \"type\" : \"string\" }, \"payload\" : { \"type\" : \"string\" } } } There are also many tools online that can convert JSON to YAML for you. If the instance data fails its validation Direktiv will throw a direktiv.schema.failed error, which will terminate the workflow unless an appropriate error catcher is defined (more on error handling in a later article).","title":"Validate State"},{"location":"getting_started/subflows/#error-state","text":"Speaking of errors, there's another new state in this demo example: the Error State. The Error State is only really useful in the context of subflows. - id : throw type : error error : notification.lint message : 'lint errors: %s' args : - jq(.warnings) If an instance executes an Error State it will store the custom-defined error and mark the instance as failed after the instance terminates. The Error State has an optional transition field just like every other state, which might surprise you. That's because the Error State won't actually cause the workflow to terminate like you might expect. This is to allow the workflow to perform any cleanup, rollback, or recovery logic. The error will still be reported when the instance does finally finish.","title":"Error State"},{"location":"getting_started/transforms/","text":"Transforms & JQ Every workflow instance always has something called the \"Instance Data\", which is a JSON object that is used to pass data around. Almost everywhere a transition can happen in a workflow definition a transform can also happen allowing the author to filter, enrich, or otherwise modify the instance data. The transform field can contain a valid jq command, which will be applied to the existing instance data to generate a new JSON object that will entirely replace it. Note that only a JSON object will be considered a valid output from this jq command: jq is capable of outputting primitives and arrays, but these are not acceptable output for a transform . Because the Noop State logs its instance data before applying its transform & transition we can follow the results of these transforms throughout the demo. Transforms can be wrapped in 'jq()' or jq() . The difference between the two is that one instructs YAML more explicitly what's in the string. This can be important if you use jq commands containing braces, for example: jq({a: 1}) . Because if this is not explicitly quoted, YAML interprets it incorrectly and throws errors. The quoted form is always valid and generally safer. First Transform The first transform defines a completely new JSON object. Command transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' Resulting Instance Data { \"number\" : 2 , \"objects\" : [ { \"k1\" : \"v1\" } ] } Second Transform The second transform enriches the existing instance data by adding a new field to it. Command transform : 'jq(.multiplier = 10)' Resulting Instance Data { \"multiplier\" : 10 , \"number\" : 2 , \"objects\" : [ { \"k1\" : \"v1\" } ] } Third Transform The third transform multiplies two fields to produce a new field, then pipes the results into another command that deletes two fields. Command transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' Resulting Instance Data { \"objects\" : [ { \"k1\" : \"v1\" } ], \"result\" : 20 } Fourth Transform The fourth transform selects a child object nested within the instance data and makes that into the new instance data. Command transform : 'jq(.objects[0])' Resulting Instance Data { \"k1\" : \"v1\" }","title":"Transforms & JQ"},{"location":"getting_started/transforms/#transforms-jq","text":"Every workflow instance always has something called the \"Instance Data\", which is a JSON object that is used to pass data around. Almost everywhere a transition can happen in a workflow definition a transform can also happen allowing the author to filter, enrich, or otherwise modify the instance data. The transform field can contain a valid jq command, which will be applied to the existing instance data to generate a new JSON object that will entirely replace it. Note that only a JSON object will be considered a valid output from this jq command: jq is capable of outputting primitives and arrays, but these are not acceptable output for a transform . Because the Noop State logs its instance data before applying its transform & transition we can follow the results of these transforms throughout the demo. Transforms can be wrapped in 'jq()' or jq() . The difference between the two is that one instructs YAML more explicitly what's in the string. This can be important if you use jq commands containing braces, for example: jq({a: 1}) . Because if this is not explicitly quoted, YAML interprets it incorrectly and throws errors. The quoted form is always valid and generally safer.","title":"Transforms &amp; JQ"},{"location":"getting_started/transforms/#first-transform","text":"The first transform defines a completely new JSON object. Command transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' Resulting Instance Data { \"number\" : 2 , \"objects\" : [ { \"k1\" : \"v1\" } ] }","title":"First Transform"},{"location":"getting_started/transforms/#second-transform","text":"The second transform enriches the existing instance data by adding a new field to it. Command transform : 'jq(.multiplier = 10)' Resulting Instance Data { \"multiplier\" : 10 , \"number\" : 2 , \"objects\" : [ { \"k1\" : \"v1\" } ] }","title":"Second Transform"},{"location":"getting_started/transforms/#third-transform","text":"The third transform multiplies two fields to produce a new field, then pipes the results into another command that deletes two fields. Command transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' Resulting Instance Data { \"objects\" : [ { \"k1\" : \"v1\" } ], \"result\" : 20 }","title":"Third Transform"},{"location":"getting_started/transforms/#fourth-transform","text":"The fourth transform selects a child object nested within the instance data and makes that into the new instance data. Command transform : 'jq(.objects[0])' Resulting Instance Data { \"k1\" : \"v1\" }","title":"Fourth Transform"},{"location":"getting_started/transitions/","text":"Transitions The ability to string a number of different operations together is a fundamental part of workflows. In this article you'll learn about transitions, transforms, and jq . Demo id : transitioner states : - id : a type : noop transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' transition : b - id : b type : noop transform : 'jq(.multiplier = 10)' transition : c - id : c type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : d - id : d type : noop transform : 'jq(.objects[0])' Output { \"k1\" : \"v1\" } Logs [10:10:30] Beginning workflow triggered by API. [10:10:30] Running state logic -- a:1 (noop) [10:10:30] State data: {} [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: b (1). [10:10:30] Running state logic -- b:2 (noop) [10:10:30] State data: { \"number\": 2, \"objects\": [ { \"k1\": \"v1\" } ] } [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: c (2). [10:10:30] Running state logic -- c:3 (noop) [10:10:30] State data: { \"multiplier\": 10, \"number\": 2, \"objects\": [ { \"k1\": \"v1\" } ] } [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: d (3). [10:10:30] Running state logic -- d:4 (noop) [10:10:30] State data: { \"objects\": [ { \"k1\": \"v1\" } ], \"result\": 20 } [10:10:30] Transforming state data. [10:10:30] Workflow completed. Transitions More than one state can be defined in a workflow definition. Each begins under the states field and multiple states can be differentiated by looking for the dash symbol that denotes a new object in the list of states. In the demo there are four separate states: State 'a' - id : a type : noop transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' transition : b State 'b' - id : b type : noop transform : 'jq(.multiplier = 10)' transition : c State 'c' - id : c type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : d State 'd' - id : d type : noop transform : 'jq(.objects[0])' We've only got Noop States here, but most state types may optionally have a transition field, with a reference to the identifier for a state in the workflow definition. After a state finishes running Direktiv uses this field to figure out whether the instance has reached its end or not. If a transition to another state is defined the instance will continue on to that state. In this demo four Noop States are defined in a simple sequence that goes a \u2192 b \u2192 c \u2192 d . The instance data for each state is inherited from its predecessor, which is why it can be helpful to use Transforms.","title":"Transitions"},{"location":"getting_started/transitions/#transitions","text":"The ability to string a number of different operations together is a fundamental part of workflows. In this article you'll learn about transitions, transforms, and jq .","title":"Transitions"},{"location":"getting_started/transitions/#demo","text":"id : transitioner states : - id : a type : noop transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' transition : b - id : b type : noop transform : 'jq(.multiplier = 10)' transition : c - id : c type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : d - id : d type : noop transform : 'jq(.objects[0])'","title":"Demo"},{"location":"getting_started/transitions/#output","text":"{ \"k1\" : \"v1\" }","title":"Output"},{"location":"getting_started/transitions/#logs","text":"[10:10:30] Beginning workflow triggered by API. [10:10:30] Running state logic -- a:1 (noop) [10:10:30] State data: {} [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: b (1). [10:10:30] Running state logic -- b:2 (noop) [10:10:30] State data: { \"number\": 2, \"objects\": [ { \"k1\": \"v1\" } ] } [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: c (2). [10:10:30] Running state logic -- c:3 (noop) [10:10:30] State data: { \"multiplier\": 10, \"number\": 2, \"objects\": [ { \"k1\": \"v1\" } ] } [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: d (3). [10:10:30] Running state logic -- d:4 (noop) [10:10:30] State data: { \"objects\": [ { \"k1\": \"v1\" } ], \"result\": 20 } [10:10:30] Transforming state data. [10:10:30] Workflow completed.","title":"Logs"},{"location":"getting_started/transitions/#transitions_1","text":"More than one state can be defined in a workflow definition. Each begins under the states field and multiple states can be differentiated by looking for the dash symbol that denotes a new object in the list of states. In the demo there are four separate states:","title":"Transitions"},{"location":"getting_started/transitions/#state-a","text":"- id : a type : noop transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' transition : b","title":"State 'a'"},{"location":"getting_started/transitions/#state-b","text":"- id : b type : noop transform : 'jq(.multiplier = 10)' transition : c","title":"State 'b'"},{"location":"getting_started/transitions/#state-c","text":"- id : c type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : d","title":"State 'c'"},{"location":"getting_started/transitions/#state-d","text":"- id : d type : noop transform : 'jq(.objects[0])' We've only got Noop States here, but most state types may optionally have a transition field, with a reference to the identifier for a state in the workflow definition. After a state finishes running Direktiv uses this field to figure out whether the instance has reached its end or not. If a transition to another state is defined the instance will continue on to that state. In this demo four Noop States are defined in a simple sequence that goes a \u2192 b \u2192 c \u2192 d . The instance data for each state is inherited from its predecessor, which is why it can be helpful to use Transforms.","title":"State 'd'"},{"location":"getting_started/using-functions/","text":"Using Functions Functions exist in the following forms: subflow isolated reusable knative-namespace knative-global This article demonstrates how each of the available function types is used within a workflow. subflow A subflow function allows a workflow to execute another workflow. Assuming that 2 workflows exist, parent and child , the parent workflow could use the child workflow as a'subflow' function. id: parent functions: - id: my-subflow type: subflow workflow: child states: - id: invoke-subflow type: action action: function: my-subflow View this article for more detailed information. isolated Isolated functions are less performant than other types, but can be useful if you need greater isolation or just want to write your own function and prefer the simpler design pattern of interacting with the file-system instead of implementing a server that adheres to our reusable function spec. For more information about isolated functions, click here . id: my-workflow functions: - id: my-isolated-function type: isolated image: example/isolated-function states: - id: invoke-isolated-function type: action action: function: my-isolated-function reusable reusable functions exist only within the scope of the workflow in which they are defined. They can not be shared between workflows. The following code block shows a workflow with a single reusable function defined. This function did not exist before this code block was written. The image field dictates which container image is used to build the function. id: my-workflow functions: - id: my-function type: reusable image: direktiv/request:v1 knative-namespace knative-namespace functions exist within the scope of a single namespace. Multiple workflows within a common namespace can use an existing knative-namespace function, regardless of whether or not other workflows also require it. The following code block shows a workflow that uses a single knative-namespace function. This function must exist, and have a name specified by the service field shown here: id: my-workflow functions: - id: ns-function type: knative-namespace service: ns-function knative-global knative-global functions exist outside of the scope of namespaces, and can be included in any workflow within the Direktiv environment. The following code block shows a workflow that uses a single knative-global function. This function must exist, and have a name specified by the service field shown here: id: my-workflow functions: - id: global-function type: knative-global service: global-function","title":"Using Functions"},{"location":"getting_started/using-functions/#using-functions","text":"Functions exist in the following forms: subflow isolated reusable knative-namespace knative-global This article demonstrates how each of the available function types is used within a workflow.","title":"Using Functions"},{"location":"getting_started/using-functions/#subflow","text":"A subflow function allows a workflow to execute another workflow. Assuming that 2 workflows exist, parent and child , the parent workflow could use the child workflow as a'subflow' function. id: parent functions: - id: my-subflow type: subflow workflow: child states: - id: invoke-subflow type: action action: function: my-subflow View this article for more detailed information.","title":"subflow"},{"location":"getting_started/using-functions/#isolated","text":"Isolated functions are less performant than other types, but can be useful if you need greater isolation or just want to write your own function and prefer the simpler design pattern of interacting with the file-system instead of implementing a server that adheres to our reusable function spec. For more information about isolated functions, click here . id: my-workflow functions: - id: my-isolated-function type: isolated image: example/isolated-function states: - id: invoke-isolated-function type: action action: function: my-isolated-function","title":"isolated"},{"location":"getting_started/using-functions/#reusable","text":"reusable functions exist only within the scope of the workflow in which they are defined. They can not be shared between workflows. The following code block shows a workflow with a single reusable function defined. This function did not exist before this code block was written. The image field dictates which container image is used to build the function. id: my-workflow functions: - id: my-function type: reusable image: direktiv/request:v1","title":"reusable"},{"location":"getting_started/using-functions/#knative-namespace","text":"knative-namespace functions exist within the scope of a single namespace. Multiple workflows within a common namespace can use an existing knative-namespace function, regardless of whether or not other workflows also require it. The following code block shows a workflow that uses a single knative-namespace function. This function must exist, and have a name specified by the service field shown here: id: my-workflow functions: - id: ns-function type: knative-namespace service: ns-function","title":"knative-namespace"},{"location":"getting_started/using-functions/#knative-global","text":"knative-global functions exist outside of the scope of namespaces, and can be included in any workflow within the Direktiv environment. The following code block shows a workflow that uses a single knative-global function. This function must exist, and have a name specified by the service field shown here: id: my-workflow functions: - id: global-function type: knative-global service: global-function","title":"knative-global"},{"location":"installation/","text":"Installation Direktiv is using Helm charts for installation. For a basic installation there are only two dependencies. A PostgreSQL database and Knative . Optional dependencies are Linkerd as service mesh and monitoring and tracing tools, e.g. backends for Direktiv's Opentlemetry configuration. The following diagram shows a high-level architecture of Direktiv and the required and optional components. The following sections explain how to install each component in a local cluster: Kubernetes Linkerd Postgres Direktiv Knative Run Docker Image For testing there is a \"all-in-one\" Docker image available. It contains all required components alrteday installed and can be used for testing or development. It has a container registry installed on port 31212 as well which can be used to push local images. Direktiv Docker Container docker run --privileged -p 8080 :80 -ti direktiv/direktiv-kube The docker image has addtional environment variables which can add other functionalities and configurations: APIKEY: Set an API key for the application HTTPS_PROXY: Sets the HTTPS_PROXY environment variable HTTP_PROXY: Sets the HTTP_PROXY environment variable NO_PROXY: Sets the NO_PROXY environment variable EVENTING: Enables Knative eventing DEBUG: Prints k3s output to stdout Direktiv Docker Container with API Key and Registry docker run -e APIKEY = 123 --privileged -p 8080 :80 -p 31212 :31212 -ti direktiv/direktiv-kube","title":"Install"},{"location":"installation/#installation","text":"Direktiv is using Helm charts for installation. For a basic installation there are only two dependencies. A PostgreSQL database and Knative . Optional dependencies are Linkerd as service mesh and monitoring and tracing tools, e.g. backends for Direktiv's Opentlemetry configuration. The following diagram shows a high-level architecture of Direktiv and the required and optional components. The following sections explain how to install each component in a local cluster: Kubernetes Linkerd Postgres Direktiv Knative","title":"Installation"},{"location":"installation/#run-docker-image","text":"For testing there is a \"all-in-one\" Docker image available. It contains all required components alrteday installed and can be used for testing or development. It has a container registry installed on port 31212 as well which can be used to push local images. Direktiv Docker Container docker run --privileged -p 8080 :80 -ti direktiv/direktiv-kube The docker image has addtional environment variables which can add other functionalities and configurations: APIKEY: Set an API key for the application HTTPS_PROXY: Sets the HTTPS_PROXY environment variable HTTP_PROXY: Sets the HTTP_PROXY environment variable NO_PROXY: Sets the NO_PROXY environment variable EVENTING: Enables Knative eventing DEBUG: Prints k3s output to stdout Direktiv Docker Container with API Key and Registry docker run -e APIKEY = 123 --privileged -p 8080 :80 -p 31212 :31212 -ti direktiv/direktiv-kube","title":"Run Docker Image"},{"location":"installation/database/","text":"Database Direktiv requires a PostgreSQL 13+ database. It acts as datastore as well as pub/sub system between Direktiv's components. It has been tested with Postgres offerings from cloud providers as well as on-premise installations. It is recommended to use a managed Postgres service from cloud providers. If that is not possible Postgres can be installed in Kubernetes as well. To install a Postgres instance in Kubernetes we are using CrunchyData's operator and helm charts . The following section will provide exmaples for different installation scenarions from basic testing setups to more complex high-availability configurations. For inidividual changes please visit the CrunchyData Operator documentation page. Installing the Operator The operator is provided as Helm chart and the installation is straighforward. Add Direktiv's helm chart repository and run the installation command. Install Postgres Operator helm repo add direktiv https://chart.direktiv.io helm install -n postgres --create-namespace postgres direktiv/pgo Backup Ports For the backup to work properly port 2022 needs to be open between the nodes Creating a Postgres Instance Basic Configuration This basic configuration is good for small instances and testing. It creates weekly backups and keeps the last 4 backups. Direktiv connects directly to the Database without connection pooling. Basic Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/basic.yaml Basic Database Configuration apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : postgresVersion : 14 instances : - name : \"direktiv\" replicas : 1 dataVolumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"1Gi\" backups : pgbackrest : global : # Keep 4 Backups repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 schedules : # Run every Sunday full : \"0 1 * * 0\" volume : volumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"4Gi\" High-Availability High-Availabilty can be achieved by scaling the database replicas. The following example has added daily differential backups and pod anti-affinity to spread the pods across the cluster. If anti-affinity is used the cluster needs to have the same number of nodes and database replicas. Basic Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/ha.yaml High-Availability Configuration apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : postgresVersion : 14 instances : - name : \"instance1\" replicas : 3 affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : \"postgres-operator.crunchydata.com/cluster\" operator : In values : - direktiv topologyKey : \"kubernetes.io/hostname\" dataVolumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"1Gi\" backups : pgbackrest : global : repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 schedules : full : \"0 1 * * 0\" differential : \"0 1 * * 1-6\" volume : volumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"4Gi\" High-Availability with S3 Backup CrunchyData's Postgres operator can store backups in AWS, Azure and Google Cloud as well. The following example shows how to use AWS S3 as backup storage. A secret is required for the S3 backend with the appropriate permission. This requires a s3.conf file with the S3 key and secret. s3.conf [ global ] repo1-s3-key = MYKEY repo1-s3-key-secret = MYSECRET After creating the file adding the secret is a simple kubectl command: Create S3 Secret kubectl create secret generic -n postgres direktiv-pgbackrest-secret --from-file = s3.conf To test if the values are correct run the following command: Show S3 Secrets kubectl get secret -n postgres direktiv-pgbackrest-secret -o go-template = '{{ index .data \"s3.conf\" | base64decode }}' High-Availabilty can be achieved by scaling the database replicas. The following example has added daily differential backups and pod anti-affinity to spread the pods across the cluster. If anti-affinity is used the cluster needs to have the same number of nodes and database replicas. S3 Backup Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/s3.yaml S3 Configuration apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : postgresVersion : 14 instances : - name : \"instance1\" replicas : 1 dataVolumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"1Gi\" backups : pgbackrest : configuration : - secret : name : direktiv-pgbackrest-secret global : repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 s3 : bucket : my-bucket endpoint : s3.eu-central-1.amazonaws.com:443 region : eu-central-1 schedules : full : \"0 1 * * 0\" Connection-Pooling Connection pooling help scaling and maintaining availability between your application and the database. The Postgres Operator provides the option to install pgBouncer as connection pooling mechanism. If it is a multi-node cluster the pgBouncer replicas can be increased and spread acorss the cluster with pod anit-affinity rules. pgBouncer Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/pgbouncer.yaml pgBouncer Configuration apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : postgresVersion : 14 instances : - name : \"direktiv\" replicas : 1 dataVolumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"1Gi\" proxy : pgBouncer : replicas : 2 affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : \"postgres-operator.crunchydata.com/cluster\" operator : In values : - direktiv topologyKey : \"kubernetes.io/hostname\" backups : pgbackrest : global : # Keep 4 Backups repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 schedules : # Run every Sunday full : \"0 1 * * 0\" volume : volumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"4Gi\" Getting Database Secrets Direktiv will need the database connection information during installation with a Helm chart. It is a good start for an installation YAML to have this information. It can be easily done by running a simple script: Database Configuration (No Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Database Configuration (With Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Restore from S3 It is always recommended to test the backup and restore before using Direktiv in production. To restore from S3 is a straightforward process. The first step is to pick the backup used for the restore process. It can be found under bd/backup in the bucket used for backups in S3. It looks like this: 20221023-042407F . There are two differtent scenarios to consider. The first one is a restore for an existing database. This can be a restore of as certain backup or a point-in-time recovery. This can be achieved with a restore attribute in the dtaabase configuration YAML. Restore From S3 (Same Database) ... backups : pgbackrest : configuration : - secret : name : direktiv-pgbackrest-secret global : repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 s3 : bucket : cd-direktiv-backup endpoint : s3.eu-central-1.amazonaws.com:443 region : eu-central-1 schedules : full : \"0 1 * * 0\" # Enable Restore restore : enabled : true repoName : repo1 options : - --set=20221023-042407F # point-in-time recovery alternative # - --type=time # - --target=\"2021-06-09 14:15:11-04\" The second scenario is if the whole database has been destroyed and it is a restore to a new database instance. In this case a datasource attribute has to be added to define the source for the backup. Restore From S3 (New Database) apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : dataSource : postgresCluster : clusterName : direktiv repoName : repo1 options : - --set=20221023-042407F postgresVersion : 14 instances : ... Additional Information For more information visit CrunchyData's documentation about disaster recovery . Restore from PVC In case the database is not using S3 backups the backups need to be stored in a safe location in case of loss of the node storing the backups. The data has to be transferred via e.g. scp in using a cron job. A restore of an existing database can be achieved with a simple restore attribute in the database configuration YAML mentioned in the S3 restore section of this documentation. The process is different if the backup node has been destroyed. It is important to not do this restore procedure if a backup is laready running. The backup needs to be rescheduled to execute it without running a backup in parallel. Identify Backup PVC The first step to store the backup is to identify the node where the backup is stored. Identify PV kubectl get pv NAME CAPACITY ... CLAIM # This is the backup node pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 1Gi ... postgres/direktiv-repo1 pvc-ce9bb226-1038-49bf-bed6-e6d0188b228c 1Gi ... postgres/direktiv-direktiv-wnh4-pgdata Describing the PV shows the node where the data is stored and the directory of the data. This directory needs to be stored in a safe location for a later restore. Identify PV kubectl describe pv pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 Name: pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 ... Claim: postgres/direktiv-repo1 ... Node Affinity: Required Terms: # Node where the data is stored Term 0 : kubernetes.io/hostname in [ db2 ] Message: Source: Type: HostPath ( bare host directory volume ) # Data directory on the node Path: /var/lib/rancher/k3s/storage/pvc-80ae5325-8b27-4695-b6df-b362dd946cb7_postgres_direktiv-repo1 HostPathType: DirectoryOrCreate Copy Data To restore the database a backup has to be selected. The available backups are in <Backup Directory>/<Old PVC Name>/backup/db . The directory will look like the following: Backup Directory drwxr-x--- 7 root root 4096 Okt 23 08 :11 . drwxr-x--- 3 root root 4096 Okt 23 08 :11 .. drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -060801F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -060901F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -061001F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -061101F drwxr-x--- 3 root root 4096 Okt 23 08 :11 backup.history -rw-r----- 1 root root 2792 Okt 23 08 :11 backup.info -rw-r----- 1 root root 2792 Okt 23 08 :11 backup.info.copy lrwxrwxrwx 1 root root 16 Okt 23 08 :11 latest -> 20221023 -061101F The next step is to identify where the new backup folder is located. It is exactly the same procedure as used in the copying process. The archive and backup have to be copied into the new backup directory of the new cluster. Copy Folders sudo cp -Rf <Backup Directory>/archive /var/lib/rancher/k3s/storage/<New PV Directory> sudo cp -Rf <Backup Directory>/backup /var/lib/rancher/k3s/storage/<New PV Directory> sudo chown -R 26 :tape /var/lib/rancher/k3s/storage/<New PV Directory> The selected restore needs to be configured in the database configuration YAML and applied with kubectl apply -f mydb.yaml . Restore from PVC apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : dataSource : postgresCluster : clusterName : direktiv repoName : repo1 options : - --set=20221023-075501F - --archive-mode=off postgresVersion : 14 Update Password If this is a new installation of the database the password will be overwritten and the command to generate the direktiv.yaml file is incorrect. Therefore it is advised to update the password to the password in the Kubernetes secret and update Direktiv with the new password. Update User Password # get the old password kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode # execute in pod kubectl exec -n postgres --stdin --tty direktiv-direktiv-<POD-ID> -- psql # update user password ALTER USER direktiv WITH PASSWORD '<PASSWORD FROM FIRST COMMAND>' ; # exit \\q Helpful Commands Fetch Master Instance kubectl -n postgres get pods \\ --selector = postgres-operator.crunchydata.com/role = master \\ -o jsonpath = '{.items[*].metadata.labels.postgres-operator\\.crunchydata\\.com/instance}' Cluster Information kubectl -n postgres describe postgrescluster direktiv Use psql in Database Instance kubectl exec -n postgres --stdin --tty direktiv-direktiv-nl9z-0 -- psql","title":"Database"},{"location":"installation/database/#database","text":"Direktiv requires a PostgreSQL 13+ database. It acts as datastore as well as pub/sub system between Direktiv's components. It has been tested with Postgres offerings from cloud providers as well as on-premise installations. It is recommended to use a managed Postgres service from cloud providers. If that is not possible Postgres can be installed in Kubernetes as well. To install a Postgres instance in Kubernetes we are using CrunchyData's operator and helm charts . The following section will provide exmaples for different installation scenarions from basic testing setups to more complex high-availability configurations. For inidividual changes please visit the CrunchyData Operator documentation page.","title":"Database"},{"location":"installation/database/#installing-the-operator","text":"The operator is provided as Helm chart and the installation is straighforward. Add Direktiv's helm chart repository and run the installation command. Install Postgres Operator helm repo add direktiv https://chart.direktiv.io helm install -n postgres --create-namespace postgres direktiv/pgo Backup Ports For the backup to work properly port 2022 needs to be open between the nodes","title":"Installing the Operator"},{"location":"installation/database/#creating-a-postgres-instance","text":"","title":"Creating a Postgres Instance"},{"location":"installation/database/#basic-configuration","text":"This basic configuration is good for small instances and testing. It creates weekly backups and keeps the last 4 backups. Direktiv connects directly to the Database without connection pooling. Basic Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/basic.yaml Basic Database Configuration apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : postgresVersion : 14 instances : - name : \"direktiv\" replicas : 1 dataVolumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"1Gi\" backups : pgbackrest : global : # Keep 4 Backups repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 schedules : # Run every Sunday full : \"0 1 * * 0\" volume : volumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"4Gi\"","title":"Basic Configuration"},{"location":"installation/database/#high-availability","text":"High-Availabilty can be achieved by scaling the database replicas. The following example has added daily differential backups and pod anti-affinity to spread the pods across the cluster. If anti-affinity is used the cluster needs to have the same number of nodes and database replicas. Basic Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/ha.yaml High-Availability Configuration apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : postgresVersion : 14 instances : - name : \"instance1\" replicas : 3 affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : \"postgres-operator.crunchydata.com/cluster\" operator : In values : - direktiv topologyKey : \"kubernetes.io/hostname\" dataVolumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"1Gi\" backups : pgbackrest : global : repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 schedules : full : \"0 1 * * 0\" differential : \"0 1 * * 1-6\" volume : volumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"4Gi\"","title":"High-Availability"},{"location":"installation/database/#high-availability-with-s3-backup","text":"CrunchyData's Postgres operator can store backups in AWS, Azure and Google Cloud as well. The following example shows how to use AWS S3 as backup storage. A secret is required for the S3 backend with the appropriate permission. This requires a s3.conf file with the S3 key and secret. s3.conf [ global ] repo1-s3-key = MYKEY repo1-s3-key-secret = MYSECRET After creating the file adding the secret is a simple kubectl command: Create S3 Secret kubectl create secret generic -n postgres direktiv-pgbackrest-secret --from-file = s3.conf To test if the values are correct run the following command: Show S3 Secrets kubectl get secret -n postgres direktiv-pgbackrest-secret -o go-template = '{{ index .data \"s3.conf\" | base64decode }}' High-Availabilty can be achieved by scaling the database replicas. The following example has added daily differential backups and pod anti-affinity to spread the pods across the cluster. If anti-affinity is used the cluster needs to have the same number of nodes and database replicas. S3 Backup Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/s3.yaml S3 Configuration apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : postgresVersion : 14 instances : - name : \"instance1\" replicas : 1 dataVolumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"1Gi\" backups : pgbackrest : configuration : - secret : name : direktiv-pgbackrest-secret global : repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 s3 : bucket : my-bucket endpoint : s3.eu-central-1.amazonaws.com:443 region : eu-central-1 schedules : full : \"0 1 * * 0\"","title":"High-Availability with S3 Backup"},{"location":"installation/database/#connection-pooling","text":"Connection pooling help scaling and maintaining availability between your application and the database. The Postgres Operator provides the option to install pgBouncer as connection pooling mechanism. If it is a multi-node cluster the pgBouncer replicas can be increased and spread acorss the cluster with pod anit-affinity rules. pgBouncer Install kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/pgbouncer.yaml pgBouncer Configuration apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : postgresVersion : 14 instances : - name : \"direktiv\" replicas : 1 dataVolumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"1Gi\" proxy : pgBouncer : replicas : 2 affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : \"postgres-operator.crunchydata.com/cluster\" operator : In values : - direktiv topologyKey : \"kubernetes.io/hostname\" backups : pgbackrest : global : # Keep 4 Backups repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 schedules : # Run every Sunday full : \"0 1 * * 0\" volume : volumeClaimSpec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"4Gi\"","title":"Connection-Pooling"},{"location":"installation/database/#getting-database-secrets","text":"Direktiv will need the database connection information during installation with a Helm chart. It is a good start for an installation YAML to have this information. It can be easily done by running a simple script: Database Configuration (No Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Database Configuration (With Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml","title":"Getting Database Secrets"},{"location":"installation/database/#restore-from-s3","text":"It is always recommended to test the backup and restore before using Direktiv in production. To restore from S3 is a straightforward process. The first step is to pick the backup used for the restore process. It can be found under bd/backup in the bucket used for backups in S3. It looks like this: 20221023-042407F . There are two differtent scenarios to consider. The first one is a restore for an existing database. This can be a restore of as certain backup or a point-in-time recovery. This can be achieved with a restore attribute in the dtaabase configuration YAML. Restore From S3 (Same Database) ... backups : pgbackrest : configuration : - secret : name : direktiv-pgbackrest-secret global : repo1-retention-full : \"4\" repo1-retention-full-type : count repos : - name : repo1 s3 : bucket : cd-direktiv-backup endpoint : s3.eu-central-1.amazonaws.com:443 region : eu-central-1 schedules : full : \"0 1 * * 0\" # Enable Restore restore : enabled : true repoName : repo1 options : - --set=20221023-042407F # point-in-time recovery alternative # - --type=time # - --target=\"2021-06-09 14:15:11-04\" The second scenario is if the whole database has been destroyed and it is a restore to a new database instance. In this case a datasource attribute has to be added to define the source for the backup. Restore From S3 (New Database) apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : dataSource : postgresCluster : clusterName : direktiv repoName : repo1 options : - --set=20221023-042407F postgresVersion : 14 instances : ... Additional Information For more information visit CrunchyData's documentation about disaster recovery .","title":"Restore from S3"},{"location":"installation/database/#restore-from-pvc","text":"In case the database is not using S3 backups the backups need to be stored in a safe location in case of loss of the node storing the backups. The data has to be transferred via e.g. scp in using a cron job. A restore of an existing database can be achieved with a simple restore attribute in the database configuration YAML mentioned in the S3 restore section of this documentation. The process is different if the backup node has been destroyed. It is important to not do this restore procedure if a backup is laready running. The backup needs to be rescheduled to execute it without running a backup in parallel.","title":"Restore from PVC"},{"location":"installation/database/#identify-backup-pvc","text":"The first step to store the backup is to identify the node where the backup is stored. Identify PV kubectl get pv NAME CAPACITY ... CLAIM # This is the backup node pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 1Gi ... postgres/direktiv-repo1 pvc-ce9bb226-1038-49bf-bed6-e6d0188b228c 1Gi ... postgres/direktiv-direktiv-wnh4-pgdata Describing the PV shows the node where the data is stored and the directory of the data. This directory needs to be stored in a safe location for a later restore. Identify PV kubectl describe pv pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 Name: pvc-80ae5325-8b27-4695-b6df-b362dd946cb7 ... Claim: postgres/direktiv-repo1 ... Node Affinity: Required Terms: # Node where the data is stored Term 0 : kubernetes.io/hostname in [ db2 ] Message: Source: Type: HostPath ( bare host directory volume ) # Data directory on the node Path: /var/lib/rancher/k3s/storage/pvc-80ae5325-8b27-4695-b6df-b362dd946cb7_postgres_direktiv-repo1 HostPathType: DirectoryOrCreate","title":"Identify Backup PVC"},{"location":"installation/database/#copy-data","text":"To restore the database a backup has to be selected. The available backups are in <Backup Directory>/<Old PVC Name>/backup/db . The directory will look like the following: Backup Directory drwxr-x--- 7 root root 4096 Okt 23 08 :11 . drwxr-x--- 3 root root 4096 Okt 23 08 :11 .. drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -060801F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -060901F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -061001F drwxr-x--- 3 root root 4096 Okt 23 08 :11 20221023 -061101F drwxr-x--- 3 root root 4096 Okt 23 08 :11 backup.history -rw-r----- 1 root root 2792 Okt 23 08 :11 backup.info -rw-r----- 1 root root 2792 Okt 23 08 :11 backup.info.copy lrwxrwxrwx 1 root root 16 Okt 23 08 :11 latest -> 20221023 -061101F The next step is to identify where the new backup folder is located. It is exactly the same procedure as used in the copying process. The archive and backup have to be copied into the new backup directory of the new cluster. Copy Folders sudo cp -Rf <Backup Directory>/archive /var/lib/rancher/k3s/storage/<New PV Directory> sudo cp -Rf <Backup Directory>/backup /var/lib/rancher/k3s/storage/<New PV Directory> sudo chown -R 26 :tape /var/lib/rancher/k3s/storage/<New PV Directory> The selected restore needs to be configured in the database configuration YAML and applied with kubectl apply -f mydb.yaml . Restore from PVC apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : dataSource : postgresCluster : clusterName : direktiv repoName : repo1 options : - --set=20221023-075501F - --archive-mode=off postgresVersion : 14","title":"Copy Data"},{"location":"installation/database/#update-password","text":"If this is a new installation of the database the password will be overwritten and the command to generate the direktiv.yaml file is incorrect. Therefore it is advised to update the password to the password in the Kubernetes secret and update Direktiv with the new password. Update User Password # get the old password kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode # execute in pod kubectl exec -n postgres --stdin --tty direktiv-direktiv-<POD-ID> -- psql # update user password ALTER USER direktiv WITH PASSWORD '<PASSWORD FROM FIRST COMMAND>' ; # exit \\q","title":"Update Password"},{"location":"installation/database/#helpful-commands","text":"Fetch Master Instance kubectl -n postgres get pods \\ --selector = postgres-operator.crunchydata.com/role = master \\ -o jsonpath = '{.items[*].metadata.labels.postgres-operator\\.crunchydata\\.com/instance}' Cluster Information kubectl -n postgres describe postgrescluster direktiv Use psql in Database Instance kubectl exec -n postgres --stdin --tty direktiv-direktiv-nl9z-0 -- psql","title":"Helpful Commands"},{"location":"installation/direktiv/","text":"Direktiv Direktiv requires a few components to run. At least the database has to be installed before proceeding with this part of the installation. Linkerd Database Knative Direktiv The following is a two-step orcess. First Knative is installed. Knative is responsible to execute Direktiv's serverless functions. It comes pre-configured to work with Direktiv. Knative Knative is an essential part of Direktiv. Although Knative provides YAML files and an operator for installation it is recommended to use the Helm installation with Direktiv's Helm charts. It uses the correct Knative (> 1.5.0) version and comes pre-configured to work seamlessly with Direktiv. Knative Installation helm repo add direktiv https://chart.direktiv.io helm install -n knative-serving --create-namespace knative direktiv/knative For high-availability for internal and external services the services need to be scaled up. The Helm chart values should be set to: High Availability replicas : 2 For more configuration options visit the Helm chart documentation . Direktiv Firstly, create a direktiv.yaml file which contains all of the database connectivity and secret information created during the database setup: Direktiv Database Configuration database : # -- database host host : \"direktiv-ha.postgres.svc\" # -- database port port : 5432 # -- database user user : \"direktiv\" # -- database password password : \"direktivdirektiv\" # -- database name, auto created if it does not exist name : \"direktiv\" # -- sslmode for database sslmode : require Database Configuration (No Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Database Configuration (With Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Using this direktiv.yaml configuration, deploy the direktiv helm chart: # This namespace might haven been created already during Linkerd installation kubectl create namespace direktiv-services-direktiv helm install -f direktiv.yaml -n direktiv direktiv direktiv/direktiv For more configuration options go to Direktiv's helm charts .","title":"Direktiv"},{"location":"installation/direktiv/#direktiv","text":"Direktiv requires a few components to run. At least the database has to be installed before proceeding with this part of the installation. Linkerd Database Knative Direktiv The following is a two-step orcess. First Knative is installed. Knative is responsible to execute Direktiv's serverless functions. It comes pre-configured to work with Direktiv.","title":"Direktiv"},{"location":"installation/direktiv/#knative","text":"Knative is an essential part of Direktiv. Although Knative provides YAML files and an operator for installation it is recommended to use the Helm installation with Direktiv's Helm charts. It uses the correct Knative (> 1.5.0) version and comes pre-configured to work seamlessly with Direktiv. Knative Installation helm repo add direktiv https://chart.direktiv.io helm install -n knative-serving --create-namespace knative direktiv/knative For high-availability for internal and external services the services need to be scaled up. The Helm chart values should be set to: High Availability replicas : 2 For more configuration options visit the Helm chart documentation .","title":"Knative"},{"location":"installation/direktiv/#direktiv_1","text":"Firstly, create a direktiv.yaml file which contains all of the database connectivity and secret information created during the database setup: Direktiv Database Configuration database : # -- database host host : \"direktiv-ha.postgres.svc\" # -- database port port : 5432 # -- database user user : \"direktiv\" # -- database password password : \"direktivdirektiv\" # -- database name, auto created if it does not exist name : \"direktiv\" # -- sslmode for database sslmode : require Database Configuration (No Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Database Configuration (With Connection Pooling) echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml Using this direktiv.yaml configuration, deploy the direktiv helm chart: # This namespace might haven been created already during Linkerd installation kubectl create namespace direktiv-services-direktiv helm install -f direktiv.yaml -n direktiv direktiv direktiv/direktiv For more configuration options go to Direktiv's helm charts .","title":"Direktiv"},{"location":"installation/kubernetes/","text":"Kubernetes Direktiv is a cloud-native solution requiring Kubernetes to run. It is working with all Kubernetes offerings of the major cloud providers as well as on-premise Kubernetes installations. The minimum version is 1.22. The easiest way to install a Kubernetes cluster for Kubernetes is using k3s . The following section explains how to install k3s on-premise with K3s. The minmi K3s Direktiv supports Kubernetes offerings from all major cloud providers and requires Kubernets 1.19+ to be installed. Direktiv supports Kubernetes setups with a single node, seperate server and agents nodes as well as small setups with nodes acting both as server and agent. The folowing section describes the installation with k3s . Single Node Setup A single node setup requires no further configuration and K3s can be used with the default settings. This setup disables Traefik to be replaced with Nginx during the installation. If proxy configuration is required please read the proxy setup section . One Node Setup curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode = 644 Multi Node Setup For production use it is recommended to run Direktiv in a multi-node environment. The K3s documentation page provides a lot of information about configuration and installation options. The following is a quick installation instruction to setup a three node cluster with nodes action as servers and agents. Server configuration In a multi-node environment the nodes have to communicate with each other. Therefore certain ports between those nodes have to be open. The following table shows the ports required to be accessible (incoming) for the nodes to enable this. On some Linux distributions firewall changes have to be applied. Please see k3s installation guide for detailed installation instructions. Protocol Port Source Description TCP 6443 K3s agent nodes Kubernetes API Server UDP 8472 K3s server and agent nodes VXLAN TCP 10250 K3s server and agent nodes Kubelet metrics TCP 2379-2380 K3s server nodes Required for HA with embedded etcd only Firewall changes (Centos/RedHat): Example Firewall Changes Centos/RedHat sudo firewall-cmd --permanent --add-port=6443/tcp sudo firewall-cmd --permanent --add-port=10250/tcp sudo firewall-cmd --permanent --add-port=8472/udp sudo firewall-cmd --permanent --add-port=2379-2380/tcp sudo firewall-cmd --reload Additional Centos/RedHat Instructions https://rancher.com/docs/k3s/latest/en/advanced/#additional-preparation-for-red-hat-centos-enterprise-linux An additional Kubernetes requirement is to disable swap on the nodes. This change need to be applied permanently to survive reboots. This might be achieved differently on different Linux distributions. Disable Swap sudo swapoff -a sudo sed -e '/swap/s/^/#/g' -i /etc/fstab Node Installation K3s provides a script to install K3s. It is recommended to use it for installation. The configuration can be done via environment variables during installation. For Direktiv the default ingress controller (Traefik) needs to be disabled because Nginx will be used. For installations using the embedded etcd the first server node requires the '--cluster-init' flag. Initial Node curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644 --cluster-init\" sh - Loadbalancer The --disable servicelb is only required if MetalLB will be used in this installation, e.g. for on-premise installation. It is not needed if the cluster is installed in a cloud environment like AWS, GCP or Azure. To add nodes to the cluster the node token is required, which is saved under /var/lib/rancher/k3s/server/node-token . With this token additional nodes can be added. Additional Nodes curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 sh - MetalLB In a on-premise environment a Kubernetes bare-metal load-balancer is required. The following example shows the use of MetalLB . K3s load-balancer needs to be disabled with --disable servicelb for this to work. Disable Loadbalancer curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 sh - MetalLB needs an IP pool to serve IP address. During the installation this pool can be configured with fhe following example YAML file: MetalLB IP Pool Configuration configInline : address-pools : - name : default protocol : layer2 addresses : - 192.168.0.122/32 - 192.168.1.0/24 After defining the IP range MetalLB is a simple Helm installation: MetalLB Installation helm repo add metallb https://metallb.github.io/metallb helm install metallb metallb/metallb -f values.yaml Proxy Setup K3s will download container images during installation and runtime. For the downloads of those internet connectivity is required. If the nodes are behind a proxy server the Linux environment variables need to provided to the service, e.g.: Proxy Settings for K3s curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 HTTP_PROXY = \"http://192.168.1.10:3128\" HTTPS_PROXY = \"http://192.168.1.10:3128\" NO_PROXY = \"localhost,127.0.0.1,svc,.cluster.local,192.168.1.100,192.168.1.101,192.168.1.102,10.0.0.0/8\" sh -","title":"Kubernetes"},{"location":"installation/kubernetes/#kubernetes","text":"Direktiv is a cloud-native solution requiring Kubernetes to run. It is working with all Kubernetes offerings of the major cloud providers as well as on-premise Kubernetes installations. The minimum version is 1.22. The easiest way to install a Kubernetes cluster for Kubernetes is using k3s . The following section explains how to install k3s on-premise with K3s. The minmi","title":"Kubernetes"},{"location":"installation/kubernetes/#k3s","text":"Direktiv supports Kubernetes offerings from all major cloud providers and requires Kubernets 1.19+ to be installed. Direktiv supports Kubernetes setups with a single node, seperate server and agents nodes as well as small setups with nodes acting both as server and agent. The folowing section describes the installation with k3s .","title":"K3s"},{"location":"installation/kubernetes/#single-node-setup","text":"A single node setup requires no further configuration and K3s can be used with the default settings. This setup disables Traefik to be replaced with Nginx during the installation. If proxy configuration is required please read the proxy setup section . One Node Setup curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode = 644","title":"Single Node Setup"},{"location":"installation/kubernetes/#multi-node-setup","text":"For production use it is recommended to run Direktiv in a multi-node environment. The K3s documentation page provides a lot of information about configuration and installation options. The following is a quick installation instruction to setup a three node cluster with nodes action as servers and agents.","title":"Multi Node Setup"},{"location":"installation/kubernetes/#server-configuration","text":"In a multi-node environment the nodes have to communicate with each other. Therefore certain ports between those nodes have to be open. The following table shows the ports required to be accessible (incoming) for the nodes to enable this. On some Linux distributions firewall changes have to be applied. Please see k3s installation guide for detailed installation instructions. Protocol Port Source Description TCP 6443 K3s agent nodes Kubernetes API Server UDP 8472 K3s server and agent nodes VXLAN TCP 10250 K3s server and agent nodes Kubelet metrics TCP 2379-2380 K3s server nodes Required for HA with embedded etcd only Firewall changes (Centos/RedHat): Example Firewall Changes Centos/RedHat sudo firewall-cmd --permanent --add-port=6443/tcp sudo firewall-cmd --permanent --add-port=10250/tcp sudo firewall-cmd --permanent --add-port=8472/udp sudo firewall-cmd --permanent --add-port=2379-2380/tcp sudo firewall-cmd --reload Additional Centos/RedHat Instructions https://rancher.com/docs/k3s/latest/en/advanced/#additional-preparation-for-red-hat-centos-enterprise-linux An additional Kubernetes requirement is to disable swap on the nodes. This change need to be applied permanently to survive reboots. This might be achieved differently on different Linux distributions. Disable Swap sudo swapoff -a sudo sed -e '/swap/s/^/#/g' -i /etc/fstab","title":"Server configuration"},{"location":"installation/kubernetes/#node-installation","text":"K3s provides a script to install K3s. It is recommended to use it for installation. The configuration can be done via environment variables during installation. For Direktiv the default ingress controller (Traefik) needs to be disabled because Nginx will be used. For installations using the embedded etcd the first server node requires the '--cluster-init' flag. Initial Node curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644 --cluster-init\" sh - Loadbalancer The --disable servicelb is only required if MetalLB will be used in this installation, e.g. for on-premise installation. It is not needed if the cluster is installed in a cloud environment like AWS, GCP or Azure. To add nodes to the cluster the node token is required, which is saved under /var/lib/rancher/k3s/server/node-token . With this token additional nodes can be added. Additional Nodes curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 sh -","title":"Node Installation"},{"location":"installation/kubernetes/#metallb","text":"In a on-premise environment a Kubernetes bare-metal load-balancer is required. The following example shows the use of MetalLB . K3s load-balancer needs to be disabled with --disable servicelb for this to work. Disable Loadbalancer curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 sh - MetalLB needs an IP pool to serve IP address. During the installation this pool can be configured with fhe following example YAML file: MetalLB IP Pool Configuration configInline : address-pools : - name : default protocol : layer2 addresses : - 192.168.0.122/32 - 192.168.1.0/24 After defining the IP range MetalLB is a simple Helm installation: MetalLB Installation helm repo add metallb https://metallb.github.io/metallb helm install metallb metallb/metallb -f values.yaml","title":"MetalLB"},{"location":"installation/kubernetes/#proxy-setup","text":"K3s will download container images during installation and runtime. For the downloads of those internet connectivity is required. If the nodes are behind a proxy server the Linux environment variables need to provided to the service, e.g.: Proxy Settings for K3s curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 HTTP_PROXY = \"http://192.168.1.10:3128\" HTTPS_PROXY = \"http://192.168.1.10:3128\" NO_PROXY = \"localhost,127.0.0.1,svc,.cluster.local,192.168.1.100,192.168.1.101,192.168.1.102,10.0.0.0/8\" sh -","title":"Proxy Setup"},{"location":"installation/linkerd/","text":"Linkerd (Optional) Linkerd is a lightweight service mesh for Kubernetes and can be used in Direktiv as a mechanism to secure communication between the components. Linkerd can enable mTLS between the core Direktiv pods as well as the containers running in a flow. The installation of Linkerd is optional. The easiest way to install Linkerd is via Helm . Creating Certificates The identity component of Linkerd requires setting up a trust anchor certificate, and an issuer certificate with its key. The following script starts a container and generates the certificates needed: Generating Linkerd Certificates certDir = $( exe = 'step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 87600h --no-password --insecure \\ --ca ca.crt --ca-key ca.key' ; \\ sudo docker run --mount \"type=bind,src= $( pwd ) ,dst=/home/step\" -i smallstep/step-cli /bin/bash -c \" $exe \" ; \\ echo $( pwd )) ; Permissions The directory where the certificates are located is stored in $certDir. If there are permission problems, please try a new directory. Install with Helm After creating the certificates the certificate folder should be located at $certDir. The expiry date provided during installation has to be the same as the value for the certificates (in this case: one year). The following script installs Linkerd with the previously generated certificates: Install Linkerd CRDs helm repo add linkerd https://helm.linkerd.io/stable ; helm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace Install Linkerd helm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM = $certDir /ca.crt \\ --set-file identity.issuer.tls.crtPEM = $certDir /issuer.crt \\ --set-file identity.issuer.tls.keyPEM = $certDir /issuer.key \\ linkerd/linkerd-control-plane --wait Annotate Namespaces To use the service mesh (and, in particular, the mTLS communication) between pods within a Direktiv cluster the namespaces need to be annotated for Linkerd to inject its proxy. The default namespaces to annotate are: direktiv knative-serving direktiv-services-direktiv This script will create and annotate the namespaces: Annotate Namespaces for ns in \"direktiv\" \"knative-serving\" \"direktiv-services-direktiv\" do kubectl create namespace $ns || true kubectl annotate ns --overwrite = true $ns linkerd.io/inject = enabled done ;","title":"Linkerd"},{"location":"installation/linkerd/#linkerd-optional","text":"Linkerd is a lightweight service mesh for Kubernetes and can be used in Direktiv as a mechanism to secure communication between the components. Linkerd can enable mTLS between the core Direktiv pods as well as the containers running in a flow. The installation of Linkerd is optional. The easiest way to install Linkerd is via Helm .","title":"Linkerd (Optional)"},{"location":"installation/linkerd/#creating-certificates","text":"The identity component of Linkerd requires setting up a trust anchor certificate, and an issuer certificate with its key. The following script starts a container and generates the certificates needed: Generating Linkerd Certificates certDir = $( exe = 'step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 87600h --no-password --insecure \\ --ca ca.crt --ca-key ca.key' ; \\ sudo docker run --mount \"type=bind,src= $( pwd ) ,dst=/home/step\" -i smallstep/step-cli /bin/bash -c \" $exe \" ; \\ echo $( pwd )) ; Permissions The directory where the certificates are located is stored in $certDir. If there are permission problems, please try a new directory.","title":"Creating Certificates"},{"location":"installation/linkerd/#install-with-helm","text":"After creating the certificates the certificate folder should be located at $certDir. The expiry date provided during installation has to be the same as the value for the certificates (in this case: one year). The following script installs Linkerd with the previously generated certificates: Install Linkerd CRDs helm repo add linkerd https://helm.linkerd.io/stable ; helm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace Install Linkerd helm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM = $certDir /ca.crt \\ --set-file identity.issuer.tls.crtPEM = $certDir /issuer.crt \\ --set-file identity.issuer.tls.keyPEM = $certDir /issuer.key \\ linkerd/linkerd-control-plane --wait","title":"Install with Helm"},{"location":"installation/linkerd/#annotate-namespaces","text":"To use the service mesh (and, in particular, the mTLS communication) between pods within a Direktiv cluster the namespaces need to be annotated for Linkerd to inject its proxy. The default namespaces to annotate are: direktiv knative-serving direktiv-services-direktiv This script will create and annotate the namespaces: Annotate Namespaces for ns in \"direktiv\" \"knative-serving\" \"direktiv-services-direktiv\" do kubectl create namespace $ns || true kubectl annotate ns --overwrite = true $ns linkerd.io/inject = enabled done ;","title":"Annotate Namespaces"},{"location":"installation/summary/","text":"Quick Install This is a list of \"copy&paste\" commands which creates a one node Direktiv cluster. K3s curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode = 644 --kube-apiserver-arg feature-gates = TTLAfterFinished = true Linkerd Create certificates certDir = $( exe = 'step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 87600h --no-password --insecure \\ --ca ca.crt --ca-key ca.key' ; \\ sudo docker run --mount \"type=bind,src= $( pwd ) ,dst=/home/step\" -i smallstep/step-cli /bin/bash -c \" $exe \" ; \\ echo $( pwd )) ; Install Linkerd helm repo add linkerd https://helm.linkerd.io/stable ; helm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace helm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM = $certDir /ca.crt \\ --set-file identity.issuer.tls.crtPEM = $certDir /issuer.crt \\ --set-file identity.issuer.tls.keyPEM = $certDir /issuer.key \\ linkerd/linkerd-control-plane --wait Annotate the Namespaces for ns in \"default\" \"knative-serving\" \"direktiv-services-direktiv\" do kubectl create namespace $ns || true kubectl annotate ns --overwrite = true $ns linkerd.io/inject = enabled done ; Database helm repo add direktiv https://chart.direktiv.io helm install -n postgres --create-namespace --set singleNamespace = true postgres direktiv/pgo kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/pg.yaml Knative helm install -n knative-serving --create-namespace knative direktiv/knative Direktiv echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml helm install -f direktiv.yaml direktiv direktiv/direktiv Get IP of Direktiv kubectl get services direktiv-ingress-nginx-controller --output jsonpath = '{.status.loadBalancer.ingress[0].ip}'","title":"Quick Install"},{"location":"installation/summary/#quick-install","text":"This is a list of \"copy&paste\" commands which creates a one node Direktiv cluster.","title":"Quick Install"},{"location":"installation/summary/#k3s","text":"curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode = 644 --kube-apiserver-arg feature-gates = TTLAfterFinished = true","title":"K3s"},{"location":"installation/summary/#linkerd","text":"","title":"Linkerd"},{"location":"installation/summary/#create-certificates","text":"certDir = $( exe = 'step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 87600h --no-password --insecure \\ --ca ca.crt --ca-key ca.key' ; \\ sudo docker run --mount \"type=bind,src= $( pwd ) ,dst=/home/step\" -i smallstep/step-cli /bin/bash -c \" $exe \" ; \\ echo $( pwd )) ;","title":"Create certificates"},{"location":"installation/summary/#install-linkerd","text":"helm repo add linkerd https://helm.linkerd.io/stable ; helm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace helm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM = $certDir /ca.crt \\ --set-file identity.issuer.tls.crtPEM = $certDir /issuer.crt \\ --set-file identity.issuer.tls.keyPEM = $certDir /issuer.key \\ linkerd/linkerd-control-plane --wait","title":"Install Linkerd"},{"location":"installation/summary/#annotate-the-namespaces","text":"for ns in \"default\" \"knative-serving\" \"direktiv-services-direktiv\" do kubectl create namespace $ns || true kubectl annotate ns --overwrite = true $ns linkerd.io/inject = enabled done ;","title":"Annotate the Namespaces"},{"location":"installation/summary/#database","text":"helm repo add direktiv https://chart.direktiv.io helm install -n postgres --create-namespace --set singleNamespace = true postgres direktiv/pgo kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/pg.yaml","title":"Database"},{"location":"installation/summary/#knative","text":"helm install -n knative-serving --create-namespace knative direktiv/knative","title":"Knative"},{"location":"installation/summary/#direktiv","text":"echo \"database: host: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode ) \\\" port: $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode ) user: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode ) \\\" password: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode ) \\\" name: \\\" $( kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode ) \\\" sslmode: require\" > direktiv.yaml helm install -f direktiv.yaml direktiv direktiv/direktiv","title":"Direktiv"},{"location":"installation/summary/#get-ip-of-direktiv","text":"kubectl get services direktiv-ingress-nginx-controller --output jsonpath = '{.status.loadBalancer.ingress[0].ip}'","title":"Get IP of Direktiv"},{"location":"spec/TODO/","text":"Spec Documentation TODOs YAML Section Inclusions: Logging Metadata Errors Section about secrets. Section about events. Section about errors & error handling. Large section explaining the spec for containers and everything related to actions. Expand variables section. Explain function files and events.","title":"Spec Documentation TODOs"},{"location":"spec/TODO/#spec-documentation-todos","text":"YAML Section Inclusions: Logging Metadata Errors Section about secrets. Section about events. Section about errors & error handling. Large section explaining the spec for containers and everything related to actions. Expand variables section. Explain function files and events.","title":"Spec Documentation TODOs"},{"location":"spec/instance-data/input/","text":"Workflow Input When a workflow is triggered and spawns a new instance it may do so with some starting value for its instance data . Here's everything you need to know about workflow input. API / Subflow If a workflow is invoked directly, either through the API or as a subflow to another instance, its input is passed in as-is. So if you call the workflow with the following input: { \"msg\" : \"Hello, world!\" } Then the instance data for the workflow will, be the same: { \"msg\" : \"Hello, world!\" } That is, unless the input data isn't a valid JSON object. If the input is valid JSON but not an object, as in the following example, it is wrapped within an object automatically under the property .input . So this input: [ 1 , 2 , 3 ] Becomes: { \"input\" : [ 1 , 2 , 3 ] } If the input data isn't valid JSON at all, it is treated as binary data. Binary data is converted into a base64 encoded string and passed into the instance the same way as above. This input: Hello, world! Becomes: { \"input\" : \"SGVsbG8sIHdvcmxkIQo=\" } This treatment of binary data allows workflows to handle non-JSON inputs. Common examples include XML and form data. Just use a function to extract the information needed from these other formats and convert them to JSON. One thing to keep in mind that might trip you up: if you provide no input data whatsoever that's not valid JSON. It is valid binary data, which means this input: Becomes: { \"input\" : \"\" } An empty string is a valid base64 representation of zero bytes. CRON By their nature, scheduled workflows have empty input. They will always be: {} This doesn't mean they have to do exactly the same thing each time, it just means you need to get a little creative. For example, begin your workflow by loading data from variables or by using an action that grabs data from an external source. CloudEvents Events Workflows that are triggered by receiving one or more events will include the received event(s) in their input data. Each received event will appear in the instance data under a property with the same value as their event type, to allow workflows to distinguish between events. For an instance triggered with the following event: { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } The instance input data becomes: { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } } If an event's payload is JSON it should be directly addressable, rather than being embedded within a string. Large Inputs Like instance data, input data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 32 MiB.","title":"Input"},{"location":"spec/instance-data/input/#workflow-input","text":"When a workflow is triggered and spawns a new instance it may do so with some starting value for its instance data . Here's everything you need to know about workflow input.","title":"Workflow Input"},{"location":"spec/instance-data/input/#api-subflow","text":"If a workflow is invoked directly, either through the API or as a subflow to another instance, its input is passed in as-is. So if you call the workflow with the following input: { \"msg\" : \"Hello, world!\" } Then the instance data for the workflow will, be the same: { \"msg\" : \"Hello, world!\" } That is, unless the input data isn't a valid JSON object. If the input is valid JSON but not an object, as in the following example, it is wrapped within an object automatically under the property .input . So this input: [ 1 , 2 , 3 ] Becomes: { \"input\" : [ 1 , 2 , 3 ] } If the input data isn't valid JSON at all, it is treated as binary data. Binary data is converted into a base64 encoded string and passed into the instance the same way as above. This input: Hello, world! Becomes: { \"input\" : \"SGVsbG8sIHdvcmxkIQo=\" } This treatment of binary data allows workflows to handle non-JSON inputs. Common examples include XML and form data. Just use a function to extract the information needed from these other formats and convert them to JSON. One thing to keep in mind that might trip you up: if you provide no input data whatsoever that's not valid JSON. It is valid binary data, which means this input: Becomes: { \"input\" : \"\" } An empty string is a valid base64 representation of zero bytes.","title":"API / Subflow"},{"location":"spec/instance-data/input/#cron","text":"By their nature, scheduled workflows have empty input. They will always be: {} This doesn't mean they have to do exactly the same thing each time, it just means you need to get a little creative. For example, begin your workflow by loading data from variables or by using an action that grabs data from an external source.","title":"CRON"},{"location":"spec/instance-data/input/#cloudevents-events","text":"Workflows that are triggered by receiving one or more events will include the received event(s) in their input data. Each received event will appear in the instance data under a property with the same value as their event type, to allow workflows to distinguish between events. For an instance triggered with the following event: { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } The instance input data becomes: { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } } If an event's payload is JSON it should be directly addressable, rather than being embedded within a string.","title":"CloudEvents Events"},{"location":"spec/instance-data/input/#large-inputs","text":"Like instance data, input data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 32 MiB.","title":"Large Inputs"},{"location":"spec/instance-data/instance-data/","text":"Instance Data Every workflow instance has its own instance data , which is data that is exclusively accessible to the instance, and only changeable by the instance. JSON Objects Instance data is represented in JSON form, and is always a valid JSON object . This detail is important because it means that not everything that is valid JSON can be valid instance data. This is valid instance data: {} So is this: { \"list\" : [ 1 , 2 , 3 ] } And this: { \"a\" : 5 , \"b\" : \"6\" , \"c\" : true , \"d\" : { \"list\" : [ 7 , \"8\" ] } } But this are not valid instance data, even though it is valid JSON: true Neither is this: \"Hello, world!\" Nor is this: [{ \"a\" : 5 }] Another way of looking at it: it's not valid instance data unless it's valid JSON beginning with { and ending with } . Size Limit The size of instance data is measured in terms of the length (in bytes) of its JSON representation. For technical reasons, there is an enforced upper limit allowed for this maximum size. This limit can vary according to the specific configuration of a Direktiv installation, but the default is 32 MiB. Lifecycle The starting value for an instance's data is set based on what triggered the workflow to spawn a new instance. See Workflow Input . Afterwards, the instance may manipulate the data in predictable ways according to the instructions in the workflow definition. The main way to change instance data is through Transforms . Other operations can also contribute to the instance data. Actions may return results, error handling may save error information, event listeners save received events, and variable getters can retrieve data saved elsewhere and add it to the instance data. After the final operation of an instance is executed the instance data becomes the instance's output data. See Instance Output . Output data is viewable by the API, and is also returned to the caller if the instance was executed as a subflow to another workflow.","title":"Instance Data"},{"location":"spec/instance-data/instance-data/#instance-data","text":"Every workflow instance has its own instance data , which is data that is exclusively accessible to the instance, and only changeable by the instance.","title":"Instance Data"},{"location":"spec/instance-data/instance-data/#json-objects","text":"Instance data is represented in JSON form, and is always a valid JSON object . This detail is important because it means that not everything that is valid JSON can be valid instance data. This is valid instance data: {} So is this: { \"list\" : [ 1 , 2 , 3 ] } And this: { \"a\" : 5 , \"b\" : \"6\" , \"c\" : true , \"d\" : { \"list\" : [ 7 , \"8\" ] } } But this are not valid instance data, even though it is valid JSON: true Neither is this: \"Hello, world!\" Nor is this: [{ \"a\" : 5 }] Another way of looking at it: it's not valid instance data unless it's valid JSON beginning with { and ending with } .","title":"JSON Objects"},{"location":"spec/instance-data/instance-data/#size-limit","text":"The size of instance data is measured in terms of the length (in bytes) of its JSON representation. For technical reasons, there is an enforced upper limit allowed for this maximum size. This limit can vary according to the specific configuration of a Direktiv installation, but the default is 32 MiB.","title":"Size Limit"},{"location":"spec/instance-data/instance-data/#lifecycle","text":"The starting value for an instance's data is set based on what triggered the workflow to spawn a new instance. See Workflow Input . Afterwards, the instance may manipulate the data in predictable ways according to the instructions in the workflow definition. The main way to change instance data is through Transforms . Other operations can also contribute to the instance data. Actions may return results, error handling may save error information, event listeners save received events, and variable getters can retrieve data saved elsewhere and add it to the instance data. After the final operation of an instance is executed the instance data becomes the instance's output data. See Instance Output . Output data is viewable by the API, and is also returned to the caller if the instance was executed as a subflow to another workflow.","title":"Lifecycle"},{"location":"spec/instance-data/output/","text":"Instance Output Unlike instance data, which is not accessible to anything other than the instance itself, instance output is exposed via API. It is also returned to caller instance if the workflow was invoked as a subflow. When an instance completes it saves its instance data as its output, which indirectly exposes the instance data. Normally this is the desired behaviour, but it can present a security risk if handled incorrectly. Workflows should take steps to trim things they don't mean to return before terminating using transforms . Large Outputs Like instance data, output data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 32 MiB.","title":"Output"},{"location":"spec/instance-data/output/#instance-output","text":"Unlike instance data, which is not accessible to anything other than the instance itself, instance output is exposed via API. It is also returned to caller instance if the workflow was invoked as a subflow. When an instance completes it saves its instance data as its output, which indirectly exposes the instance data. Normally this is the desired behaviour, but it can present a security risk if handled incorrectly. Workflows should take steps to trim things they don't mean to return before terminating using transforms .","title":"Instance Output"},{"location":"spec/instance-data/output/#large-outputs","text":"Like instance data, output data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 32 MiB.","title":"Large Outputs"},{"location":"spec/instance-data/security/","text":"Security There are a number of security concerns to keep in mind with instance data: Input & Output Data Is Remembered A copy of the starting value is saved separately so that instances can be replayed. It also helps to debug workflows. It is worth keeping this in mind. Even if your workflow deletes sensitive fields, they are still stored somewhere they could be reused or read later. Likewise, output data is saved. It is returned to parent instances if the instance was executed as a subflow. Both input and output data is requestable via API. For a good way to handle most sensitive data, consider using secrets. Input Validation To protect your workflows from behaving in unexpected ways, including intentional exploits by attackers, it is good practice to validate your input data before acting upon it. That is why we recommend beginning every workflow with a validate state. .private As a basic precaution, anything stored under .private is redacted over the APIs that retrieve instance input and output data. This data is still usable by the instance. Could still be returned to a parent instance. It is still stored in the database in plaintext. And there is nothing preventing you from transforming it or passing it somewhere that exposes this information. Use this feature with caution.","title":"Security"},{"location":"spec/instance-data/security/#security","text":"There are a number of security concerns to keep in mind with instance data:","title":"Security"},{"location":"spec/instance-data/security/#input-output-data-is-remembered","text":"A copy of the starting value is saved separately so that instances can be replayed. It also helps to debug workflows. It is worth keeping this in mind. Even if your workflow deletes sensitive fields, they are still stored somewhere they could be reused or read later. Likewise, output data is saved. It is returned to parent instances if the instance was executed as a subflow. Both input and output data is requestable via API. For a good way to handle most sensitive data, consider using secrets.","title":"Input &amp; Output Data Is Remembered"},{"location":"spec/instance-data/security/#input-validation","text":"To protect your workflows from behaving in unexpected ways, including intentional exploits by attackers, it is good practice to validate your input data before acting upon it. That is why we recommend beginning every workflow with a validate state.","title":"Input Validation"},{"location":"spec/instance-data/security/#private","text":"As a basic precaution, anything stored under .private is redacted over the APIs that retrieve instance input and output data. This data is still usable by the instance. Could still be returned to a parent instance. It is still stored in the database in plaintext. And there is nothing preventing you from transforming it or passing it somewhere that exposes this information. Use this feature with caution.","title":".private"},{"location":"spec/instance-data/structured-jx/","text":"Structured JX Many fields of the workflow definition are described as \"Structured JX\". That's a name we use for fields that support complex and powerful query logic that we'll describe in greater detail here. JQ Since instance data is represented as JSON, the most natural way to work with that data is with the powerful JSON query language called jq. Whenever a string appears within a Structured JX field that includes jq(...) , everything between the brackets is evaluated as a jq query against the instance data. Then the entire jq(...) part is replaced by the results of that query. Note: YAML allows for strings without quotation marks, but this should be avoided when using Structured JX. The characters in the queries will commonly be interpreted in unintended ways by the YAML parser. If the jq(...) part constitutes the entirety of the string then the entire string is replaced by whatever data type was returned. If not, the results are marshalled into a JSON string and substituted into the parent string. The one exception to this rule is if the returned data type is a string, in which case it is substituted as-is without marshalling into JSON. This enables you to build strings without filling them with quotation marks. Example 1 Instance Data { \"a\" : [ 1 , 2 , 3 ] } Structured JX 'jq(.a)' Evaluated Result [ 1 , 2 , 3 ] Example 2 Instance Data { \"a\" : [ 1 , 2 , 3 ] } Structured JX 'a: jq(.a)' Evaluated Result \"a: [1, 2, 3]\" Example 3 Instance Data { \"a\" : \"hello\" } Structured JX 'a: jq(.a)' Evaluated Result \"a: hello\" JS JQ isn't the only option available to interact with the instance data. Javascript is also supported using js(...) in a very similar way. Entire Javascript scripts can be embedded in strings within Structured JX. Note: YAML supports several ways of including large or multi-line strings. But each of these ways is treated a little bit differently by the YAML parser. To preserve newlines, we recommend using the | form. With Javascript this often necessary. When writing scripts this way, the instance data is copied and exposed to the script in an object called data . Example 1 JQ transform : 'jq({x: 5})' Analogous Javascript transform : | js( items = new Object() items.x = 5 return items ) Example 2 JQ transform : 'jq({x: .a})' Analogous Javascript transform : | js( items = new Object() items.x = data['a'] return items ) YAML So far we've seen how you can use jq or Javascript to produce a value for your Structured JX field, but it's also possible to use neither, or both. The \"Structured\" part of Structured JX is so named because you don't have to provide a single string. You can provide any type of data you like. The entirety of what is provided will be converted from its YAML representation to a JSON representation. And then every field within will be searched recursively for embedded jq/Javascript. Example Instance Data Before Transform { \"a\" : [ 1 , 2 , 3 ] } Transform tranform : x : 'jq(.a)' y : | js( var output = data['a'].map((x) => {return ++x;}) return output ) z : 5 listA : [ \"a\" , \"b\" , \"c\" ] listB : - d - e - f obj : i : 10 j : 'jq(.a[2])' Evaluated Result { \"listA\" : [ \"a\" , \"b\" , \"c\" ], \"listB\" : [ \"d\" , \"e\" , \"f\" ], \"obj\" : { \"i\" : 10 , \"j\" : 3 }, \"x\" : [ 1 , 2 , 3 ], \"y\" : [ 2 , 3 , 4 ], \"z\" : 5 }","title":"Structured JX"},{"location":"spec/instance-data/structured-jx/#structured-jx","text":"Many fields of the workflow definition are described as \"Structured JX\". That's a name we use for fields that support complex and powerful query logic that we'll describe in greater detail here.","title":"Structured JX"},{"location":"spec/instance-data/structured-jx/#jq","text":"Since instance data is represented as JSON, the most natural way to work with that data is with the powerful JSON query language called jq. Whenever a string appears within a Structured JX field that includes jq(...) , everything between the brackets is evaluated as a jq query against the instance data. Then the entire jq(...) part is replaced by the results of that query. Note: YAML allows for strings without quotation marks, but this should be avoided when using Structured JX. The characters in the queries will commonly be interpreted in unintended ways by the YAML parser. If the jq(...) part constitutes the entirety of the string then the entire string is replaced by whatever data type was returned. If not, the results are marshalled into a JSON string and substituted into the parent string. The one exception to this rule is if the returned data type is a string, in which case it is substituted as-is without marshalling into JSON. This enables you to build strings without filling them with quotation marks.","title":"JQ"},{"location":"spec/instance-data/structured-jx/#example-1","text":"Instance Data { \"a\" : [ 1 , 2 , 3 ] } Structured JX 'jq(.a)' Evaluated Result [ 1 , 2 , 3 ]","title":"Example 1"},{"location":"spec/instance-data/structured-jx/#example-2","text":"Instance Data { \"a\" : [ 1 , 2 , 3 ] } Structured JX 'a: jq(.a)' Evaluated Result \"a: [1, 2, 3]\"","title":"Example 2"},{"location":"spec/instance-data/structured-jx/#example-3","text":"Instance Data { \"a\" : \"hello\" } Structured JX 'a: jq(.a)' Evaluated Result \"a: hello\"","title":"Example 3"},{"location":"spec/instance-data/structured-jx/#js","text":"JQ isn't the only option available to interact with the instance data. Javascript is also supported using js(...) in a very similar way. Entire Javascript scripts can be embedded in strings within Structured JX. Note: YAML supports several ways of including large or multi-line strings. But each of these ways is treated a little bit differently by the YAML parser. To preserve newlines, we recommend using the | form. With Javascript this often necessary. When writing scripts this way, the instance data is copied and exposed to the script in an object called data .","title":"JS"},{"location":"spec/instance-data/structured-jx/#example-1_1","text":"JQ transform : 'jq({x: 5})' Analogous Javascript transform : | js( items = new Object() items.x = 5 return items )","title":"Example 1"},{"location":"spec/instance-data/structured-jx/#example-2_1","text":"JQ transform : 'jq({x: .a})' Analogous Javascript transform : | js( items = new Object() items.x = data['a'] return items )","title":"Example 2"},{"location":"spec/instance-data/structured-jx/#yaml","text":"So far we've seen how you can use jq or Javascript to produce a value for your Structured JX field, but it's also possible to use neither, or both. The \"Structured\" part of Structured JX is so named because you don't have to provide a single string. You can provide any type of data you like. The entirety of what is provided will be converted from its YAML representation to a JSON representation. And then every field within will be searched recursively for embedded jq/Javascript.","title":"YAML"},{"location":"spec/instance-data/structured-jx/#example","text":"Instance Data Before Transform { \"a\" : [ 1 , 2 , 3 ] } Transform tranform : x : 'jq(.a)' y : | js( var output = data['a'].map((x) => {return ++x;}) return output ) z : 5 listA : [ \"a\" , \"b\" , \"c\" ] listB : - d - e - f obj : i : 10 j : 'jq(.a[2])' Evaluated Result { \"listA\" : [ \"a\" , \"b\" , \"c\" ], \"listB\" : [ \"d\" , \"e\" , \"f\" ], \"obj\" : { \"i\" : 10 , \"j\" : 3 }, \"x\" : [ 1 , 2 , 3 ], \"y\" : [ 2 , 3 , 4 ], \"z\" : 5 }","title":"Example"},{"location":"spec/instance-data/transforms/","text":"Transforms Whenever an instance finishes executing a state there is an opportunity to perform a Transform. Usually with a field called transform , but sometimes in other forms. The switch state also has a defaultTransform , for example. All transforms use structured jx , giving you powerful options to enrich, sanitize, or modify instance data. All transforms must produce output that remains valid instance data , otherwise an error will be thrown. // TODO: what error, specifically? Examples Here are some common use-case helpful examples of transforms. Completely Replacing Instance Data Instance Data Before Transform { \"msg\" : \"Hello, world! } Transform Snippet - id : snippet type : noop transform : x : 5 Instance Data After Transform { \"x\" : 5 } Replacing A Subset Of Instance Data Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(.a = 5 | .b = 6)' Instance Data After Transform { \"a\" : 5 , \"b\" : 6 , \"c\" : 3 } Deleteing A Subset of Instance Data Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(del(.a) | del(.b))' Instance Data After Transform { \"c\" : 3 } Adding A New Value. Instance Data Before Transform { \"a\" : 1 } Transform Snippet - id : snippet type : noop transform : 'jq(.b = 2)' Instance Data After Transform { \"a\" : 1 , \"b\" : 2 } Renaming A Subset of Instance Data Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(.x = .a | del(.a))' Instance Data After Transform { \"b\" : 2 , \"c\" : 3 , \"x\" : 1 }","title":"Transforms"},{"location":"spec/instance-data/transforms/#transforms","text":"Whenever an instance finishes executing a state there is an opportunity to perform a Transform. Usually with a field called transform , but sometimes in other forms. The switch state also has a defaultTransform , for example. All transforms use structured jx , giving you powerful options to enrich, sanitize, or modify instance data. All transforms must produce output that remains valid instance data , otherwise an error will be thrown. // TODO: what error, specifically?","title":"Transforms"},{"location":"spec/instance-data/transforms/#examples","text":"Here are some common use-case helpful examples of transforms.","title":"Examples"},{"location":"spec/instance-data/transforms/#completely-replacing-instance-data","text":"Instance Data Before Transform { \"msg\" : \"Hello, world! } Transform Snippet - id : snippet type : noop transform : x : 5 Instance Data After Transform { \"x\" : 5 }","title":"Completely Replacing Instance Data"},{"location":"spec/instance-data/transforms/#replacing-a-subset-of-instance-data","text":"Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(.a = 5 | .b = 6)' Instance Data After Transform { \"a\" : 5 , \"b\" : 6 , \"c\" : 3 }","title":"Replacing A Subset Of Instance Data"},{"location":"spec/instance-data/transforms/#deleteing-a-subset-of-instance-data","text":"Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(del(.a) | del(.b))' Instance Data After Transform { \"c\" : 3 }","title":"Deleteing A Subset of Instance Data"},{"location":"spec/instance-data/transforms/#adding-a-new-value","text":"Instance Data Before Transform { \"a\" : 1 } Transform Snippet - id : snippet type : noop transform : 'jq(.b = 2)' Instance Data After Transform { \"a\" : 1 , \"b\" : 2 }","title":"Adding A New Value."},{"location":"spec/instance-data/transforms/#renaming-a-subset-of-instance-data","text":"Instance Data Before Transform { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } Transform Snippet - id : snippet type : noop transform : 'jq(.x = .a | del(.a))' Instance Data After Transform { \"b\" : 2 , \"c\" : 3 , \"x\" : 1 }","title":"Renaming A Subset of Instance Data"},{"location":"spec/variables/system/","text":"System Variables In addition to the standard scopes, there is a special system scope. This scope is a utility to make miscellaneous information accessible to an instance. The following special variables exist in the system scope: Key Description instance Returns the instance ID of the running instance. uuid Returns a randomly generated UUID. epoch Returns the current time in unix/epoch format.","title":"System"},{"location":"spec/variables/system/#system-variables","text":"In addition to the standard scopes, there is a special system scope. This scope is a utility to make miscellaneous information accessible to an instance. The following special variables exist in the system scope: Key Description instance Returns the instance ID of the running instance. uuid Returns a randomly generated UUID. epoch Returns the current time in unix/epoch format.","title":"System Variables"},{"location":"spec/variables/variables/","text":"Variables Direktiv can store data separately to instance data . An instance can read and change its instance data at will so you might wonder why this separation needs to exist, but it turns out variables solve a number of problems: Efficiently passing around large datasets or files to actions, especially ones that exceed instance data size limits. Persisting data between instances of a workflow. Sharing data between different workflows. Scopes All variables belong to a scope. The scopes are instance , workflow , and namespace . Instance scoped variables are only accessible to the singular instance that created them. Workflow scoped variables can be used and shared between multiple instances of the same workflow. Namespace scoped variables are available to all instances of all workflows on the namespace. All variables are identified by a name, and each name is unique within its scope. States Two types of states in the workflow spec interact directly with variables: getter and setter . Files Due to size limitations on action inputs and instance data it can sometimes be impossible to pass data to actions without using variables. Actions can interact with variables directly, loading them onto their file-system and sometimes creating/changing variables as well.","title":"Variables"},{"location":"spec/variables/variables/#variables","text":"Direktiv can store data separately to instance data . An instance can read and change its instance data at will so you might wonder why this separation needs to exist, but it turns out variables solve a number of problems: Efficiently passing around large datasets or files to actions, especially ones that exceed instance data size limits. Persisting data between instances of a workflow. Sharing data between different workflows.","title":"Variables"},{"location":"spec/variables/variables/#scopes","text":"All variables belong to a scope. The scopes are instance , workflow , and namespace . Instance scoped variables are only accessible to the singular instance that created them. Workflow scoped variables can be used and shared between multiple instances of the same workflow. Namespace scoped variables are available to all instances of all workflows on the namespace. All variables are identified by a name, and each name is unique within its scope.","title":"Scopes"},{"location":"spec/variables/variables/#states","text":"Two types of states in the workflow spec interact directly with variables: getter and setter .","title":"States"},{"location":"spec/variables/variables/#files","text":"Due to size limitations on action inputs and instance data it can sometimes be impossible to pass data to actions without using variables. Actions can interact with variables directly, loading them onto their file-system and sometimes creating/changing variables as well.","title":"Files"},{"location":"spec/workflow-yaml/action/","text":"Action State - id : a type : action action : function : myfunc input : 'jq(.x)' ActionStateDefinition The action state is the simplest and most common way to call a function or invoke a workflow to act as a subflow. See Actions . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to action . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no aync If set to true , the workflow execution will continue without waiting for the action to return. boolean no action Defines the action to perform. ActionDefinition yes","title":"action"},{"location":"spec/workflow-yaml/action/#action-state","text":"- id : a type : action action : function : myfunc input : 'jq(.x)'","title":"Action State"},{"location":"spec/workflow-yaml/action/#actionstatedefinition","text":"The action state is the simplest and most common way to call a function or invoke a workflow to act as a subflow. See Actions . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to action . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no aync If set to true , the workflow execution will continue without waiting for the action to return. boolean no action Defines the action to perform. ActionDefinition yes","title":"ActionStateDefinition"},{"location":"spec/workflow-yaml/actions/","text":"Actions - id : a type : action action : function : myfunc input : 'jq(.x)' ActionDefinition Parameter Description Type Required function Name of the referenced function. See FunctionDefinition . string yes input Selects or generates the data to send as input to the function. Structured JQ no secrets Defines a list of secrets to temporarily add to the instance data under .secrets , before evaluating the input . []string no retries []RetryPolicyDefinition no files Determines a list of files to load onto the function's file-system from variables. Only valid if the referenced function supports it. []FunctionFileDefinition no RetryPolicyDefinition - id : a type : action action : function : myfunc input : 'jq(.x)' retries : - codes : [ \".*\" ] max_attempts : 3 delay : PT3S multiplier : 1.5 Parameter Description Type Required codes A list of \"glob\" patterns that will be compared to catchable error codes returned by the function to determine if this retry policy applies. []string yes max_attempts Maximum number of retry attempts. If the function has been retried this many times or more when this policy is invoked the retry will be skipped, and instead the error will be escalated to the state's error handling logic. integer yes delay ISO8601 duration string giving a time delay between retry attempts. string no multiplier Value by which the delay is multiplied after each attempt. float no FunctionFileDefinition - id : a type : action action : function : myfunc input : 'jq(.x)' files : - key : VAR_A scope : namespace as : a Some function types support loading variable directly from storage onto their file-systems. This object defines what variable to load and what to save it as. Parameter Description Type Required key Identifies which variable to load into a file. string yes scope Specifies the scope from which to load the variable. VariableScopeDefinition no as Names the resulting file. If left unspecified, the key will be used instead. string no VariableScopeDefinition Every variable exists within a single scope. The scope dictates what can access it and how persistent it is. There are three defined scopes : instance workflow namespace","title":"Actions"},{"location":"spec/workflow-yaml/actions/#actions","text":"- id : a type : action action : function : myfunc input : 'jq(.x)'","title":"Actions"},{"location":"spec/workflow-yaml/actions/#actiondefinition","text":"Parameter Description Type Required function Name of the referenced function. See FunctionDefinition . string yes input Selects or generates the data to send as input to the function. Structured JQ no secrets Defines a list of secrets to temporarily add to the instance data under .secrets , before evaluating the input . []string no retries []RetryPolicyDefinition no files Determines a list of files to load onto the function's file-system from variables. Only valid if the referenced function supports it. []FunctionFileDefinition no","title":"ActionDefinition"},{"location":"spec/workflow-yaml/actions/#retrypolicydefinition","text":"- id : a type : action action : function : myfunc input : 'jq(.x)' retries : - codes : [ \".*\" ] max_attempts : 3 delay : PT3S multiplier : 1.5 Parameter Description Type Required codes A list of \"glob\" patterns that will be compared to catchable error codes returned by the function to determine if this retry policy applies. []string yes max_attempts Maximum number of retry attempts. If the function has been retried this many times or more when this policy is invoked the retry will be skipped, and instead the error will be escalated to the state's error handling logic. integer yes delay ISO8601 duration string giving a time delay between retry attempts. string no multiplier Value by which the delay is multiplied after each attempt. float no","title":"RetryPolicyDefinition"},{"location":"spec/workflow-yaml/actions/#functionfiledefinition","text":"- id : a type : action action : function : myfunc input : 'jq(.x)' files : - key : VAR_A scope : namespace as : a Some function types support loading variable directly from storage onto their file-systems. This object defines what variable to load and what to save it as. Parameter Description Type Required key Identifies which variable to load into a file. string yes scope Specifies the scope from which to load the variable. VariableScopeDefinition no as Names the resulting file. If left unspecified, the key will be used instead. string no","title":"FunctionFileDefinition"},{"location":"spec/workflow-yaml/actions/#variablescopedefinition","text":"Every variable exists within a single scope. The scope dictates what can access it and how persistent it is. There are three defined scopes : instance workflow namespace","title":"VariableScopeDefinition"},{"location":"spec/workflow-yaml/consume-event/","text":"ConsumeEvent State - id : a type : consumeEvent timeout : PT15M event : type : com.github.pull.create context : subject : '123' ConsumeEventStateDefinition To pause the workflow and wait until a CloudEvents event is received before proceeding, the consumeEvent is the simplest state that can be used. It is one of three states that can do so, along with eventsAnd and eventsXor . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to consumeEvent . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no event Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes ConsumeEventDefinition The StartEventDefinition is a structure shared by various event-consuming states. Parameter Description Type Required type Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own type context value. string yes context Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" must be strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. Structured JQ no The received data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following data: CloudEvents Event { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Input Data { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } }","title":"consumeEvent"},{"location":"spec/workflow-yaml/consume-event/#consumeevent-state","text":"- id : a type : consumeEvent timeout : PT15M event : type : com.github.pull.create context : subject : '123'","title":"ConsumeEvent State"},{"location":"spec/workflow-yaml/consume-event/#consumeeventstatedefinition","text":"To pause the workflow and wait until a CloudEvents event is received before proceeding, the consumeEvent is the simplest state that can be used. It is one of three states that can do so, along with eventsAnd and eventsXor . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to consumeEvent . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no event Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"ConsumeEventStateDefinition"},{"location":"spec/workflow-yaml/consume-event/#consumeeventdefinition","text":"The StartEventDefinition is a structure shared by various event-consuming states. Parameter Description Type Required type Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own type context value. string yes context Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" must be strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. Structured JQ no The received data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following data: CloudEvents Event { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Input Data { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } }","title":"ConsumeEventDefinition"},{"location":"spec/workflow-yaml/delay/","text":"Delay State - id : a type : delay duration : PT10S DelayStateDefinition If the workflow needs to pause for a specific length of time, the delay state is usually the simplest way to do that. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to delay . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no duration An ISO8601 duration string. string yes","title":"delay"},{"location":"spec/workflow-yaml/delay/#delay-state","text":"- id : a type : delay duration : PT10S","title":"Delay State"},{"location":"spec/workflow-yaml/delay/#delaystatedefinition","text":"If the workflow needs to pause for a specific length of time, the delay state is usually the simplest way to do that. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to delay . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no duration An ISO8601 duration string. string yes","title":"DelayStateDefinition"},{"location":"spec/workflow-yaml/error/","text":"Error State - id : a type : error error : badinput message : 'Missing or invalid value for required input.' ErrorStateDefinition When workflow logic end up in a failure mode, the error state can be used to mark the instance as failed. This allows the instance to report what went wrong to the caller, which may then be handled or reported appropriately. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to error . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no error A short descriptive error code that can be caught by a parent workflow. string yes message Generates a more detailed message or object that can contain instance data, to provide more information for users. Structured JQ yes","title":"error"},{"location":"spec/workflow-yaml/error/#error-state","text":"- id : a type : error error : badinput message : 'Missing or invalid value for required input.'","title":"Error State"},{"location":"spec/workflow-yaml/error/#errorstatedefinition","text":"When workflow logic end up in a failure mode, the error state can be used to mark the instance as failed. This allows the instance to report what went wrong to the caller, which may then be handled or reported appropriately. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to error . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no error A short descriptive error code that can be caught by a parent workflow. string yes message Generates a more detailed message or object that can contain instance data, to provide more information for users. Structured JQ yes","title":"ErrorStateDefinition"},{"location":"spec/workflow-yaml/errors/","text":"Errors Errors can happen for many reasons. Direktiv allows you to catch and handle these errors using a common field 'catch'. This field takes an array of ErrorCatchDefinition objects, each specifying one or more errors that apply and where to transition to next in order to handle them. When an error is thrown, the list of error catchers is evaluated in order until a match is found. If no match is found, the instance fails. - id : a type : delay duration : PT5M catch : - error : \"direktiv.cancels.timeout.soft\" transition : b ErrorCatchDefinition Parameter Description Type Required error Specified what error code(s) this catcher applies to. This should be a \"glob\" pattern that will be compared to catchable error codes to determine if this retry policy applies. string yes transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no","title":"Errors"},{"location":"spec/workflow-yaml/errors/#errors","text":"Errors can happen for many reasons. Direktiv allows you to catch and handle these errors using a common field 'catch'. This field takes an array of ErrorCatchDefinition objects, each specifying one or more errors that apply and where to transition to next in order to handle them. When an error is thrown, the list of error catchers is evaluated in order until a match is found. If no match is found, the instance fails. - id : a type : delay duration : PT5M catch : - error : \"direktiv.cancels.timeout.soft\" transition : b","title":"Errors"},{"location":"spec/workflow-yaml/errors/#errorcatchdefinition","text":"Parameter Description Type Required error Specified what error code(s) this catcher applies to. This should be a \"glob\" pattern that will be compared to catchable error codes to determine if this retry policy applies. string yes transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no","title":"ErrorCatchDefinition"},{"location":"spec/workflow-yaml/events-and/","text":"EventsAnd State - id : a type : eventsAnd timeout : PT15M events : - type : com.github.pull.create context : subject : '123' - type : com.github.pull.delete context : subject : '123' EventsAndStateDefinition To pause the workflow and wait until multiple CloudEvents events are received before proceeding, the eventsAnd is used. Every listed event must be received for the state to complete. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to eventsAnd . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no events Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"eventsAnd"},{"location":"spec/workflow-yaml/events-and/#eventsand-state","text":"- id : a type : eventsAnd timeout : PT15M events : - type : com.github.pull.create context : subject : '123' - type : com.github.pull.delete context : subject : '123'","title":"EventsAnd State"},{"location":"spec/workflow-yaml/events-and/#eventsandstatedefinition","text":"To pause the workflow and wait until multiple CloudEvents events are received before proceeding, the eventsAnd is used. Every listed event must be received for the state to complete. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to eventsAnd . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no events Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"EventsAndStateDefinition"},{"location":"spec/workflow-yaml/events-xor/","text":"EventsXor State - id : a type : eventsXor timeout : PT15M events : - type : com.github.pull.create context : subject : '123' - type : com.github.pull.delete context : subject : '123' EventsXorStateDefinition To pause the workflow and wait until one of multiple CloudEvents events is received before proceeding, the eventsXor state might be used. Any event match received will cause this state to complete. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to eventsXor . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no events Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"eventsXor"},{"location":"spec/workflow-yaml/events-xor/#eventsxor-state","text":"- id : a type : eventsXor timeout : PT15M events : - type : com.github.pull.create context : subject : '123' - type : com.github.pull.delete context : subject : '123'","title":"EventsXor State"},{"location":"spec/workflow-yaml/events-xor/#eventsxorstatedefinition","text":"To pause the workflow and wait until one of multiple CloudEvents events is received before proceeding, the eventsXor state might be used. Any event match received will cause this state to complete. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to eventsXor . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout An ISO8601 duration string. string no events Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes","title":"EventsXorStateDefinition"},{"location":"spec/workflow-yaml/foreach/","text":"Foreach State - id : a type : foreach array : 'jq([.names[] | {name: .}])' action : function : myfunc input : 'jq(.x)' ForeachStateDefinition The foreach state is a convenient way to divide some data and then perform an action on each element in parallel. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to foreach . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no array Selects or generates an array, from which each element will be separately acted upon. The action.input will be evaluated against each element in this array, rather than the usual instance data. Structured JQ yes action Defines the action to perform. ActionDefinition yes","title":"foreach"},{"location":"spec/workflow-yaml/foreach/#foreach-state","text":"- id : a type : foreach array : 'jq([.names[] | {name: .}])' action : function : myfunc input : 'jq(.x)'","title":"Foreach State"},{"location":"spec/workflow-yaml/foreach/#foreachstatedefinition","text":"The foreach state is a convenient way to divide some data and then perform an action on each element in parallel. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to foreach . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no array Selects or generates an array, from which each element will be separately acted upon. The action.input will be evaluated against each element in this array, rather than the usual instance data. Structured JQ yes action Defines the action to perform. ActionDefinition yes","title":"ForeachStateDefinition"},{"location":"spec/workflow-yaml/functions/","text":"Functions FunctionDefinition Functions refer to anything executable by Direktiv as a unit of logic within a subflow that isn't otherwise part of basic state functionality. Usually this means either a purpose-built container or another workflow executed as a subflow. In some cases functions can be extensively configured, and they are often reused repeatedly within a workflow. To manage the size of Direktiv workflow definitions functions are predefined as much as possible and referenced when called. These are the currently available function types: Functions FunctionDefinition GlobalKnativeFunctionDefinition NamespacedKnativeFunctionDefinition WorkflowKnativeFunctionDefinition ContainerSizeDefinition SubflowFunctionDefinition The following example demonstrate how to define and reference a function within a workflow: Workflow description : | A basic demonstration of functions. functions : - type : knative-workflow id : request image : direktiv/request:latest size : small states : - id : getter type : action action : function : request input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Input {} Output { \"return\" : { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"completed\" : false } } GlobalKnativeFunctionDefinition A knative-global refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service configured to be available \"globally\" (to all namespaces on the Direktiv servers). This function type supports files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to knative-global . string yes id A unique identifier for the function within the workflow definition. string yes service URI to a globally accessible function on the Direktiv servers. string yes NamespacedKnativeFunctionDefinition A knative-namespace refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service configured to be available on the namespace. This function type supports files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to knative-namespace . string yes id A unique identifier for the function within the workflow definition. string yes service URI to a function on the namespace. string yes WorkflowKnativeFunctionDefinition A knative-workflow refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service that Direktiv can create on-demand for the exclusive use by this workflow. Historically this was called reusable , but this keyword has been deprecated. This function type supports files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to knative-workflow . string yes id A unique identifier for the function within the workflow definition. string yes image URI to a knative-workflow compliant container. string yes size Specifies the container size. ContainerSizeDefinition no cmd Custom command to execute within the container. string no scale Used as a suggestion to Direktiv for a minimum number of pods to keep running. Direktiv is not required to adhere to this minimum. The default value is zero, which may result in higher latency if a service goes unused for a while, but saves on resources. integer no ContainerSizeDefinition When functions use containers you may be able to specify what size the container should be. This is done using one of three keywords, each representing a different size preset : small medium large SubflowFunctionDefinition A subflow refers to a function that is actually another workflow. The other workflow is called with some input and its output is returned to this workflow. This function type does not support files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to subflow . string yes id A unique identifier for the function within the workflow definition. string yes workflow URI to a workflow within the same namespace. string yes","title":"Functions"},{"location":"spec/workflow-yaml/functions/#functions","text":"","title":"Functions"},{"location":"spec/workflow-yaml/functions/#functiondefinition","text":"Functions refer to anything executable by Direktiv as a unit of logic within a subflow that isn't otherwise part of basic state functionality. Usually this means either a purpose-built container or another workflow executed as a subflow. In some cases functions can be extensively configured, and they are often reused repeatedly within a workflow. To manage the size of Direktiv workflow definitions functions are predefined as much as possible and referenced when called. These are the currently available function types: Functions FunctionDefinition GlobalKnativeFunctionDefinition NamespacedKnativeFunctionDefinition WorkflowKnativeFunctionDefinition ContainerSizeDefinition SubflowFunctionDefinition The following example demonstrate how to define and reference a function within a workflow: Workflow description : | A basic demonstration of functions. functions : - type : knative-workflow id : request image : direktiv/request:latest size : small states : - id : getter type : action action : function : request input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Input {} Output { \"return\" : { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"completed\" : false } }","title":"FunctionDefinition"},{"location":"spec/workflow-yaml/functions/#globalknativefunctiondefinition","text":"A knative-global refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service configured to be available \"globally\" (to all namespaces on the Direktiv servers). This function type supports files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to knative-global . string yes id A unique identifier for the function within the workflow definition. string yes service URI to a globally accessible function on the Direktiv servers. string yes","title":"GlobalKnativeFunctionDefinition"},{"location":"spec/workflow-yaml/functions/#namespacedknativefunctiondefinition","text":"A knative-namespace refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service configured to be available on the namespace. This function type supports files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to knative-namespace . string yes id A unique identifier for the function within the workflow definition. string yes service URI to a function on the namespace. string yes","title":"NamespacedKnativeFunctionDefinition"},{"location":"spec/workflow-yaml/functions/#workflowknativefunctiondefinition","text":"A knative-workflow refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service that Direktiv can create on-demand for the exclusive use by this workflow. Historically this was called reusable , but this keyword has been deprecated. This function type supports files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to knative-workflow . string yes id A unique identifier for the function within the workflow definition. string yes image URI to a knative-workflow compliant container. string yes size Specifies the container size. ContainerSizeDefinition no cmd Custom command to execute within the container. string no scale Used as a suggestion to Direktiv for a minimum number of pods to keep running. Direktiv is not required to adhere to this minimum. The default value is zero, which may result in higher latency if a service goes unused for a while, but saves on resources. integer no","title":"WorkflowKnativeFunctionDefinition"},{"location":"spec/workflow-yaml/functions/#containersizedefinition","text":"When functions use containers you may be able to specify what size the container should be. This is done using one of three keywords, each representing a different size preset : small medium large","title":"ContainerSizeDefinition"},{"location":"spec/workflow-yaml/functions/#subflowfunctiondefinition","text":"A subflow refers to a function that is actually another workflow. The other workflow is called with some input and its output is returned to this workflow. This function type does not support files . Parameter Description Type Required type Identifies which kind of FunctionDefinition is being used. In this case it must be set to subflow . string yes id A unique identifier for the function within the workflow definition. string yes workflow URI to a workflow within the same namespace. string yes","title":"SubflowFunctionDefinition"},{"location":"spec/workflow-yaml/generate-event/","text":"GenerateEvent State - id : a type : generateEvent event : type : myeventtype source : myeventsource data : 'jq(.data)' datacontenttype : application/json GenerateEventStateDefinition Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to generateEvent . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no delay ISO8601 duration string defining how long to hold the event before broadcasting it. string no event Defines the event to generate. GenerateEventDefinition yes GenerateEventDefinition Parameter Description Type Required type Sets the CloudEvents event type. string yes source Sets the CloudEvents event source. string yes data Defines the content of the payload for the CloudEvents event. Structured JQ no datacontenttype An RFC2046 string specifying the payload content type. string no context If defined, must evaluate to an object of key-value pairs. These will be used to define CloudEvents event context data. Structured JQ no","title":"generateEvent"},{"location":"spec/workflow-yaml/generate-event/#generateevent-state","text":"- id : a type : generateEvent event : type : myeventtype source : myeventsource data : 'jq(.data)' datacontenttype : application/json","title":"GenerateEvent State"},{"location":"spec/workflow-yaml/generate-event/#generateeventstatedefinition","text":"Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to generateEvent . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no delay ISO8601 duration string defining how long to hold the event before broadcasting it. string no event Defines the event to generate. GenerateEventDefinition yes","title":"GenerateEventStateDefinition"},{"location":"spec/workflow-yaml/generate-event/#generateeventdefinition","text":"Parameter Description Type Required type Sets the CloudEvents event type. string yes source Sets the CloudEvents event source. string yes data Defines the content of the payload for the CloudEvents event. Structured JQ no datacontenttype An RFC2046 string specifying the payload content type. string no context If defined, must evaluate to an object of key-value pairs. These will be used to define CloudEvents event context data. Structured JQ no","title":"GenerateEventDefinition"},{"location":"spec/workflow-yaml/getter/","text":"Getter State - id : a type : getter variables : - key : x scope : workflow GetterStateDefinition To load variables, use the getter state. See Variables . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to getter . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no variables Defines variables to load. []VariableGetterDefinition yes VariableGetterDefinition Parameter Description Type Required key Variable name. string yes scope Selects the scope to which the variable belongs. If undefined, defaults to instance . See Variables . yes no as Names the resulting data. If left unspecified, the key will be used instead. string no","title":"getter"},{"location":"spec/workflow-yaml/getter/#getter-state","text":"- id : a type : getter variables : - key : x scope : workflow","title":"Getter State"},{"location":"spec/workflow-yaml/getter/#getterstatedefinition","text":"To load variables, use the getter state. See Variables . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to getter . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no variables Defines variables to load. []VariableGetterDefinition yes","title":"GetterStateDefinition"},{"location":"spec/workflow-yaml/getter/#variablegetterdefinition","text":"Parameter Description Type Required key Variable name. string yes scope Selects the scope to which the variable belongs. If undefined, defaults to instance . See Variables . yes no as Names the resulting data. If left unspecified, the key will be used instead. string no","title":"VariableGetterDefinition"},{"location":"spec/workflow-yaml/logging/","text":"Logging All states can write to instance logs via a common field log . This field uses structured jx to support querying instance data and inserting it into the logs. - id : a type : noop log : 'Hello, world!'","title":"Logging"},{"location":"spec/workflow-yaml/logging/#logging","text":"All states can write to instance logs via a common field log . This field uses structured jx to support querying instance data and inserting it into the logs. - id : a type : noop log : 'Hello, world!'","title":"Logging"},{"location":"spec/workflow-yaml/metadata/","text":"Instance Metadata Instance metadata is a way to monitor an instance. An instance can update its metadata at any time, replacing it with whatever information it needs to expose via the API. All states can write to instance metadata via a common field metadata . This field uses structured jx to support querying instance data and inserting it into the metadata. - id : a type : noop metadata : 'jq(.)'","title":"Instance Metadata"},{"location":"spec/workflow-yaml/metadata/#instance-metadata","text":"Instance metadata is a way to monitor an instance. An instance can update its metadata at any time, replacing it with whatever information it needs to expose via the API. All states can write to instance metadata via a common field metadata . This field uses structured jx to support querying instance data and inserting it into the metadata. - id : a type : noop metadata : 'jq(.)'","title":"Instance Metadata"},{"location":"spec/workflow-yaml/noop/","text":"Noop State - id : a type : noop NoopStateDefinition Often workflows need to do something that can be achieved using logic built into most state types. For example, to log something, or to transform the instance data by running a jq command. In many cases this can be done by an existing state within the workflow, but sometimes it's necessary to split it out into a separate state. The noop state exists for this purpose. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to noop . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no","title":"noop"},{"location":"spec/workflow-yaml/noop/#noop-state","text":"- id : a type : noop","title":"Noop State"},{"location":"spec/workflow-yaml/noop/#noopstatedefinition","text":"Often workflows need to do something that can be achieved using logic built into most state types. For example, to log something, or to transform the instance data by running a jq command. In many cases this can be done by an existing state within the workflow, but sometimes it's necessary to split it out into a separate state. The noop state exists for this purpose. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to noop . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no","title":"NoopStateDefinition"},{"location":"spec/workflow-yaml/parallel/","text":"Parallel State - id : a type : parallel mode : and actions : - function : myfunc input : 'jq(.x)' - function : myfunc input : 'jq(.y)' ParallelStateDefinition The parallel state is an alternative to the action state when a workflow can perform multiple threads of logic simultaneously. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to parallel . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no mode If defined, must be either and or or . The default is and . This setting determines whether the state is considered successfully completed only if all threads have returned without error ( and ) or as soon as any single thread returns without error ( or ). string no actions Defines the action to perform. []ActionDefinition yes","title":"parallel"},{"location":"spec/workflow-yaml/parallel/#parallel-state","text":"- id : a type : parallel mode : and actions : - function : myfunc input : 'jq(.x)' - function : myfunc input : 'jq(.y)'","title":"Parallel State"},{"location":"spec/workflow-yaml/parallel/#parallelstatedefinition","text":"The parallel state is an alternative to the action state when a workflow can perform multiple threads of logic simultaneously. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to parallel . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no timeout ISO8601 duration string to set a non-default timeout. string no mode If defined, must be either and or or . The default is and . This setting determines whether the state is considered successfully completed only if all threads have returned without error ( and ) or as soon as any single thread returns without error ( or ). string no actions Defines the action to perform. []ActionDefinition yes","title":"ParallelStateDefinition"},{"location":"spec/workflow-yaml/setter/","text":"Setter State - id : a type : setter variables : - key : x scope : workflow mimeType : text/plain value : 'jq(.x)' SetterStateDefinition To create or change variables, use the setter state. See Variables . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to setter . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no variables Defines variables to push. []VariableSetterDefinition yes VariableSetterDefinition Parameter Description Type Required key Variable name. string yes scope Selects the scope to which the variable belongs. If undefined, defaults to instance . See Variables . yes no mimeType Store a MIME type with the variable. If left undefined, it will default to application/json . Two specific MIME types cause this state to behave differently: text/plain and application/octet-stream . If the value evaluates to a JSON string the MIME type is text/plain , that string will be stored in plaintext (without JSON quotes and escapes). If if the value is a JSON string containing base64 encoded data and the MIME type is application/octet-stream , the base64 data will be decoded and stored as binary data. string no value Select or generate the data to store. Structured JQ yes","title":"setter"},{"location":"spec/workflow-yaml/setter/#setter-state","text":"- id : a type : setter variables : - key : x scope : workflow mimeType : text/plain value : 'jq(.x)'","title":"Setter State"},{"location":"spec/workflow-yaml/setter/#setterstatedefinition","text":"To create or change variables, use the setter state. See Variables . Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to setter . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no variables Defines variables to push. []VariableSetterDefinition yes","title":"SetterStateDefinition"},{"location":"spec/workflow-yaml/setter/#variablesetterdefinition","text":"Parameter Description Type Required key Variable name. string yes scope Selects the scope to which the variable belongs. If undefined, defaults to instance . See Variables . yes no mimeType Store a MIME type with the variable. If left undefined, it will default to application/json . Two specific MIME types cause this state to behave differently: text/plain and application/octet-stream . If the value evaluates to a JSON string the MIME type is text/plain , that string will be stored in plaintext (without JSON quotes and escapes). If if the value is a JSON string containing base64 encoded data and the MIME type is application/octet-stream , the base64 data will be decoded and stored as binary data. string no value Select or generate the data to store. Structured JQ yes","title":"VariableSetterDefinition"},{"location":"spec/workflow-yaml/starts/","text":"Starts StartDefinition A StartDefinition may be defined using one of the following, depending on the desired behaviour: Starts StartDefinition DefaultStartDefinition ScheduledStartDefinition EventStartDefinition EventsXorStartDefinition EventsAndStartDefinition StartEventDefinition If omitted from the workflow definition the DefaultStartDefinition will be used, which means the workflow will only be executed when called. DefaultStartDefinition The default start definition is used for workflows that should only execute when called. This means subflows, workflows triggered by scripts, and workflows triggered manually by humans. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to default . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no ScheduledStartDefinition The scheduled start definition is used for workflows that should execute at regularly defined times. Scheduled workflow can be manually triggered for convenience and testing. They never have input data, so accurate testing should use {} as input. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to scheduled . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no cron Defines the time(s) when the workflow should execute using a CRON expression. string yes Example (snippet) start : type : scheduled cron : '* * * * *' # Trigger a new instance every minute. EventStartDefinition The event start definition is used for workflows that should be executed whenever a relevant CloudEvents event is received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to event . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no event Defines what events can trigger the workflow. StartEventDefinition yes EventsXorStartDefinition The event \"xor\" start definition is used for workflows that should be executed whenever one of multiple possible CloudEvents events is received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to eventsXor . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no events Defines what events can trigger the workflow. []StartEventDefinition yes EventsAndStartDefinition The event \"and\" start definition is used for workflows that should be executed when multiple matching CloudEvents events are received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to eventsAnd . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no lifespan An ISO8601 duration string. Sets the maximum duration an event can be stored before being discarded while waiting for other events. string no correlate CloudEvents event context keys can must exist on every event and have matching values to be grouped together. []string no events Defines what events can trigger the workflow. []StartEventDefinition yes StartEventDefinition The StartEventDefinition is a structure shared by various start definitions involving events. Parameter Description Type Required type Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own type context value. string yes filters Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" are strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. object no The input data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following input data in a workflow triggered by a single event: CloudEvents Event { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Input Data { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } }","title":"Triggers / Starts"},{"location":"spec/workflow-yaml/starts/#starts","text":"","title":"Starts"},{"location":"spec/workflow-yaml/starts/#startdefinition","text":"A StartDefinition may be defined using one of the following, depending on the desired behaviour: Starts StartDefinition DefaultStartDefinition ScheduledStartDefinition EventStartDefinition EventsXorStartDefinition EventsAndStartDefinition StartEventDefinition If omitted from the workflow definition the DefaultStartDefinition will be used, which means the workflow will only be executed when called.","title":"StartDefinition"},{"location":"spec/workflow-yaml/starts/#defaultstartdefinition","text":"The default start definition is used for workflows that should only execute when called. This means subflows, workflows triggered by scripts, and workflows triggered manually by humans. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to default . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no","title":"DefaultStartDefinition"},{"location":"spec/workflow-yaml/starts/#scheduledstartdefinition","text":"The scheduled start definition is used for workflows that should execute at regularly defined times. Scheduled workflow can be manually triggered for convenience and testing. They never have input data, so accurate testing should use {} as input. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to scheduled . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no cron Defines the time(s) when the workflow should execute using a CRON expression. string yes Example (snippet) start : type : scheduled cron : '* * * * *' # Trigger a new instance every minute.","title":"ScheduledStartDefinition"},{"location":"spec/workflow-yaml/starts/#eventstartdefinition","text":"The event start definition is used for workflows that should be executed whenever a relevant CloudEvents event is received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to event . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no event Defines what events can trigger the workflow. StartEventDefinition yes","title":"EventStartDefinition"},{"location":"spec/workflow-yaml/starts/#eventsxorstartdefinition","text":"The event \"xor\" start definition is used for workflows that should be executed whenever one of multiple possible CloudEvents events is received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to eventsXor . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no events Defines what events can trigger the workflow. []StartEventDefinition yes","title":"EventsXorStartDefinition"},{"location":"spec/workflow-yaml/starts/#eventsandstartdefinition","text":"The event \"and\" start definition is used for workflows that should be executed when multiple matching CloudEvents events are received. See StartEventDefinition for an explanation of the input data of event-triggered workflows. Parameter Description Type Required type Identifies which kind of StartDefinition is being used. In this case it must be set to eventsAnd . string yes state References a defined state's id . This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the states list. string no lifespan An ISO8601 duration string. Sets the maximum duration an event can be stored before being discarded while waiting for other events. string no correlate CloudEvents event context keys can must exist on every event and have matching values to be grouped together. []string no events Defines what events can trigger the workflow. []StartEventDefinition yes","title":"EventsAndStartDefinition"},{"location":"spec/workflow-yaml/starts/#starteventdefinition","text":"The StartEventDefinition is a structure shared by various start definitions involving events. Parameter Description Type Required type Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own type context value. string yes filters Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" are strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. object no The input data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following input data in a workflow triggered by a single event: CloudEvents Event { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Input Data { \"com.github.pull.create\" : { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } }","title":"StartEventDefinition"},{"location":"spec/workflow-yaml/states/","text":"States StateDefinition A StateDefinition may be defined using one of the following, depending on the desired behaviour: action consumeEvent delay error eventsAnd eventsXor foreach generateEvent getter noop parallel setter switch validate","title":"States"},{"location":"spec/workflow-yaml/states/#states","text":"","title":"States"},{"location":"spec/workflow-yaml/states/#statedefinition","text":"A StateDefinition may be defined using one of the following, depending on the desired behaviour: action consumeEvent delay error eventsAnd eventsXor foreach generateEvent getter noop parallel setter switch validate","title":"StateDefinition"},{"location":"spec/workflow-yaml/switch/","text":"Switch State - id : a type : switch defaultTransform : 'jq(del(.x))' defaultTransition : b conditions : - condition : 'jq(.y == true)' transform : 'jq(.x)' transition : c - condition : 'jq(.z == true)' transform : 'jq(.x)' transition : d SwitchStateDefinition To change the behaviour of a workflow based on the instance data, use a switch state. This state does nothing except choose between any number of different possible state transitions. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to switch . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no defaultTransform If defined, modifies the instance's data upon completing the state logic. But only if none of the conditions are met. See StateTransforms . Structured JQ no defaultTransition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. But only if none of the conditions are met. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no conditions List of conditions, which are evaluated in-order until a match is found. []SwitchConditionDefinition yes SwitchConditionDefinition Parameter Description Type Required condition Selects or generates the data used to determine if condition is met. The condition is considered met if the result is anything other than null , false , {} , [] , \"\" , or 0 . Structured JQ yes transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, matching this condition terminates the workflow. string no","title":"switch"},{"location":"spec/workflow-yaml/switch/#switch-state","text":"- id : a type : switch defaultTransform : 'jq(del(.x))' defaultTransition : b conditions : - condition : 'jq(.y == true)' transform : 'jq(.x)' transition : c - condition : 'jq(.z == true)' transform : 'jq(.x)' transition : d","title":"Switch State"},{"location":"spec/workflow-yaml/switch/#switchstatedefinition","text":"To change the behaviour of a workflow based on the instance data, use a switch state. This state does nothing except choose between any number of different possible state transitions. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to switch . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no defaultTransform If defined, modifies the instance's data upon completing the state logic. But only if none of the conditions are met. See StateTransforms . Structured JQ no defaultTransition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. But only if none of the conditions are met. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no conditions List of conditions, which are evaluated in-order until a match is found. []SwitchConditionDefinition yes","title":"SwitchStateDefinition"},{"location":"spec/workflow-yaml/switch/#switchconditiondefinition","text":"Parameter Description Type Required condition Selects or generates the data used to determine if condition is met. The condition is considered met if the result is anything other than null , false , {} , [] , \"\" , or 0 . Structured JQ yes transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, matching this condition terminates the workflow. string no","title":"SwitchConditionDefinition"},{"location":"spec/workflow-yaml/timeouts/","text":"Timeouts TimeoutsDefinition In addition to any timeouts applied on a state-by-state basis, every workflow has two global timeouts that begin ticking from the moment the workflow starts. This is where you can configure these timeouts differently to their defaults. Parameter Description Type Required interrupt An ISO8601 duration string. Sets the time to wait before throwing a catchable direktiv.cancels.timeout.soft error. Consider this a soft timeout. string no kill An ISO8601 duration string. Sets the time to wait before throwing an uncatchable direktiv.cancels.timeout.hard error. This is a hard timeout. string no","title":"Timeouts"},{"location":"spec/workflow-yaml/timeouts/#timeouts","text":"","title":"Timeouts"},{"location":"spec/workflow-yaml/timeouts/#timeoutsdefinition","text":"In addition to any timeouts applied on a state-by-state basis, every workflow has two global timeouts that begin ticking from the moment the workflow starts. This is where you can configure these timeouts differently to their defaults. Parameter Description Type Required interrupt An ISO8601 duration string. Sets the time to wait before throwing a catchable direktiv.cancels.timeout.soft error. Consider this a soft timeout. string no kill An ISO8601 duration string. Sets the time to wait before throwing an uncatchable direktiv.cancels.timeout.hard error. This is a hard timeout. string no","title":"TimeoutsDefinition"},{"location":"spec/workflow-yaml/validate/","text":"Validate State - id : a type : validate schema : title : Files type : object properties : file : type : string format : data-url title : Single file ValidateStateDefinition Since workflows receive external input it may be necessary to check that instance data is valid. The validate state exists for this purpose. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to validate . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no subject Selects or generates the data that will be compared to the schema . If undefined, it will be default to 'jq(.)' . Structured JQ no schema A YAMLified representation of a JSON Schema that defines whether the subject is considered valid. object yes","title":"validate"},{"location":"spec/workflow-yaml/validate/#validate-state","text":"- id : a type : validate schema : title : Files type : object properties : file : type : string format : data-url title : Single file","title":"Validate State"},{"location":"spec/workflow-yaml/validate/#validatestatedefinition","text":"Since workflows receive external input it may be necessary to check that instance data is valid. The validate state exists for this purpose. Parameter Description Type Required type Identifies which kind of StateDefinition is being used. In this case it must be set to validate . string yes id An identifier unique within the workflow to this one state. string yes log If defined, the workflow will generate a log when it commences this state. See StateLogging . Structured JQ no metadata If defined, updates the instance's metadata. See InstanceMetadata . Structured JQ no transform If defined, modifies the instance's data upon completing the state logic. See StateTransforms . Structured JQ no transition Identifies which state to transition to next, referring to the next state's unique id . If undefined, this state terminates the workflow. string no catch Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no subject Selects or generates the data that will be compared to the schema . If undefined, it will be default to 'jq(.)' . Structured JQ no schema A YAMLified representation of a JSON Schema that defines whether the subject is considered valid. object yes","title":"ValidateStateDefinition"},{"location":"spec/workflow-yaml/workflow/","text":"Workflow Definition Direktiv Workflow Definition This document describes the rules for Direktiv workflow definition files. These files are written in YAML and dictate the behaviour of a workflow running on Direktiv. Workflow description : | A simple \"Hello, world\" demonstration. states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' Input {} Output { \"msg\" : \"Hello, world!\" } Workflows have inputs and outputs, usually in JSON. Where examples appear in this document they will often be accompanied by inputs and outputs as seen above. WorkflowDefinition This is the top-level structure of a Direktiv workflow definition. All workflows must have one. Parameter Description Type Required url Link to further information. string no description Short description of the workflow. string no functions List of function definitions for use by function-based states . []FunctionDefinition no start Configuration for how the workflow should start. StartDefinition no states List of all possible workflow states. []StateDefinition yes timeouts Configuration of workflow-level timeouts. TimeoutsDefinition no","title":"Workflow"},{"location":"spec/workflow-yaml/workflow/#workflow-definition","text":"","title":"Workflow Definition"},{"location":"spec/workflow-yaml/workflow/#direktiv-workflow-definition","text":"This document describes the rules for Direktiv workflow definition files. These files are written in YAML and dictate the behaviour of a workflow running on Direktiv. Workflow description : | A simple \"Hello, world\" demonstration. states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' Input {} Output { \"msg\" : \"Hello, world!\" } Workflows have inputs and outputs, usually in JSON. Where examples appear in this document they will often be accompanied by inputs and outputs as seen above.","title":"Direktiv Workflow Definition"},{"location":"spec/workflow-yaml/workflow/#workflowdefinition","text":"This is the top-level structure of a Direktiv workflow definition. All workflows must have one. Parameter Description Type Required url Link to further information. string no description Short description of the workflow. string no functions List of function definitions for use by function-based states . []FunctionDefinition no start Configuration for how the workflow should start. StartDefinition no states List of all possible workflow states. []StateDefinition yes timeouts Configuration of workflow-level timeouts. TimeoutsDefinition no","title":"WorkflowDefinition"}]}