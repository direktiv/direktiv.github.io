{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quickstart","text":"<p>Direktiv is a cloud-agnostic, serverless flow engine that capitalizes on microservices, containers and custom code to create efficient processes. Using Kubernetes and Knative under the hood, this platform empowers you with the scalability of modern cloud computing. Direktiv provides a set of YAML definitions to define how the data should be processed, allowing developers to quickly and efficiently write their own business logic. </p> <p>Integrate</p> <p>Direktiv's event-driven system and container approach make it simple to link different systems. As a broker for multiple backends, Direktiv provides error handling, retries, logging and tracing capabilities to ensure seamless integration as well as greater visibility into the processes.</p> <p>Orchestrate</p> <p>Direktiv simplifies the orchestration of APIs to create higher-level services that can be used by any organization, either externally or internally. This is made possible through a YAML-based flow description system which makes it easy and fast to customize flows as well as expand capabilities.</p> <p>Automate</p> <p>Streamline repetitive duties within your team or organization by e.g. moving scripts and playbooks into an easily accessible platform. Whether you need to automate Continuous Integration, Infrastructure Management, Onboarding tasks or something else that is currently done manually - automating these processes can be a major advantage. Use a single platform which multiple users in your team have access to.</p>"},{"location":"#see-also","title":"See Also","text":"<ul> <li>The Getting Started Guide.</li> <li>The direktiv.io website.</li> <li>The direktiv.io repository.</li> <li>The Godoc library documentation.</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"environment/cli/","title":"Direktiv CLI","text":"<p>Although developing flows with the web UI is easy, a command line tool can be used to make local flow development faster and more convenient. Direktiv's cli <code>direktivctl</code> is used for pushing and executing flows remotely. This enables the developer to stay in his development environment, e.g. Visual Studio Code. </p>"},{"location":"environment/cli/#installing","title":"Installing","text":"<p>The direktivctl is available for Linux, Windows, and Mac platforms and is distributed as a <code>tar.gz</code> file with every new release of Direktiv. The asset can be downloaded and unpacked to get the <code>direktiv-sync</code> binary.</p> <ul> <li>Linux</li> <li>Mac</li> <li>Mac ARM</li> <li>Windows</li> </ul> Linux Installation Example<pre><code>curl -L https://github.com/direktiv/direktiv/releases/latest/download/direktivctl_amd64.tar.gz | tar -xz &amp;&amp; \\\n sudo mv direktivctl /usr/local/bin\n</code></pre>"},{"location":"environment/cli/#setting-up-a-namespace","title":"Setting up a Namespace","text":"<p>Working with the CLI assumes that you create a directory which is mirroring a namespace in Direktiv. This directory can be empty or can be a populated from a <code>github clone</code> command. The only requirement is that the namespace already exists. </p>"},{"location":"environment/cli/#configuring-the-cli-tool","title":"Configuring the CLI-Tool","text":"<p>Connection information (address, token, and namespace) can be provided as arguments, but it is easier to use a configutration file. You can use the direktivctl tool to populate a cinfiguration like this: </p> <pre><code># Use direktivctl to initialize a configuration file\ndirektivctl profile add -n mynamespace -p profilename  -a http://myserver.com\n</code></pre> <p>This creates a new file, if it does not exist already, at <code>$HOME//.direktiv/profiles.yaml</code> and adds the new profile. </p> Example profiles.yaml<pre><code>profilename:\n    address: http://myserver.com\n    insecure: true\n    namespace: mynamespace\n    token: \"\"\n</code></pre> <p>The token value, provided with the <code>-t</code> flag is optional if authentication is required via a token. After the creation of a configuration you must use the profile flag to to use a specific profile. All the profile variables can be overwritten by environment variables:</p> Environment Variables<pre><code>DIREKTIV_ADDRESS\nDIREKTIV_TOKEN\nDIREKTIV_NAMESPACE\n</code></pre>"},{"location":"environment/cli/#working-directory","title":"Working Directory","text":"<p>Each project folder requires a <code>.direktivignore</code> file in the root folder. This is required to calculate the path on the server. This files can be empty but works exactly like a <code>.gitignore</code> file to ignore certain files to be pushed to the server. A project structure can look like this:</p> Project Structure<pre><code>/myfolder\n .direktivignore\n myflow.yaml\n /services\n  myservice.yaml\n</code></pre>"},{"location":"environment/cli/#pushing-and-executing","title":"Pushing and Executing","text":"<p>After setup there are two commands available. The <code>push</code> command pushes a flow to Direktiv but does not execute it. This command works recursively e.g. <code>direktivctl workflows push .</code>. The <code>exec</code> command uploads and executes the flow. During execution the logs are printed to <code>stdout</code>.</p> CLI Examples<pre><code>direktivctl filesystem push myproject/ -p test\ndirektivctl filesystem push . -p test\ndirektivctl filesystem exec myflow.yaml -p test\n</code></pre>"},{"location":"environment/env/","title":"Dev Enironment","text":"<p>To improve function and flow development it is recommended to setup a local development environment. This section explains how to setup the development environment. Details about developing custom functions is described in this section.</p>"},{"location":"environment/env/#running-direktiv","title":"Running Direktiv","text":"<p>As mentioned in the \"Getting Started\" guide there are two ways to set up a local development environment besides setting up a full Kubernetes with Direktiv. There is a Docker image and a multipass configuration. This section describes how they can be configured and used. </p>"},{"location":"environment/env/#docker","title":"Docker","text":"<p>Setting up a development Direktiv instance on a local machine is very simple. Assuming docker is installed, run the following command:</p> Starting Direktiv<pre><code>docker run --privileged -p 8080:80 -p 31212:31212 -d --name direktiv direktiv/direktiv-kube\n</code></pre> <p>This command starts direktiv as container 'direktiv'. The initial boot-time will take a few minutes. The progress can be followed with:</p> Direktiv Docker Logs<pre><code>docker logs direktiv -f\n</code></pre> <p>Once all pods reach 'running' status, direktiv is ready and the URL <code>http://localhost:8080/api/namespaces</code> is accessible.</p> <p>The database uses a persistent volume so the data stored should survive restarts with 'docker stop/start'. The port-forward of 31212 is the included docker registry.</p> <p>If there is a requirement to execute <code>kubectl</code> commands the container can be accessed via <code>docker exec</code>. For convenience there is a <code>kubectl</code> shortcut <code>kc</code> and command completion is installed as well.</p> Accessing Shell<pre><code>docker exec -it direktiv /bin/bash\n\nkc get pods -A\n</code></pre>"},{"location":"environment/env/#enabling-proxy","title":"Enabling Proxy","text":"<p>The following settings can be passed as environmental variables to use this image in environments with a proxy. This has to be done on the first startup. </p> Proxy Settings<pre><code>docker run --privileged -p 8080:80 -p 31212:31212 --env HTTPS_PROXY=\"http://&lt;proxy-address&gt;:443\" --env NO_PROXY=\"127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,172.16.0.0/12,.svc,.default,.local,.cluster.local,localhost,.direktiv-services-direktiv\" -d --name direktiv -ti direktiv/direktiv-kube\n</code></pre>"},{"location":"environment/env/#api-key","title":"API Key","text":"<p>If the instance requires an API key it can be added with an environment variable as well.</p> Enable API Key<pre><code>docker run --privileged -p 8080:80 -p 31212:31212 -e APIKEY=123 -d --name direktiv -ti direktiv/direktiv-kube\n</code></pre>"},{"location":"environment/env/#enable-eventing","title":"Enable Eventing","text":"<p>Knative Eventing is disabled by default in the Docker image but can be easily enabled during startup with <code>EVENTING=true</code> as environemtn variable.</p> Enable Eventing<pre><code>docker run --privileged -p 8080:80 -p 31212:31212 -e EVENTING=true -d --name direktiv -ti direktiv/direktiv-kube\n</code></pre>"},{"location":"environment/env/#debug","title":"Debug","text":"<p>If there are issues starting nested Kubernetes it is possible to see the K3S debug logs on startup with the variable <code>DEBUG</code>.</p> Enable Eventing<pre><code>docker run --privileged -p 8080:80 -p 31212:31212 -e DEBUG=true direktiv/direktiv-kube\n</code></pre> <p>There is always the option to use the multipass configuration if the Docker image does not work.</p>"},{"location":"environment/env/#multipass","title":"Multipass","text":"<p>Multipass creates a virtual machine with Direktiv pre-configured. The configuration is different from the Docker image but all features are available to that approach as well. The cloud-init script will do the configuration during first boot and takes a few minutes to complete. Eventing is anebled by default.</p> Start Multipass Instance<pre><code>multipass launch --cpus 4 --disk 20G --memory 6G --name direktiv --cloud-init https://raw.githubusercontent.com/direktiv/direktiv/main/scripts/build/docker/all/multipass/init.yaml\n</code></pre> <p>After startup the machine can be access with a simple command. For convenience there is a <code>kubectl</code> shortcut and code completion installed. </p> Accessing Shell<pre><code>multipass exec direktiv -- /bin/bash\n</code></pre> <p>Warning</p> <p>multipass does not work in a VPN. The VPN needs to be turned off for this example installation.</p> <p>If the installation is not successful there is a cloud-init log available on the virtual machine <code>/var/log/cloud-init-output.log</code> to check the logs.</p> <p>The instance has an accessible network configured and the IP is accessible from the host. After startup the UI can be accessed with first IP listed under <code>IPv4</code>. </p> Display IP<pre><code>multipass info direktiv\n\nName:           direktiv\nState:          Running\nIPv4:           10.100.91.90\n                10.42.0.0\n                10.42.0.1\nRelease:        Ubuntu 22.04.2 LTS\nImage hash:     345fbbb6ec82 (Ubuntu 22.04 LTS)\nCPU(s):         4\nLoad:           0.53 0.51 0.25\nDisk usage:     5.0GiB out of 9.5GiB\nMemory usage:   2.1GiB out of 3.8GiB\n</code></pre>"},{"location":"environment/env/#enabling-proxy_1","title":"Enabling Proxy","text":"<p>Enabling a proxy has to be done by changing the cloud-init file manually. The first step is to download the file from Github with e.g. curl.</p> Download Cloud-Init<pre><code>curl https://raw.githubusercontent.com/direktiv/direktiv/main/build/docker/all/multipass/init.yaml &gt; myinit.yaml\n</code></pre> <p>The proxy configuration values need to be added as a file under <code>/env</code>. The following snippet is an example for such a configuration.</p> Proxy YAML<pre><code>...\nwrite_files:\n- encoding: b64\n  content: SCRIPT\n  path: /home/install.sh\n  permissions: '0755'\n- path: /env\n  content: |\n    HTTP_PROXY=http://10.100.6.16:3128\n    HTTPS_PROXY=http://10.100.6.16:3128\n    NO_PROXY=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,172.16.0.0/12,.svc,.default,.local,.cluster.local,localhost,.direktiv-services-direktiv\n  append: true\n</code></pre> <p>After changing the file multipass requires this file instead of the default one.</p> Custom Cloud-Init<pre><code>multipass launch --cpus 4 --disk 20G --memory 6G --name direktiv --cloud-init myinit.yaml\n</code></pre> <p>Warning</p> <p>Multipass can not read and use files in the <code>/tmp</code> directory. Do not place the custom init file in <code>/tmp</code>.</p>"},{"location":"environment/env/#enabling-api-key","title":"Enabling API Key","text":"<p>To add an API key it is also required to create a custom cloud-init configuration like the proxy does. The required variables is <code>APIKEY</code>.</p> API Key YAML<pre><code>...\nwrite_files:\n- encoding: b64\n  content: SCRIPT\n  path: /home/install.sh\n  permissions: '0755'\n- path: /env\n  content: |\n    APIKEY=123\n  append: true\n</code></pre>"},{"location":"environment/env/#deleting-multipass-intance","title":"Deleting Multipass Intance","text":"<p>To remove the instance the <code>delete</code> and <code>purge</code> command is required.</p> Delete Multipass Instance<pre><code>multipass delete direktiv\nmultipass purge\n</code></pre>"},{"location":"environment/env/#docker-registry","title":"Docker registry","text":"<p>Direktiv pulls containers from a registry and use them as functions in flows. For development purposes the direktiv docker container as well as the multipass instances come with a registry installed. It is accessible via :31212. The value for  is either the localhost for Docker or the IP of the multipass instance. <p>To test the local repository the golang example from direktiv-apps can be used:</p> <pre><code>git clone https://github.com/direktiv-apps/bash.git\n\ndocker build bash/ -t &lt;IP&gt;:31212/bash\n\ndocker push &lt;IP&gt;:31212/bash\n\n# confirm upload\ncurl http://&lt;IP&gt;:31212/v2/_catalog\n</code></pre> <p>Multipass Instances</p> <p>Docker doesn not support pushing to <code>http</code> registries. Therefore it has to be added as an insecure registry to the docker service. Add something like the following to <code>/etc/docker/daemon.json</code> and restart the Docker service.</p> <pre><code>{\n  \"insecure-registries\" : [\"10.100.91.188:31212\"]\n}\n</code></pre>"},{"location":"environment/env/#testing-configuration","title":"Testing Configuration","text":"<p>To test if everything is working this example creates a namespace and a flow and executes it. The value for <code>&lt;ADDRESS&gt;</code> has to be replaced with either <code>localhost:8080</code> for Docker or the IP of the multipass instance. </p> Testing Installation<pre><code># create namespace 'test'\ncurl -X PUT http://&lt;ADDRESS&gt;/api/namespaces/test\n\n# create the workflow file\ncat &gt; helloworld.yml &lt;&lt;- EOF\ndirektiv_api: workflow/v1\nfunctions:\n- id: get\n  type: reusable\n  image: gcr.io/direktiv/functions/bash:1.0\nstates:\n- id: getter\n  type: action\n  action:\n    function: get\n    input:\n      commands:\n      - command: echo Hello\nEOF\n\n# upload flow\ncurl -X PUT  --data-binary @helloworld.yml \"http://&lt;ADDRESS&gt;/api/namespaces/test/tree/test?op=create-workflow\"\n\n# execute flow (initial call will be slightly slower than subsequent calls)\ncurl \"http://&lt;ADDRESS&gt;/api/namespaces/test/tree/test?op=wait\"\n</code></pre>"},{"location":"environment/git/","title":"Git Mirrors","text":"<p>Direktiv supports configuring a namespace to use a git repository as the source of its contents and configuration.</p>"},{"location":"environment/git/#setting-up-a-git-mirror","title":"Setting Up A Git Mirror","text":"<p>The following arguments can be supplied when creating a mirror as a namespace:</p> <ul> <li><code>namespace</code></li> <li><code>url</code></li> <li><code>ref</code></li> <li>Auth:</li> <li>None</li> <li>Git access token:<ul> <li><code>access_token</code></li> </ul> </li> <li>SSH<ul> <li><code>private_key</code></li> <li><code>public_key</code></li> <li><code>passphrase</code></li> </ul> </li> </ul> <p>The following arguments are considered sensitive, and will never be returned via the API, except in a redacted form: <code>access_token</code>, <code>private_key</code>, <code>passphrase</code>. </p>"},{"location":"environment/git/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"environment/git/#none","title":"None","text":"<p>If a repository is public some providers will allow clients to access it without any form of authentication. SSH requires authentication, that's why the only way to use a zero-auth configuration is with git over HTTP. When using a zero-auth configuration very few fields are needed to set up a mirror. The following is an example:</p> <pre><code>namespace: apps-svc\nurl: https://github.com/direktiv/apps-svc.git\nref: main\n</code></pre>"},{"location":"environment/git/#access-token","title":"Access Token","text":"<p>Github allows users to create personal access tokens (link). These can be used as a form of authentication. These can only be used with git over HTTP. </p> <pre><code>namespace: apps-svc\nurl: https://github.com/direktiv/apps-svc.git\nref: main\naccess_token: my-access-token...\n</code></pre>"},{"location":"environment/git/#ssh","title":"SSH","text":"<p>If the two previous approaches aren't sufficient for your needs, the reliable way of reaching a repository under any and all circumstances is with git over SSH. For this to work, you will need to provide both a public and a private key, and optionally a passphrase used to decrypt the private key, if it is password protected. You will also need to configure the remote server to recognize your SSH key. </p> <pre><code>namespace: apps-svc\nurl: https://github.com/direktiv/apps-svc.git\nref: main\npublic_key: my-public-key...\nprivate_key: my-private-key...\npassphrase: my-passphrase...\n</code></pre>"},{"location":"environment/git/#activities","title":"Activities","text":"<p>All Direktiv mirror operations are encapsulated within an \"activity\". This serves as a way of organizing the logic and logs of an operation in a convenient way. Cloning a remote repository can take time, and it's possible that the operation will fail or produce unexpected results. Check the list of recent activities and their logs to learn more about any issues you encounter.</p>"},{"location":"environment/git/#modifying-the-configuration-of-a-local-mirror","title":"Modifying the Configuration of a Local Mirror","text":"<p>For various reasons you may need to update the settings of a mirror. Whether it's to change which branch or commit it's referencing, or to update your credentials. All of this is supported. For convenience, Direktiv will only apply changes to settings you ask it to, anything else will remain unchanged. This means for example that you can swap from branch <code>v1.0.x</code> to <code>v1.1.x</code> without resupplying your SSH keys if you want to.</p>"},{"location":"environment/git/#direktiv-projects","title":"Direktiv Projects","text":"<p>When a filetree is intended for mirroring by Direktiv, we call it a Direktiv Project. What follows is an explanation of the rules for Direktiv Projects.</p>"},{"location":"environment/git/#file-system","title":"File-System","text":"<p>When performing a sync, Direktiv will produce a copy of every directory and file from the git project into your Direktiv file-system, with some exceptions and caveats. Other kinds of file-system objects such as symlinks are skipped.</p> <ol> <li> <p>Files and directories will only be copied if their names are valid within Direktiv. At this time, valid names must conform to the following regex pattern: <code>(([a-zA-Z][a-zA-Z0-9_\\-\\.]*[a-zA-Z0-9])|([a-zA-Z]))</code>. Also, names beginning with <code>.</code> (hidden files) are also excluded. This helps to reduce pollution in your file-tree by avoiding common unhelpful git project contents such as <code>.gitignore</code> and <code>.git/</code>.</p> </li> <li> <p>If a <code>/.direktivignore</code> file exists, Direktiv will use this file in exactly the same way <code>.gitignore</code> files are used by <code>git</code> to more precisely control what is and isn't copied.</p> </li> <li> <p>If a file ending with the <code>.yaml</code> or <code>.yml</code> extension is unambiguously identified as a Direktiv resource definition (by including a top level field <code>direktiv_api</code>), it will not be copied. Instead, such a file will be processed and its definitions added to the namespace. More on this later.</p> </li> <li> <p>If a file ending with the <code>.yaml</code> or <code>.yml</code> extension is ambiguous, Direktiv will attempt to parse it as a workflow. If it succeeds without error, it will be added to the file-system as a Direktiv Workflow. Otherwise, it will be added to the file-system as a generic yaml file. \u00a0</p> </li> </ol> <p>Deprecated: the correct way to add a workflow is to unambiguously define one. This ambiguousness step is only included short-term to maintain backwards compatibility.</p> <p>The above steps only determine whether a file will be evaluated as a potential Direktiv resource definition or copied into the file-tree. Exclusion for any of the above reasons won't break direct references to such files in Direktiv resource definitions. This means, for example, that you can define a filter (more on this later) that references a Javascript file, while still excluding the javascript file from appearing in the file-tree using <code>.direktivignore</code>.</p>"},{"location":"environment/git/#workflows","title":"Workflows","text":"<p>As mentioned in the File-System section above, workflows will be loaded from any <code>.yaml</code> or <code>.yml</code> file that parses without error. This processing of ambiguous files may in rare circumstances lead to Direktiv creating workflows from yaml files that had nothing to do with Direktiv. This behaviour is therefore deprecated. The correct way to define a workflow is to include the <code>direktiv_api</code> field, set to <code>workflow/v1</code>. For example, a file <code>/hello.yaml</code> containing the following contents defines a simple helloworld workflow of the same name and location in the file-tree:</p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: hello\n\u00a0 type: noop\n\u00a0 transform: 'jq({ msg: \"Hello, world!\" })'\n</code></pre> <p>When workflows are unambiguously defined this way, they do not need to parse successfully as valid workflows to be copied into the file-system.</p>"},{"location":"environment/git/#namespace-services","title":"Namespace Services","text":"<p>Namespace services can be defined within a Direktiv Project so they are automatically created during mirroring. This is done using appropriate Direktiv resource definition files. For example, a <code>/services.yaml</code> file containing the following contents defines a simple http requester service named <code>requester</code>. The <code>services</code> field is an array, meaning that these files may define one or more services each. The user may divide or combine their service definitions amongst as many files as they like.</p> <pre><code>direktiv_api: services/v1\nservices:\n- name: requester\n\u00a0 image: direktiv/request:v4\n</code></pre>"},{"location":"environment/git/#namespaceservicesdefinition","title":"NamespaceServicesDefinition","text":"Parameter Description Type Required <code>direktiv_api</code> Set it to 'services/v1'. string yes <code>services</code> []NamespaceServiceDefinition yes"},{"location":"environment/git/#namespaceservicedefinition","title":"NamespaceServiceDefinition","text":"Parameter Description Type Required <code>name</code> The name for this service, which should be unique amonst all services within the namespace / project. string yes <code>image</code> The image to use for this service. string yes"},{"location":"environment/git/#namespace-variables","title":"Namespace Variables","text":"<p>If a file is named like <code>var.*</code> then it is treated as a namespace variable. For example, <code>var.style.css</code> will become a namespace variable called <code>style.css</code>.</p>"},{"location":"environment/git/#workflow-variables","title":"Workflow Variables","text":"<p>If a file is named such that it is prefixed with the full name of another file that was evaluated to be a workflow (followed by a <code>.</code>), then the file is treated as a workflow variable attached to that workflow. For example, if you have a workflow <code>/hello.yaml</code>, and another file called <code>/hello.yaml.page.html</code>, then the <code>/hello.yaml</code> workflow will gain a workflow variable called <code>page.html</code>.</p>"},{"location":"environment/git/#unsyncables","title":"Unsyncables","text":""},{"location":"environment/git/#container-registries","title":"Container Registries","text":"<p>Configuring container registries requires a password, which is sensitive information that should not be committed to a git project. We therefore do not support any way to sync these at the moment.</p>"},{"location":"environment/git/#secrets","title":"Secrets","text":"<p>Sensitive information should not be committed to git projects. We therefore do not support any way to sync these at the moment.</p> <p>As a small help to users, when workflows are created referencing secrets that haven't been defined, uninitialized secrets are created. These don't contain any helpful information, but they populate the list of secrets within the namespace for easy adjustment.</p>"},{"location":"environment/git/#errors","title":"Errors","text":"<p>The syncing process adopts a fault-tolerant approach to errors. This means that as often as possible, detected errors should be logged as such, but not prevent the greater sync operation from succeeding. Users are strongly encouraged to check the logs of their sync operation to confirm that there were no unexpected problems with their git repository.</p>"},{"location":"environment/vscode/","title":"Direktiv Workflow Validation in Visual Studio Code","text":"<p>For efficient management of Direktiv workflows outside our UI, we offer a schema validation using Direktiv's official schema and the Red Hat YAML extension in Visual Studio Code. This provides automatic validation and intellisense. Follow the guide below to set it up.</p>"},{"location":"environment/vscode/#prerequisites","title":"Prerequisites","text":"<ul> <li>Visual Studio Code: Ensure that you have Visual Studio Code installed on your system.</li> <li>Internet Connection: This is essential for fetching the Direktiv schema.</li> </ul>"},{"location":"environment/vscode/#step-by-step-guide","title":"Step-by-step Guide","text":""},{"location":"environment/vscode/#1-install-the-red-hat-yaml-extension","title":"1. Install the Red Hat YAML Extension","text":"<p>Begin by opening Visual Studio Code. Once it's up, navigate to the Extensions view by clicking on the <code>Extensions</code> icon on the Activity Bar situated on the side of the window. In the search bar, type <code>YAML</code> and then proceed to install the extension offered by Red Hat.</p>"},{"location":"environment/vscode/#2-modify-the-settings","title":"2. Modify the Settings","text":"<p>After the extension is installed, you'll need to add the Direktiv schema configuration. Here's how:</p> <ul> <li>Click on the gear icon found in the lower left corner of the Visual Studio Code window.</li> <li>From the dropdown that appears, select <code>Settings (JSON)</code>.</li> </ul> <p>Within the settings JSON file that opens up, merge or add the following configuration:</p> <pre><code>\"yaml.schemas\": {\n  \"https://raw.githubusercontent.com/direktiv/direktiv/main/resources/direktiv.schema.json\": \".wf.yaml\"\n}\n</code></pre>"},{"location":"environment/vscode/#3-using-the-schema-validation","title":"3. Using the Schema Validation","text":"<p>From now on, each time you open a <code>.wf.yaml</code> file in Visual Studio Code, the program will apply the schema validations grounded on the Direktiv schema automatically. If the schema is not automatically applied or to make use of the schema validation when using simply <code>.yaml</code> as extension:</p> <ul> <li>Direct your attention to the lower right corner of the status bar, where you should find an option labeled \"Select Language (YAML)\" or something similar.</li> <li>Click on this option and from the dropdown list that appears, choose <code>direktiv-workflow</code>. This action will apply the schema.</li> </ul>"},{"location":"events/","title":"Events","text":"<p>Direktiv utilizes the HTTP Protocol Binding for CloudEvents, and offers two distinct ways to produce and consume events. The easiest approach is by using Direktiv's API directly in order to route your desired events. However, if you require more flexibility, Knative can assist with a more powerful and dynamic approach when it comes to eventing. What type of integration is ideal depends on which use cases are meant to be addressed by Direktiv. Independent from this integration approach system internal events within Direktiv are always supported.</p> <p></p>"},{"location":"events/#event-api","title":"Event API","text":"<p>The event API provides direct access to Direktiv's eventing system. The general API path is <code>/api/namespaces/{namespace}/broadcast</code>. Following the cloud-event specification events can be send to Direktiv in three different formats. </p> <p>Event ID</p> <pre><code>The specification requires an event ID. Direktiv generates a random ID if not provided by the client.\n</code></pre>"},{"location":"events/#binary-content-mode","title":"Binary Content Mode","text":"<p>Th binary content mode uses headers to describe the event metadata with a \"ce-\" prefix and allows for efficient transfer and without transcoding effort. The header \"content-type\" must be set to the content-type of the body of the event.</p> <pre><code>POST /api/namespaces/{namespace}/broadcast HTTP/1.1\nHost: direktiv.io\nce-specversion: 1.0\nce-type: com.example.event\nce-id: 1234-1234-1234\nce-source: /mycontext/subcontext\nContent-Type: application/json; charset=utf-8\n\n{\n   \"hello\": \"world\"\n}\n</code></pre>"},{"location":"events/#structured-content-mode","title":"Structured Content Mode","text":"<p>In structured mode the whole cloudevent is in the payload. The content-type header needs to be set to \"application/cloudevents+json\". </p> <pre><code>{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull_request.opened\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"A234-1234-1234\",\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n}\n</code></pre>"},{"location":"events/#batched-content-mode","title":"Batched Content Mode","text":"<p>In batch mode multiple events can be send to direktiv. The content-type has to be \"application/cloudevents-batch+json\" and the body is a JSON array of cloud events.</p> <pre><code>[\n    {\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull_request.opened\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"C234-1234-1234\",\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n    },\n    {\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull_request.opened\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"B234-1234-1234\",\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n    }\n]\n</code></pre>"},{"location":"events/#other-data","title":"Other Data","text":"<p>If unknown data arrives at the API endpoint Direktiv does not drop the data but converts it into a cloud event. The value for <code>type</code> is set to <code>noncompliant</code> and <code>source</code> to <code>unknown</code>. The payload of the original requets will be base64 encoded and added as <code>data_base64</code> to the event. If the content type can be guessed or is provided in the header it will be part of the cloud event as well.</p> <pre><code>{\n  \"noncompliant\": {\n    \"data_base64\": \"aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1kUXc0dzlXZ1hjUQ==\",\n    \"datacontenttype\": \"text/plain\",\n    \"error\": \"unknown Message encoding\",\n    \"id\": \"60290f4b-3971-411a-b824-73b60eb8b72d\",\n    \"source\": \"unknown\",\n    \"specversion\": \"1.0\",\n    \"type\": \"noncompliant\"\n  }\n}\n</code></pre>"},{"location":"events/#events-in-flows","title":"Events in Flows","text":"<p>Events in a Direktiv flow can be a start condition and initiate a flow or a workflow can wait for an event during flow execution. Direktiv can wait for single events or on AND and OR combinations of events. </p>"},{"location":"events/#event-start-type-example","title":"Event Start Type Example","text":"<p>The following is an example of a simple start condition for a Direktiv flow. A start condition requires the type and additional <code>context</code> values can be provided. If context values are defined the cloud event has to match the context attribute. For matching glob values can be used. </p> <pre><code>start:\n  type: event\n  state: helloworld\n  event:\n    type: io.direktiv.myevent\n    context:\n      myvalue: my*\nstates:\n- id: helloworld\n  type: noop\n  log: jq(.)\n</code></pre> <p>The above example flow would trigger if the following cloud event would arrive:</p> <pre><code>{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"io.direktiv.myevent\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"myvalue\": \"mydata\"\n}\n</code></pre> <p>Because the defintion uses a glob pattern valid values fo <code>myvalue</code> could be everything starting with <code>my</code>. If the attribute is missing or does not start with <code>my</code> the workflow would not trigger.</p>"},{"location":"events/#events-in-flow-example","title":"Events in Flow Example","text":"<p>Waiting for events within a flow is similar to a start definition except the <code>context</code> can be used to \"link\" flows to each other via context attributes in events. Additionally it can handle timeouts if an event has not been received within a certain time. </p> <pre><code>states:\n\n- id: wait-event\n  type: consumeEvent\n  timeout: PT1M\n  event:\n    type: io.direktiv.myevent\n    context:\n      customer: jq(.customer)\n  catch: \n  - error: \"direktiv.cancels.timeout.soft\"\n    transition: timedout\n\n- id: timedout\n  type: noop\n  log: this event timed out\n</code></pre>"},{"location":"events/cloud/amazon/","title":"Amazon EventBridge","text":"<p>We're going to go through the process of setting up a rule for 'ec2' to send events to our Direktiv service. This explains how to create an api destination and transform the aws event input to cloud event format. </p> <p>Note: the below tutorial assumes that the user has already created the IAM role for the EventBridge API integration as described in Amazon EventBridge User Guide</p> <p>From the Role create above - keep the Role Arn details as it is needed in the final step. A screenshot is shown below:</p> <p> </p>"},{"location":"events/cloud/amazon/#create-a-rule","title":"Create a rule","text":"<pre><code>aws events put-rule --name \"direktiv-rule\" --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"]}\"\n</code></pre> <p>The following output should appear (make sure you hold onto the ARN as it is used further down to attach a target to the rule):</p> <pre><code>{\n    \"RuleArn\": \"&lt;RULE_ARN&gt;\"\n}\n</code></pre>"},{"location":"events/cloud/amazon/#create-a-connection","title":"Create a connection","text":"<p>After creating an Authorization token from the Direktiv interface, create the connection using the token as follow:</p> <pre><code>aws events create-connection --name direktiv-connection --authorization-type API_KEY --auth-parameters \"{\\\"ApiKeyAuthParameters\\\": {\\\"ApiKeyName\\\":\\\"direktiv-token\\\", \\\"ApiKeyValue\\\":\\\"&lt;DIREKTIV_TOKEN&gt;\\\"}}\"\n</code></pre> <p>Upon creating the connection the following output from the CLI should appear.</p> <pre><code>{\n    \"ConnectionArn\": \"&lt;CONNECTION_ARN&gt;\",\n    \"ConnectionState\": \"AUTHORIZED\",\n    \"CreationTime\": \"2021-08-04T05:28:24+00:00\",\n    \"LastModifiedTime\": \"2021-08-04T05:28:24+00:00\"\n}\n</code></pre> <p>We will need to use the connection arn in the next command.</p>"},{"location":"events/cloud/amazon/#create-an-api-destination","title":"Create an Api-Destination","text":"<pre><code>aws events create-api-destination --name direktiv-api --connection-arn \"&lt;CONNECTION_ARN&gt;\" --invocation-endpoint https://&lt;DIREKTIV_URL&gt;/api/namespaces/&lt;NAMESPACE&gt;/broadcast --http-method POST\n</code></pre> <p>The output should resemble this:</p> <pre><code>{\n    \"ApiDestinationArn\": \"&lt;API_ARN&gt;\",\n    \"ApiDestinationState\": \"ACTIVE\",\n    \"CreationTime\": \"2021-08-04T05:30:50+00:00\",\n    \"LastModifiedTime\": \"2021-08-04T05:30:50+00:00\"\n}\n</code></pre>"},{"location":"events/cloud/amazon/#put-targets-to-the-aws-eventbridge-rule","title":"Put Targets to the AWS EventBridge Rule","text":"<p>Adding the targets to the EventBridge rule also requires us to define an Input Path and Input Template.</p> <pre><code>aws events put-targets --rule direktiv-rule --targets '[ { \"Id\": \"direktiv-api\", \"RoleArn\": \"&lt;ROLE_ARN&gt;\", \"Arn\": \"&lt;API_ARN&gt;\", \"InputTransformer\": { \"InputPathsMap\": { \"id\":\"$.id\", \"source\":\"$.source\", \"state\":\"$.detail.state\", \"subject\":\"$.source\", \"time\":\"$.time\", \"type\":\"$.detail-type\" }, \"InputTemplate\": \" {\\\"specversion\\\":\\\"1.0\\\", \\\"id\\\":&lt;id&gt;, \\\"source\\\":&lt;source&gt;, \\\"type\\\":&lt;type&gt;, \\\"subject\\\":&lt;subject&gt;, \\\"time\\\":&lt;time&gt;, \\\"data\\\":&lt;aws.events.event.json&gt;}\" } } ]'\n</code></pre> <p>The output (if successful) below:</p> <pre><code>{\n    \"FailedEntryCount\": 0,\n    \"FailedEntries\": []\n}\n</code></pre>"},{"location":"events/cloud/amazon/#input-path-map-example","title":"Input Path Map Example","text":"<p>Input Path Map captures the EventBridge event so we can easily filter into a cloud event to send to Direktiv</p> <pre><code>    {\n      \"id\": \"$.id\",\n      \"source\": \"$.source\",\n      \"subject\": \"$.source\",\n      \"time\": \"$.time\",\n      \"type\": \"$.detail-type\"\n    }\n</code></pre>"},{"location":"events/cloud/amazon/#input-template-example","title":"Input Template Example","text":"<p>The Input Template allows you to spec out what you want the JSON to look like parsing the values from the input path.</p> <pre><code>     {\n       \"specversion\":\"1.0\", \n       \"id\": \"&lt;id&gt;\", \n       \"source\": \"&lt;source&gt;\", \n       \"type\": \"&lt;type&gt;\", \n       \"subject\": \"&lt;subject&gt;\", \n       \"time\": \"&lt;time&gt;\",\n       \"data\": &lt;aws.events.event.json&gt;\n     }\n</code></pre> <p>So now when you change the state of an instance on EC2 a workflow will be triggered on Direktiv if it is listening to 'aws.ec2'. For reference, when an AWS event is generated, the default event structure (for an EC2 status change as an example) is shown below:</p> <pre><code>{\n  \"version\": \"0\",\n  \"id\": \"7bf73129-1428-4cd3-a780-95db273d1602\",\n  \"detail-type\": \"EC2 Instance State-change Notification\",\n  \"source\": \"aws.ec2\",\n  \"account\": \"123456789012\",\n  \"time\": \"2015-11-11T21:29:54Z\",\n  \"region\": \"us-east-1\",\n  \"resources\": [\"arn:aws:ec2:us-east-1:123456789012:instance/i-abcd1111\"],\n  \"detail\": {\n    \"instance-id\": \"i-abcd1111\",\n    \"state\": \"pending\"\n  }\n}\n</code></pre> <p>The CloudEvent received by Direktiv after the transformation is shown below:</p> <pre><code>{\n  \"specversion\": \"1.0\",\n  \"id\": \"f694954a-c307-368c-005a-d4279473e156\",\n  \"source\": \"aws.ec2\",\n  \"type\": \"EC2 Instance State-change Notification\",\n  \"subject\": \"aws.ec2\",\n  \"time\": \"2022-05-04T01:57:06Z\",\n  \"data\": {\n    \"version\": \"0\",\n    \"id\": \"f694954a-c307-368c-005a-d4279473e156\",\n    \"detail-type\": \"EC2 Instance State-change Notification\",\n    \"source\": \"aws.ec2\",\n    \"account\": \"338328518639\",\n    \"time\": \"2022-05-04T01:57:06Z\",\n    \"region\": \"ap-southeast-2\",\n    \"resources\": [\n      \"arn:aws:ec2:ap-southeast-2:338328518639:instance/i-0cf5a83f321fbed55\"\n    ],\n    \"detail\": {\n      \"instance-id\": \"i-0cf5a83f321fbed55\",\n      \"state\": \"pending\"\n    }\n  }\n}\n</code></pre>"},{"location":"events/cloud/amazon/#testing","title":"Testing","text":"<p>Create this simple workflow that gets executed when it receives a cloud-event of a specific type.</p> <pre><code>id: listen-for-event\ndescription: Listen to a custom cloud event\nstart:\n  type: event\n  state: helloworld\n  event:\n    type: \"EC2 Instance State-change Notification\"\nstates:\n  - id: helloworld\n    type: noop\n    transform: 'jq({ result: . })'\n</code></pre>"},{"location":"events/cloud/azure/","title":"Azure EventGrid","text":"<p>Goes through the process of setting up a storage account that listens for events on upload. Being that Azure uses native cloud events we won't need to run anything apart from the initial setup.</p>"},{"location":"events/cloud/azure/#setup","title":"Setup","text":"<p>To follow along you will need access to the resource group you wish to setup in. This example includes the creation of a storage account but an existing one can be used.</p>"},{"location":"events/cloud/azure/#create-a-storage-account-container","title":"Create a Storage Account &amp; Container","text":"<p>Create a storage account under a resource group</p> <pre><code>az storage account create --name direktivstoragetest --resource-group trentis-direktiv-apps-test\n</code></pre> <p>Create a container under that storage account. You can get the --account-key by doing the following</p> <pre><code>az storage account keys list --account-name direktivstoragetest\n</code></pre> <pre><code>az storage container create  --name direktiv-container --account-name direktivstorage100  --account-key ACCOUNT-KEY\n</code></pre>"},{"location":"events/cloud/azure/#create-an-event-subscription","title":"Create an Event Subscription","text":"<p>webhook-request-callback sends option request</p> <p>Create an event subscription attached to the storage account.</p> <pre><code>az eventgrid event-subscription create \\\n--name direktiv-event \\\n--source-resource-id=$(az storage account show --name direktivstoragetest --resource-group trentis-direktiv-apps-test --query id --output tsv) \\\n--endpoint=https://playground.direktiv.io/api/namespaces/trent/event \\\n--endpoint-type=webhook --event-delivery-schema cloudeventschemav1_0 \\\n--delivery-attribute-mapping Authorization Static \"Bearer ACCESS_TOKEN\" true\n</code></pre>"},{"location":"events/cloud/azure/#testing","title":"Testing","text":"<pre><code>id: listen-for-azure-event\ndescription: Listen to a custom cloud event\nstart:\n  type: event\n  state: helloworld\n  event:\n    type: Microsoft.Storage.BlobCreated\nstates:\n  - id: helloworld\n    type: noop\n    transform: 'jq({ result: . })'\n</code></pre>"},{"location":"events/cloud/gcp/","title":"Google Cloud EventArc","text":"<p>To send Google Cloud Audit log events to EventArc you will need a container service running on Cloud Run. We provide you a container located at 'gcr.io/direktiv/event-arc-listener'. That container's job is to read the cloud event it receives and relays it back to a Direktiv service.</p>"},{"location":"events/cloud/gcp/#setup","title":"Setup","text":""},{"location":"events/cloud/gcp/#setup-audit-logs-to-be-managed","title":"Setup Audit Logs to be managed","text":"<p>Read policy file to /tmp/policy.yaml <pre><code>gcloud projects get-iam-policy PROJECT_ID &gt; /tmp/policy.yaml\n</code></pre></p> <p>Add the follow section above 'bindings:'</p> <pre><code>auditConfigs:\n- auditLogConfigs:\n  - logType: ADMIN_READ\n  - logType: DATA_WRITE\n  - logType: DATA_READ\n  service: storage.googleapis.com\n</code></pre> <p>Set the new policy</p> <pre><code>gcloud projects set-iam-policy PROJECT_ID /tmp/policy.yaml\n</code></pre>"},{"location":"events/cloud/gcp/#setup-configs-for-gcloud-to-run-properly","title":"Setup Configs for Gcloud to run properly","text":"<pre><code>gcloud config set project PROJECT_ID\ngcloud config set run/region us-central1\ngcloud config set run/platform managed\ngcloud config set eventarc/location us-central1\n</code></pre>"},{"location":"events/cloud/gcp/#configure-the-cloud-run-service","title":"Configure the Cloud Run Service","text":""},{"location":"events/cloud/gcp/#using-authentication","title":"Using Authentication","text":"<p>Create a secret to use as the DIREKTIV_TOKEN </p> <pre><code>gcloud secrets create DIREKTIV_TOKEN \\\n    --replication-policy=\"automatic\"\n</code></pre> <p>Create a file that contains the ACCESS_TOKEN generated from Direktiv that has 'namespaceEvent' privilege. I chose to create the file as '/tmp/ac'.</p> <p>Add the secret data to the secret <pre><code>gcloud secrets versions add DIREKTIV_TOKEN --data-file=/tmp/ac\n</code></pre></p>"},{"location":"events/cloud/gcp/#create-a-cloud-run-service","title":"Create a Cloud Run Service","text":"<p>Deploy the container to your environment</p> <pre><code>gcloud beta run deploy event-arc-listener --image gcr.io/direktiv/event-arc-listener \\\n    --update-secrets=DIREKTIV_TOKEN=DIREKTIV_TOKEN:1 \\\n    --set-env-vars \"DIREKTIV_NAMESPACE=trent\" \\\n    --set-env-vars \"DIREKTIV_ENDPOINT=https://playground.direktiv.io\" \\\n    --allow-unauthenticated\n</code></pre>"},{"location":"events/cloud/gcp/#create-a-trigger-for-the-cloud-run-service","title":"Create a Trigger for the Cloud Run Service","text":"<p>Create a new trigger to listen for storage events on this project. <pre><code>gcloud eventarc triggers create storage-upload-trigger \\\n    --destination-run-service=event-arc-listener  \\\n    --destination-run-region=us-central1 \\\n    --event-filters=\"type=google.cloud.audit.log.v1.written\" \\\n    --event-filters=\"serviceName=storage.googleapis.com\" \\\n    --event-filters=\"methodName=storage.objects.create\" \\\n    --service-account=SERVICE_ACCOUNT_ADDRESS\n</code></pre></p> <p>Note: Keep in mind this trigger will take 10 minutes to work</p>"},{"location":"events/cloud/gcp/#testing","title":"Testing","text":"<p>Create this simple workflow that gets executed when it receives a cloud-event of a specific type.</p> <pre><code>id: listen-for-event\ndescription: Listen to a custom cloud event\nstart:\n  type: event\n  state: helloworld\n  event:\n    type: google.cloud.audit.log.v1.written\nstates:\n  - id: helloworld\n    type: noop\n    transform: 'jq({ result: . })'\n</code></pre>"},{"location":"events/knative/example/","title":"Kafka Example","text":"<p>This example uses Kafka as Knative event broker and event source and sink as well. After receiving a message from Kafka, Knative forwards it to Direktiv which subsequently initiates a flow and publishes an event back to Knative which will broker the event to a receive topic in Kafka. To run this example the following steps are required:</p> <ul> <li>Installing Kafka</li> <li>Installing Knative with Kafka</li> <li>Configure Kafka Source</li> <li>Configure Direktiv Source</li> <li>Configuring Kafka Sink</li> <li>Flow</li> </ul> <p>Example Versions</p> <p>The version numbers in this example might have changed over time. Please make sure to update them accordingly if required.</p> <p></p>"},{"location":"events/knative/example/#installing-kafka","title":"Installing Kafka","text":"<p>To enable Knative Eventing in a production environment, Knative requires the installation of an event broker. By setting up triggers and subscriptions, Knative brokers like RabbitMQ or Kafka can build an event mesh architecture. Here we will be using Kafka and the Strimzi Operator for the installation. This is a two-step process, installing the Kafka operator and creating the Kafka cluster itself.</p> Installing Strimzi Operator<pre><code>kubectl create namespace kafka\nkubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka\nkubectl wait --for=condition=ready pod -l name=strimzi-cluster-operator -n kafka --timeout=300s\n</code></pre> <p>Kafka Installation</p> <p>The Kafka installation instructions provided here is just an example and can not be used as-is in production environments. Please go to https://strimzi.io for full documentation. </p> <p>After the operator is running the following command enables KRaft for the operator. This allows an installation without Zookeeper and should simplify this setup.</p> Enable KRaft<pre><code>kubectl -n kafka set env deployment/strimzi-cluster-operator STRIMZI_FEATURE_GATES=+UseKRaft\nkubectl wait --for=condition=ready pod -l name=strimzi-cluster-operator -n kafka --timeout=300s\n</code></pre> <p>The following command will create the actual Kafka cluster which will be used in this example. </p> Create Kafka Instance<pre><code>cat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: my-cluster\n  namespace: kafka\nspec:\n  kafka:\n    version: 3.4.0\n    replicas: 1\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n    config:\n      offsets.topic.replication.factor: 1\n      transaction.state.log.replication.factor: 1\n      transaction.state.log.min.isr: 1\n      default.replication.factor: 1\n      min.insync.replicas: 1\n      inter.broker.protocol.version: \"3.4\"\n    storage:\n      type: ephemeral\n  zookeeper:\n    replicas: 1\n    storage:\n      type: ephemeral\nEOF\n</code></pre>"},{"location":"events/knative/example/#knative-with-kafka","title":"Knative with Kafka","text":"<p>To use Kafka as the underlying mechanism for message brokering Knative needs to be configured during installation. The YAML here will create Knative Eventing instance with the required settings.</p> Knative Eventing with Kafka<pre><code>cat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-eventing\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  config:\n    config-br-default-channel:\n      channel-template-spec: |\n        apiVersion: messaging.knative.dev/v1beta1\n        kind: KafkaChannel\n        spec:\n          numPartitions: 6\n          replicationFactor: 1\n    default-ch-webhook:\n      default-ch-config: |\n        clusterDefault:\n          apiVersion: messaging.knative.dev/v1beta1\n          kind: KafkaChannel\n          spec:\n            numPartitions: 10\n            replicationFactor: 1\nEOF\n</code></pre> <p>This installation requires the Knative Kafka controller and data plane as well. This can be installed with two <code>kubectl</code> commands.</p> Knative Kafka Dependencies<pre><code>kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-controller.yaml\nsleep 3\nkubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-broker.yaml\n</code></pre> <p>The last step is to create the actual broker. The following two commands are creating the broker configuration and the broker using the configuration.</p> Kafka Configuration<pre><code>cat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-broker-config\n  namespace: knative-eventing\ndata:\n  default.topic.partitions: \"10\"\n  default.topic.replication.factor: \"1\"\n  bootstrap.servers: \"my-cluster-kafka-bootstrap.kafka:9092\"\nEOF\n</code></pre> Creating Broker<pre><code>cat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    eventing.knative.dev/broker.class: Kafka\n  name: default\n  namespace: knative-eventing\nspec:\n  config:\n    apiVersion: v1\n    kind: ConfigMap\n    name: kafka-broker-config\n    namespace: knative-eventing\nEOF\n</code></pre> <p>The Kafka broker is now up and running. The setup can be tested with <code>kubectl</code>.</p> Working Knative Eventing<pre><code>kubectl get brokers.eventing.knative.dev\nNAME      URL                                                                              AGE   READY   REASON\ndefault   http://kafka-broker-ingress.knative-eventing.svc.cluster.local/default/default   16m   True    \n</code></pre>"},{"location":"events/knative/example/#configuring-kafka-source","title":"Configuring Kafka Source","text":"<p>Kafka will be an event source and a sink in this example. Therefore we need two channels. One channel sending messages and a second channeld to receive the outcome of the whole message process. </p> Sender &amp; Receiver Topics<pre><code>cat &lt;&lt;-EOF | kubectl apply -f -\n---\n  apiVersion: kafka.strimzi.io/v1beta2\n  kind: KafkaTopic\n  metadata:\n    name: sender-topic\n    namespace: kafka\n    labels:\n      strimzi.io/cluster: my-cluster\n  spec:\n    partitions: 3\n    replicas: 1\n    config:\n      retention.ms: 7200000\n      segment.bytes: 1073741824\n---\n  apiVersion: kafka.strimzi.io/v1beta2\n  kind: KafkaTopic\n  metadata:\n    name: receiver-topic\n    namespace: kafka\n    labels:\n      strimzi.io/cluster: my-cluster\n  spec:\n    partitions: 3\n    replicas: 1\n    config:\n      retention.ms: 7200000\n      segment.bytes: 1073741824\nEOF\n\nkubectl get kafkatopics.kafka.strimzi.io  -n kafka\n</code></pre> <p>With that setup a Kafka source can be installed which will trigger the event flow. This YAML creates the source which sends all messages to the Kafka broker. This shows the decoupling of the events. The producer or sender is unaware of the receiver(s) of the message. </p> Install Kafka Source<pre><code>kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-source.yaml\n\ncat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: sources.knative.dev/v1beta1\nkind: KafkaSource\nmetadata:\n  name: direktiv-kafka-source\n  namespace: knative-eventing\nspec:\n  consumerGroup: knative-group\n  bootstrapServers:\n  - my-cluster-kafka-bootstrap.kafka:9092\n  topics:\n  - sender-topic\n  sink:\n    ref:\n      apiVersion: eventing.knative.dev/v1\n      kind: Broker\n      name: default\nEOF\n</code></pre> Working Kafka Source<pre><code>kubectl get kafkasources.sources.knative.dev\n\nNAME                    TOPICS                       BOOTSTRAPSERVERS                            READY   REASON   AGE\ndirektiv-kafka-source   [\"sender-topic\"]             [\"my-cluster-kafka-bootstrap.kafka:9092\"]   True             4m16s\n</code></pre> <p>With this source enabled Knative can receive events but it requires a trigger to have another system consume the event. A trigger is a simple mechnism in Knative to \"forward\" certain events to subscribers. In this YAML there is a trigger filter defined this trigger consumes all events of type <code>dev.knative.kafka.event</code> and forwards it to Direktiv's direktiv-eventing service. The <code>uri</code> value specifies the target namespace in Direktiv. For more information about eventing filters visit the Knative documentation page about filters.</p> Trigger to Direktiv<pre><code>cat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: direktiv-in\n  namespace: knative-eventing\nspec:\n  broker: default\n  filter:\n    attributes:\n      type: dev.knative.kafka.event\n  subscriber:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: direktiv-eventing\n    uri: /direktiv\nEOF\n</code></pre> <p>This setup can already send events to a namespace called <code>direktiv</code> if data arrives at the <code>sender-topic</code> topic in Kafka. This can be easily tested if the namespace <code>direktiv</code> already exists in Direktiv. To test it we start a pod which connects to the sender topic.</p> Kafka Client Pod<pre><code>kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic sender-topic\n</code></pre> <p>After running the pod add JSON into the command prompt, e.g. {}. This sends the JSON object to Kafka. Knative's broker will pick up the message and execute the trigger for Direktiv. The event will appear on the direktiv namespace dashboard.</p> <p></p>"},{"location":"events/knative/example/#configuring-direktiv-source","title":"Configuring Direktiv Source","text":"<p>To connect Direktiv back to Knative we need to install <code>direktiv-knative-source</code>. This source listens to events generated in Direktiv and pushes them to Knative. In this example the message is pushed back to the broker which can then use triggers to distribute the event. The required argument for this source is the direktiv URI within the cluster, e.g. <code>direktiv-flow.default:3333</code>.</p> Direktiv Source<pre><code>cat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: sources.knative.dev/v1\nkind: ContainerSource\nmetadata:\n  name: direktiv-source\n  namespace: knative-eventing\nspec:\n  template:\n    spec:\n      containers:\n        - image: vorteil/direktiv-knative-source\n          name: direktiv-source\n          args:\n            - --direktiv=direktiv-flow.default:3333\n  sink:\n    ref:\n      apiVersion: eventing.knative.dev/v1\n      kind: Broker\n      name: default\nEOF\n</code></pre>"},{"location":"events/knative/example/#configuring-kafka-sink","title":"Configuring Kafka Sink","text":"<p>The last step is to create a Kafka sink which consumes the event coming from Direktiv. This closes the communication cycle from Kafka to Direktiv and back to Kafka again. For this to work a Kafka sink has to be installed.</p> Kafka Sink Installation<pre><code>kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/knative-v1.9.3/eventing-kafka-sink.yaml\n\ncat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: eventing.knative.dev/v1alpha1\nkind: KafkaSink\nmetadata:\n  name: direktiv-kafka-sink\n  namespace: knative-eventing\nspec:\n  topic: receiver-topic\n  bootstrapServers:\n  - my-cluster-kafka-bootstrap.kafka:9092\nEOF\n</code></pre> <p>Sink Topic</p> <p>Send a message to the <code>receiver-topic</code> if the sink reports an error about a missing topic:  <pre><code>kubectl -n kafka run kafka-receiver -ti --image=quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic receiver-topic\n</code></pre></p> <p>After installing the sink a trigger is required to tie them together. A filter can applied to that trigger as well. In this case the trigger accepts events if the type of the cloudevent is <code>myevent</code>.</p> Kafka Receiver Sink<pre><code>cat &lt;&lt;-EOF | kubectl apply -f -\n---\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: direktiv-receive\n  namespace: knative-eventing\nspec:\n  broker: default\n  filter:\n    attributes:\n      type: myevent\n  subscriber:\n    ref:\n      apiVersion: eventing.knative.dev/v1alpha1\n      kind: KafkaSink\n      name: direktiv-kafka-sink\nEOF\n</code></pre>"},{"location":"events/knative/example/#flow","title":"Flow","text":"<p>After all components are installed and connected a flow in Direktiv is required to actually transfrom the message and send it back. The example flow in the <code>direktiv</code> namespace here will listen to all <code>dev.knative.kafka.event</code> events and return the event under the new attribute <code>x</code>.</p> Simple Flow<pre><code>start:\n  type: event\n  state: tellme\n  event:\n    type: dev.knative.kafka.event\nstates:\n- id: tellme\n  type: generateEvent\n  event:\n    type: myevent\n    source: Direktiv\n    data:\n      x: jq(.\"dev.knative.kafka.event\".data)\n</code></pre> <p>With that setup a new message e.g. <code>\"Hello\"</code> on the <code>sender-topic</code> queue should show as <code>{ \"x\": { \"Hello\" }}</code> in the receiver topic. Please make sure to send valid JSON because this is being used as the data playload for the event. </p> Listen to Receiver Topic<pre><code>kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:latest-kafka-3.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic receiver-topic --from-beginning\n</code></pre>"},{"location":"events/knative/knative/","title":"Eventing","text":"<p>Direktiv provides a sink and a source for integration into Knative Eventing. Knative uses a broker to relay events between systems and the Kafka example shows how to use Kafka as broker. This section however explains the concept via direct connections between sinks and sources. </p>"},{"location":"events/knative/knative/#preparing-direktiv","title":"Preparing Direktiv","text":"<p>Knative requires a sink to send events to Direktiv. Direktiv comes with a ready-to-use Knative sink but it has to be enabled. This can be done during installation or afterward with an <code>helm upgrade</code>. The following configuration in Direktiv's <code>value.yaml</code> adds the required sink service.</p> Enabling Eventing<pre><code>eventing:\n    enabled: true\n</code></pre> Upgrade Direktiv<pre><code>helm upgrade -f direktiv.yaml -n direktiv direktiv direktiv/direktiv\n</code></pre> <p>After that change there is an additional service <code>direktiv-eventing</code> available in Direktiv's namespace. </p>"},{"location":"events/knative/knative/#knative-installation","title":"Knative Installation","text":"<p>During the default installation Knative's operator has been installed an makes installing Knative eventing an easy task with the default settings. </p> Operator Installation<pre><code>kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.9.4/operator.yaml\n</code></pre> Create Eventing Namespace<pre><code>kubectl create ns knative-eventing\n</code></pre> Install Default Knative Eventing<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nEOF\n</code></pre> <p>Default Installation</p> <p>The default installation uses an in-memory channel which is not recommended in production use because it is best-effort. </p>"},{"location":"events/knative/knative/#simple-ping-source","title":"Simple Ping Source","text":"<p>An easy way to test test the installation is to install a \"ping\" source. This is one of many sources provided by the Knative project. The examples below are almost identical except the <code>uri</code> parameter. Direktiv uses this to define the target namespaces. If the value is empty or <code>/</code> it will send the event to all namespace. If it contains a value e.g. <code>/mynamespace</code> it will send it to that namespace only. Event filters can be defined with a query parameter <code>filter</code>, e.g. <code>/mynamespace?filter=myfilter</code>.</p> Events For All Namespaces<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: my-ping\n  namespace: default\nspec:\n  schedule: \"*/1 * * * *\"\n  contentType: application/json\n  data: '{\"message\": \"Hello world!\"}'\n  sink:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: direktiv-eventing   \nEOF\n</code></pre> Events For One Namespace<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: my-ping\n  namespace: default\nspec:\n  schedule: \"*/1 * * * *\"\n  contentType: application/json\n  data: '{\"message\": \"Hello world!\"}'\n  sink:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: direktiv-eventing   \n    uri: /hello\nEOF\n</code></pre>"},{"location":"events/knative/vmware/","title":"VMWare Example","text":"<p>Connecting Direktiv and VMWare vSphere or ESXi via Knative Eventing is a very simple process because VMWare provides a Knative eventing source for their products. If Knative Eventing is configured with Direktiv there are just three simple steps required to connect these components.  </p> <p>VMWare Version</p> <p>This example has been tested with 7.x and 8.x</p>"},{"location":"events/knative/vmware/#installing-vmware-tanzu-sources","title":"Installing VMWare Tanzu Sources","text":"<p>The VMWare sources can be directly installed from the source repository with a <code>kubectl</code> command.</p> Apply Tanzu Source<pre><code>kubectl apply -f https://github.com/vmware-tanzu/sources-for-knative/releases/download/v0.36.3/release.yaml\n</code></pre> <p>VMWare Source Version</p> <p>Please check for the latest version of the VMWare sources</p> <p>After running the command there shouild be three pods available in the namespace <code>vmware-sources</code>:</p> <ul> <li>horizon-source-webhook</li> <li>vsphere-source-webhook</li> <li>horizon-source-controller</li> </ul>"},{"location":"events/knative/vmware/#creating-credentials","title":"Creating Credentials","text":"<p>To connect to vSphere or ESXi the source needs the credentials and connectivity information. It requires a kubernetes secrets which will be consumed later by the actual source.</p> VMWare Secret<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-credentials\n  namespace: vmware-sources\ntype: kubernetes.io/basic-auth\nstringData:\n  username: root\n  password: MySecretPassword\nEOF\n</code></pre> <p>The next stpe is to create the actual source. It requires the address and the reference to the crednetials used. The sink is the default Direktiv sink. The namespace of the sink might need to be adjusted to fit the installation namespace.</p> Create Source<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: sources.tanzu.vmware.com/v1alpha1\nkind: VSphereSource\nmetadata:\n  name: source\n  namespace: vmware-sources\nspec:\n  # Where to fetch the events, and how to auth.\n  address: https://192.168.220.128\n  skipTLSVerify: true\n  secretRef:\n    name: vsphere-credentials\n\n  # Where to send the events.\n  sink:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: direktiv-eventing\n      namespace: default\n    uri: /hello?filter=test # sending to namespace vmware\n\n  # Adjust checkpointing and event replay behavior\n  checkpointConfig:\n    maxAgeSeconds: 300\n    periodSeconds: 10\n\n  # Set the CloudEvent data encoding scheme to JSON\n  payloadEncoding: application/json\nEOF\n</code></pre>"},{"location":"events/knative/vmware/#testing-and-filtering","title":"Testing And Filtering","text":"<p>By default there should be enough events generated within VMWare to see incoming events on the monitoring page of Direktiv. These events can be e.g. of type <code>com.vmware.vsphere.VmBeingCreatedEvent.v0</code> or <code>com.vmware.vsphere.VmStartingEvent.v0</code>. Because there a many events it might be worthwhile to filter them. </p> <p>For filtering there are two options. Either use the native Knative trigger/filter mechanism or Direktiv's built-in event filter in the gateway. </p> <p>Direktiv's gateway supports filtering of event for events and this example will rename logout events, drop login events and pass through all other events as-is.</p> Direktiv Gateway Filter<pre><code>direktiv_api: \"endpoint/v1\"\nallow_anonymous: true\npath: \"/hello2\"\nmethods:\n  - \"POST\"\nplugins:\n  target:\n    type: \"target-event\"\n    configuration:\n      namespace: \"ddd\"\n  inbound:\n    - type: \"event-filter\"\n      configuration:\n        script: |\n          if (event[\"type\"] == \"com.vmware.vsphere.UserLogoutSessionEvent.v0\") {\n            event[\"type\"] = \"vmware-logout\"\n          }\n          if (event[\"type\"] == \"com.vmware.vsphere.UserLoginSessionEvent.v0\") {\n              return null\n          }\n          return event\n        allow_non_events: false\n</code></pre>"},{"location":"examples/aws/","title":"AWS Examples","text":"<p>AWS Examples on Github</p> <p>These examples should how you can communicate with AWS using the aws images. There are two examples, one is how to run a ec2 instance and the other is how to upload a file to a s3 bucket. </p> <p>These examples require the following namespace secrets to be set:</p> <ul> <li>ACCESS_KEY</li> <li>SECRET_ACCESS_KEY</li> </ul> <p>The flow will use these secrets to configure AWS access.</p>"},{"location":"examples/aws/#run-ec2-instance-flow-example","title":"Run EC2 Instance Flow Example","text":"<p>This flow will create a new t2.small instance on ec2 ap-southeast-2 region. The flow uses the 'awsgo' action which executes the cli command passed in the command input property.</p> Start AWS Instance<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: aws-cli\n  image: direktiv/aws-cli:dev\n  type: knative-workflow\n\nstates:\n- id: start-instance\n  type: action\n  action:\n    secrets: [\"ACCESS_KEY\", \"ACCESS_SECRET\"]\n    function: aws-cli\n    input: \n      access-key: jq(.secrets.ACCESS_KEY)\n      secret-key: jq(.secrets.ACCESS_SECRET)\n      region: ap-southeast-2\n      commands: \n      - command: aws ec2 run-instances --image-id ami-07620139298af599e --instance-type t2.small\n</code></pre>"},{"location":"examples/aws/#upload-file-to-s3-bucket-example","title":"Upload File to S3 Bucket Example","text":"<p>This flow will upload a file to a S3 bucket. The file name and data are set in the input. The input property <code>fileData</code> can be a url-encoded base64 string or a standard base64 string.</p> Start AWS Instance<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: s3\n  image: direktiv/aws-cli:dev\n  type: knative-workflow\n\nstates:\n- id: validate-input\n  type: validate\n  transform: 'jq(. + {fileData: .fileData | split(\"base64,\")[-1]})'\n  schema:\n    type: object\n    required:\n      - fileName\n      - fileData\n    properties:\n      fileName:\n        title: Filename\n        description: Filename to be set in S3 bucket\n        type: string\n      fileData:\n        title: File\n        description: File to upload\n        type: string\n        format: data-url\n  transition: store\n\n# stores the uploaded file as binary\n- id: store\n  type: setter\n  variables:\n  - key: data\n    scope: workflow\n    mimeType: application/octet-stream\n    value: 'jq(.fileData)'\n  transition: upload-file\n\n- id: upload-file\n  type: action\n  action:\n    function: s3\n    secrets: [\"ACCESS_KEY\", \"ACCESS_SECRET\"]\n    files: \n    - key: data\n      scope: workflow\n      as: jq(.fileName)\n    input:\n      access-key: jq(.secrets.ACCESS_KEY)\n      secret-key: jq(.secrets.ACCESS_SECRET)\n      region: ap-southeast-2\n      commands: \n      - command: aws s3 cp jq(.fileName) s3://direktiv/\n</code></pre> Input<pre><code>{\n  \"fileData\": \"SGVsbG8sIHdvcmxkIQ==\",\n  \"fileName\": \"message.txt\"\n}\n</code></pre>"},{"location":"examples/conditional-states/","title":"Conditional State","text":"<p>Conditional State on Github</p> <p>This example demonstrates the use of a switch state to conditional transition to different states based on a jq expression.  To show this, the example below is a flow that either approves or rejects a loan depending on the provided credit score and required minimum credit score.</p> Simple Switch Statement<pre><code>direktiv_api: workflow/v1\n\ndescription: |\n  Conditionally transition to states depending if input credit score is higher\n  or lower than creditMinRequired.\n\nstates:\n  - id: validate-input\n    type: validate\n    schema:\n      type: object\n      required:\n      - creditScore\n      - creditMinRequired\n      properties:\n        creditMinRequired:\n          type: number\n          title: Minimum credit score\n          description: minimum credit score required for approval \n          default: 500\n        creditScore:\n          type: number\n          description: credit score of user\n          title: Credit Score\n    transition: check-credit\n\n    #\n    # Check if the user's threshold is above minimum credit requirements.\n    # If credit score meets requirements transition to approve-loan. Otherwise\n    # transition to reject-loan.\n    #\n  - id: check-credit\n    type: switch\n    conditions:\n    - condition: jq(.creditScore &gt; .creditMinRequired)\n      transition: approve-loan\n    defaultTransition: reject-loan\n  - id: reject-loan\n    type: noop\n    transform: 'jq({ \"msg\": \"You have been rejected for this loan\" })'\n  - id: approve-loan\n    type: noop\n    transform: 'jq({ \"msg\": \"You have been approved for this loan\" })'\n</code></pre> Input<pre><code>{\n  \"creditMinRequired\": 500,\n  \"creditScore\": 600\n}\n</code></pre> Output<pre><code>{\n  \"msg\": \"You have been approved for this loan\"\n}\n</code></pre>"},{"location":"examples/consumers/","title":"Consumers","text":"<p>Consumers on Github</p> <p>The concept of consumers is used in Direktiv's gateway. The following example uses two consumers and an endpoint requiring authentication. </p> <p>For auithentication the <code>key-auth</code> plugin is used where <code>key_name</code> defines the name of the API key. By default all consumers in Direktiv can access an endpoint. </p> <p>In this example there ius an additional ACL plugin configured which limits the access by groups. In this case only consumers with <code>group1</code> can access the route.</p> <p>This request would be succesful because it is using the API key of <code>consumer1</code>.</p> <pre><code>curl --request GET \\\n  --url http://MYSERVER/ns/examples/consumer \\\n  --header 'mykey: apikey'\n</code></pre> <p>The second request would fail because although the user can be authenticated the ACL plugin denies the request because of the group membership.</p> <pre><code>curl --request GET \\\n  --url http://MYSERVER/ns/examples/consumer \\\n  --header 'mykey: apikey2'\n</code></pre> Route with Authentication<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"/consumer\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"/gw/wf1.yaml\"\n      async: false\n  inbound:\n    - type: \"acl\"\n      configuration:\n        allow_groups:\n          - \"group1\"\n  auth:\n    - type: \"key-auth\"\n      configuration:\n        add_username_header: false\n        add_tags_header: false\n        add_groups_header: false\n        key_name: \"mykey\"\n</code></pre> Consumer 1<pre><code>direktiv_api: \"consumer/v1\"\nusername: \"demo\"\npassword: \"password\"\napi_key: \"apikey\"\ntags:\n  - \"tag1\"\ngroups:\n  - \"group1\"\n</code></pre> Consumer 2<pre><code>direktiv_api: \"consumer/v1\"\nusername: \"demo2\"\npassword: \"password2\"\napi_key: \"apikey2\"\ngroups:\n  - \"group2\"\n</code></pre>"},{"location":"examples/counter-persistent-data/","title":"Counter","text":"<p>Counter on Github</p> <p>A simple example that shows how to store a counter as a flow variable for persistent data. Any state data can be set to a variable to be used in later instances. If the variable does not exist it is empty but is getting created the first time it will be stored.</p> Counter Example<pre><code>direktiv_api: workflow/v1\n\ndescription: \"Simple Counter getter and setter variable example\"\nstates:\n  #\n  # Get flow counter variable and increment value\n  #\n  - id: counter-get\n    type: getter \n    transition: counter-set\n    variables:\n    - key: counter\n      scope: workflow\n    transform: 'jq(. += {\"newCounter\": (.var.counter + 1)})'\n\n  #\n  # Set workflow counter variable\n  #\n  - id: counter-set\n    type: setter\n    variables:\n      - key: counter\n        scope: workflow \n        value: 'jq(.newCounter)'\n</code></pre>"},{"location":"examples/counter-persistent-data/#output","title":"Output","text":"Output<pre><code>{\n  \"newCounter\": 1,\n  \"var\": {\n    \"counter\": 0\n  }\n}\n</code></pre>"},{"location":"examples/cron/","title":"Cron","text":"<p>Cron on Github</p> <p>Direktiv flows can have different start actions. This can be a direct call or waiting for events.  Another way of executing flows is the cron start definition.</p> Cron<pre><code>direktiv_api: workflow/v1\n\nstart:\n  type: scheduled\n  cron: '* * * * *' # Trigger a new instance every minute.\n\nstates:\n- id: run\n  type: noop\n  log: Run Cron\n</code></pre>"},{"location":"examples/envs-wf/","title":"Environment Variables","text":"<p>Environment Variables on Github</p> <p>Direktiv allows to add environment variabels when using functions and services. In both cases the syntax is similar. </p> <pre><code>envs:\n- name: MYVAR\n  value: my-value\n- name: MYOTHERVAR\n  value: my-other-value\n</code></pre> <p>These values will be set when executing the user function. If those values change the service will be redeployed by Direktiv.</p> <p>The following services is an example how to use environment variabels in namespace services.</p> Service With Environment Variables<pre><code>direktiv_api: service/v1\nimage: gcr.io/direktiv/functions/bash:1.0\nsize: small\nenvs: \n- name: HELLO \n  value: world\n</code></pre> <p>Environment variabels can be used in flow functions as well. This flow is using a namespace service and a flow function with environment variables and adds the return of the functions to the final output of the flow. </p> Function with Environment Variables<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: bash\n  image: gcr.io/direktiv/functions/bash:1.0\n  type: knative-workflow\n  envs:\n  - name: HELLO\n    value: world\n- id: bash-svc\n  service: /envs-wf/svc.yaml\n  type: knative-namespace\n\nstates:\n- id: hello \n  type: action\n  action:\n    function: bash\n    input: \n      commands:\n      - command: bash -c \"echo $HELLO\"\n  transition: hello-again\n  transform:\n    hello: 'jq(.return.bash[0].result)'\n- id: hello-again\n  type: action\n  action:\n    function: bash-svc\n    input: \n      commands:\n      - command: bash -c \"echo $HELLO\"\n  transform:\n    result: 'jq(. + { \"hello-again\": .return.bash[0].result } | del(.return))'\n</code></pre>"},{"location":"examples/foreach/","title":"Foreach","text":"<p>Foreach  on Github</p> <p>The <code>foreach</code> state requires the <code>array</code> attribute to loop over. The difference to other states is that the data in the action of the <code>foreach</code> function is not getting the state data of the flow but the values provided in the array. </p>"},{"location":"examples/foreach/#simple-foreach","title":"Simple Foreach","text":"<p>This is the most basic example. It shows that each action call in the foreach loop has it's own object during execution. In the flow scope there is a variable <code>.names</code>. But the <code>array</code> definition uses jq to iterate through <code>.names</code> and creates a list of JSON objects with the variable <code>name</code>. This means that each action only sees an object with the value <code>name</code> and has no access to <code>names</code>.</p> Simple Foreach<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: echo\n  image: direktiv/echo:dev\n  type: knative-workflow\n\nstates:\n\n- id: data\n  type: noop\n  log: preparing foreach data\n  transform:\n    names:\n    - hello\n    - world\n    - goodbye\n  transition: foreach\n\n- id: foreach\n  type: foreach\n  array: 'jq([.names[] | { name: . }])'\n  action:\n    function: echo\n    input: 'jq(.)'\n</code></pre> <p>The output for this flow should be something like the following:</p> Output<pre><code>{\n  \"names\": [\n    \"hello\",\n    \"world\",\n    \"goodbye\"\n  ],\n  \"return\": [\n    {\n      \"name\": \"hello\"\n    },\n    {\n      \"name\": \"world\"\n    },\n    {\n      \"name\": \"goodbye\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/foreach/#foreach-with-jq","title":"Foreach with JQ","text":"<p>This examples shows how to use JQ for a more complex foreach scenario. It generates an array based on <code>.data</code> in the first state. The JQ command is storing the state data <code>.otherdata</code> in the variable <code>od</code>. This result will be piped into the actual array generation with <code>.data[]</code>. In this case it is more obvious how each <code>foreach</code> action gets it's own JSON object. In this case the JQ command sets the <code>name</code> to the name in the array, <code>time</code> to the actual time with the JQ <code>time</code> function. The last attribute <code>otherdata</code> passes the original value from the flow state data into the action.</p> JQ Foreach<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: echo\n  image: direktiv/echo:dev\n  type: knative-workflow\n\nstates:\n\n- id: data\n  type: noop\n  transform:\n    data:\n    - name: key1\n      value: value1\n    - name: key2\n      value: value2\n    - name: key3\n      value: value3\n    otherdata: somedata\n  transition: foreach\n\n- id: foreach\n  type: foreach\n  array: 'jq(.otherdata as $od | [.data[] | { name: .name, time: now, otherdata: $od }])'\n  action:\n    function: echo\n    input: 'jq(.)'\n</code></pre> Output<pre><code>{\n  \"data\": [\n    {\n      \"name\": \"key1\",\n      \"value\": \"value1\"\n    },\n    {\n      \"name\": \"key2\",\n      \"value\": \"value2\"\n    },\n    {\n      \"name\": \"key3\",\n      \"value\": \"value3\"\n    }\n  ],\n  \"otherdata\": \"somedata\",\n  \"return\": [\n    {\n      \"name\": \"key1\",\n      \"otherdata\": \"somedata\",\n      \"time\": 1680972341.2246315\n    },\n    {\n      \"name\": \"key2\",\n      \"otherdata\": \"somedata\",\n      \"time\": 1680972341.224634\n    },\n    {\n      \"name\": \"key3\",\n      \"otherdata\": \"somedata\",\n      \"time\": 1680972341.2246354\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/foreach/#foreach-with-js","title":"Foreach with JS","text":"<p>This example uses Javascript to achieve the same outcome. If data structures are getting too complex it might be better to use Javascript for readability. If Javascript is used Direktiv passes in an object <code>data</code> which contains the flow state. Data can be accessed in the usual way like <code>data[\"otherdata\"]</code>. In the case of a <code>foreach</code> the Javascript function needs to return an array.</p> JS Foreach<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: echo\n  image: direktiv/echo:dev\n  type: knative-workflow\n\nstates:\n- id: data\n  type: noop\n  transform:\n    data:\n    - name: key1\n      value: value1\n    - name: key2\n      value: value2\n    - name: key3\n      value: value3\n    otherdata: somedata\n  transition: foreach\n- id: foreach\n  type: foreach\n  array: |\n    js(\n      // empty array\n      const items = []\n\n      // loop over \"data\" attribute created in first state of flow\n      for (let i = 0; i &lt; data[\"data\"].length; i++) { \n          // create object and set attributes\n          item = new Object();  \n          item.name = data[\"data\"][i][\"name\"]\n          item.time = Date.now()\n          item.otherdata = data[\"otherdata\"]\n\n          // add item\n          items[i] = item\n      }\n\n      // return array of items\n      return items\n    )\n  action:\n    function: echo\n    input: 'jq(.)'\n</code></pre>"},{"location":"examples/gcp-vm-destroy/","title":"Self-Destroying VM in GCP","text":"<p>Self-Destroying VM in GCP on Github</p> <p>This example shows how to create virtual machines (VM) in Google cloud and delete after a certain time. This can be used for build processes or creating test instances. This example requires a service account JSON key in Google Cloud.</p> <p>It consist of three workflows. The create flow is responsible for creating the virtual machine. This example uses a Google Cloud VM but conceptually it works with every cloud provider. This flow returns all the important information about the created machine. At the end it starts a subflow which waits for an event to delete the virtual machine. If that event does not arrive and times out, the delete process starts even in the absence of that event. </p> <p>This flow has a validate state at the beginning and a transform to set defaults for the virtual machine creation.</p> Create VM Flow<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: gcp\n  image: gcr.io/direktiv/functions/gcp:1.0\n  type: knative-workflow\n- id: deleter \n  type: subflow\n  workflow: deleter.yaml\n\nstates:\n\n# validate input for flow\n- id: input\n  type: validate\n  schema:\n    title: Create GCP VM\n    type: object\n    required: [\"name\"]\n    properties:\n      name:\n        type: string\n        title: VM Name\n      disk:\n        type: string\n        title: Disk Size\n      zone:\n        type: string\n        title: Zone\n      machine:\n        type: string\n        title: Machine Type\n      tags:\n        type: array\n        items:\n          type: string\n  transform:\n    name: jq(.name)\n    disk: jq(.disk // \"10GB\")\n    zone: jq(.zone // \"us-west2-a\")\n    machine: jq(.machine // \"e2-standard-16\")\n    tags: jq(.tags // [])\n  transition: gcp\n\n# create vm with parameters provided\n- id: gcp\n  type: action\n  action:\n    function: gcp\n    secrets: [\"gcpJSONKey\", \"gcpProject\", \"gcpAccount\"]\n    input: \n      account: jq(.secrets.gcpAccount)\n      project: jq(.secrets.gcpProject)\n      key: jq(.secrets.gcpJSONKey | @base64 )\n      commands:\n      - command: gcloud compute instances create jq(.name) --boot-disk-size jq(.disk) --zone jq(.zone) --machine-type jq(.machine) jq(if .tags then \"--tags \" + (.tags | join(\",\")) end) --format=json\n  transition: load-delete\n\n# start the delete flow\n- id: load-delete\n  type: action\n  async: true\n  action:\n    function: deleter\n    input:\n      name: jq(.name)\n      zone: jq(.zone)\n</code></pre> <p>The <code>timeout</code> defines how long that flow waits before it starts to delete the virtual machine. </p> Delete Flow<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: gcp\n  image: gcr.io/direktiv/functions/gcp:1.0\n  type: knative-workflow\n\nstates:\n\n# waits for the delete event, if it times out it deletes the VM anyways\n- id: wait\n  type: consumeEvent\n  timeout: PT1H\n  log: waiting for delete event for jq(.name)\n  event:\n    type: io.direktiv.gcp.vm.delete \n    context:\n      name: jq(.name)\n  transition: check-instance\n  catch:\n  - error: \"direktiv.cancels.timeout.soft\"\n    transition: check-instance\n\n# lists instances to check if there is something to delete\n- id: check-instance\n  type: action\n  action:\n    function: gcp\n    secrets: [\"gcpJSONKey\", \"gcpProject\", \"gcpAccount\"]\n    input: \n      account: jq(.secrets.gcpAccount)\n      project: jq(.secrets.gcpProject)\n      key: jq(.secrets.gcpJSONKey | @base64 )\n      commands:\n      - command: gcloud compute instances list --filter=\"name=jq(.name)\" --format json\n  transition: length-check\n\n# if the previous state returns a VM it proceeds to deleting\n- id: length-check\n  type: switch\n  conditions:\n  - condition: 'jq(.return.gcp[0].result | length &gt; 0)'\n    transition: delete\n\n- id: delete\n  type: action\n  action:\n    function: gcp\n    secrets: [\"gcpJSONKey\", \"gcpProject\", \"gcpAccount\"]\n    input: \n      account: jq(.secrets.gcpAccount)\n      project: jq(.secrets.gcpProject)\n      key: jq(.secrets.gcpJSONKey | @base64 )\n      commands:\n      - command: gcloud compute instances delete jq(.name) --zone=jq(.zone) -q\n</code></pre> <p>The virtual machine can be deleted by sending an event. This can come from outside via an API call or within a flow with the <code>generateEvent</code> state.</p> Trigger Event Example<pre><code>direktiv_api: workflow/v1\n\nstates:\n- id: a\n  type: generateEvent\n  event:\n    type: io.direktiv.gcp.vm.delete\n    source: direktiv\n    context:\n      name: jq(.name)\n</code></pre>"},{"location":"examples/greeting-event-listener/","title":"Event-based Workflow","text":"<p>Event-based Workflow on Github</p> <p>This example demonstrates a flow that waits for a cloud event with type <code>greetingcloudevent</code>. When the event is received, a state will be triggered using the data provided by the event. Because this flow has a start of type event, directly executing this flow is not necessary. </p> <p>To trigger the listener flow, a second flow will be created to generate the cloud event. </p> <p>The <code>generate-greeting</code> flow generates the <code>greetingcloudevent</code> that the <code>eventbased-greeting</code> flow is waiting for.</p> Listener Workflow<pre><code># Example Input:\n# This input is a cloud event and was generated from the greeting-generate flow.\n# {\n#   \"greetingcloudevent\": {\n#     \"data\": {\n#       \"name\": \"Trent\"\n#     },\n#     \"datacontenttype\": \"application/json\",\n#     \"id\": \"2638e2d6-754e-409f-9038-f725e0d9d0af\",\n#     \"source\": \"Direktiv\",\n#     \"specversion\": \"1.0\",\n#     \"type\": \"greetingcloudevent\"\n#   }\n# }\n#\n# Example Output\n# {\n#     \"return\": {\n#         \"greeting\": \"Welcome to Direktiv, World!\"\n#     }\n# }\n\ndirektiv_api: workflow/v1\n\ndescription: |\n  Passively listen for cloud events where the type equals \"greetingcloudevent\" and\n  then execute a action state to call the direktiv/greeting action, which 'greets' \n  the user specified in the \"name\" field of the input provided to the flow.\n\n  Because this flow has a start of type event, directly executing this flow \n  is not necessary.\n\n#\n# Start of type event definition sets the flow to be executed when a event\n# is triggered with the defined type 'greetingcloudevent'\n#\nstart:\n  type: event\n  state: greeter\n  event:\n    type: greetingcloudevent\n\nfunctions:\n- id: hello-world\n  image: direktiv/hello-world:dev\n  type: knative-workflow\n\n\nstates:\n- id: greeter\n  type: action\n  log: jq(.greetingcloudevent.data.name)\n  action: \n    function: hello-world\n    input: \n      name: jq(.greetingcloudevent.data.name)\n  transform: 'jq({ \"greeting\": .return.\"hello-world\" })'\n</code></pre> Output<pre><code>{\n    \"return\": {\n        \"greeting\": \"Welcome to Direktiv, World!\"\n    }\n}\n</code></pre> Generator Workflow<pre><code>direktiv_api: workflow/v1\n\ndescription: |\n  Generate a cloud with of type \"greetingcloudevent\" with name data as input.\n\nstates:\n  # Example Generated Cloud Event:\n  # {\n  #   \"greetingcloudevent\": {\n  #     \"data\": {\n  #       \"name\": \"World\"\n  #     },\n  #     \"datacontenttype\": \"application/json\",\n  #     \"id\": \"2638e2d6-754e-409f-9038-f725e0d9d0af\",\n  #     \"source\": \"Direktiv\",\n  #     \"specversion\": \"1.0\",\n  #     \"type\": \"greetingcloudevent\"\n  #   }\n  # }\n- id: gen\n  type: generateEvent\n  event:\n    type: greetingcloudevent\n    source: Direktiv\n    data:\n      name: \"World\"\n</code></pre>"},{"location":"examples/greeting/","title":"Greeting Example","text":"<p>Greeting Example on Github</p> <p>This simple example flow uses a single <code>action</code> state to call the <code>hello-world</code> action, which 'greets' the user specified in the <code>\"name\"</code> field of the input provided to the flow. The validate state ensures the input is valid.</p> Greeter Flow<pre><code># Example Input:\n# {\n#     \"name\": \"World\"\n# }\n#\n# Example Output:\n# The results of this action will contain a greeting addressed to the provided name.\n# {\n#     \"return\": {\n#         \"greeting\": \"Welcome to Direktiv, World!\"\n#     }\n# }\n\ndirektiv_api: workflow/v1\n\ndescription: |\n  Execute a action state to call the direktiv/greeting action, which 'greets' \n  the user specified in the \"name\" field of the input provided to the flow.\n\nfunctions:\n- id: greeter\n  image: direktiv/hello-world:dev\n  type: knative-workflow\n\nstates:\n- id: validate-input\n  type: validate\n  schema:\n    type: object\n    required:\n    - name\n    properties:\n      name:\n        type: string\n        description: Name to greet\n        title: Name\n  transition: greeter\n\n#\n# Execute greeter action.\n#\n- id: greeter\n  type: action\n  log: jq(.)\n  action: \n    function: greeter\n    input: \n      name: jq(.name)\n  transform: 'jq({ \"greeting\": .return.\"hello-world\" })'\n</code></pre> Input<pre><code>{\n    \"name\": \"World\"\n}\n</code></pre> <p>The results of this action will contain a greeting addressed to the provided name.</p> Output<pre><code>{\n  \"greeting\": \"Hello World\"\n}\n</code></pre>"},{"location":"examples/gw/","title":"Simple Route","text":"<p>Simple Route   on Github</p> <p>Routing is part of Direktiv's gateway. If an endoint file is detected Direktiv creates the route and serves it to clients. The following example routes a GET request to a flow. </p> <p>Depending on the name of the namespace the URL would be something like <code>https://YOURSERVER/ns/examples/hello</code>.</p> Route<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"/hello\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"/gw/wf1.yaml\"\n      async: false\nallow_anonymous: true\n</code></pre> Flow<pre><code>direktiv_api: workflow/v1\nstates:\n- id: helloworld\n  type: noop\n  transform:\n    result: Hello From Gateway!\n</code></pre>"},{"location":"examples/input-convert/","title":"Convert Input","text":"<p>Convert Input on Github</p> <p>This example show how to handle input which is not JSON. This example uses a XLSX file and converts it to JSON to be used in the workflow. </p> <p>If Direktiv gets non-JSON input, in this case a binary file, it encodes it as Base64 and starts the workflow with a an <code>input</code> variable containing the binary file. </p> Convert Flow<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: csvkit\n  image: direktiv/csvkit:dev\n  type: knative-workflow\n\n# Fetch base64 input and store it workflow variable\nstates:\n- id: set\n  type: setter\n  log: jq(.)\n  variables:\n  - key: in.xlsx\n    # mark this a binary file\n    mimeType: application/octet-stream\n    # for non-JSON input the data ends up as base64 in .input\n    value: 'jq(.input)'\n    scope: workflow\n  transition: convert \n\n# Takes the workflow variable and converts it\n- id: convert\n  type: action\n  action:\n    function: csvkit\n    files: \n    - key: in.xlsx\n      scope: workflow\n    input: \n      commands:\n      - command: bash -c 'in2csv in.xlsx &gt; out.csv'\n      - command: csvjson out.csv\n  transform:\n    json: jq(.return.csvkit[1].result[0])\n</code></pre> Push Data to Flow<pre><code>curl -XPOST --data-binary @data.xlsx http://MYSERVER/api/namespaces/examples/tree/input-convert/workflow?op=wait\n</code></pre>"},{"location":"examples/js-plugin/","title":"Javascript Plugin","text":"<p>Javascript Plugin on Github</p> <p>This example uses the Javascript and Request converter plugin. The first plugin adds a header and the request converter sends the whole request split into an object to the flow.</p> <p>The flow receiving that request will have an additional header called <code>Header1</code>.</p> Javascript Route<pre><code>direktiv_api: \"endpoint/v1\"\nallow_anonymous: true\npath: \"/js\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"/js-plugin/wf.yaml\"\n      async: false\n  inbound:\n    - type: \"js-inbound\"\n      configuration:\n        script: |\n          input[\"Headers\"].Add(\"Header1\", \"Value1\")\n    - type: \"request-convert\"\n      configuration:\n        omit_headers: false\n        omit_queries: false\n        omit_body: false\n        omit_consumer: false\n</code></pre> Simple Workflow<pre><code>direktiv_api: workflow/v1\nstates:\n- id: helloworld\n  type: noop\n  transform:\n    result: jq(.)\n</code></pre>"},{"location":"examples/js-plugin/#advanced-example","title":"Advanced Example","text":"<p>This example uses a path parameter and a bit more complex Javascript. </p> <ul> <li>The plugin moves the original request in an object <code>original</code></li> <li>The plugin gets the value of a query param <code>action</code> and puts it in <code>action</code> in the object</li> </ul> <p>The URL would look like this: <code>http://YOUR-SERVER/ns/examples/js-action?action=do-it</code></p> Advanced Javascript<pre><code>direktiv_api: \"endpoint/v1\"\nallow_anonymous: true\npath: \"/js-action\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"/js-plugin/wf.yaml\"\n      async: false\n  inbound:\n    - type: \"js-inbound\"\n      configuration:\n        script: |\n          b = JSON.parse(input[\"Body\"]) \n          const body = new Map();\n          body['action'] = input[\"Queries\"].Get(\"action\")[0]\n          body[\"original\"] = b\n          input[\"Body\"] = JSON.stringify(body)  \n</code></pre>"},{"location":"examples/patching/","title":"Patching Functions","text":"<p>Patching Functions on Github</p> <p>Patcing functions is a simple way to use Kubernetes functionality, which is not supported by the default configuration in Direktiv. This can be extra annotations or metadata for the pod or values within the user container.</p> <p>This example add an annotation and a label to the pod. Additionally it changes the CPU requests for that user function.</p> Simple Patch<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: patch\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\n  patches:\n  - op: add\n    path: /spec/template/metadata/annotations\n    Value: { \"my\": \"annotations\" }\n  - op: add\n    path: /spec/template/metadata/labels\n    Value: { \"my\": \"labels\" }\n  - op: add\n    path: /spec/template/spec/containers/0/resources/requests/cpu\n    Value: 250m\nstates:\n- id: getter \n  type: action\n  action:\n    function: patch\n    input: \n      method: \"GET\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n</code></pre> <p>The following example uses annotations to enable container image builds on Direktiv. The annotation<code>/spec/template/metadata/annotations/container.apparmor.security.beta.kubernetes.io~1direktiv-container</code> change sthe apparmor settings for the container and an additional environment variable changes the filesystem for <code>buildah</code>. </p> <p>This function uses the helper command <code>/usr/share/direktiv/direktiv-cmd</code> which enables Direktiv to use standrad containers from registries. In this case it is the official <code>buildah</code> container. </p> Patch for Build<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: build\n  image: quay.io/buildah/stable:v1.32.2 \n  cmd: /usr/share/direktiv/direktiv-cmd\n  type: knative-workflow\n  size: large\n  envs:\n  - name: STORAGE_DRIVER\n    value: vfs\n  patches:\n  - op: add\n    path: /spec/template/metadata/annotations/container.apparmor.security.beta.kubernetes.io~1direktiv-container\n    value: unconfined\nstates:\n- id: builder\n  type: action\n  action:\n    function: build\n    secrets: [\"user\", \"pwd\"]\n    input:\n      files:\n      - name: Dockerfile\n        content: FROM docker.io/nginx:1.23.3-alpine\n        permission: 0755\n      data:\n        commands:\n        - command: buildah login -u jq(.secrets.user) -p jq(.secrets.pwd) docker.io\n          suppress_command: true\n        - command: buildah bud --tag \"jq(.secrets.user)/deleteme\" --manifest multi --arch amd64 .\n        - command: buildah bud --tag \"jq(.secrets.user)/deleteme\" --manifest multi --arch arm64 .\n        - command: buildah manifest push --all multi \"docker://docker.io/jq(.secrets.user)/deleteme\" \n</code></pre>"},{"location":"examples/python-container/","title":"Python Container","text":"<p>Python Container on Github</p> <p>With Direktiv's special command it is easy to use a standard Python container from Docker Hub. In the following example the flow uses one of those containers and uses the special command <code>/usr/share/direktiv/direktiv-cmd</code> to use it in Direktiv. </p> <p>This special command eanbles the user to pass in files as well. Thisd can be done with the standard files in Direktiv's filesystem or on-demand like in this example. The file <code>script.py</code> is getting created when the function  runs and can be executed because the permission is set to <code>0755</code>. These scripts can contain flow variables and secrets as well if required. </p> Python Container<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: python\n  image: python\n  type: knative-workflow\n  cmd: /usr/share/direktiv/direktiv-cmd\nstates:\n- id: python\n  type: action\n  action:\n    function: python\n    input: \n      files:\n      - name: script.py\n        content: |\n          print(\"Hello, World!\")\n        permission: 0755\n      data:\n        commands:\n        - command: python ./script.py\n</code></pre>"},{"location":"examples/request-external-api/","title":"Request API","text":"<p>Request API on Github</p> <p>This example shows how we can write a flow to communicate with a external API service. In this flowflow we will use the Direktiv request image to make a HTTP GET request to https://fakerapi.it/ and fetch the details of a fake person. A transform is also used to clean up the returned value from the action, but it can be commented out to see the full return value.</p> API Request<pre><code># Example Output:\n# {\n#   \"person\": {\n#     \"address\": {\n#       \"buildingNumber\": \"8422\",\n#       \"city\": \"Ashleytown\",\n#       \"country\": \"Ethiopia\",\n#       \"county_code\": \"AD\",\n#       \"id\": 0,\n#       \"latitude\": -21.509297,\n#       \"longitude\": -48.162169,\n#       \"street\": \"47933 Kennedi View Apt. 395\",\n#       \"streetName\": \"Margie Stream\",\n#       \"zipcode\": \"44788\"\n#     },\n#     \"birthday\": \"1944-09-28\",\n#     \"email\": \"qmetz@gmail.com\",\n#     \"firstname\": \"Gabriella\",\n#     \"gender\": \"female\",\n#     \"id\": 1,\n#     \"image\": \"http://placeimg.com/640/480/people\",\n#     \"lastname\": \"Steuber\",\n#     \"phone\": \"+5542223225627\",\n#     \"website\": \"http://wiza.com\"\n#   }\n# }\n\ndirektiv_api: workflow/v1\n\ndescription: |\n  Execute a HTTP request to generate a persons details from the fake data API fakerapi. \nfunctions:\n- id: http-request\n  image: direktiv/http-request:dev\n  type: knative-workflow\nstates:\n#\n# HTTP GET Fake person from fakerapi\n# Transform data to get data out of body\n#\n- id: get-fake-persons\n  transform: \"jq({person: .return[0].result.data[0]})\"\n  type: action\n  action:\n    function: http-request\n    input: \n      method: \"GET\"\n      url: \"https://fakerapi.it/api/v1/persons?_quantity=1\"\n</code></pre> Output<pre><code>{\n  \"person\": {\n    \"address\": {\n      \"buildingNumber\": \"8422\",\n      \"city\": \"Ashleytown\",\n      \"country\": \"Ethiopia\",\n      \"county_code\": \"AD\",\n      \"id\": 0,\n      \"latitude\": -21.509297,\n      \"longitude\": -48.162169,\n      \"street\": \"47933 Kennedi View Apt. 395\",\n      \"streetName\": \"Margie Stream\",\n      \"zipcode\": \"44788\"\n    },\n    \"birthday\": \"1944-09-28\",\n    \"email\": \"qmetz@gmail.com\",\n    \"firstname\": \"Gabriella\",\n    \"gender\": \"female\",\n    \"id\": 1,\n    \"image\": \"http://placeimg.com/640/480/people\",\n    \"lastname\": \"Steuber\",\n    \"phone\": \"+5542223225627\",\n    \"website\": \"http://wiza.com\"\n  }\n}\n</code></pre>"},{"location":"examples/scripting/","title":"Scripting","text":"<p>Scripting on Github</p> <p>To use scripts like Python, Javascript, Powershell etc. A script can be loaded from flow or namespace variables. These files can be provided to the actions and executed. If the namespace is synced via Git a naming convention adds variables to worklflows. If a file has a prefix of a flow it will be added as variable. This enables Direktiv to use it as script in a flow, e.g.:</p> <ul> <li>myflow.yaml</li> <li>myflow.yaml.script.sh</li> </ul> <p>In the above example there would be a <code>script.sh</code> flow variable. The following flow example uses Python but any file type can be used, even binaries.</p> Python Flow<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: python\n  image: direktiv/python:dev\n  type: knative-workflow\nstates:\n- id: python\n  type: action\n  action:\n    function: python\n    # use AWS key and secret\n    secrets: [\"AWS_ACCESS_KEY_ID\",\"AWS_SECRET_ACCESS_KEY\"]\n    files:\n    - key: python.py\n      scope: workflow\n    input: \n      commands:\n      - command: pip install boto3\n      - command: python3 python.py\n        envs: \n        - name: AWS_ACCESS_KEY_ID\n          value: jq(.secrets.AWS_ACCESS_KEY_ID)\n        - name: AWS_SECRET_ACCESS_KEY\n          value: jq(.secrets.AWS_SECRET_ACCESS_KEY)\n      - command: cat out.json\n  transform:\n    regions: jq(.return.python[1].result)\n</code></pre> python.py<pre><code>import boto3\nimport os\nimport json\n\nsession = boto3.session.Session()\n\nregions = {}\n\nclient = boto3.client('ec2',region_name='us-east-1')\nec2_regions = [region['RegionName'] for region in client.describe_regions()['Regions']]\nfor region in ec2_regions:\n    print(\"executing region \" + region)\n    vs = []\n    ec2_resource = session.resource('ec2', region_name=region)\n    for volume in ec2_resource.volumes.filter():\n        if volume.state == 'available':\n            vs.append(volume.id)\n\n    if len(vs) &gt; 0:\n        regions[region] = vs\n        print(\"added \" + str(len(vs)) +  \"volumes for region  \" + region)\n\n# writes json to a file\nwith open('out.json', 'w') as out_file:\n     json.dump(regions, out_file)\n\n# writes json to a workflow variable\nwith open('out/workflow/out.json', 'w') as out_file:\n     json.dump(regions, out_file)\n</code></pre>"},{"location":"examples/services/","title":"Namespace Services","text":"<p>Namespace Services on Github</p> <p>Direktiv can use namespace-wide services. These services have to be configured as individual files. In those files different attriubtes can bet set to change the behaviour of the service, e.g. environment variables. </p> <p>The avaiable attributes are available in the specification</p> Service Definition<pre><code>direktiv_api: service/v1\nimage: gcr.io/direktiv/functions/http-request:1.0\nsize: small\nscale: 1\n</code></pre> <p>Multiple workflows can use this service in their function definition.</p> Workflow<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: get\n  service: /services/s1.yaml\n  type: knative-namespace\nstates:\n- id: getter \n  type: action\n  action:\n    function: get\n    input: \n      method: \"GET\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n</code></pre>"},{"location":"examples/solving-math-expressions/","title":"Solving Math Expressions","text":"<p>Solving Math Expressions on Github</p> <p>/spec/workflow-yaml/foreach/</p> <p>This example shows how we can iterate over data using the ForEach state. Which executes an action that solves a math expression. The flow data input are the expressions you want to solve as a string array.</p> <p>The example demonstrates the use of an action isolate to solve a number of mathematical expressions using a <code>foreach</code> state. For each expression in the input array, the isolate will be run once. </p> Solver Flow<pre><code># Example Input:\n# {\n#  \"expressions\": [\n#    \"4+10\",\n#    \"15-14\",\n#    \"100*3\",\n#    \"200/2\"\n#  ]\n# }\n#\n# Example Output:\n# The results of this foreach loop will be a json array of strings that have the solved answers.\n# {\n#   \"solved\": [\n#     \"14\",\n#     \"1\",\n#     \"300\",\n#     \"100\"\n#   ]\n# }\n\ndirektiv_api: workflow/v1\n\ndescription: |\n  Executes an action that solves a math expression. \n  The workflow data input are the expressions you want to solve as a string array.\n\nfunctions:\n- id: solve-math-expression\n  image: direktiv/bash:dev\n  type: knative-workflow\n\nstates:\n  - id: validate-input\n    type: validate\n    schema:\n      type: object\n      required:\n      - expressions\n      properties:\n        expressions:\n          type: array\n          description: expressions to solve\n          title: Expressions\n          items:\n            type: string\n    transition: solve\n\n  #\n  # Execute solve action.\n  #\n  - id: solve\n    type: foreach\n    array: 'jq([.expressions[] | { expression: . }])'\n    action:\n      function: solve-math-expression\n      input: \n        commands: \n        - command: bash -c \"echo $((jq(.expression)))\"\n    transform: 'jq({ solved: [.return[] | .bash[0].result ] })'\n</code></pre> Input<pre><code>{\n  \"expressions\": [\n    \"4+10\",\n    \"15-14\",\n    \"100*3\",\n    \"200/2\"\n  ]\n}\n</code></pre> <p>The results of this foreach loop will be a json array of strings that have the solved answers.</p> Output<pre><code>{\n  \"solved\": [\n    \"14\",\n    \"1\",\n    \"300\",\n    \"100\"\n  ]\n}\n</code></pre> <p>Note: The array for a foreach state must be passed as an array of objects. This is why to iterate over the <code>expressions</code> string array, we must pipe it and construct a new array of objects using <code>[.expressions[] | { expression: . }]</code>.</p>"},{"location":"examples/solving-math-expressions/#jq-expressions","title":"jq: <code>.expressions</code>","text":"<pre><code>[\n  \"4+10\",\n  \"15-14\",\n  \"100*3\",\n  \"200/2\"\n]\n</code></pre>"},{"location":"examples/solving-math-expressions/#jq-expressions-expression","title":"jq: <code>[.expressions[] | { expression: . }]</code>","text":"<pre><code>[\n  {\n    \"expression\": \"4+10\"\n  },\n  {\n    \"expression\": \"15-14\"\n  },\n  {\n    \"expression\": \"100*3\"\n  },\n  {\n    \"expression\": \"200/2\"\n  }\n]\n</code></pre>"},{"location":"examples/special_command/","title":"Special Command","text":"<p>Special Command on Github</p> <p>Direktiv can provide a generic server on port <code>8080</code> for containers not providing one. An example could be a <code>python</code> container from Docker Hub. To avoid creating a custom container for Direktiv the <code>cmd</code> atttribute in functions can be used to let Direktiv know that a server is required. </p> <p>This function uses the default <code>python</code> container and wit hthe <code>cmd</code> set Direktiv starts up a server. The container executes all functions under the <code>data/commands/command</code> section.</p> Python Function<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: get\n  image: python:3.13.0a3-bookworm\n  type: knative-workflow\n  cmd: /usr/share/direktiv/direktiv-cmd\nstates:\n- id: getter \n  type: action\n  action:\n    function: get\n    input: \n      data:\n        commands:\n        - command: python3 -c \"print('hello world')\"\n</code></pre> <p>Additional Attributes for commands:</p> Attribute Description stop Stops the excution of subsequent ocmmands if this command fails. suppress_command Does not show the command executed in the logs for the instance suppress_output Does not write stdout of the command to the logs"},{"location":"examples/subflows/","title":"Subflows","text":"<p>Subflows on Github</p> <p>Direktiv can use containers as actions but can also call subflows in the same way.  It uses the same parameters and provides the same functionality.</p> Parent Flow<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n# Define subflow function\n- id: sub\n  workflow: subflow\n  type: subflow\n\n# Call subflow with input values\nstates:\n- id: call-sub \n  type: action\n  action:\n    function: sub\n    input: \n      key: value\n</code></pre> Subflow<pre><code>direktiv_api: workflow/v1\nstates:\n- id: print\n  type: noop\n  log: jq(.)\n</code></pre>"},{"location":"examples/variable-mime-type/","title":"Variable Mime Type Example","text":"<p>Variable Mime Type Example on Github</p> <p>All variables have an associated mime type to distinguish the content type of its value. This example will show two examples, and the special behaviour that happens when mimeType is <code>text/plain</code> or <code>application/octet-stream</code>. </p>"},{"location":"examples/variable-mime-type/#storing-a-string-as-a-raw-plaintext-variable","title":"Storing a string as a raw plaintext variable.","text":"<p>By default (mimeType=application/json) all variables are treated as JSON values. So this means even if you store a string in a variable, it's value is stored with quotes wrapped around it.</p> JSON String Data<pre><code>direktiv_api: workflow/v1\n\ndescription: |\n  Store the workflow variable 'StringVar' as a json encoded string.  \n\nstates:\n#\n# Set StringVar Value: \n# \"hello\\nworld\"\n#\n- id: set-var\n  type: setter\n  variables:\n    - key: StringVar\n      scope: workflow \n      value: |\n        hello\n        world\n</code></pre> JSON String Variable<pre><code>\"hello\\nworld\"\n</code></pre> <p>If the data is YAML it will be converted to JSON in the variable.</p> JSON Data<pre><code>direktiv_api: workflow/v1\n\ndescription: |\n  Store the workflow variable 'StringVar' as a json.  \n\nstates:\n#\n# Set StringVar Value: \n# \"hello\\nworld\"\n#\n- id: set-var\n  type: setter\n  variables:\n    - key: StringVar\n      scope: workflow \n      value: \n        - key: value\n</code></pre> JSON Variable<pre><code>[{\"key\":\"value\"}]\n</code></pre> <p>There are certain scenarios where you would not want to store the variable with its quotes. To do this all need to do is simply set the mimeType to <code>text/plain</code> or <code>text/plain; charset=utf-8</code>. This will store the variable as a raw string without quotes. </p> Plain Text<pre><code>direktiv_api: workflow/v1\n\ndescription: |\n  Store the workflow variable 'StringVar' as a plaintext string.  \n\nstates:\n#\n# Set StringVar Value: \n# hello\n# world\n#\n- id: set-var\n  type: setter\n  variables:\n    - key: StringVar\n      scope: workflow \n      mimeType: 'text/plain'\n      value: |\n        hello\n        world\n</code></pre>"},{"location":"examples/variable-mime-type/#variable-stringvar-value","title":"Variable - StringVar Value","text":"Plain Text Variable<pre><code>hello\nworld\n</code></pre>"},{"location":"examples/variable-mime-type/#auto-decoding-base64-string","title":"Auto-Decoding Base64 string","text":"<p>Another special behaviour is that it's also possible to auto decode a base64 string by setting the <code>mimeType</code> to <code>application/octet-stream</code>. This is used for binaries like Excel files, images etc.</p> Base64 Variable<pre><code>direktiv_api: workflow/v1\n\ndescription: |\n  Auto decode base64 string and store the resulting value \n  as the workflow variable 'MessageVar'.  \n\nstates:\n#\n# Set MessageVar Value: \n# hello from direktiv\n#\n- id: set-var\n  type: setter\n  variables:\n    - key: MessageVar\n      scope: workflow \n      value: 'aGVsbG8gZnJvbSBkaXJla3Rpdg=='\n      mimeType: 'application/octet-stream'\n</code></pre>"},{"location":"examples/variable-mime-type/#variable-messagevar-value","title":"Variable - MessageVar Value","text":"Binary Data<pre><code>hello from direktiv\n</code></pre> <p>These are the only two mime types with special behaviour. Any other <code>mimeType</code> will be treated internally by the default <code>JSON</code> behaviour. The default value for mimeType is <code>application/json</code></p>"},{"location":"examples/variables/","title":"Variable Scopes","text":"<p>Variable Scopes on Github</p> <p>Variable can be set on different scopes. Later in the flow they can be accessed within the same scope. The following scopes are available.</p> <ul> <li>instance: Only valid during the execution of the flow</li> <li>workflow: Stored as workflow variable and can be accessed from every intsance of the flow</li> <li>namespace: Namespace global scope and every workflow in the namespace can access it</li> </ul> <p>This example uses a setter state to set a variable in the <code>instance</code> scope. The second state set a workflow variable with the special output folder <code>out</code> in actions. Values can be stored in <code>out/&lt;SCOPE&gt;</code> and will be set after executing the action. The last state uses a <code>transform</code> to return the variables.</p> Set Variables<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: bash\n  image: direktiv/bash:dev\n  type: knative-workflow\n\nstates:\n\n# Sets the variable in instance scope\n- id: set-value\n  type: setter\n  variables:\n  - key: x\n    scope: instance\n    value: This is my value\n  transition: set-value-fn\n\n# Sets the variable in workflow scope with writing to the special \"out\" folder\n- id: set-value-fn\n  type: action\n  action:\n    function: bash\n    input: \n      commands:\n      - command: bash -c 'echo \\\"my fn value\\\" &gt; out/workflow/y'\n  transition: get-values\n\n# fetch values\n- id: get-values\n  type: getter\n  variables:\n  - key: x\n    scope: instance\n  - key: y\n    scope: workflow\n  transform:\n    my-x: jq(.var.x)\n    my-y: jq(.var.y)\n</code></pre>"},{"location":"gateway/","title":"Gateway","text":"<p>Direktiv offers a gateway (beta), providing access to internal resources via custom routes. The gateway primarily serves to modify or enrich incoming requests and implement authentication.  By adopting this approach, external clients can be decoupled from direct access to the internal Direktiv API but use custom routes instead.  With the additional use of plugins the consumption of flows by clients can be made easier and faster. In particular in integration scenarios Direktiv's gateway and it's routes can speed up that process.</p> <p> </p> <p>Direktiv's gateway contsist of three main components:</p> <ul> <li>Routes</li> <li>Consumers</li> <li>Plugins</li> </ul>"},{"location":"gateway/consumers/","title":"Consumers","text":"<p>Consumers are a concept to authenticate and authorise users in Direktiv. A consumer requires a unique username and a password or an API key but can have both. Additionally groups and tags can be appalied to a consumer. By default all consumers apply to routes requiring authentication. This means if the access is not limited by e.g. the ACL Plugin every authenticated consumer can access the route.</p> <p>Clear Passwords</p> <p>In the beta version of the gateway the consumer passwords and API keys are clear text but this will be changed in subsequent releases.</p> <p>Because routes and consumers can be stored at any place in the tree it is recommended to store them under one directory for one route or a group of routes.</p> Example Consumer<pre><code>direktiv_api: \"consumer/v1\"\nusername: \"demo\"\npassword: \"mypassword\"\napi_key: \"myapikey\"\ngroups:\n  - \"group1\"\ntags:\n  - \"mytags\"\n</code></pre>"},{"location":"gateway/routes/","title":"Routes","text":"<p>Routes are individual URLs defining an entry point into Direktiv. There are three basic settings for a route:</p> <ul> <li> <p><code>path</code>: The path defines the URL being used by Direktiv. Usually it will be <code>https://yourserver.com/ns/NAMESPACE/ROUTEPATH</code> where <code>NAMESPACE</code> is the namespace the route is in. If the namespace is called <code>gateway</code> the URL looks usually different: <code>/gw/ROUTEPATH</code>. The namespace is not required in this case. An additional feature of this namespace is that the gateway can address targets in other namespaces whereas routes in other namespaces can only address targets in it's own namespace. A path can be static but can also contain variables. These variables can be used in plugins or can be passed through to the flow, e.g. <code>/product/{id}</code>.</p> </li> <li> <p><code>timeout</code>: Timeout for the request in seconds.</p> </li> <li> <p><code>methods</code>: The HTTP methods this route supports. This is only for the route and is not getting used for making requests to Direktiv, e.g. a GET request to a workflow target is still a POST internally from the gateway to Direktiv.</p> </li> <li> <p><code>allow_anonymous</code>: This boolean defines if a route is accessible for unauthenticated users. Consumers can still be used but the service is still accessible without a valid consumer.</p> </li> </ul> <p>A route can have multiple plugins active to provide the required functionality. There are different types of plugins:</p> <p><code>target</code>: Unlike the other plugins only one target plugin can exist in each route. This plugin defines what is being requested in Direktiv. This can be flows, files or variables. </p> <p><code>auth</code>: Auth plugins are for authentication and are getting executed in order. Subsequent plugins will use the first successful authentication response.</p> <p><code>inbound</code>: Inbound plugins can modify the request. This can include headers, query parameters or even the body.</p> <p><code>outbound</code>: Plugins in the <code>outbound</code> section can modify the response. This is an expensive operation because every response needs to be loaded in memory for it to be modified.</p> <p>All plugins have a <code>type</code> which is the name of the plugin. They can have a <code>configuration</code> section. The content of the configuration depends on the plugin selected.</p> Example Route<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"/hello\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"/envs-wf/wf.yaml\"\n      async: false\nallow_anonymous: true\n</code></pre>"},{"location":"gateway/plugins/auth/","title":"Authentication Plugins","text":"<p>Plugins to authenticate consumers or clients.</p> <ul> <li>Basic Authentication</li> <li>Key Authentication</li> <li>Github Authentication</li> <li>Gitlab Authentication</li> <li>Slack Authentication</li> </ul>"},{"location":"gateway/plugins/auth/basic_auth/","title":"Basic Authentication","text":"<p>Adds Basic Authentication to the route. This requires at least one valid consumer in the system.</p>"},{"location":"gateway/plugins/auth/basic_auth/#configuration","title":"Configuration","text":"Value Description add_username_header Adds a <code>Direktiv-Consumer-User</code> header for authenticated user. add_tags_header Adds a <code>\"Direktiv-Consumer-Tags</code> header for authenticated user. add_groups_header Adds a <code>Direktiv-Consumer-Groups</code> header for authenticated user."},{"location":"gateway/plugins/auth/basic_auth/#example","title":"Example","text":"Basic Authentication<pre><code>direktiv_api: \"endpoint/v1\"\nallow_anonymous: false\npath: \"basicauth\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow-var\"\n    configuration:\n      flow: \"/envs-wf/wf.yaml\"\n      variable: \"hello\"\n  auth:\n    - type: \"basic-auth\"\n      configuration:\n        add_username_header: true\n        add_tags_header: false\n        add_groups_header: true\n</code></pre>"},{"location":"gateway/plugins/auth/git/","title":"Git Event Authentication","text":"<p>Using Github event webhooks require a secret which is used to check the HMAC hex digest. This can be used e.g. for a webhook for each <code>commit</code> or <code>tag</code> in Github.</p>"},{"location":"gateway/plugins/auth/git/#configuration","title":"Configuration","text":"Value Description secret Configured secret in Github."},{"location":"gateway/plugins/auth/git/#example","title":"Example","text":"Github Webhook Authentication<pre><code>direktiv_api: \"endpoint/v1\"\nallow_anonymous: false\npath: \"github-event\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow-var\"\n    configuration:\n      flow: \"/envs-wf/wf.yaml\"\n      variable: \"hello\"\n  auth:\n    - type: \"github-webhook-auth\"\n      configuration:\n        secret: \"hello123\"\n</code></pre>"},{"location":"gateway/plugins/auth/gitlab/","title":"Gitlab Event Authentication","text":"<p>Using Gitlab event webhooks require a secret which is used to check Gitlab's <code>X-Gitlab-Token</code> header. This plugin can be used e.g. for a webhook for each <code>commit</code> or <code>tag</code> in Gitlab.</p>"},{"location":"gateway/plugins/auth/gitlab/#configuration","title":"Configuration","text":"Value Description secret Configured secret in Gitlab. Will be checked agains Gitlab's header."},{"location":"gateway/plugins/auth/gitlab/#example","title":"Example","text":"Gitlab Webhook Authentication<pre><code>direktiv_api: endpoint/v1\nallow_anonymous: false\nplugins:\n  target:\n    type: target-flow\n    configuration:\n        flow: /target.yaml\n        content_type: application/json\n  auth:\n    - type: gitlab-webhook-auth\n      configuration:\n        secret: secretmysecret\nmethods: \n  - POST\npath: /target\n</code></pre>"},{"location":"gateway/plugins/auth/key/","title":"Key Authentication","text":"<p>Adds API key Authentication to the route. This requires at least one valid consumer in the system.</p>"},{"location":"gateway/plugins/auth/key/#configuration","title":"Configuration","text":"Value Description add_username_header Adds a <code>Direktiv-Consumer-User</code> header for authenticated user. add_tags_header Adds a <code>\"Direktiv-Consumer-Tags</code> header for authenticated user. add_groups_header Adds a <code>Direktiv-Consumer-Groups</code> header for authenticated user. key_name Name of the header for this API key."},{"location":"gateway/plugins/auth/key/#example","title":"Example","text":"Key Authentication<pre><code>direktiv_api: \"endpoint/v1\"\nallow_anonymous: false\npath: \"keyauth\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow-var\"\n    configuration:\n      flow: \"/envs-wf/wf.yaml\"\n      variable: \"hello\"\n  auth:\n    - type: \"key-auth\"\n      configuration:\n        add_username_header: false\n        add_tags_header: false\n        add_groups_header: false\n        key_name: \"myapikey\"\n</code></pre>"},{"location":"gateway/plugins/auth/slack/","title":"Slack Event Authentication","text":"<p>Slack can be configured to send events on certain actions, e.g. a file has been added or a message has been posted. Another feature of Slack are the <code>slash commands</code>.  In both cases a message is getting posted to a confiurable URL. This plugin authenticates the request with the signing secret of the application and in case of a form-encoded request, e.g. <code>slash commands</code>, converts the payload to JSON. </p>"},{"location":"gateway/plugins/auth/slack/#configuration","title":"Configuration","text":"Value Description secret Signing Secret for the Slack application. Found in <code>Basic Information</code> of the Slack application."},{"location":"gateway/plugins/auth/slack/#example","title":"Example","text":"Slack Webhook Authentication with Signing Secret<pre><code>direktiv_api: endpoint/v1\n\nallow_anonymous: false\nplugins:\n  target:\n    type: target-flow\n    configuration:\n        flow: /target.yaml\n        content_type: application/json\n  auth:\n    - type: slack-webhook-auth\n      configuration:\n        secret: 123MySigningSecret456\nmethods: \n  - POST\npath: /target\n</code></pre>"},{"location":"gateway/plugins/auth/slack/#example-slash-command","title":"Example Slash Command","text":"<p>The following is a simple example of a slash command. The flow fetches the <code>respond_url</code> from the initial request and calls a subflow with the URL as parameter. Because a <code>slash command</code> has to respond immediately the subflow is called with <code>async: true</code>. </p> Example Flow with Response<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: response\n  type: subflow\n  workflow: respond.yaml\n\nstates:\n- id: start\n  type: noop\n  transform:\n    url: jq(.response_url[0])\n  transition: respond\n- id: respond\n  type: action\n  async: true\n  action:i\n\n```yaml title=\"Post Message to Slack to Response URL\"\ndirektiv_api: workflow/v1\n\nfunctions:\n- id: get\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\nstates:\n- id: getter \n  type: action\n  log: Requesting jq(.url)\n  action:\n    function: get\n    input: \n      debug: true\n      method: \"POST\"\n      url: jq(.url)\n      content:\n        value:\n          text: Hello \n</code></pre>"},{"location":"gateway/plugins/inbound/","title":"Inbound Plugins","text":"<p>Inbound plugins executed in order after authentication plugins. </p> <ul> <li>ACL</li> <li>Javascript Inbound</li> <li>Request Converter</li> <li>Modify Headers</li> </ul>"},{"location":"gateway/plugins/inbound/acl/","title":"Access Control List","text":"<p>If a route requires authentication all valid consumers have access to the route. This plugin can limit the consumers to certain tags or groups.</p>"},{"location":"gateway/plugins/inbound/acl/#configuration","title":"Configuration","text":"Value Description allow_groups Whitelist groups. deny_groups Blacklist groups. allow_tags Whitelist tags. deny_tags Blacklist tags."},{"location":"gateway/plugins/inbound/acl/#example","title":"Example","text":"ACL Example<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"/consumer\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"/gw/wf1.yaml\"\n      async: false\n  inbound:\n    - type: \"acl\"\n      configuration:\n        allow_groups:\n          - \"group1\"\n          - \"group2\"\n        allow_tags:\n          - \"tag1\"\n  auth:\n    - type: \"key-auth\"\n      configuration:\n        add_username_header: false\n        add_tags_header: false\n        add_groups_header: false\n        key_name: \"mykey\"\n</code></pre>"},{"location":"gateway/plugins/inbound/headers/","title":"Modify Headers","text":"<p>This plugin can add, modify or remove headers fro the request coming in.</p>"},{"location":"gateway/plugins/inbound/headers/#configuration","title":"Configuration","text":"Value Description headers_to_add Name/Value pairs of headers to add. headers_to_modify Name/Value pairs of headers to set or modify. headers_to_remove Names of headers to remove."},{"location":"gateway/plugins/inbound/headers/#example","title":"Example","text":"Header Example<pre><code>direktiv_api: endpoint/v1\nallow_anonymous: true\nplugins:\n  target:\n    type: target-flow\n    configuration:\n        flow: /target.yaml\n        content_type: application/json\n  inbound:\n    - type: header-manipulation\n      configuration:\n        headers_to_add:\n        - name: hello\n          value: world\n        headers_to_modify: \n        - name: header1\n          value: newvalue\n        headers_to_remove:\n          - name: header \n    - type: \"request-convert\"\n      configuration:\n        omit_headers: false\n        omit_queries: true\n        omit_body: true\n        omit_consumer: true\nmethods: \n  - POST\npath: /target\n</code></pre>"},{"location":"gateway/plugins/inbound/js/","title":"Javascript Manipulation","text":"<p>The Javascript plugin receives the request as an object. This object can be manipulated with the script. The object will be used as a new request in subsequent plugins or send to a flow. </p> <p>The <code>input</code> object contains <code>Headers</code>, <code>Queries</code>, <code>Body</code>, <code>Consumer</code> and <code>URLParams</code> (Parameters like <code>/{id}</code> in the route path). They can be addressed with the Javascript script in the plugin. </p> Javascript Request Access<pre><code># Delete Header\ninput[\"Headers\"].Delete(\"Header1\")\n\n# Add Header\ninput[\"Queries\"].Add(\"new\", \"param\")\n\n# Modify Body\nb = JSON.parse(input[\"Body\"])\nb[\"newvalue\"] = 1200\ninput[\"Body\"] = JSON.stringify(b) \n</code></pre>"},{"location":"gateway/plugins/inbound/js/#configuration","title":"Configuration","text":"Value Description script Javascript to execute."},{"location":"gateway/plugins/inbound/js/#example","title":"Example","text":"Javascript Example<pre><code>direktiv_api: \"endpoint/v1\"\nallow_anonymous: true\npath: \"/js\"\nmethods:\n  - \"GET\"\nplugins:\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"/js-plugin/wf.yaml\"\n      async: false\n  inbound:\n    - type: \"js-inbound\"\n      configuration:\n        script: |\n          input[\"Body\"] = JSON.stringify(\"HELLO\")  \n</code></pre>"},{"location":"gateway/plugins/inbound/reconv/","title":"Request Converter","text":"<p>The converter takes the request and creates an object out of it. This object can be used in subsequent plugins or within a flow. </p> <p>The request converter translates URL parameters as well. In case a parameter is used in the path, e.g. <code>/mypath/{id}</code> it will be part of the  converted request like the following:</p> URL Parameters<pre><code>...\n\"url_params\": {\n    \"id\": \"hello\"\n}\n...\n</code></pre>"},{"location":"gateway/plugins/inbound/reconv/#configuration","title":"Configuration","text":"Value Description omit_headers Don't convert HTTP headers of the request. omit_queries Don't convert URL query parameters in the request. omit_body Don't convert the body of the request. omit_consumer Don't convert the consumer of the request. Only set if route and user is authenticated."},{"location":"gateway/plugins/inbound/reconv/#example","title":"Example","text":"Request Converter Example<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"/convert/{id}\"\nmethods:\n  - \"GET\"\nallow_anonymous: true\nplugins:\n  inbound:\n    - type: \"request-convert\"\n      configuration:\n        omit_headers: false\n        omit_queries: false\n        omit_body: true\n        omit_consumer: false\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"wf.yaml\"\n      async: false\n</code></pre>"},{"location":"gateway/plugins/outbound/","title":"Outbound Plugins","text":"<p>Outbound plugins executed in order after returning from the target plugin in Direktiv.</p> <ul> <li>Javascript Outbound</li> </ul>"},{"location":"gateway/plugins/outbound/js/","title":"Javascript Manipulation (Outbound)","text":"<p>The Javascript plugin receives the response from Direktiv as an object. This object can be manipulated with the script. The object will be used as a new response in subsequent plugins or send to the client. </p> <p>The <code>input</code> object contains <code>Headers</code>, <code>Code</code> and <code>Body</code> and they can be addressed with the Javascript script in the plugin. </p> Javascript Outbound Access<pre><code># Add Header\ninput[\"Headers\"].Add(\"new\", \"param\")\n\n# Modify Body\nb = JSON.parse(input[\"Body\"])\nb[\"newvalue\"] = \"hello world\"\ninput[\"Body\"] = JSON.stringify(b) \n\n# Change Response Code\ninput[\"Code\"] = 201\n</code></pre>"},{"location":"gateway/plugins/outbound/js/#configuration","title":"Configuration","text":"Value Description script Javascript to execute."},{"location":"gateway/plugins/outbound/js/#example","title":"Example","text":"Javascript Example<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"convert\"\nmethods:\n  - \"GET\"\nallow_anonymous: true\nplugins:\n  inbound: []\n  outbound:\n    - type: \"js-outbound\"\n      configuration:\n        script: |\n            input[\"Code\"] = 201\n            input[\"Headers\"].Add(\"new\", \"param\")\n            b = JSON.parse(input[\"Body\"])\n            b[\"newvalue\"] = \"hello world\"\n            input[\"Body\"] = JSON.stringify(b) \n  auth: []\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"wf.yaml\"\n      async: false\n</code></pre>"},{"location":"gateway/plugins/target/","title":"Target Plugins","text":"<p>A target plugin configures the target the route is using. A target can e.g. execute a flow or return workflow variables. </p> <ul> <li>Instant Response</li> <li>Flow</li> <li>Namespace File</li> <li>Namespace Variable</li> <li>Workflow Variable</li> </ul>"},{"location":"gateway/plugins/target/flow/","title":"Flow Target","text":"<p>Executes a flow in Direktiv.</p>"},{"location":"gateway/plugins/target/flow/#configuration","title":"Configuration","text":"Value Description namespace Only configurable in the <code>gateway</code> namespace. In all other namespaces it can only call flows within that namespace. flow Path to flow, e.g. <code>/gw/wf1.yaml</code> async If true, the flow is getting executed and the request returns without waiting content_type If the flow returns another content-type that JSON"},{"location":"gateway/plugins/target/flow/#example","title":"Example","text":"Flow Target<pre><code>direktiv_api: \"endpoint/v1\"\nallow_anonymous: true\npath: \"/flow-target\"\nmethods:\n  - \"GET\"\n  - \"PUT\"\nplugins:\n  target:\n    type: \"target-flow\"\n    configuration:\n      flow: \"/envs-wf/wf.yaml\"\n      async: false\n</code></pre>"},{"location":"gateway/plugins/target/instant/","title":"Instant Response","text":"<p>Instantly responds tp the request.</p>"},{"location":"gateway/plugins/target/instant/#configuration","title":"Configuration","text":"Value Description status_message String value for the response . status_code The HTTP code to return content_type The value of the Content-Type header"},{"location":"gateway/plugins/target/instant/#example","title":"Example","text":"Instant Target<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"/instant\"\nmethods:\n  - \"GET\"\n  - \"HEAD\"\nallow_anonymous: true\nplugins:\n  target:\n    type: \"instant-response\"\n    configuration:\n      content_type: \"application/json\"\n      status_code: 201\n      status_message: \"{ \\\"hello\\\": \\\"world\\\" }\"\n</code></pre>"},{"location":"gateway/plugins/target/ns-file/","title":"Namespace File","text":"<p>Returns a file in the filesystem tree in Direktiv.</p>"},{"location":"gateway/plugins/target/ns-file/#configuration","title":"Configuration","text":"Value Description namespace Only configurable in the <code>gateway</code> namespace. In all other namespaces it can only call flows within that namespace. file Path to the file content_type The value of the Content-Type header"},{"location":"gateway/plugins/target/ns-file/#example","title":"Example","text":"File Target<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"/file\"\nmethods:\n  - \"GET\"\nallow_anonymous: true\nplugins:\n  target:\n    type: \"target-namespace-file\"\n    configuration:\n      file: \"/aws/README.md\"\n      content_type: \"text/markdown\"\n</code></pre>"},{"location":"gateway/plugins/target/ns-var/","title":"Namespace Variable","text":"<p>Returns a namespace-scoped variable in Direktiv.</p>"},{"location":"gateway/plugins/target/ns-var/#configuration","title":"Configuration","text":"Value Description namespace Only configurable in the <code>gateway</code> namespace. In all other namespaces it can only call flows within that namespace. variable Name of the variable. Returns an empty body if not found. content_type The value of the Content-Type header"},{"location":"gateway/plugins/target/ns-var/#example","title":"Example","text":"Namespace Variable Target<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"nsvar\"\nmethods:\n  - \"GET\"\nallow_anonymous: true\nplugins:\n  target:\n    type: \"target-namespace-var\"\n    configuration:\n      variable: \"hello\"\n      content_type: \"plain/text\"\n</code></pre>"},{"location":"gateway/plugins/target/wf-var/","title":"Flow Variable","text":"<p>Returns a workflow-scoped variable in Direktiv.</p>"},{"location":"gateway/plugins/target/wf-var/#configuration","title":"Configuration","text":"Value Description namespace Only configurable in the <code>gateway</code> namespace. In all other namespaces it can only call flows within that namespace. flow Name of the flow the variable is attached to. variable Name of the variable. Returns an empty body if not found. content_type The value of the Content-Type header."},{"location":"gateway/plugins/target/wf-var/#example","title":"Example","text":"Flow Variable Target<pre><code>direktiv_api: \"endpoint/v1\"\npath: \"wfvar\"\nmethods:\n  - \"GET\"\nallow_anonymous: true\nplugins:\n  target:\n    type: \"target-flow-var\"\n    configuration:\n      flow: \"/envs-wf/wf.yaml\"\n      variable: \"hello\"\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>If you're beginning your journey with Direktiv, there are two easy ways to do so. You can opt for a full installation or take the simpler route and use a Docker container that is fully equipped with everything required to get started - including a nested Kubernetes instance - or by utilizing a Multipass virtual machine setup.</p>"},{"location":"getting_started/#docker-linux-only","title":"Docker (Linux only)","text":"<p>The Docker image works on Linux only and can be used for easy development of flows on a local machine.</p> Running Docker Image<pre><code>docker run --privileged -p 8080:80 -ti direktiv/direktiv-kube\n</code></pre>"},{"location":"getting_started/#multipass-linux-mac-windows","title":"Multipass (Linux, Mac, Windows)","text":"<p>For Windows and Mac users in particular there is a Multipass cloud-init script to set up a Direktiv instance for testing and development.</p> Running Multipass<pre><code>multipass launch --cpus 4 --disk 20G --memory 6G --name direktiv --cloud-init https://raw.githubusercontent.com/direktiv/direktiv/main/scripts/build/docker/all/multipass/init.yaml\n</code></pre> <p>Warning</p> <p>multipass does not work in a VPN. The VPN needs to be turned off for this example installation.</p> <p>The development section has more details how to configure these instances and how to use them.</p>"},{"location":"getting_started/conditional-transitions/","title":"Conditional Transitions","text":"<p>Oftentimes a flow needs to be a little bit smarter than an immutable sequence of states. That's when conditional transitions are required. For these cases Direktiv provides a <code>switch</code> state which can route the flow based on conditions. Each condition can route the flow to a different state but there can be a <code>defaultTransition</code> to transition to if none of the conditions are true. </p> Loop Demo<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: httprequest\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\nstates:\n- id: ifelse\n  type: switch\n  defaultTransition: done\n  conditions:\n  - condition: jq(.names)\n    transition: poster\n- id: poster\n  type: action\n  action:\n    function: httprequest\n    input: \n      method: POST\n      url: https://jsonplaceholder.typicode.com/posts\n      content: \n        name: jq(.names[0])\n  transform: jq(del(.names[0]))\n  transition: ifelse\n- id: done\n  type: noop\n  transform: done\n</code></pre> Input<pre><code>{\n  \"names\": [\n    \"Michael\",\n    \"Thomas\",\n    \"Kevin\"\n  ]\n}\n</code></pre> Output<pre><code>{\n  \"done\": \"yes\"\n}\n</code></pre> <p>In this example the switch state will transition to <code>poster</code> until the list of names is empty, at which point the flow will transition to the default transition <code>done</code>.</p>"},{"location":"getting_started/conditional-transitions/#switch-state","title":"Switch State","text":"<p>The Switch State can make decisions about where to transition to next based on the instance data by evaluating a number of <code>jq</code> expressions and checking the results. Here's an example switch state definition:</p> <pre><code>- id: ifelse\n  type: switch\n  conditions:\n  - condition: 'jq(.person.age &gt; 18)'\n    transition: accept\n    #transform:\n  - condition: 'jq(.person.age != nil)'\n    transition: reject\n    #transform:\n  defaultTransition: failure\n  #defaultTransform:\n</code></pre> <p>Each of the <code>conditions</code> will be evaluated in the order it appears by running the <code>jq</code> command in <code>condition</code>. Any result other than <code>null</code>, <code>false</code>, <code>{}</code>, <code>[]</code>, <code>\"\"</code>, or <code>0</code> will cause the condition to be considered a successful match. If no conditions match the default transition will be used.</p>"},{"location":"getting_started/conditional-transitions/#other-conditional-transitions","title":"Other Conditional Transitions","text":"<p>The Switch State is not the only way to do conditional transitions. The eventsXor state also transitions conditionally based on which CloudEvent was received. All states can also define handlers for catching various types of errors.</p>"},{"location":"getting_started/conditional-transitions/#loops","title":"Loops","text":"<p>By transitioning to a state that has already happened it's possible to create loops in flow instances. In this example we have got a type of range loop, iterating over the contents of an array. Direktiv sets limits for the number of transitions an instance can make in order to protect itself from infinitely-looping flows. This is an example only and in this case a foreach is a better solution.</p>"},{"location":"getting_started/error-handling/","title":"Error Handling","text":"<p>One obvious use for loops is to retry some logic if an error occurs, but there's no need to design looping flow because Direktiv has configurable error catching &amp; retrying available on every action-based state. This will be discussed in a later article.</p> <p>Handling errors can be an important part of a flow.</p>"},{"location":"getting_started/error-handling/#demo","title":"Demo","text":"<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: http-request\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\nstates:\n- id: do-request\n  type: action\n  action:\n    function: http-request\n    input:\n      url: http://doesnotexist.xy\n    retries:\n      max_attempts: 2\n      delay: PT5S\n      multiplier: 2.0\n      codes: [\".*\"]\n</code></pre> <p>In this example a request is being made to an URL. This URL does not exist to simulate the retry mechanism. It uses the multiplier to try within 5 seconds the first time and 10 seconds the second time.</p>"},{"location":"getting_started/error-handling/#catchable-errors","title":"Catchable Errors","text":"<p>Errors that occur during instance execution usually are considered \"catchable\". Any flow state may optionally define error catchers, and if a catchable error is raised Direktiv will check to see if any catchers can handle it.</p> <p>Errors have a \"code\", which is a string formatted in a style similar to a domain name. Error catchers can explicitly catch a single error code or they can use <code>*</code> wildcards in their error codes to catch ranges of errors. Setting the error catcher to just \"<code>*</code>\" means it will handle any error, so long as no catcher defined higher up in the list has already caught it.</p> <p>If no catcher is able to handle an error, the flow will fail immediately.</p> <pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: http-request\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\nstates:\n- id: do-request\n  type: action\n  action:\n    function: http-request\n    input:\n      url: http://doesnotexist.xy\n    retries:\n      max_attempts: 2\n      delay: PT5S\n      multiplier: 2.0\n      codes: [\".*\"]\n  catch:\n  - error: \"direktiv.retries.exceeded\"\n    transition: handle-error\n- id: handle-error\n  type: noop\n  log: this did not work\n</code></pre> <p>In this case the flow catches the failed retries and transitions to <code>handle-error</code> and the flow finished successful. Every other error will mark the flow execution as failed.</p>"},{"location":"getting_started/error-handling/#uncatchable-errors","title":"Uncatchable Errors","text":"<p>Rarely, some errors are considered \"uncatchable\", but generally an uncatchable error becomes catchable if escalated to a calling flow. One example of this is the error triggered by Direktiv if a flow fails to complete within its maximum timeout.</p> <p>If a flow fails to complete within its maximum timeout it will not be given an opportunity to catch the error and continue running. But if that flow is running as a subflow its parentflow will be able to detect and handle that error.</p>"},{"location":"getting_started/error-handling/#retries","title":"Retries","text":"<p>Action definitions may optionally define a retry strategy. If a retry strategy is defined the catcher's transition won't be used and no error will be escalated for retryable errors until all retries have failed. A retry strategy might look like the following:</p> <pre><code>    retry:\n      max_attempts: 3\n      delay: PT30S\n      multiplier: 2.0\n      codes: [\".*\"]\n</code></pre> <p>In this example you can see that a maximum number of attempts is defined, alongside an initial delay between attempts and a multiplication factor to apply to the delay between subsequent attempts.</p>"},{"location":"getting_started/error-handling/#recovery","title":"Recovery","text":"<p>Flows sometimes perform actions which may need to be reverted or undone if the flow as a whole cannot complete successfully. Solving these problems requires careful use of error catchers and transitions.</p>"},{"location":"getting_started/error-handling/#cause-errors","title":"Cause Errors","text":"<p>Sometimes it is important to fail the flow with a custom error. This is possible with the <code>error</code> state. This can used e.g. in switch states.</p> <pre><code>direktiv_api: workflow/v1\n\nstates:\n- id: a\n  type: switch\n  defaultTransition: fail\n  conditions:\n  - condition: 'jq(.y == true)'\n\n- id: fail\n  type: error\n  error: badinput\n  message: 'value y not set'\n</code></pre> <p>In this example if the payload does not contain <code>y: true</code> the flow fails. The error throwns <code>badinput</code> is thrown and the flow failed. The error <code>badinput</code> could be caught by a parent flow.</p>"},{"location":"getting_started/events/","title":"Events","text":"<p>Direktiv has built-in support for CloudEvents, which can be a great way to interact with flows. The following flow has a start condition based on events. It would only start if an event of type <code>com.github.pull.create</code> arrives with the source set to <code>https://github.com/cloudevents/spec/pull</code></p> Example Workflow<pre><code>direktiv_api: workflow/v1\nstart:\n  type: event\n  event:\n    type: com.github.pull.create\n    context:\n      source: https://github.com/cloudevents/spec/pull\nfunctions:\n- id: httprequest\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\nstates:\n- id: notify\n  type: action\n  action:\n    function: httprequest\n    input:\n      method: \"POST\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n      body: 'jq(.\"com.github.pull.create\")'\n</code></pre>"},{"location":"getting_started/events/#cloudevents","title":"CloudEvents","text":"<p>CloudEvents are specification for describing event data in a common way. They're JSON objects with a number of required fields, some optional fields, and a payload.</p> Sample Cloudevent<pre><code>{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull.create\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n}\n</code></pre> <p>CloudEvents can be sent via the API to a namespace or generated by flow within Direktiv, to be handled by any number of interested receivers on that namespace.</p>"},{"location":"getting_started/events/#start-types","title":"Start Types","text":"<p>The most common use for events in Direktiv is to have external services generate CloudEvents and send them to Direktiv to trigger your flows. But to make your flows trigger on an event you need to register the flow's interest in the event by adding the appropriate start type to your workflow definition:</p> <pre><code>direktiv_api: workflow/v1\nstart:\n  type: event\n  event:\n    type: com.github.pull.create\n    filters:\n      source: \"https://github.com/cloudevents/spec/pull\"\n</code></pre> <p>In this example a new instance will be created whenever a cloudevent is received that has the matching <code>type</code> and <code>source</code> values.</p> <p>Two other event-based start types exist in Direktiv: the <code>eventsXor</code>, and the <code>eventsAnd</code>.</p> <p>The <code>eventsXor</code> registers an interest in multiple events and will trigger a new instance as soon as any one of them is received. The <code>eventsAnd</code> also registers an interest in multiple events, but will only trigger once all have been received.</p>"},{"location":"getting_started/events/#event-payloads","title":"Event Payloads","text":"<p>Whenever an event is received its payload will be added to the instance data under a field with the same name as the event \"type\". This allows for a uniform approach to accepting events that supports single events, eventsXor, and eventsAnd. The payload itself consists of the full cloudevent including attributes, extension context attributes and data.</p>"},{"location":"getting_started/events/#instances-waiting-for-events","title":"Instances Waiting for Events","text":"<p>Triggering flows is not the only thing you can do with events. Flows can be constructed to run some logic and then wait for an event before proceeding. Like the event-based start types, there are three event consuming states: <code>consumeEvent</code>, <code>eventsXor</code>, and <code>eventsAnd</code>. </p> Waiting Within A Flow<pre><code>- id: wait-event\n  type: consumeEvent\n  event:\n    type: com.github.pull.create\n    context:\n      source: \"https://github.com/cloudevents/spec/pull\"\n      repository: 'jq(.repo)'\n  timeout: PT5M\n  transform: 'jq(.\"com.github.pull.create\")'\n  transition: next-state\n</code></pre>"},{"location":"getting_started/events/#timeouts","title":"Timeouts","text":"<p>It's rarely a good idea to leave a flow waiting indefinitely. Direktiv allows you to define timeouts in ISO8601 format when waiting on an event. If the state is not ready to proceed before the timeout has elapsed an error will be thrown. It's possible to catch the error <code>direktiv.cancels.timeout.soft</code>.</p> <p>The <code>timeout</code> field is not required, but Direktiv caps the maximum timeout whether specified or not to prevent flows from living forever. The default timeout is 15 minutes.</p>"},{"location":"getting_started/events/#context","title":"Context","text":"<p>Event-consuming states have a <code>context</code> field. The context field can restrict which events are considered matches by requiring an exact match on a CloudEvent context field. This can be used to link certain events to e.g. customer ids or transaction ids.</p>"},{"location":"getting_started/events/#generateevent-state","title":"GenerateEvent State","text":"<p>Flows can generate events for their namespace. The fields for this state are fairly self-explanatory. Here's an example:</p> <pre><code>- id: gen-event\n  type: generateEvent\n  event:\n    type: \"my.custom.event\"\n    source: \"direktiv\"\n    data: 'jq(.)'\n    datacontenttype: \"application/json\"\n</code></pre> <p>If the <code>jq</code> command that populates the <code>data</code> field outputs a plain base64 encoded string and the <code>datacontenttype</code> field is set to anything other than <code>application/json</code> Direktiv will decode the string before sending the event.</p>"},{"location":"getting_started/functions-intro/","title":"Introduction to Functions","text":"<p>Flows wouldn't be very powerful if they were limited to just the predefined states. That's why Direktiv can run \"functions\" which are basically serverless containers or even a separate flow, referred to as a <code>subflow</code>. Direktiv uses the <code>action</code> state to provide this functionality.</p>"},{"location":"getting_started/functions-intro/#example","title":"Example","text":"<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: http-request\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\n\nstates:\n- id: getter\n  type: action\n  action:\n    function: http-request\n    input:\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n</code></pre> <p>If the flow requires function to run they need to be defined in the <code>functions</code> section. There are different types of functions and the type is specified in the <code>type</code> attribute. The value can be one of the following:</p>"},{"location":"getting_started/functions-intro/#function-types","title":"Function Types","text":""},{"location":"getting_started/functions-intro/#knative-workflow","title":"knative-workflow","text":"<p>This function is for the flow only and will not be re-used across different flows. This type requires an image name to use. The image name should point to a valid container image in a remote registry like Docker Hub, GCR, Azure etc. Two additional attributes can be provided:</p> <p>size: Sometimes functions need a different size in terms of CPU and memory. Possible values are <code>small</code>, <code>medium</code> and <code>large</code>. The definition of those values can be configured in Direktiv's configuration files via Helm chart. </p> <p>cmd: The function can use a different command in the container if it is supported by the function container. </p>"},{"location":"getting_started/functions-intro/#knative-namespace","title":"knative-namespace","text":"<p>If a function is used frequently by different flows it can be shared across flows with this type. They can be created under <code>Services</code> in the user interface or via API. </p> <pre><code>- id: http-request\n  service: request\n  type: knative-namespace\n</code></pre> <p>For these types a <code>scale</code> attribute can be defined on creation of the service which sets the minimum instances to run in the cluster and therefore reducing or eliminating the warm-up time and the function can run immediately. </p>"},{"location":"getting_started/functions-intro/#subflow","title":"subflow","text":"<p>In Direktiv a function can be a subflows as well. The behaviour is the same as calling serverless container functions. If used the flow provides the input for the subflow and accepts the response of the subflow as result. </p> <pre><code>- id: http-request\n  workflow: my-subflow\n  type: subflow\n</code></pre>"},{"location":"getting_started/functions-intro/#input-value","title":"Input Value","text":"<p>The input value for the function is set in <code>input</code> in. This YAML object under <code>input</code> will be send as JSON to the function container and can a multi-level nested object as well. Different containers require different inputs depending on their functionality. This concept is important for custom functions. </p> <pre><code>- id: getter\n  type: action\n  action:\n    function: http-request\n    input:\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n</code></pre>"},{"location":"getting_started/functions-intro/#return-value","title":"Return Value","text":"<p>Every time a function is called the response is stored in <code>return</code> in the state data and can be processed via e.g. <code>transform</code> or <code>switch</code>. The next function call overwrites the <code>return</code> value so if data is required from a function accross multiple states it needs to be stored with a transition. </p> <p>In the example above the state data after executing the flow would have an additional JSON object with information about the headers and the content of the HTTP request in the <code>return</code> attribute.</p> <pre><code>{\n  \"return\": [\n    {\n      \"code\": 200,\n      \"headers\": {\n        \"Access-Control-Allow-Credentials\": [\n          \"true\"\n        ],\n        \"Age\": [\n          \"20706\"\n        ]\n      },\n      \"result\": {\n        \"completed\": false,\n        \"id\": 1,\n        \"title\": \"delectus aut autem\",\n        \"userId\": 1\n      },\n      \"status\": \"200 OK\",\n      \"success\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"getting_started/functions-intro/#store-value","title":"Store Value","text":"<p>As mentioned earlier, every return of a function is getting overwritten with the next function call. Therefore it is important to store the data in the state if it is needed later in the flow. This can be done with a simple transform at the end of the action state. In the example here we store only the response status.</p> <pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: http-request\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\n\nstates:\n- id: getter\n  type: action\n  action:\n    function: http-request\n    input:\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n  transform:\n    status: jq(.return[0].status)\n</code></pre> <p>The http request function returns an array so the JQ command would be <code>.return[0]</code> to get the 0th item from it and the <code>.status</code> fetches the status of that item. More function can be found at apps.direktiv.io.</p>"},{"location":"getting_started/functions-intro/#foreach","title":"Foreach","text":"<p>Another way to call functions is the <code>foreach</code> function. This is useful if an array of objects need to be processed the same way, e.g. executing multiple http requests. </p> <pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: http-request\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\n\nstates:\n- id: getter\n  type: foreach\n  array: |-\n    jq(\n      [ \n        { \"url\": \"https://jsonplaceholder.typicode.com/todos/1\"}, \n        { \"url\": \"https://www.direktiv.io\"}]\n    )\n  action:\n    function: http-request\n    input:\n      url: jq(.url)\n</code></pre> <p>This <code>foreach</code> call the same function but uses an array of objects. There are a few simple requirements for <code>foreach</code> states.</p> <ul> <li>The array has to be a list of objects not e.g. an array of strings.</li> <li>The <code>input</code> attribute has only access to the object it is iterating over at that time. It does not have access to state data at all. </li> </ul>"},{"location":"getting_started/functions-intro/#parallel","title":"Parallel","text":"<p>The parallel execution can be used if the flow needs to execute functions in parallel with the same state data. An example would be quality gates during a release process where functional tests and load test can potentially be run in parallel. </p> <pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: http-request\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\n- id: python\n  image: gcr.io/direktiv/functions/python:1.0\n  type: knative-workflow\n\nstates:\n- id: execute-both\n  type: parallel\n  mode: and\n  actions:\n  - function: http-request\n    input: \n      url: https://www.direktiv.io\n  - function: python\n    input:\n      commands:\n      - command: python3 -c 'import os;print(os.environ[\"hello\"])'\n        envs: \n        - name: hello\n          value: world\n</code></pre> <p>Additionally a <code>mode</code> attribute can be set to either <code>or</code> or <code>and</code> to define if all actions need to return successfully or only one. </p>"},{"location":"getting_started/namespaces/","title":"Namespaces","text":"<p>Direktiv namespaces allow you the flexibility to divide projects, teams or use-cases. These spaces are totally seperate and independent of each other in terms of e.g. flows, secrets and services. You can easily create a namespace using the user interface or through an API call.</p> <p>Namespaces come in two different types. The <code>standard</code> version only stores data in Direktiv, while the <code>mirror</code> namespaces use Git as their source of truth for configuration and flows. It is recommended to use Git-backed namespaces for projects but for this guide a <code>standard</code> namespace will suffice.</p>"},{"location":"getting_started/namespaces/#create-standard-namespace","title":"Create Standard Namespace","text":"<pre><code>curl -X PUT \"http://localhost:8080/api/namespaces/demo\"\n</code></pre> <p>Response <pre><code>{\n  \"namespace\":  {\n    \"createdAt\":  \"2023-02-23T08:47:05.490124153Z\",\n    \"updatedAt\":  \"2023-02-23T08:47:05.490124801Z\",\n    \"name\":  \"demo\",\n    \"oid\":  \"\"\n  }\n}\n</code></pre></p> <p>Server Name</p> <p>Please adjust the server name to your environment if you are not using the all-in-one image for this \"Getting Started\" guide.</p>"},{"location":"getting_started/namespaces/#create-mirror-git-namespace","title":"Create Mirror (Git) Namespace","text":"<p>To create a Git namespace Direktiv requires at least the two attributes <code>url</code> and <code>ref</code>. The <code>ref</code> value is the tag, branch or commit to use as the base whereas the <code>url</code> points to the Git repository to use. If there are only those two attributes provided the access to the repository needs to be <code>public</code>.  </p> <p>Public Git <pre><code>curl -X PUT http://localhost:8080/api/namespaces/demo \\\n--data-binary @- &lt;&lt; EOF\n{ \n    \"url\": \"https://github.com/direktiv/direktiv-examples.git\", \n    \"ref\": \"main\" \n}\nEOF\n</code></pre></p> <p>If it is a <code>private</code> repository Direktiv requires either <code>passphrase</code>, which can be Github or Gitlab token or a <code>publicKey</code>/<code>privateKey</code> combination where the public key is registered with the Git instance. </p> <p>Private Git with Token <pre><code>curl -X PUT http://localhost:8080/api/namespaces/demo \\\n--data-binary @- &lt;&lt; EOF\n{ \n    \"url\": \"https://github.com/direktiv/direktiv-examples.git\", \n    \"ref\": \"main\",\n    \"passphrase\": \"abhsh2763gshs\"\n}\nEOF\n</code></pre></p> <p>GitLab Passphrases</p> <p>GitLab requires a username for the token. The username needs to be prepended like <code>username:glpat-152zshj2756</code></p>"},{"location":"getting_started/namespaces/#delete-namespace","title":"Delete Namespace","text":"<p>Delting a namepsace with the API is very simple. The command requires the <code>recursive</code> attribute if there is already content in the namespace.</p> <pre><code>curl -X DELETE http://localhost:8080/api/namespaces/demo?recursive=true\n</code></pre>"},{"location":"getting_started/persistent-data/","title":"Persistent Data","text":"<p>Direktiv supports storing and retrieving data that is persisted beyond the scope of a single state or flow instance. This article shows how to store and retrieve these variables. </p>"},{"location":"getting_started/persistent-data/#demo","title":"Demo","text":"<pre><code>direktiv_api: workflow/v1\nstates:\n- id: a\n  type: getter\n  variables:\n  - key: x\n    scope: workflow\n  transform: 'jq(.var.x += 1)'\n  transition: b\n- id: b\n  type: setter\n  variables:\n  - key: x\n    scope: workflow\n    value: 'jq(.var.x)'\n</code></pre> <p>This demo increments a counter each time the flow is executed. It gets the variable <code>x</code> from <code>workflow</code> scope and increments it vi <code>jq</code>. The secons state stores the data in the same variable on the same scope.</p>"},{"location":"getting_started/persistent-data/#scopes","title":"Scopes","text":"<p>There are three scopes for storing persistent data: <code>instance</code>, <code>workflow</code>, and <code>namespace</code>.</p> <p>Data stored in the <code>instance</code> scope only exists for the duration of the running flow instance.</p> <p>Data stored in the <code>workflow</code> scope exists until the flow definition is deleted, and is accessible to all instances of that flow.</p> <p>Data stored in the <code>namespace</code> scope exists until the namespace itself is deleted, and is accessible to all instances of all flows originating on that namespace.</p>"},{"location":"getting_started/persistent-data/#setter-state","title":"Setter State","text":"<p>The Setter State can be used to store any number of variables. Each variable must be explicitly scoped, and the value stored for a variable is generated by the output of a <code>jq</code> query.</p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: a\n  type: setter\n  variables:\n  - key: MyVar\n    scope: namespace\n    value: 'Hello'\n</code></pre> <p>The only way to delete a stored value is to set it to <code>null</code>.</p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: a\n  type: setter\n  variables:\n  - key: MyVar\n    scope: namespace\n    value: \n</code></pre>"},{"location":"getting_started/persistent-data/#getter-state","title":"Getter State","text":"<p>The Getter State is used to retrieve any number of variables in persistent storage. Each variable must be explicitly scoped, and the value retrieved will be stored under <code>.var.KEY</code> where <code>KEY</code> is the variable's name.</p> <pre><code>- id: a\n  type: getter\n  variables:\n  - key: x\n    scope: namespace\n</code></pre> <p>A key doesn't need to exist in storage to return successfully, but the value returned will be <code>null</code> if it doesn't exist.</p>"},{"location":"getting_started/persistent-data/#concurrency","title":"Concurrency","text":"<p>Direktiv makes no effort to guarantee any thread-safety on persistent data. Multiple instances that interact with the same variable may have inconsistent results.</p>"},{"location":"getting_started/persistent-data/#getting-setting-from-functions","title":"Getting &amp; Setting from Functions","text":""},{"location":"getting_started/persistent-data/#getting","title":"Getting","text":"<p>Accessing persistent data from within a function is a fairly straightforward process. The request that the custom function receives from Direktiv contains a header 'Direktiv-TempDir', which contains all of the variables specified in the function definition. The <code>as</code>, <code>key</code>, <code>scope</code>, and <code>type</code> fields can all play a role in the placement and naming of files within this directory:</p> <ul> <li><code>key</code></li> <li>The key used to select a variable from within the flow definition. If no <code>as</code> field is provided, the file on a custom function will correspond to the value of <code>key</code>.</li> <li><code>scope</code></li> <li>Which scope to get the variable from: <code>instance</code>, <code>workflow</code>, or <code>namespace</code>. Defaults to <code>instance</code> if omitted.</li> <li><code>as</code></li> <li>An optional field used to set the name of the file as it appears on the isolate.</li> <li><code>type</code></li> <li><code>plain</code><ul> <li>The variable data inside of the file will be written 'as-is'.</li> </ul> </li> <li><code>base64</code><ul> <li>If the variable is stored as base64-encoded data, it will be decoded before being written to the file system.</li> </ul> </li> <li><code>tar</code><ul> <li>If the variable is a valid tar archive, a directory will be created instead of a file, with the contents of the tar archive populating it.</li> </ul> </li> <li><code>tar.gz</code><ul> <li>Similar to <code>tar</code>, this will result in a populated directory being created from a valid <code>.tar.gz</code> file.</li> </ul> </li> </ul> <p>For example, given the following state definition, a directory named 'myFiles' should exist within the directory specified by the <code>Direktiv-TempDir</code> header. Assuming that this header has a value of <code>/mnt/shared/example</code>, the following structure would be expected:</p> <pre><code>  - id: get\n    image: localhost:5000/iv-getter:v1\n    files:\n    - key: \"myFiles\"\n      scope: instance\n      type: tar\n</code></pre> <pre><code>/mnt/shared/example/\n\u2514\u2500\u2500 myFiles\n    \u2514\u2500\u2500 file-1\n    \u2514\u2500\u2500 file-2\n    \u2514\u2500\u2500 file-3\n</code></pre>"},{"location":"getting_started/persistent-data/#setting","title":"Setting","text":"<p>From within a function running on Direktiv, variables can be set by sending a <code>POST</code> request:</p> <pre><code>POST http://localhost:8889/var?aid=&lt;EXAMPLE&gt;&amp;scope=instance&amp;key=myFiles\n\nBody: &lt;VARIABLE DATA&gt;\n</code></pre> <ul> <li>query parameters</li> <li>aid<ul> <li>The action ID, found from the <code>Direktiv-ActionID</code> header of the request being served by the isolate.</li> </ul> </li> <li>scope<ul> <li>The scope for which the variable is set (<code>namespace</code>, <code>workflow</code>, or <code>instance</code>)</li> </ul> </li> <li>key<ul> <li>The key used by subsequent actions to access the variable.</li> </ul> </li> </ul> <p>An alternative approach is to write files into certain directories. The direktiv sidecar will store those files as variables. There are three different folders for the three different scopes. For the above example they would be:</p> <pre><code>/mnt/shared/example/out/instance\n/mnt/shared/example/out/workflow\n/mnt/shared/example/out/namespace\n</code></pre> <p>Files under these folders will be stored with their names under the scope of the folder. Diretories will be stored as tar.gz files.</p>"},{"location":"getting_started/scheduling/","title":"Scheduling","text":"<p>Sometimes a flow needs to run periodically. Direktiv supports scheduling based on \"cron\". The <code>cron</code> is one of the start definitions.</p>"},{"location":"getting_started/scheduling/#demo","title":"Demo","text":"<pre><code>direktiv_api: workflow/v1\nstart:\n  type: scheduled\n  cron: \"* 0/2 * * *\"\nfunctions:\n- id: httprequest\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: reusable\nstates:\n- id: getter\n  type: action\n  action:\n    function: httprequest\n    input: \n      method: \"GET\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n</code></pre>"},{"location":"getting_started/scheduling/#start-types","title":"Start Types","text":"<p>Flow definitions can have one of many different start types. If the <code>start</code> section left out entirely, causes it to <code>default</code>, which is appropriate for a direct-invoke/subflow flow. </p> <p><pre><code>start:\n  type: scheduled\n  cron: \"0 */2 * * *\"\n</code></pre> Direktiv supports valid cron expressions and prevents scheduled flows from being directly invoked or used as a subflow, which is why this example does not specify any input data. Scheduled flows can not accept any payloads. </p>"},{"location":"getting_started/scheduling/#activeinactive-flows","title":"Active/Inactive Flows","text":"<p>Every flow definition can be considered \"active\" or \"inactive\". Being \"active\" doesn't mean that there's an instance running right now, it means that Direktiv will allow instances to be created from it. This setting is part of the API, not a part of the flow definition.</p> <p>With scheduled flows this is a useful setting. It can toggle the schedule on and off without modifying the flow definition itself.</p>"},{"location":"getting_started/scheduling/#cron","title":"Cron","text":"<p>Cron is a time-based job scheduler in Unix-like operating systems. Direktiv doesn't run cron, but it does borrow their syntax and expressions for scheduling. In the example above the cron expression is \"<code>0 */2 * * *</code>\". This tells Direktiv to run the flow once every two hours. There are many great resources online to help creating custom cron expressions.</p>"},{"location":"getting_started/secrets-registries/","title":"Secrets &amp; Registries","text":"<p>Many flows require sensitive information such as passwords or authentication tokens to access third-party APIs. This article shows the best way to handle sensitive data such as this so that they don not need to be stored as plaintext in flow definitions. Additionally this article shows how to pull containers from a private repository.</p> <p>Stored secrets can be requested in a function via the <code>secrets</code> attribute and is available as <code>.secrets.SECRETNAME</code></p> Secrets<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: httprequest\n  image: direktiv/request:v1\n  type: reusable\nstates:\n- id: getter\n  type: action\n  action:\n    secrets: [\"secretToken\"]\n    function: httprequest\n    input:\n      method: \"GET\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n      headers:\n        \"Content-type\": \"application/json; charset=UTF-8\"\n        \"Authorization\": \"bearer jq(.secrets.secretToken)\"\n</code></pre>"},{"location":"getting_started/secrets-registries/#registries","title":"Registries","text":"<p>Direktiv can store authentication information for a container repositories on a namespace-by-namespace basis. Creating secrets can be done via the Direktiv API or web interface in the settings page.</p> <p>With the relevant registry defined, functions referencing containers on that registry become accessible. For example, if a registry was created via the api with the following curl command:</p> <pre><code>curl -X 'POST' \\\n  'URL/api/functions/registries/namespaces/NAMESPACE' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"data\": \"admin:8QwFLg%D$qg*\",\n  \"reg\": \"https://index.docker.io\"\n}'\n</code></pre> <p>This registry would be used automatically by Direktiv when running the flow in the demo.</p>"},{"location":"getting_started/secrets-registries/#example-google-artifact-registry","title":"Example Google Artifact Registry","text":"<p>To use the Google Artifact Registry a service account with a key is required. How to create a service account and generate a key is documented here.</p> <p>The keys needs to be in base64 format. On linux it can be converted with the following command:</p> <pre><code>base64 -w 0 mykey-da8c8b573601.json &gt; base64google.json\n</code></pre> <p>Please make sure that there are no line wraps in the base64 file. For base64 encoded files the username is <code>_json_key_base64</code>. Example details for this registry would be something like the following:</p> Key Value URL https://us-central1-docker.pkg.dev Username _json_key_base64 Password ewogICJ0eXBlIjo...WlJkMWhqK1RRRF <p>Note</p> <p>If a registry is created after a service, the service will need to be recreated to use the latest registry.</p>"},{"location":"getting_started/secrets-registries/#secrets","title":"Secrets","text":"<p>Similar to how registry tokens are stored, arbitrary secrets can also be stored. That includes passwords, API tokens, certificates, or anything else. Secrets are stored on a namespace-by-namespace basis as key-value pairs. Secreats can be defined with the Direktiv API or web interface.</p> <p>Wherever actions appear in flow definitions there's always an optional <code>secrets</code> field. For every secret named in this field, Direktiv will find and decrypt the relevant secret from your namespace and add it to the data from which the action input is generated just before running the <code>jq</code> command that generates that logic. This means your <code>jq</code> commands can reference your secret and place it wherever it needs to be.</p> <p>Direktiv discards the secret-enriched data after generating the action input, so the secrets won't naturally appear in your instance output or logs. But once Direktiv passes that data to your action it has no control over how it's used. It's up to you to ensure your action doesn't log sensitive information and doesn't send sensitive information where it shouldn't go.</p> <p>IMPORTANT: Be especially wary of subflows. Try to avoid passing secrets to subflows if you can, subflows can reference secrets the same way as their parents after all. Remember, your secret-enriched data will become the input for a subflow, which means it will be logged. It's also stored in that subflow's instance data and could be passed around automatically if you're not careful. If your subflow doesn't strip secrets out before it terminates those secrets could also end up in the caller's <code>return</code> object.</p>"},{"location":"getting_started/secrets-registries/#security","title":"Security","text":"<p>Secrets are securely stored in an encrypted format within Direktiv's database. Registry tokens are directly passed as Kubernetes Secrets, and they are therefore only Base64 encoded. For enhanced security, it is advised to configure system logs (Kubernetes) to the Info level in production environments, as debug-level logs may expose secrets in plain text. As a best practice, we recommend opting for tokens over passwords whenever feasible.</p>"},{"location":"getting_started/states/","title":"Flows & States","text":"<p>Direktiv Flows are YAML-based definitions of states connected in a directed acyclic graph (DAG). During runtime, the flow controls how execution progresses and which states are being called. It provides different state types to allow e.g. decision making, execute functions, event triggering and subflow calls. </p> <p>During execution data will be stored as JSON which can be accessed or modified in any given state.  Direktiv takes any kind of input to start the process off and returns a result as JSON output once finished.</p> <p>Direktiv Flow </p>"},{"location":"getting_started/states/#workflow-definition","title":"Workflow definition","text":"<p>It is best practices for all workflows to begin with the following line, so that tools can identify it as a Direktiv workflow: <pre><code>direktiv_api: workflow/v1\n</code></pre></p> <p>All states for a flow are listed under <code>states</code>. Every flow must have at least one state. The first state under <code>states</code> will be executed first and all subsequent states need to be connected  via transitions. If a state has no <code>transition</code> attribute the flow ends at that point of the execution. </p>"},{"location":"getting_started/states/#simple-state","title":"Simple State","text":"<pre><code>direktiv_api: workflow/v1\nstates:\n- id: hello\n  type: noop\n  log: this is the log\n  transform: \n    hello: world\n</code></pre> <p>The above flow contains a single <code>noop</code> (\"no operation\") and shows the common attributes in all available states within Dirketiv. When the flow is getting executed Direktiv creates an <code>instance</code> of that flow definition and tracks the progress and state data of that instance. The output of that flow would be the following:</p> <pre><code>{\n  \"hello\": \"world\"\n}\n</code></pre>"},{"location":"getting_started/states/#state-id","title":"State ID","text":"<pre><code>- id: hello\n</code></pre> <p>Every state has to have its own identifier. The state identifier is used in logging and to define transitions, which will come up in a later example when we define more than one state. A state identifier must be unique within the flow definition. </p>"},{"location":"getting_started/states/#state-type","title":"State Type","text":"<pre><code>  type: noop\n</code></pre> <p>There are many state types that do all sorts of different things. It is required to provide the state type. </p>"},{"location":"getting_started/states/#log","title":"Log","text":"<pre><code>  log: this is the log\n</code></pre> <p>Every state has the <code>log</code> attribute and the content of the log attribute will be stored in the logs of the instance. </p>"},{"location":"getting_started/states/#transform-command","title":"Transform Command","text":"<pre><code>  transform: \n    hello: world\n</code></pre> <p>Any state may optionally define a \"transform\" to modify the state data. The transform can add and delete data in the state or even wipe all data in the state. </p>"},{"location":"getting_started/states/#simple-transition","title":"Simple Transition","text":"<p>A <code>transition</code> attribute in a state instructs Direktiv to move to the next state. Transitions can also be conditional or during error handling but the following is a simple sequential transition.</p> <pre><code>states:\n\n- id: hello\n  type: noop\n  log: this is the log\n  transform: \n    hello: world\n  transition: next-step\n\n- id: next-step\n  type: noop\n  log: last-step\n\n- id: last-step\n  type: noop\n  log: second stage\n</code></pre>"},{"location":"getting_started/subflows/","title":"Subflows","text":"<p>Just like scripting or programming, with Direktiv it's possible to organize your logic into reusable modules. Anytime a flow is invoked by another we it is called subflow. A subflow can be called like actions and it uses the same parameters as functions.</p> Subflow 'checker'<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: httprequest\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\nstates:\n\n# validates input and protects flow from wrong input data\n- id: validate-input\n  type: validate\n  schema:\n    type: object\n    required:\n    - contact\n    - payload\n    additionalProperties: false\n    properties:\n      contact:\n        type: string\n      payload:\n        type: string\n  transition: notify\n\n# run http request\n- id: notify\n  type: action\n  action:\n    function: httprequest\n    input:\n      method: \"GET\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n      content: \n        input: jq(.)\n  transition: check-results\n\n# check if http code is 200\n- id: check-results\n  type: switch\n  conditions:\n  - condition: 'jq(.return[0].code != 200)'\n    transition: throw\n  defaultTransform:\n    result: jq(.return[0].code)\n\n# throw an error if not 200 response code\n- id: throw\n  type: error\n  error: notification.lint\n  message: \"not 200 response\"\n</code></pre> Parent Flow<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: checker-sub\n  type: subflow\n  # relative reference to subflow\n  workflow: checker.yaml\nstates:\n- id: notify\n  type: action\n  action:\n    function: checker-sub\n    input:\n      contact: hello\n      payload: data\n</code></pre> Output<pre><code>{\n  \"return\": {\n    \"result\": 200\n  }\n}\n</code></pre>"},{"location":"getting_started/transforms/","title":"Transforms &amp; JQ/JS","text":"<p>Every flow instance always has something called the \"Instance Data\", which is a JSON object that is used to pass data around. Almost everywhere a <code>transition</code> can happen in a flow definition a <code>transform</code> can also happen allowing the author to filter, enrich, or otherwise modify the instance data. Transforms can be static, as seen in previous parts of this guide, or use JQ or Javascript to dynamically change it. </p>"},{"location":"getting_started/transforms/#jq-introduction","title":"JQ introduction","text":"<p>Direktiv uses JQ, JSON query language, to dynamically change data within the system. It is used in transformations, transitions, logs or function calls. </p> <p>JQ Hints</p> <p>Setting Defaults: JQ throws an error if the value you are accessing is empty. It is easy to set a default value with JQ like the following example:  <pre><code>- id: hello\n  type: noop\n  transform:\n    hello: jq(.myvalue // \"world\")\n</code></pre></p> <p>Multi-Line: Sometimes JQ can be hard to read if it is too long. YAML provides an easy way to use multi-line input. </p> <pre><code>- id: hello\n  type: noop\n  transform:\n    hello: |-\n      jq(\n        if .mydata // \"myvalue\" == \"hello\"\n        then \"it is hello\" \n        elif . == \"world\" then \"it is world\" \n        else \"none of the above\" end\n      )\n</code></pre> <p>The <code>transform</code> field can contain a valid <code>jq</code> command, which will be applied to the existing instance data to generate a new JSON object that will entirely replace it. Note that only a JSON object will be considered a valid output from this <code>jq</code> command: <code>jq</code> is capable of outputting primitives and arrays, but these are not acceptable output for a <code>transform</code>. </p> <p>Transforms can be wrapped in <code>'jq()'</code> or <code>jq()</code>. The difference between the two is that one instructs YAML more explicitly what's in the string. This can be important if you use <code>jq</code> commands containing braces, for example: <code>jq({a: 1})</code>. Because if this is not explicitly quoted, YAML interprets it incorrectly and throws errors. The quoted form is always valid and generally safer.</p> <p>Hint</p> <p>The UI provides a JQ playground to write andf test JQ queries. </p>"},{"location":"getting_started/transforms/#js-introduction","title":"JS introduction","text":"<p>An alternative to JQ is Javascript. Direktiv provides a <code>data</code> Javascript object which can be modified to change state data. It assumes the script runs in a function and the Javascript section needs to return data even if it is an empty string. A <code>null</code> value is not allowed. </p> <pre><code>- id: hello\n  type: noop\n  transform:\n    epoch: js(return Date.now())\n</code></pre> <p>Javascript snippets have access to the state data as well. The state data in that object is accessible through regular Javascript commands.</p> <pre><code>- id: hello\n  type: noop\n  transform: |- \n      js(\n        data[\"hello\"] = \"world\"\n        return data\n      )\n</code></pre>"},{"location":"getting_started/transforms/#first-transform","title":"First Transform","text":"<p>Although a <code>transform</code> can use <code>jq</code> or <code>js</code> to modify data plain YAML can be used to do the transform. The following example does such a static transform. This can be used to e.g. set-up defaults or a basic object to work with in that flow. </p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: transform1\n  type: noop\n  transform:\n    number: 5\n    objects:\n    - key1: value1\n    - key2: value2\n</code></pre> <p>Resulting Instance Data</p> <pre><code>{\n  \"number\": 5,\n  \"objects\": [\n    {\n      \"key1\": \"value1\"\n    },\n    {\n      \"key2\": \"value2\"\n    }\n  ]\n}\n</code></pre>"},{"location":"getting_started/transforms/#second-transform","title":"Second Transform","text":"<p>The second transform enriches the existing instance data by adding a new field to it.</p> <p>Command</p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: transform1\n  type: noop\n  transform:\n    number: 5\n    objects:\n    - key1: value1\n    - key2: value2\n  transition: transform2\n- id: transform2\n  type: noop\n  transform: 'jq(.multiplier = 10)' \n</code></pre> <p>Resulting Instance Data</p> <pre><code>{\n  \"multiplier\": 10,\n  \"number\": 5,\n  \"objects\": [\n    {\n      \"key1\": \"value1\"\n    },\n    {\n      \"key2\": \"value2\"\n    }\n  ]\n}\n</code></pre>"},{"location":"getting_started/transforms/#third-transform","title":"Third Transform","text":"<p>The third transform multiplies two fields to produce a new field, then pipes the results into another command that deletes two fields.</p> <p>Command</p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: transform1\n  type: noop\n  transform:\n    number: 5\n    objects:\n    - key1: value1\n    - key2: value2\n  transition: transform2\n- id: transform2\n  type: noop\n  transform: 'jq(.multiplier = 10)' \n  transition: transform3\n- id: transform3\n  type: noop\n  transform: 'jq(.result = .multiplier * .number | del(.multiplier, .number))'\n</code></pre> <p>Resulting Instance Data</p> <pre><code>{\n  \"objects\": [\n    {\n      \"key1\": \"value1\"\n    },\n    {\n      \"key2\": \"value2\"\n    }\n  ],\n  \"result\": 50\n}\n</code></pre>"},{"location":"getting_started/transforms/#fourth-transform","title":"Fourth Transform","text":"<p>The fourth transform selects a child object nested within the instance data and makes that into the new instance data.</p> <p>Command</p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: transform1\n  type: noop\n  transform:\n    number: 5\n    objects:\n    - key1: value1\n    - key2: value2\n  transition: transform2\n- id: transform2\n  type: noop\n  transform: 'jq(.multiplier = 10)' \n  transition: transform3\n- id: transform3\n  type: noop\n  transform: 'jq(.result = .multiplier * .number | del(.multiplier, .number))'\n  transition: transform4\n- id: transform4\n  type: noop\n  transform: 'jq(.objects[0])'\n</code></pre> <p>Resulting Instance Data</p> <pre><code>{\n  \"key1\": \"value1\"\n}\n</code></pre>"},{"location":"getting_started/transitions/","title":"Input & Transitions","text":"<p>Input data and transitions, in particular conditional transitions, are an important part in Direktiv. As previously shown a state can define a transition as the next state in the flow. If there is no transition defined the flow ends at that point in the execution. So far the examples have only shown sequential transition but here there will be a conditional transition based on input data of the flow. </p>"},{"location":"getting_started/transitions/#conditional-transition","title":"Conditional Transition","text":"<p>To execute conditional transitions Direktiv provides a <code>switch</code> which makes decisions about where to transition to next based on the instance data by evaluating a number of <code>jq</code> or <code>js</code> expressions and checking the results. </p> <pre><code>direktiv_api: workflow/v1\n\nstates:\n- id: ifelse\n  type: switch\n  conditions:\n  - condition: 'jq(.age &gt; 17)'\n    transition: accepted\n  - condition: 'jq(.age != null)'\n    transition: rejected\n  defaultTransition: failure\n\n- id: accepted\n  type: noop\n  transform:\n    message: request accepted\n\n- id: rejected\n  type: noop\n  transform:\n    message: rejected based on age\n\n- id: failure\n  type: error\n  error: age.error\n  message: no age provided\n</code></pre> <p>Each of the <code>conditions</code> will be evaluated in the order it appears by running the <code>jq</code> command in <code>condition</code>. Any result other than <code>null</code>, <code>false</code>, <code>{}</code>, <code>[]</code>, <code>\"\"</code>, or <code>0</code> will cause the condition to be considered a successful match. If no conditions match the default transition will be used. </p> <p>Transform</p> <p>Each condition has a <code>transform</code> attribute and there is a <code>defaultTransform</code> so every condition can modfiy the state data if there is a successful match. </p> <p>Running the above example will always go to the <code>failure</code> state because no input data has been provided for this flow. In this case the <code>failure</code> state is an <code>error</code> state which marks the flow as failed. More about errors can be found in the error handling section.</p>"},{"location":"getting_started/transitions/#input-data","title":"Input Data","text":"<p>To make the above example more useful the flow needs input data. Input data in Direktiv will never be empty. If the flow is called with no data it will be executed with an empty JSON object <code>{}</code>. If the payload is in JSON format it will be base64 encoded and provided with the attribute <code>input</code>. </p> <pre><code>{\n  \"input\": \"T1hSisBaSE64Data==\"\n}\n</code></pre> <p>The above flow can be called with a simple JSON providing a value for age. </p> <pre><code>{ \n    \"age\": 18\n}\n</code></pre> <p>The curl command to call the flow via the API is the following. Please adjust the flow and server name if required.</p> <pre><code>curl -X POST http://localhost:8080/api/namespaces/demo/tree/MYWORKFLOWNAME?op=wait \\\n--data-binary @- &lt;&lt; EOF\n{ \n    \"age\": 18\n}\nEOF\n</code></pre> <p>The response is always the last state data of a flow. Because the final states include a <code>transform</code> the response of the flow would be the transformed data.</p> <pre><code>{\n  \"message\": \"request accepted\"\n}\n</code></pre>"},{"location":"getting_started/validating/","title":"Validating Input","text":"<p>In some cases it is important to validate the state of the flow. This can be done as the first state in the flow to protect the flow from rogue data or within the flow to check the state data before proceeding. Direktiv is using JSON schema to validate the state data.</p> Check Attribute<pre><code>direktiv_api: workflow/v1\nstates:\n- id: data\n  type: noop\n  transform: \n    name: Michael\n  transition: check\n- id: check\n  type: validate\n  schema:\n    type: object\n    required: \n    - name\n    properties:\n      name:\n        type: string\n</code></pre> <p>The above example will succedd because the attribute <code>name</code> is set and it is a string, in this case <code>Michael</code>. If the the value would be an integer the flow would fail.</p> Failed Attribute<pre><code>direktiv_api: workflow/v1\nstates:\n- id: start\n  type: validate\n  schema:\n    type: object\n    required: \n    - name\n    properties:\n      name:\n        type: string\n</code></pre> <p>JSON schema is used to validate JSON structures and not contents. It can not validate if the value of <code>name</code> has a certain content. If that is a requirement Direktiv's switch statement has to be used. </p>"},{"location":"getting_started/validating/#first-state","title":"First State","text":"<p>Direktiv can generate a form if the validate state is the first state in the flow. </p> Validate Form<pre><code>direktiv_api: workflow/v1\nstates:\n- id: start\n  type: validate\n  schema:\n    type: object\n    required: \n    - name\n    properties:\n      name:\n        type: string\n        title: Name\n        description: Please enter your name\n        default: My Name\n</code></pre> <p>In this example the <code>default</code> attribute is used and it is shown in the form. Direktiv can not set defaults via API or in a running flow. It is for  form generation only. If defaults are required <code>jq</code> can be used like <code>jq(.name // \"Michael\")</code>. The following validate would ask for the name but set it to <code>Michael</code> if it is empty.</p> Setting Defaults<pre><code>direktiv_api: workflow/v1\nstates:\n- id: start\n  type: validate\n  schema:\n    type: object\n    properties:\n      name:\n        type: string\n        title: Name\n        description: Please enter your name\n  transition: next-state\n- id: next-state\n  type: noop\n  transform: 'jq(. + { name: (.name // \"Michael\") })'\n</code></pre>"},{"location":"getting_started/advanced/making-functions/","title":"Making Custom Functions","text":"<p>If a custom function is required in case there are no Direktiv functions available it is easy to create those. Direktiv is pulling the images defined in the <code>functions</code> section and executes the container in the flow. They can be in any repository.</p> Custom Function<pre><code>functions:\n- id: custom\n  image: mycompany/customfunction\n  type: knative-workflow\n</code></pre> <p>The custom container just need to implement a few things. The most important requirement is to listen to port <code>8080</code>. The data in the flow will be posted to the container on that port. </p> Input<pre><code>- id: notify\n  type: action\n  action:\n    function: custom\n    input:\n      hello: world\n</code></pre> <p>In the above example Direktiv would post JSON <code>{ \"hello\": \"world\" }</code> to the function. The function can use his date to execute whatever it needs to do. After that the function has to return in JSON formart. Direktiv will fetch the response and add it to the state data in the <code>return</code> attribute. The flow can use that data and proceed. </p>"},{"location":"getting_started/advanced/making-functions/#reporting-errors","title":"Reporting Errors","text":"<p>If something goes wrong a function can report an error to the calling flow instance by adding HTTP headers to the response. If these headers are populated the execution of the function will be considered a failure regardless of what's stored in response data.</p> <p>The headers to report errors are: <code>Direktiv-ErrorCode</code> and <code>Direktiv-ErrorMessage</code>. If an error message is defined without defining an error code the calling flow instance will be marked as \"crashed\" without exposing any helpful information, so it's important to always define both. Errors raised by functions are always 'catchable' by their error codes.</p> Error Headers<pre><code>  \"Direktiv-ErrorCode\": \"myapp.input\",\n  \"Direktiv-ErrorMessage\": \"Missing 'customerId' property in JSON input.\"\n</code></pre>"},{"location":"getting_started/advanced/making-functions/#logging","title":"Logging","text":"<p>Logging for functions is a simple HTTP POST or GET request to the address:</p> <p><code>http://localhost:8889/log?aid=$ACTIONID</code></p> <p>If POST is used the body of the request is getting logged for GET requests add a log request parameter. The important parameter is $ACTIONID. Each requests gets an action id header which identifies the flow instance. This parameter has to be passed back to attach the log to the instance. This information is passed in as in the initial request (Direktiv-ActionID).</p>"},{"location":"getting_started/advanced/making-functions/#examples","title":"Examples","text":"<ul> <li>Dotnet</li> <li>Python FastAPI</li> <li>Golang</li> <li>Java</li> <li>Node</li> <li>Python</li> <li>Rust</li> </ul>"},{"location":"getting_started/advanced/making-functions/#using-generic-containers","title":"Using Generic Containers","text":"<p>Direktiv has a special command to use any conatiner from a container registry even if it does not have a server running on port <code>8080</code>. If the the command <code>/usr/share/direktiv/direktiv-cmd</code> in the <code>cmd</code> field is used Direktiv provides a server for this function / container.  This can be useful if e.g. a Python container is used for scripting  or <code>bash</code> for ssh/scp comands. </p> Special Command<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: get\n  image: alpine\n  type: knative-workflow\n  cmd: /usr/share/direktiv/direktiv-cmd\nstates:\n- id: getter \n  type: action\n  action:\n    function: get\n    input: \n      data:\n        commands:\n        - command: echo -n \"hello\"\n          envs:\n          - name: MYENV\n            value: MYVALUE\n          stop: true\n</code></pre> <p>The <code>commands</code> block holds an array of individual commands which will be excuted in the function. The commands can have three additional parameters.</p> <ul> <li><code>stop</code>: If the execution should stop if an error occurs during this command (default: false).</li> <li><code>suppress_command</code>: If the command should be printed when executed. Should be set to true if passwords are part of the command (default: false).</li> <li><code>suppress_output</code>: If the stdout output of the command should be printed (default: false).</li> <li><code>env</code>: A list ov environment variables with  <code>name</code>/<code>value</code> pairs for this single command.</li> </ul> <p>The is an additional files block which allows to pass in files on-demand. A file requires a <code>name</code> and <code>content</code>. The content can be text-based data including Direktiv secrets. An additional setting is <code>permission</code>. If the file is an executable script or certificate the permissions can be set via that option.</p> Files in Function<pre><code>direktiv_api: workflow/v1\nfunctions:\n- id: get\n  image: alpine\n  type: knative-workflow\n  cmd: /usr/share/direktiv/direktiv-cmd\nstates:\n- id: getter \n  type: action\n  action:\n    function: get\n    input:\n      files:\n      - name: script.sh\n        content: |\n          #!/bin/sh\n\n          echo -n \"HELLO WORLD\"\n        permission: 0755 \n      data:\n        commands:\n        - command: ./script.sh\n</code></pre>"},{"location":"getting_started/advanced/metadata/","title":"Metadata","text":"<p>If Direktiv flow are consumed by external applications metadata can be used to request the state of a flow. It is data which can be set by the flow and requested via API. This in particular useful if the flow is executed asynchronously.</p>"},{"location":"getting_started/advanced/metadata/#executing-flow-asynchronously","title":"Executing Flow Asynchronously","text":"<p>A flow can be started with a simple API call. By default this is done asynchronously and can be called e.g. via shell with curl:</p> <p><code>curl -X POST http://&lt;DIREKTIV-ADDRESS&gt;/api/namespaces/&lt;NAMESPACE&gt;/tree/&lt;FLOW-NAME&gt;?op=execute</code></p> <p>This call would return information about the started flow and it looks like the following:</p> Workflow Info<pre><code>{\n  \"namespace\": \"asdas\",\n  \"instance\": \"24d6b04b-6e3c-47ba-a300-462f02c8fcae\"\n}\n</code></pre> <p>To request the metadata the <code>instance</code> attribute is the value required for subsequent requests to fetch the metadata. </p> <p><code>curl http://10.100.91.85/api/namespaces/&lt;NAMESPACE&gt;/instances/&lt;INSTANCE ID FROM THE PREVIOUS CALL&gt;/metadata | jq -r .data | base64 -d</code></p> <p>The following flow with just <code>delay</code> states can be used to test the result of the metadata call.</p> Metadata Flow Example<pre><code>direktiv_api: workflow/v1\n\nstates:\n- id: step1\n  type: delay\n  metadata:\n    state: waiting at the moment at state one\n  duration: PT30S\n  transition: step2\n\n- id: step2 \n  type: delay\n  metadata:\n    state: waiting at the moment at state two\n  duration: PT30S\n  transition: step3\n\n- id: step3\n  type: delay\n  metadata:\n    state: waiting at the moment at state three\n  duration: PT30S\n</code></pre>"},{"location":"getting_started/advanced/timeout/","title":"Timeouts","text":"<p>Direktiv supports timeouts on different levels. The main reason for having timeouts is to avoid having long-running or orphaned flows. The default timeout for flows and actions is 15 minutes.</p>"},{"location":"getting_started/advanced/timeout/#flow-timeouts","title":"Flow Timeouts","text":"<p>There is a general flow time out setting which controls how Direktiv will try to gracefully stop or interrupt the flow and eventually kill the flow if that is not possible. The time has to be provided in <code>ISO8601</code> format.</p> <pre><code>direktiv_api: workflow/v1\n\ntimeouts:\n  interrupt: PT20M\n  kill: PT30M\n\nstates:\n- id: nothing\n  type: noop\n  log: I'm doing nothing\n</code></pre>"},{"location":"getting_started/advanced/timeout/#state-timeouts","title":"State Timeouts","text":"<p>Every state has a timeout attribute as well. This is in particular interesting for functions and the action state. If the timeout is triggered in an action Direktiv sends an interrupt to the action and it is up to the function to handle it.</p> Action Timeouts<pre><code>direktiv_api: workflow/v1\n\nfunctions:\n- id: httprequest\n  image: gcr.io/direktiv/functions/http-request:1.0\n  type: knative-workflow\n\nstates:\n- id: timeout-test\n  type: action\n  # this flow fails if it exceeds 10 seconds\n  timeout: PT10S\n  action:\n    function: httprequest\n    input:\n      method: \"GET\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n</code></pre>"},{"location":"getting_started/advanced/timeout/#events","title":"Events","text":"<p>Another use for timeouts is events. There are three states consuming events: consumeEvent, eventsAnd and eventsXor. If the timeout is reached the flow fails or the error can be caught and handled. </p> Event Wait And Timeout<pre><code>direktiv_api: workflow/v1\n\nstates:\n- id: something\n  type: noop\n  transition: consume\n\n- id: consume\n  type: consumeEvent\n  # wait for the event for one minute, otherwise fail\n  timeout: PT1M\n  event:\n    type: com.github.pull.create\n    context:\n      subject: '123'\n</code></pre>"},{"location":"getting_started/advanced/timeout/#catching-timeouts","title":"Catching Timeouts","text":"<p>Timeouts in actions can be caught and the error thrown is <code>direktiv.cancels.timeout.soft</code>. Based on that error the flow can be re-routed. </p> Catch Timeout<pre><code>direktiv_api: workflow/v1\n\nstates:\n- id: something\n  type: noop\n  transition: consume\n\n- id: consume\n  type: consumeEvent\n  timeout: PT1S\n  event:\n    type: com.github.pull.create\n    context:\n      subject: '123'\n  catch:\n  - error: \"direktiv.cancels.timeout.soft\"\n    transition: handle-error\n\n- id: handle-error\n  type: noop\n  log: error handling\n</code></pre> <p>Timeouts in States</p> <p>Direktiv is not automatically calculating timeouts. If an action has a 30 minute timeout the flow timeout has to be increased as well to cater for long-running actions. </p>"},{"location":"installation/","title":"Installation","text":"<p>Direktiv is using Helm charts for installation. For a basic installation there are only two dependencies. A PostgreSQL database and Knative. Optional dependencies are Linkerd as service mesh and monitoring and tracing tools, e.g. backends for Direktiv's Opentelemetry configuration. The following diagram shows a high-level architecture of Direktiv and the required and optional components.</p> <p> </p> <p>The following sections explain how to install each component in a local cluster:</p> <ul> <li> <p>Kubernetes</p> </li> <li> <p>Linkerd</p> </li> <li> <p>Postgres</p> </li> <li> <p>Direktiv</p> </li> <li> <p>Knative</p> </li> </ul>"},{"location":"installation/#run-docker-image","title":"Run Docker Image","text":"<p>For testing there is a \"all-in-one\" Docker image available. It contains all required components already installed and can be used for testing or development. It has a container registry installed on port 31212 as well which can be used to push local images.</p> Direktiv Docker Container<pre><code>docker run --privileged -p 8080:80 -ti direktiv/direktiv-kube\n</code></pre> <p>The docker image has additional environment variables which can add other functionalities and configurations:</p> <ul> <li>APIKEY: Set an API key for the application</li> <li>HTTPS_PROXY: Sets the HTTPS_PROXY environment variable</li> <li>HTTP_PROXY: Sets the HTTP_PROXY environment variable</li> <li>NO_PROXY: Sets the NO_PROXY environment variable</li> <li>EVENTING: Enables Knative eventing</li> <li>DEBUG: Prints k3s output to stdout</li> </ul> Direktiv Docker Container with API Key and Registry<pre><code>docker run -e APIKEY=123 --privileged -p 8080:80 -p 31212:31212 -ti direktiv/direktiv-kube\n</code></pre>"},{"location":"installation/database/","title":"Database","text":"<p>Direktiv requires a PostgreSQL 16+ database. It acts as datastore as well as pub/sub system between Direktiv's components. It has been tested with Postgres offerings from cloud providers as well as on-premise installations. It is recommended to use a managed Postgres service from cloud providers. If that is not possible Postgres can be installed in Kubernetes as well.</p> <p>To install a Postgres instance in Kubernetes we are using Percona's Postgres operator. The following section will provide exmaples for different installation scenarios from basic testing setups to more complex high-availability configurations. For inidividual changes please visit the Percona Operator documentation page.</p>"},{"location":"installation/database/#installing-the-operator","title":"Installing the Operator","text":"<p>The operator is provided as Helm chart and the installation is straighforward. Add Direktiv's helm chart repository and run the installation command.</p> Install Postgres Operator<pre><code>kubectl create namespace postgres\nhelm repo add percona https://percona.github.io/percona-helm-charts/\nhelm install --create-namespace --version 2.3.1  -n postgres pg-operator percona/pg-operator --wait\n</code></pre> <p>Backup Ports</p> <p>For the backup to work properly port 2022 needs to be open between the nodes</p>"},{"location":"installation/database/#creating-a-postgres-instance","title":"Creating a Postgres Instance","text":""},{"location":"installation/database/#basic-configuration","title":"Basic Configuration","text":"<p>This basic configuration is good for small instances and testing. It creates weekly backups and keeps the last 4 backups. Direktiv connects directly to the Database without connection pooling.</p> Basic Install<pre><code>kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/scripts/kubernetes/install/db/basic.yaml\n</code></pre> Basic Database Configuration<pre><code>apiVersion: pgv2.percona.com/v2\nkind: PerconaPGCluster\nmetadata:\n  name: direktiv-cluster\n  namespace: postgres\n  #  finalizers:\n  #  - percona.com/delete-pvc\nspec:\n  crVersion: 2.3.0\n\n  users:\n    - name: direktiv\n      databases:\n        - direktiv\n      # access to public schema\n      # if no superuser is required, grant privileges manually: GRANT ALL ON SCHEMA public TO direktiv;\n      options: \"SUPERUSER\"\n    - name: postgres\n\n  image: perconalab/percona-postgresql-operator:main-ppg16-postgres\n  imagePullPolicy: Always\n  postgresVersion: 16\n  port: 5432\n\n  instances:\n    - name: instance1\n      replicas: 1\n      dataVolumeClaimSpec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 1Gi\n\n  proxy:\n    pgBouncer:\n      replicas: 1\n      image: perconalab/percona-postgresql-operator:main-ppg16-pgbouncer\n\n  backups:\n    pgbackrest:\n      image: perconalab/percona-postgresql-operator:main-ppg16-pgbackrest\n      global:\n        # Keep 4 Backups\n        repo1-retention-full: \"4\"\n        repo1-retention-full-type: count\n      manual:\n        repoName: repo1\n        options:\n          - --type=full\n      repos:\n        - name: repo1\n          schedules:\n            full: \"0 0 * * 6\"\n          volume:\n            volumeClaimSpec:\n              accessModes:\n                - ReadWriteOnce\n              resources:\n                requests:\n                  storage: 1Gi\n  pmm:\n    enabled: false\n    image: percona/pmm-client:2.41.0\n</code></pre>"},{"location":"installation/database/#high-availability","title":"High-Availability","text":"<p>High-Availabilty can be achieved by scaling the database replicas. The following example has added daily differential backups and pod anti-affinity to spread the pods across the cluster. If anti-affinity is used the cluster needs to have the same number of nodes and database replicas.</p> Basic Install<pre><code>kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/scripts/kubernetes/install/db/ha.yaml\n</code></pre> High-Availability Configuration<pre><code>apiVersion: pgv2.percona.com/v2\nkind: PerconaPGCluster\nmetadata:\n  name: direktiv-cluster\n  namespace: postgres\nspec:\n  crVersion: 2.3.0\n\n  users:\n    - name: direktiv\n      databases:\n        - direktiv\n      # access to public schema\n      # if no superuser is required, grant privileges manually: GRANT ALL ON SCHEMA public TO direktiv;\n      options: \"SUPERUSER\"\n    - name: postgres\n\n  image: perconalab/percona-postgresql-operator:main-ppg16-postgres\n  imagePullPolicy: Always\n  postgresVersion: 16\n  port: 5432\n\n  instances:\n    - name: instance1\n      replicas: 2\n      resources:\n        limits:\n          cpu: 2.0\n          memory: 4Gi\n      dataVolumeClaimSpec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 4Gi\n      topologySpreadConstraints:\n        - maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: DoNotSchedule\n          labelSelector:\n            matchLabels:\n              postgres-operator.crunchydata.com/instance-set: instance1\n\n  proxy:\n    pgBouncer:\n      replicas: 2\n      image: perconalab/percona-postgresql-operator:main-ppg16-pgbouncer\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 1\n              podAffinityTerm:\n                labelSelector:\n                  matchLabels:\n                    postgres-operator.crunchydata.com/cluster: keycloakdb\n                    postgres-operator.crunchydata.com/role: pgbouncer\n                topologyKey: kubernetes.io/hostname\n\n  backups:\n    pgbackrest:\n      image: perconalab/percona-postgresql-operator:main-ppg16-pgbackrest\n      global:\n        repo1-retention-full: \"4\"\n        repo1-retention-full-type: count\n      manual:\n        repoName: repo1\n        options:\n          - --type=full\n      repos:\n        - name: repo1\n          schedules:\n            full: \"0 0 * * 6\"\n            differential: \"0 1 * * 1-6\"\n          volume:\n            volumeClaimSpec:\n              accessModes:\n                - ReadWriteOnce\n              resources:\n                requests:\n                  storage: 4Gi\n  pmm:\n    enabled: false\n    image: percona/pmm-client:2.41.0\n</code></pre>"},{"location":"installation/database/#high-availability-with-s3-backup","title":"High-Availability with S3 Backup","text":"<p>Percona's Postgres operator can store backups in AWS, Azure and Google Cloud as well. The following example shows how to use AWS S3 as backup storage. A secret is required for the S3 backend with the appropriate permission. This requires a <code>s3.conf</code> file with the S3 key and secret.</p> s3.conf<pre><code>[global]\nrepo1-s3-key=MYKEY\nrepo1-s3-key-secret=MYSECRET\n</code></pre> <p>After creating the file adding the secret is a simple <code>kubectl</code> command:</p> Create S3 Secret<pre><code>kubectl create secret generic -n postgres direktiv-pgbackrest-secret --from-file=s3.conf\n</code></pre> <p>To test if the values are correct run the following command:</p> Show S3 Secrets<pre><code>kubectl get secret -n postgres direktiv-pgbackrest-secret -o go-template='{{ index .data \"s3.conf\" | base64decode }}'\n</code></pre> <p>High-Availabilty can be achieved by scaling the database replicas. The following example has added daily differential backups and pod anti-affinity and topology spread constraints to spread the pods across the cluster. If anti-affinity is used the cluster needs to have the same number of nodes and database replicas.</p> S3 Backup Install<pre><code>kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/scripts/kubernetes/install/db/s3.yaml\n</code></pre> S3 Configuration<pre><code>apiVersion: pgv2.percona.com/v2\nkind: PerconaPGCluster\nmetadata:\n  name: direktiv-cluster\n  namespace: postgres\nspec:\n  crVersion: 2.3.0\n\n  users:\n    - name: direktiv\n      databases:\n        - direktiv\n      # access to public schema\n      # if no superuser is required, grant privileges manually: GRANT ALL ON SCHEMA public TO direktiv;\n      options: \"SUPERUSER\"\n    - name: postgres\n\n  image: perconalab/percona-postgresql-operator:main-ppg16-postgres\n  imagePullPolicy: Always\n  postgresVersion: 16\n  port: 5432\n\n  instances:\n    - name: instance1\n      replicas: 1\n      dataVolumeClaimSpec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 1Gi\n\n  proxy:\n    pgBouncer:\n      replicas: 1\n      image: perconalab/percona-postgresql-operator:main-ppg16-pgbouncer\n\n  backups:\n    pgbackrest:\n      image: perconalab/percona-postgresql-operator:main-ppg16-pgbackrest\n      global:\n        repo1-retention-full: \"4\"\n        repo1-retention-full-type: time\n      configuration:\n        - secret:\n            name: direktiv-pgbackrest-secret\n      manual:\n        repoName: repo1\n        options:\n          - --type=full\n      repos:\n        - name: repo1\n          s3:\n            bucket: direktiv-backup\n            endpoint: \"https://eu-central-1.linodeobjects.com\"\n            region: \"US\"\n          schedules:\n            full: \"0 1 * * 0\"\n  pmm:\n    enabled: false\n    image: percona/pmm-client:2.41.0\n</code></pre>"},{"location":"installation/database/#connection-pooling","title":"Connection-Pooling","text":"<p>Connection pooling help scaling and maintaining availability between your application and the database. The Postgres Operator provides <code>pgBouncer</code> as connection pooling mechanism. If it is a multi-node cluster the <code>pgBouncer</code> replicas can be increased and spread across the cluster with pod anit-affinity rules. Direktiv can connect to the Postgres instances as well as to <code>pgBouncer</code>.</p>"},{"location":"installation/database/#getting-database-secrets","title":"Getting Database Secrets","text":"<p>Direktiv will need the database connection information during installation with a Helm chart. It is a good start for an installation YAML to have this information. It can be easily done by running a simple script:</p> Database Configuration (No Connection Pooling)<pre><code>echo \"database:\n  host: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode)\\\"\n  port: $(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode)\n  user: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode)\\\"\n  password: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode)\\\"\n  name: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode)\\\"\n  sslmode: require\" &gt; direktiv.yaml\n</code></pre> Database Configuration (With Connection Pooling)<pre><code>echo \"database:\n  host: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode)\\\"\n  port: $(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode)\n  user: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode)\\\"\n  password: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode)\\\"\n  name: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode)\\\"\n  sslmode: require\" &gt; direktiv.yaml\n</code></pre>"},{"location":"installation/database/#restore-from-s3","title":"Restore from S3","text":"<p>It is always recommended to test the backup and restore before using Direktiv in production. To restore from S3 is a straightforward process. The first step is to pick the backup used for the restore process. It can be found under <code>pgbackrest/backup/db</code> in the bucket used for backups in S3. It looks like this: <code>20221023-042407F</code>. There are two differtent scenarios to consider. The first one is a restore for an existing database. This can be a restore of as certain backup or a point-in-time recovery.</p> Restore From S3 (Same Database)<pre><code>---\napiVersion: pgv2.percona.com/v2\nkind: PerconaPGRestore\nmetadata:\n  name: restore\n  namespace: postgres\nspec:\n  pgCluster: direktiv-cluster\n  repoName: repo1\n  options:\n    - --set=20230914-061517F\n  # point-in-time recovery alternative\n  # - --type=time\n  # - --target=\"2023-06-09 14:15:11-04\"\n</code></pre> <p>The second scenario is if the whole database has been destroyed and it is a restore to a new database instance. In this case a <code>datasource</code> attribute has to be added to define the source for the backup.</p> Restore From S3 (New Database)<pre><code>---\ndataSource:\n  postgresCluster:\n    clusterName: direktiv\n    repoName: repo1\n    options:\n      - --set=20221023-042407F\n</code></pre> <p>Additional Information</p> <p>For more information visit Percona's documentation about backup and restore.</p>"},{"location":"installation/database/#restore-from-pvc","title":"Restore from PVC","text":"<p>In case the database is not using S3 backups the backups need to be stored in a safe location in case of loss of the node storing the backups. The data has to be transferred via e.g. <code>scp</code> using a cron job. A restore of an existing database can be achieved with a simple <code>restore</code> attribute in the database configuration YAML mentioned in the S3 restore section of this documentation. The process is different if the backup node has been destroyed. It is important to not do this restore procedure if a backup is already running. The backup needs to be rescheduled to execute it without running a backup in parallel.</p>"},{"location":"installation/database/#identify-backup-pvc","title":"Identify Backup PVC","text":"<p>The first step to store the backup is to identify the node where the backup is stored.</p> Identify PV<pre><code>kubectl get pv\n\nNAME                                       CAPACITY   ...     CLAIM\n# This is the backup node\npvc-80ae5325-8b27-4695-b6df-b362dd946cb7   1Gi        ...     postgres/direktiv-cluster-repo1\npvc-ce9bb226-1038-49bf-bed6-e6d0188b228c   1Gi        ...     postgres/direktiv-cluster-instance1-q9sm-pgdata\n</code></pre> <p>Describing the PV shows the node where the data is stored and the directory of the data. This directory needs to be stored in a safe location for a later restore.</p> Identify PV<pre><code>kubectl describe  pv pvc-80ae5325-8b27-4695-b6df-b362dd946cb7\n\nName:              pvc-80ae5325-8b27-4695-b6df-b362dd946cb7\n...\nClaim:             postgres/direktiv-repo1\n...\nNode Affinity:\n  Required Terms:\n    # Node where the data is stored\n    Term 0:        kubernetes.io/hostname in [db2]\nMessage:\nSource:\n    Type:          HostPath (bare host directory volume)\n    # Data directory on the node\n    Path:          /var/lib/rancher/k3s/storage/pvc-80ae5325-8b27-4695-b6df-b362dd946cb7_postgres_direktiv-repo1\n    HostPathType:  DirectoryOrCreate\n</code></pre>"},{"location":"installation/database/#copy-data","title":"Copy Data","text":"<p>To restore the database a backup has to be selected. The available backups are in <code>&lt;Backup Directory&gt;/&lt;Old PVC Name&gt;/backup/db</code>. The directory will look like the following:</p> Backup Directory<pre><code>drwxr-x--- 7 root root 4096 Okt 23 08:11 .\ndrwxr-x--- 3 root root 4096 Okt 23 08:11 ..\ndrwxr-x--- 3 root root 4096 Okt 23 08:11 20221023-060801F\ndrwxr-x--- 3 root root 4096 Okt 23 08:11 20221023-060901F\ndrwxr-x--- 3 root root 4096 Okt 23 08:11 20221023-061001F\ndrwxr-x--- 3 root root 4096 Okt 23 08:11 20221023-061101F\ndrwxr-x--- 3 root root 4096 Okt 23 08:11 backup.history\n-rw-r----- 1 root root 2792 Okt 23 08:11 backup.info\n-rw-r----- 1 root root 2792 Okt 23 08:11 backup.info.copy\nlrwxrwxrwx 1 root root   16 Okt 23 08:11 latest -&gt; 20221023-061101F\n</code></pre> <p>The next step is to identify where the new backup folder is located. It is exactly the same procedure as used in the copying process. The <code>archive</code> and <code>backup</code> have to be copied into the new backup directory of the new cluster.</p> Copy  Folders<pre><code>sudo cp -Rf  &lt;Backup Directory&gt;/archive /var/lib/rancher/k3s/storage/&lt;New PV Directory&gt;\nsudo cp -Rf  &lt;Backup Directory&gt;/backup /var/lib/rancher/k3s/storage/&lt;New PV Directory&gt;\n\nsudo chown -R 26:tape  /var/lib/rancher/k3s/storage/&lt;New PV Directory&gt;\n</code></pre> <p>The selected restore needs to be configured in the database configuration YAML and applied with <code>kubectl apply -f mydb.yaml</code>.</p> Restore from PVC<pre><code>---\nspec:\n  dataSource:\n    postgresCluster:\n      clusterName: direktiv\n      repoName: repo1\n      options:\n        - --set=20221023-075501F\n        - --archive-mode=off\n</code></pre>"},{"location":"installation/database/#update-password","title":"Update Password","text":"<p>If this is a new installation of the database the password will be overwritten and the command to generate the <code>direktiv.yaml</code> file is incorrect. Therefore it is advised to update the password to the password in the Kubernetes secret and update Direktiv with the new password.</p> Update User Password<pre><code># get the old password\nkubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode\n\n# execute in pod\nkubectl exec -n postgres --stdin --tty direktiv-cluster-instance1-&lt;POD-ID&gt; -- psql\n\n# update user password\nALTER USER direktiv WITH PASSWORD '&lt;PASSWORD FROM FIRST COMMAND&gt;';\n\n# exit\n\\q\n</code></pre>"},{"location":"installation/database/#helpful-commands","title":"Helpful Commands","text":"Fetch Master Instance<pre><code>kubectl -n postgres get pods \\\n  --selector=postgres-operator.crunchydata.com/role=master \\\n  -o jsonpath='{.items[*].metadata.labels.postgres-operator\\.crunchydata\\.com/instance}'\n</code></pre> Cluster Information<pre><code>kubectl -n postgres describe postgrescluster direktiv\n</code></pre> Use psql in Database Instance<pre><code>kubectl exec -n postgres --stdin --tty direktiv-cluster-instance1-&lt;ID&gt; -- psql\n</code></pre>"},{"location":"installation/direktiv/","title":"Direktiv","text":"<p>Direktiv requires a few components to run. At least the database has to be installed before proceeding with this part of the installation. </p> <ul> <li>Linkerd</li> <li>Database</li> <li>Knative</li> <li>Direktiv</li> </ul> <p>The following is a two-step process. First Knative is installed. Knative is responsible to execute Direktiv's serverless functions. It comes pre-configured to work with Direktiv. </p>"},{"location":"installation/direktiv/#knative","title":"Knative","text":"<p>Knative is an essential part of Direktiv and can be installed with Knative's operator. The following command installs this operator in the default namespace.</p> Install Knative Operator<pre><code>kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.12.2/operator.yaml\n</code></pre> <p>After the deployment of the operator a new instance of Knative Serving can be created. Direktiv requires a certain configuration for Knative to work. There are two examples of configurations in the (Github repository). The first one is the standard configuration and the other one is an example with proxy settings. </p> Install Knative<pre><code>kubectl create ns knative-serving\nkubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/scripts/kubernetes/install/knative/basic.yaml\n</code></pre> <p>Direktiv supports Contour as network component. </p> Install Contour<pre><code>kubectl apply --filename https://github.com/knative/net-contour/releases/download/knative-v1.11.1/contour.yaml\n</code></pre> <p>This installs Contour in two namespaces <code>contour-internal</code> and <code>contour-external</code>. The second namespace is not needed for Direktiv to run and might even block the ingress controller from getting an external IP. This can be deleted with:</p> Delete Contour External<pre><code>kubectl delete namespace contour-external\n</code></pre>"},{"location":"installation/direktiv/#direktiv_1","title":"Direktiv","text":"<p>Firstly, create a <code>direktiv.yaml</code> file which contains all of the database connectivity and secret information created during the database setup:</p> Direktiv Database Configuration<pre><code>database:\n  # -- database host\n  host: \"direktiv-ha.postgres.svc\"\n  # -- database port\n  port: 5432\n  # -- database user\n  user: \"direktiv\"\n  # -- database password\n  password: \"direktivdirektiv\"\n  # -- database name, auto created if it does not exist\n  name: \"direktiv\"\n  # -- sslmode for database\n  sslmode: require\n</code></pre> Database Configuration (No Connection Pooling)<pre><code>echo \"database:\n  host: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode)\\\"\n  port: $(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode)\n  user: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode)\\\"\n  password: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode)\\\"\n  name: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode)\\\"\n  sslmode: require\" &gt; direktiv.yaml\n</code></pre> Database Configuration (With Connection Pooling)<pre><code>echo \"database:\n  host: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-host\"}}' | base64 --decode)\\\"\n  port: $(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"pgbouncer-port\"}}' | base64 --decode)\n  user: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode)\\\"\n  password: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode)\\\"\n  name: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode)\\\"\n  sslmode: require\" &gt; direktiv.yaml\n</code></pre> <p>Using this <code>direktiv.yaml</code> configuration, deploy the direktiv helm chart:</p> <pre><code>helm repo add fluent-bit https://fluent.github.io/helm-charts\nhelm repo add direktiv https://charts.direktiv.io\nhelm install -f direktiv.yaml -n direktiv direktiv direktiv/direktiv\n</code></pre> <p>Direktiv should now be running. Run this to get the IP of the UI:</p> <pre><code>kubectl -n direktiv get services direktiv-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre> <p>For more configuration options go to Direktiv's helm charts.</p>"},{"location":"installation/direktiv/#changing-logos","title":"Changing Logos","text":"<p>Direktiv can use custom logos for the UI. The only requirement is that the replacements need to be in SVG format (except the favicon). The following files are required:</p> <ul> <li>icon-light.svg: Small light icon for mobile view.</li> <li>icon-dark.svg: Small dark icon for mobile view.</li> <li>logo-light.svg: Light logo for normal sized UI.</li> <li>logo-dark.svg: Dark logo for normal sized UI.</li> <li>favicon.png: Favicon for Direktiv UI.</li> </ul> <p>The icons can be set via Direktiv's Helm chart.</p> Custom Logos<pre><code>helm install --set-file frontend.logos.icon-light=/home/jensg/logos/icon-light.svg  \\\n  --set-file frontend.logos.icon-dark=/home/jensg/logos/icon-dark.svg \\\n  --set-file frontend.logos.logo-light=/home/jensg/logos/logo-light.svg \\\n  --set-file frontend.logos.logo-dark=/home/jensg/logos/logo-dark.svg \\\n  --set-file frontend.logos.favicon=/home/jensg/logos/favicon.png \\\n  -f direktiv.yaml direktiv charts/direktiv/\n</code></pre>"},{"location":"installation/kubernetes/","title":"Kubernetes","text":"<p>Direktiv is a cloud-native solution requiring Kubernetes to run. It is working with all Kubernetes offerings of the major cloud providers as well as on-premise Kubernetes installations. The easiest way to install a Kubernetes cluster for Kubernetes is using k3s. The following section explains how to install k3s on-premise. The minimum version is 1.24.</p>"},{"location":"installation/kubernetes/#k3s","title":"k3s","text":"<p>Direktiv supports Kubernetes offerings from all major cloud providers and requires Kubernets 1.24+ to be installed. Direktiv supports Kubernetes setups with a single node, seperate server and agents nodes as well as small setups with nodes acting both as server and agent. The following section describes the installation with k3s.</p>"},{"location":"installation/kubernetes/#single-node-setup","title":"Single Node Setup","text":"<p>A single node setup requires no further configuration and k3s can be used with the default settings. This setup disables Traefik to be replaced with Nginx during the installation. If proxy configuration is required please read the proxy setup section. </p> One Node Setup<pre><code>curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode=644\n</code></pre> <p>k3s Version</p> <p>If an error message occurs during installation, e.g. <code>resource mapping not found for name: \"linkerd-heartbeat\" namespace: \"linkerd\" from \"\": no matches for kind \"CronJob\" in version \"batch/v1beta1\"</code> it is most likely the wrong k3s version. To keep k3S small there is only a subset of Kubernetes APIs available. Please try to update k3s to the latest version or at least 1.24</p>"},{"location":"installation/kubernetes/#multi-node-setup","title":"Multi Node Setup","text":"<p>For production use it is recommended to run Direktiv in a multi-node environment. The k3s documentation page provides a lot of information about configuration and installation options. The following is a quick installation instruction to setup a three node cluster with nodes action as servers and agents. </p>"},{"location":"installation/kubernetes/#server-configuration","title":"Server configuration","text":"<p>In a multi-node environment the nodes have to communicate with each other. Therefore certain ports between those nodes have to be open. The following table shows the ports required to be accessible (incoming) for the nodes to enable this. On some Linux distributions firewall changes have to be applied. Please see k3s installation guide for detailed installation instructions.</p> Protocol Port Source Description TCP 6443 k3s agent nodes Kubernetes API Server UDP 8472 k3s server and agent nodes VXLAN TCP 10250 k3s server and agent nodes Kubelet metrics TCP 2379-2380 k3s server nodes Required for HA with embedded etcd only <p>Firewall changes (Centos/RedHat):</p> Example Firewall Changes Centos/RedHat<pre><code>sudo firewall-cmd --permanent --add-port=6443/tcp\nsudo firewall-cmd --permanent --add-port=10250/tcp\nsudo firewall-cmd --permanent --add-port=8472/udp\nsudo firewall-cmd --permanent --add-port=2379-2380/tcp\nsudo firewall-cmd --reload\n</code></pre> <p>Additional Centos/RedHat Instructions</p> <p>https://rancher.com/docs/k3s/latest/en/advanced/#additional-preparation-for-red-hat-centos-enterprise-linux</p> <p>An additional Kubernetes requirement is to disable swap on the nodes. This change need to be applied permanently to survive reboots. This might be achieved differently on different Linux distributions.</p> Disable Swap<pre><code>sudo swapoff -a\nsudo sed -e '/swap/s/^/#/g' -i /etc/fstab\n</code></pre>"},{"location":"installation/kubernetes/#node-installation","title":"Node Installation","text":"<p>k3s provides a script to install k3s. It is recommended to use it for installation. The configuration can be done via environment variables during installation. For Direktiv the default ingress controller (Traefik) needs to be disabled because Nginx will be used. For installations using the embedded etcd the first server node requires the '--cluster-init' flag.</p> Initial Node<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable traefik --write-kubeconfig-mode=644 --cluster-init\" sh -\n</code></pre> <p>Loadbalancer</p> <p>To use MetalLB add <code>--disable servicelb</code> to the arguments, e.g. for on-premise installation. It is not needed if the cluster is installed in a cloud environment like AWS, GCP or Azure.</p> <p>To add nodes to the cluster the node token is required, which is saved under /var/lib/rancher/k3s/server/node-token. With this token additional nodes can be added.</p> Additional Nodes<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN=\"&lt;TOKEN FROM NODE-TOKEN FILE&gt;\" K3S_URL=https://&lt;cluster ip&gt;:6443 sh -\n</code></pre>"},{"location":"installation/kubernetes/#metallb","title":"MetalLB","text":"<p>In a on-premise environment a Kubernetes bare-metal load-balancer is required. The following example shows the use of MetalLB. k3s load-balancer needs to be disabled with <code>--disable servicelb</code> for this to work.</p> Disable Loadbalancer<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable servicelb --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN=\"&lt;TOKEN FROM NODE-TOKEN FILE&gt;\" K3S_URL=https://&lt;cluster ip&gt;:6443 sh -\n</code></pre> <p>To install MetalLB add the Helm repository and configure the avaiable IPs. </p> <pre><code>helm repo add metallb https://metallb.github.io/metallb\nhelm install metallb metallb/metallb\n</code></pre> <p>MetalLB needs an IP pool to serve IP address. During the installation this pool can be configured with fhe following example YAML file:</p> MetalLB IP Pool Configuration<pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: myip\n  namespace: default\nspec:\n  addresses:\n  - 192.168.0.199/32\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: ipadvertise\n  namespace: default\n</code></pre>"},{"location":"installation/kubernetes/#proxy-setup","title":"Proxy Setup","text":"<p>K3s will download container images during installation and runtime. For the downloads of those internet connectivity is required. If the nodes are behind a proxy server the Linux environment variables need to provided to the service, e.g.:</p> Proxy Settings for k3s<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN=\"&lt;TOKEN FROM NODE-TOKEN FILE&gt;\" K3S_URL=https://&lt;cluster ip&gt;:6443 HTTP_PROXY=\"http://192.168.1.10:3128\" HTTPS_PROXY=\"http://192.168.1.10:3128\" NO_PROXY=\"localhost,127.0.0.1,svc,.cluster.local,192.168.1.100,192.168.1.101,192.168.1.102,10.0.0.0/8\" sh -\n</code></pre> <p>Alternatively the environment variables HTTP_PROXY, HTTPS_PROXY and NO_PROXY can be set and k3s will automatically add them to the service configuration file.</p>"},{"location":"installation/linkerd/","title":"Linkerd (Optional)","text":"<p>Linkerd is a lightweight service mesh for Kubernetes and can be used in Direktiv as a mechanism to secure communication between the components. Linkerd can enable mTLS between the core Direktiv pods as well as the containers running in a flow. The installation of Linkerd is optional. The easiest way to install Linkerd is via Helm. </p>"},{"location":"installation/linkerd/#creating-certificates","title":"Creating Certificates","text":"<p>The identity component of Linkerd requires setting up a trust anchor certificate, and an issuer certificate with its key. The following script starts a container and generates the certificates needed:</p> Generating Linkerd Certificates<pre><code>certDir=$(exe='step certificate create root.linkerd.cluster.local ca.crt ca.key \\\n--profile root-ca --no-password --insecure \\\n&amp;&amp; step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\\n--profile intermediate-ca --not-after 87600h --no-password --insecure \\\n--ca ca.crt --ca-key ca.key'; \\\n  sudo docker run --mount \"type=bind,src=$(pwd),dst=/home/step\"  -i smallstep/step-cli /bin/bash -c \"$exe\";  \\\necho $(pwd));\n</code></pre> <p>Permissions</p> <p>The directory where the certificates are located is stored in $certDir. If there are permission problems, please try a different directory with write permissions.</p>"},{"location":"installation/linkerd/#install-with-helm","title":"Install with Helm","text":"<p>After creating the certificates the certificate folder should be located at $certDir. The expiry date provided during installation has to be the same as the value for the certificates (in this case: one year). The following script installs Linkerd with the previously generated certificates:</p> Install Linkerd CRDs<pre><code>helm repo add linkerd https://helm.linkerd.io/stable;\n\nhelm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace \n</code></pre> Install Linkerd<pre><code>helm install linkerd-control-plane \\\n  -n linkerd \\\n  --set-file identityTrustAnchorsPEM=$certDir/ca.crt \\\n  --set-file identity.issuer.tls.crtPEM=$certDir/issuer.crt \\\n  --set-file identity.issuer.tls.keyPEM=$certDir/issuer.key \\\n  linkerd/linkerd-control-plane --wait\n</code></pre>"},{"location":"installation/linkerd/#annotate-namespaces","title":"Annotate Namespaces","text":"<p>To use the service mesh (and, in particular, the mTLS communication) between pods within a Direktiv cluster the namespaces need to be annotated for Linkerd to inject its proxy. The default namespace to annotate is <code>direktiv</code>.</p> Create Namespace<pre><code>kubectl create namespace direktiv\n</code></pre> Annotate Namespace<pre><code>kubectl annotate ns --overwrite=true direktiv linkerd.io/inject=enabled\n</code></pre>"},{"location":"installation/summary/","title":"Quick Install","text":"<p>This is a list of \"copy&amp;paste\" commands which creates a one node Direktiv cluster.</p>"},{"location":"installation/summary/#k3s","title":"K3s","text":"<pre><code>curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode=644\n\nsudo swapoff -a\nsudo sed -e '/swap/s/^/#/g' -i /etc/fstab\n</code></pre>"},{"location":"installation/summary/#linkerd","title":"Linkerd","text":""},{"location":"installation/summary/#create-certificates","title":"Create certificates","text":"<pre><code>certDir=$(exe='step certificate create root.linkerd.cluster.local ca.crt ca.key \\\n--profile root-ca --no-password --insecure \\\n&amp;&amp; step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\\n--profile intermediate-ca --not-after 87600h --no-password --insecure \\\n--ca ca.crt --ca-key ca.key'; \\\n  sudo docker run --mount \"type=bind,src=$(pwd),dst=/home/step\"  -i smallstep/step-cli /bin/bash -c \"$exe\";  \\\necho $(pwd));\n</code></pre>"},{"location":"installation/summary/#install-linkerd","title":"Install Linkerd","text":"<pre><code>helm repo add linkerd https://helm.linkerd.io/stable\n\nhelm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace\n\nhelm install linkerd-control-plane \\\n  -n linkerd \\\n  --set-file identityTrustAnchorsPEM=$certDir/ca.crt \\\n  --set-file identity.issuer.tls.crtPEM=$certDir/issuer.crt \\\n  --set-file identity.issuer.tls.keyPEM=$certDir/issuer.key \\\n  linkerd/linkerd-control-plane --wait\n</code></pre>"},{"location":"installation/summary/#annotate-the-namespace","title":"Annotate the Namespace","text":"<pre><code>kubectl create namespace direktiv\n\nkubectl annotate ns --overwrite=true direktiv linkerd.io/inject=enabled\n</code></pre>"},{"location":"installation/summary/#database","title":"Database","text":"<pre><code>kubectl create namespace postgres\nhelm repo add percona https://percona.github.io/percona-helm-charts/\nhelm install --create-namespace --version 2.3.1  -n postgres pg-operator percona/pg-operator --wait\n</code></pre> <pre><code>kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/scripts/kubernetes/install/db/basic.yaml\n</code></pre>"},{"location":"installation/summary/#knative","title":"Knative","text":"<pre><code>kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.12.2/operator.yaml\nkubectl create ns knative-serving\nkubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/scripts/kubernetes/install/knative/basic.yaml\nkubectl apply --filename https://github.com/knative/net-contour/releases/download/knative-v1.11.1/contour.yaml\nkubectl delete namespace contour-external\n</code></pre>"},{"location":"installation/summary/#direktiv","title":"Direktiv","text":"<pre><code>echo \"database:\n  host: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"host\"}}' | base64 --decode)\\\"\n  port: $(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"port\"}}' | base64 --decode)\n  user: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"user\"}}' | base64 --decode)\\\"\n  password: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode)\\\"\n  name: \\\"$(kubectl get secrets -n postgres direktiv-cluster-pguser-direktiv -o 'go-template={{index .data \"dbname\"}}' | base64 --decode)\\\"\n  sslmode: require\" &gt; direktiv.yaml\n</code></pre> <pre><code>helm repo add fluent-bit https://fluent.github.io/helm-charts\nhelm repo add direktiv https://charts.direktiv.io\nhelm install -f direktiv.yaml -n direktiv direktiv direktiv/direktiv\n</code></pre>"},{"location":"installation/summary/#get-ip-of-direktiv","title":"Get IP of Direktiv","text":"<pre><code>kubectl -n direktiv get services direktiv-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre>"},{"location":"spec/TODO/","title":"Spec Documentation TODOs","text":"<ul> <li>YAML Section Inclusions:</li> <li>Logging</li> <li>Metadata </li> <li> <p>Errors</p> </li> <li> <p>Section about secrets.</p> </li> <li>Section about events.</li> <li>Section about errors &amp; error handling.</li> <li>Large section explaining the spec for containers and everything related to actions.</li> <li>Expand variables section. Explain function files and events.</li> </ul>"},{"location":"spec/db_migration/","title":"0.8.3","text":""},{"location":"spec/db_migration/#db-schema-change","title":"DB Schema Change","text":"<p>Exec the following sql queries:</p> <pre><code>ALTER TABLE \"instances_v2\" ADD COLUMN IF NOT EXISTS \"server\" uuid;\n</code></pre>"},{"location":"spec/db_migration/#082","title":"0.8.2","text":""},{"location":"spec/db_migration/#db-schema-change_1","title":"DB Schema Change","text":"<p>Exec the following sql queries:</p> <pre><code>ALTER TABLE \"filesystem_roots\" ADD COLUMN IF NOT EXISTS \"created_at\" timestamptz NOT NULL DEFAULT CURRENT_TIMESTAMP;\nALTER TABLE \"filesystem_roots\" ADD COLUMN IF NOT EXISTS \"updated_at\" timestamptz NOT NULL DEFAULT CURRENT_TIMESTAMP;\n\nALTER TABLE \"filesystem_files\" ADD COLUMN IF NOT EXISTS \"data\" bytea;\nALTER TABLE \"filesystem_files\" ADD COLUMN IF NOT EXISTS \"checksum\" text;\n\n\nALTER TABLE \"instances_v2\" DROP COLUMN IF EXISTS \"revision_id\";\nALTER TABLE \"metrics\" DROP COLUMN IF EXISTS \"revision\";\nDROP TABLE IF EXISTS \"filesystem_revisions\";\n\n\nALTER TABLE \"event_topics\" ADD COLUMN IF NOT EXISTS \"filter\" text;\nALTER TABLE \"event_topics\" DROP CONSTRAINT \"no_dup_topics_check\";\nALTER TABLE \"event_topics\" ADD  CONSTRAINT \"no_dup_topics_check\" UNIQUE (\"event_listener_id\", \"topic\", \"filter\");\n</code></pre>"},{"location":"spec/instance-data/input/","title":"Workflow Input","text":"<p>When a workflow is triggered and spawns a new instance it may do so with some starting value for its instance data. Here's everything you need to know about workflow input. </p>"},{"location":"spec/instance-data/input/#api-subflow","title":"API / Subflow","text":"<p>If a workflow is invoked directly, either through the API or as a subflow to another instance, its input is passed in as-is. </p> <p>So if you call the workflow with the following input:</p> <pre><code>{\n  \"msg\": \"Hello, world!\"\n}\n</code></pre> <p>Then the instance data for the workflow will, be the same:</p> <pre><code>{\n  \"msg\": \"Hello, world!\"\n}\n</code></pre> <p>That is, unless the input data isn't a valid JSON object. If the input is valid JSON but not an object, as in the following example, it is wrapped within an object automatically under the property <code>.input</code>. </p> <p>So this input:</p> <pre><code>[1, 2, 3]\n</code></pre> <p>Becomes:</p> <pre><code>{\n  \"input\": [1, 2, 3]\n}\n</code></pre> <p>If the input data isn't valid JSON at all, it is treated as binary data. Binary data is converted into a base64 encoded string and passed into the instance the same way as above.</p> <p>This input:</p> <pre><code>Hello, world!\n</code></pre> <p>Becomes:</p> <pre><code>{\n  \"input\": \"SGVsbG8sIHdvcmxkIQo=\"\n}\n</code></pre> <p>This treatment of binary data allows workflows to handle non-JSON inputs. Common examples include XML and form data. Just use a function to extract the information needed from these other formats and convert them to JSON.</p> <p>One thing to keep in mind that might trip you up: if you provide no input data whatsoever that's not valid JSON. It is valid binary data, which means this input:</p> <pre><code>\n</code></pre> <p>Becomes:</p> <pre><code>{\n  \"input\": \"\"\n}\n</code></pre> <p>An empty string is a valid base64 representation of zero bytes.</p>"},{"location":"spec/instance-data/input/#cron","title":"CRON","text":"<p>By their nature, scheduled workflows have empty input. They will always be:</p> <pre><code>{}\n</code></pre> <p>This doesn't mean they have to do exactly the same thing each time, it just means you need to get a little creative. For example, begin your workflow by loading data from variables or by using an action that grabs data from an external source.</p>"},{"location":"spec/instance-data/input/#cloudevents-events","title":"CloudEvents Events","text":"<p>Workflows that are triggered by receiving one or more events will include the received event(s) in their input data. Each received event will appear in the instance data under a property with the same value as their event type, to allow workflows to distinguish between events.</p> <p>For an instance triggered with the following event:</p> <pre><code>{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull.create\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n}\n</code></pre> <p>The instance input data becomes:</p> <pre><code>{\n  \"com.github.pull.create\": {\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull.create\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n  }\n}\n</code></pre> <p>If an event's payload is JSON it should be directly addressable, rather than being embedded within a string.</p>"},{"location":"spec/instance-data/input/#large-inputs","title":"Large Inputs","text":"<p>Like instance data, input data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 128 MiB. </p>"},{"location":"spec/instance-data/instance-data/","title":"Instance Data","text":"<p>Every workflow instance has its own instance data, which is data that is exclusively accessible to the instance, and only changeable by the instance. </p>"},{"location":"spec/instance-data/instance-data/#json-objects","title":"JSON Objects","text":"<p>Instance data is represented in JSON form, and is always a valid JSON object. This detail is important because it means that not everything that is valid JSON can be valid instance data. </p> <p>This is valid instance data:</p> <pre><code>{}\n</code></pre> <p>So is this:</p> <pre><code>{\n  \"list\": [1, 2, 3]\n}\n</code></pre> <p>And this:</p> <pre><code>{\n  \"a\": 5,\n  \"b\": \"6\",\n  \"c\": true,\n  \"d\": {\n    \"list\": [7, \"8\"]\n  }\n}\n</code></pre> <p>But this are not valid instance data, even though it is valid JSON:</p> <pre><code>true\n</code></pre> <p>Neither is this:</p> <pre><code>\"Hello, world!\"\n</code></pre> <p>Nor is this:</p> <pre><code>[{\n  \"a\": 5\n}]\n</code></pre> <p>Another way of looking at it: it's not valid instance data unless it's valid JSON beginning with <code>{</code> and ending with <code>}</code>.</p>"},{"location":"spec/instance-data/instance-data/#size-limit","title":"Size Limit","text":"<p>The size of instance data is measured in terms of the length (in bytes) of its JSON representation. For technical reasons, there is an enforced upper limit allowed for this maximum size. This limit can vary according to the specific configuration of a Direktiv installation, but the default is 128 MiB.</p>"},{"location":"spec/instance-data/instance-data/#lifecycle","title":"Lifecycle","text":"<p>The starting value for an instance's data is set based on what triggered the workflow to spawn a new instance. See Workflow Input.</p> <p>Afterwards, the instance may manipulate the data in predictable ways according to the instructions in the workflow definition. The main way to change instance data is through Transforms. </p> <p>Other operations can also contribute to the instance data. Actions may return results, error handling may save error information, event listeners save received events, and variable getters can retrieve data saved elsewhere and add it to the instance data.</p> <p>After the final operation of an instance is executed the instance data becomes the instance's output data. See Instance Output. Output data is viewable by the API, and is also returned to the caller if the instance was executed as a subflow to another workflow.</p>"},{"location":"spec/instance-data/output/","title":"Instance Output","text":"<p>Unlike instance data, which is not accessible to anything other than the instance itself, instance output is exposed via API. It is also returned to caller instance if the workflow was invoked as a subflow. </p> <p>When an instance completes it saves its instance data as its output, which indirectly exposes the instance data. Normally this is the desired behaviour, but it can present a security risk if handled incorrectly. </p> <p>Workflows should take steps to trim things they don't mean to return before terminating using transforms.</p>"},{"location":"spec/instance-data/output/#large-outputs","title":"Large Outputs","text":"<p>Like instance data, output data has size limits. These size limits are usually the same, but not necessarily. This will vary according to the configuration of each Direktiv installation, and is usually about 128 MiB. </p>"},{"location":"spec/instance-data/security/","title":"Security","text":"<p>There are a number of security concerns to keep in mind with instance data:</p>"},{"location":"spec/instance-data/security/#input-output-data-is-remembered","title":"Input &amp; Output Data Is Remembered","text":"<p>A copy of the starting value is saved separately so that instances can be replayed. It also helps to debug workflows. It is worth keeping this in mind. Even if your workflow deletes sensitive fields, they are still stored somewhere they could be reused or read later. </p> <p>Likewise, output data is saved. It is returned to parent instances if the instance was executed as a subflow. Both input and output data is requestable via API. For a good way to handle most sensitive data, consider using secrets.</p>"},{"location":"spec/instance-data/security/#input-validation","title":"Input Validation","text":"<p>To protect your workflows from behaving in unexpected ways, including intentional exploits by attackers, it is good practice to validate your input data before acting upon it. That is why we recommend beginning every workflow with a validate state.</p>"},{"location":"spec/instance-data/security/#private","title":".private","text":"<p>As a basic precaution, anything stored under <code>.private</code> is redacted over the APIs that retrieve instance input and output data. This data is still usable by the instance. Could still be returned to a parent instance. It is still stored in the database in plaintext. And there is nothing preventing you from transforming it or passing it somewhere that exposes this information. Use this feature with caution. </p>"},{"location":"spec/instance-data/structured-jx/","title":"Structured JX","text":"<p>Many fields of the workflow definition are described as \"Structured JX\". That's a name we use for fields that support complex and powerful query logic that we'll describe in greater detail here.</p>"},{"location":"spec/instance-data/structured-jx/#jq","title":"JQ","text":"<p>Since instance data is represented as JSON, the most natural way to work with that data is with the powerful JSON query language called jq. </p> <p>Whenever a string appears within a Structured JX field that includes <code>jq(...)</code>, everything between the brackets is evaluated as a jq query against the instance data. Then the entire <code>jq(...)</code> part is replaced by the results of that query. </p> <p>Note: YAML allows for strings without quotation marks, but this should be avoided when using Structured JX. The characters in the queries will commonly be interpreted in unintended ways by the YAML parser.</p> <p>If the <code>jq(...)</code> part constitutes the entirety of the string then the entire string is replaced by whatever data type was returned. If not, the results are marshalled into a JSON string and substituted into the parent string. </p> <p>The one exception to this rule is if the returned data type is a string, in which case it is substituted as-is without marshalling into JSON. This enables you to build strings without filling them with quotation marks.</p>"},{"location":"spec/instance-data/structured-jx/#example-1","title":"Example 1","text":"Instance Data<pre><code>{\n  \"a\": [1, 2, 3]\n}\n</code></pre> Structured JX<pre><code>'jq(.a)'\n</code></pre> Evaluated Result<pre><code>[1, 2, 3]\n</code></pre>"},{"location":"spec/instance-data/structured-jx/#example-2","title":"Example 2","text":"Instance Data<pre><code>{\n  \"a\": [1, 2, 3]\n}\n</code></pre> Structured JX<pre><code>'a: jq(.a)'\n</code></pre> Evaluated Result<pre><code>\"a: [1, 2, 3]\"\n</code></pre>"},{"location":"spec/instance-data/structured-jx/#example-3","title":"Example 3","text":"Instance Data<pre><code>{\n  \"a\": \"hello\"\n}\n</code></pre> Structured JX<pre><code>'a: jq(.a)'\n</code></pre> Evaluated Result<pre><code>\"a: hello\"\n</code></pre>"},{"location":"spec/instance-data/structured-jx/#js","title":"JS","text":"<p>JQ isn't the only option available to interact with the instance data. Javascript is also supported using <code>js(...)</code> in a very similar way. Entire Javascript scripts can be embedded in strings within Structured JX.</p> <p>Note: YAML supports several ways of including large or multi-line strings. But each of these ways is treated a little bit differently by the YAML parser. To preserve newlines, we recommend using the <code>|</code> form. With Javascript this often necessary. </p> <p>When writing scripts this way, the instance data is copied and exposed to the script in an object called <code>data</code>. </p>"},{"location":"spec/instance-data/structured-jx/#example-1_1","title":"Example 1","text":"JQ<pre><code>transform: 'jq({x: 5})'\n</code></pre> Analogous Javascript<pre><code>transform: |\n  js(\n    items = new Object()\n    items.x = 5\n    return items\n  )\n</code></pre>"},{"location":"spec/instance-data/structured-jx/#example-2_1","title":"Example 2","text":"JQ<pre><code>transform: 'jq({x: .a})'\n</code></pre> Analogous Javascript<pre><code>transform: |\n  js(\n    items = new Object()\n    items.x = data['a']\n    return items\n  )\n</code></pre>"},{"location":"spec/instance-data/structured-jx/#yaml","title":"YAML","text":"<p>So far we've seen how you can use jq or Javascript to produce a value for your Structured JX field, but it's also possible to use neither, or both. </p> <p>The \"Structured\" part of Structured JX is so named because you don't have to provide a single string. You can provide any type of data you like. The entirety of what is provided will be converted from its YAML representation to a JSON representation. And then every field within will be searched recursively for embedded jq/Javascript. </p>"},{"location":"spec/instance-data/structured-jx/#example","title":"Example","text":"Instance Data Before Transform<pre><code>{\n  \"a\": [1, 2, 3]\n}\n</code></pre> Transform<pre><code>tranform:\n  x: 'jq(.a)'\n  y: |\n    js(\n    var output = data['a'].map((x) =&gt; {return ++x;})\n      return output\n  )\n  z: 5\n  listA: [\"a\", \"b\", \"c\"]\n  listB:\n  - d\n  - e\n  - f\n  obj:\n    i: 10\n  j: 'jq(.a[2])'\n</code></pre> Evaluated Result<pre><code>{\n  \"listA\": [\"a\", \"b\", \"c\"],\n  \"listB\": [\"d\", \"e\", \"f\"],\n  \"obj\": {\n    \"i\": 10,\n    \"j\": 3\n  },\n  \"x\": [1, 2, 3],\n  \"y\": [2, 3, 4],\n  \"z\": 5\n}\n</code></pre>"},{"location":"spec/instance-data/transforms/","title":"Transforms","text":"<p>Whenever an instance finishes executing a state there is an opportunity to perform a Transform. Usually with a field called <code>transform</code>, but sometimes in other forms. The <code>switch</code> state also has a <code>defaultTransform</code>, for example. </p> <p>All transforms use structured jx, giving you powerful options to enrich, sanitize, or modify instance data. All transforms must produce output that remains valid instance data, otherwise an error will be thrown: <code>direktiv.jq.notObject</code>.</p>"},{"location":"spec/instance-data/transforms/#examples","title":"Examples","text":"<p>Here are some common use-case helpful examples of transforms.</p>"},{"location":"spec/instance-data/transforms/#completely-replacing-instance-data","title":"Completely Replacing Instance Data","text":"Instance Data Before Transform<pre><code>{\n  \"msg\": \"Hello, world!\n}\n</code></pre> Transform Snippet<pre><code>- id: snippet\n  type: noop\n  transform: \n    x: 5\n</code></pre> Instance Data After Transform<pre><code>{\n  \"x\": 5\n}\n</code></pre>"},{"location":"spec/instance-data/transforms/#replacing-a-subset-of-instance-data","title":"Replacing A Subset Of Instance Data","text":"Instance Data Before Transform<pre><code>{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3\n}\n</code></pre> Transform Snippet<pre><code>- id: snippet\n  type: noop\n  transform: 'jq(.a = 5 | .b = 6)'\n</code></pre> Instance Data After Transform<pre><code>{\n  \"a\": 5,\n  \"b\": 6,\n  \"c\": 3\n}\n</code></pre>"},{"location":"spec/instance-data/transforms/#deleteing-a-subset-of-instance-data","title":"Deleteing A Subset of Instance Data","text":"Instance Data Before Transform<pre><code>{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3\n}\n</code></pre> Transform Snippet<pre><code>- id: snippet\n  type: noop\n  transform: 'jq(del(.a) | del(.b))'\n</code></pre> Instance Data After Transform<pre><code>{\n  \"c\": 3\n}\n</code></pre>"},{"location":"spec/instance-data/transforms/#adding-a-new-value","title":"Adding A New Value.","text":"Instance Data Before Transform<pre><code>{\n  \"a\": 1\n}\n</code></pre> Transform Snippet<pre><code>- id: snippet\n  type: noop\n  transform: 'jq(.b = 2)'\n</code></pre> Instance Data After Transform<pre><code>{\n  \"a\": 1,\n  \"b\": 2\n}\n</code></pre>"},{"location":"spec/instance-data/transforms/#renaming-a-subset-of-instance-data","title":"Renaming A Subset of Instance Data","text":"Instance Data Before Transform<pre><code>{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3\n}\n</code></pre> Transform Snippet<pre><code>- id: snippet\n  type: noop\n  transform: 'jq(.x = .a | del(.a))'\n</code></pre> Instance Data After Transform<pre><code>{\n  \"b\": 2,\n  \"c\": 3,\n  \"x\": 1\n}\n</code></pre>"},{"location":"spec/variables/system/","title":"System Variables","text":"<p>In addition to the standard scopes, there is a special <code>system</code> scope. This scope is a utility to make miscellaneous information accessible to an instance. The following special variables exist in the system scope:</p> Key Description instance Returns the instance ID of the running instance. uuid Returns a randomly generated UUID. epoch Returns the current time in unix/epoch format."},{"location":"spec/variables/variables/","title":"Variables","text":"<p>Direktiv can store data separately to instance data. An instance can read and change its instance data at will so you might wonder why this separation needs to exist, but it turns out variables solve a number of problems: </p> <ul> <li>Efficiently passing around large datasets or files to actions, especially ones that exceed instance data size limits. </li> <li>Persisting data between instances of a workflow.</li> <li>Sharing data between different workflows.</li> </ul>"},{"location":"spec/variables/variables/#scopes","title":"Scopes","text":"<p>All variables belong to a scope. The scopes are <code>instance</code>, <code>workflow</code>, <code>namespace</code>, and <code>file</code>. Instance scoped variables are only accessible to the singular instance that created them. Workflow scoped variables can be used and shared between multiple instances of the same workflow. Namespace scoped variables are available to all instances of all workflows on the namespace. For these first three scopes, variables are identified by a name, and each name is unique within its scope.</p> <p>The <code>file</code> scope is treated similarly. It looks for a file in the namespace's filetree to use. This means <code>file</code> scoped variables are available to all instances of all workflows on the namespace, which is similar to <code>namespace</code> scoped variables. However, instead of using just a name, you need to reference them by a filepath. Additionally, the <code>file</code> scope is considered read-only: instances do not have permission to delete, modify, or create files on the filetree.</p>"},{"location":"spec/variables/variables/#states","title":"States","text":"<p>Two types of states in the workflow spec interact directly with variables: <code>getter</code> and <code>setter</code>.</p>"},{"location":"spec/variables/variables/#files","title":"Files","text":"<p>Due to size limitations on action inputs and instance data it can sometimes be impossible to pass data to actions without using variables. Actions can interact with variables directly, loading them onto their file-system and sometimes creating/changing variables as well. </p>"},{"location":"spec/workflow-yaml/action/","title":"Action State","text":"<pre><code>- id: a\n  type: action\n  action:\n    function: myfunc\n    input: 'jq(.x)'\n</code></pre>"},{"location":"spec/workflow-yaml/action/#actionstatedefinition","title":"ActionStateDefinition","text":"<p>The <code>action</code> state is the simplest and most common way to call a function or invoke a workflow to act as a subflow. See Actions. </p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>action</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>timeout</code> ISO8601 duration string to set a non-default timeout. string no <code>async</code> If set to <code>true</code>, the workflow execution will continue without waiting for the action to return. boolean no <code>action</code> Defines the action to perform. ActionDefinition yes"},{"location":"spec/workflow-yaml/actions/","title":"Actions","text":"<pre><code>- id: a\n  type: action\n  action:\n    function: myfunc\n    input: 'jq(.x)'\n</code></pre>"},{"location":"spec/workflow-yaml/actions/#actiondefinition","title":"ActionDefinition","text":"Parameter Description Type Required <code>function</code> Name of the referenced function. See FunctionDefinition. string yes <code>input</code> Selects or generates the data to send as input to the function. Structured JQ no <code>secrets</code> Defines a list of secrets to temporarily add to the instance data under <code>.secrets</code>, before evaluating the <code>input</code>. []string no <code>retries</code> []RetryPolicyDefinition no <code>files</code> Determines a list of files to load onto the function's file-system from variables. Only valid if the referenced function supports it. []FunctionFileDefinition no"},{"location":"spec/workflow-yaml/actions/#retrypolicydefinition","title":"RetryPolicyDefinition","text":"<pre><code>- id: a\n  type: action\n  action:\n    function: myfunc\n    input: 'jq(.x)'\n    retries:\n      codes: [\".*\"]\n      max_attempts: 3\n      delay: PT3S\n      multiplier: 1.5\n</code></pre> Parameter Description Type Required codes A list of \"glob\" patterns that will be compared to catchable error codes returned by the function to determine if this retry policy applies. []string yes max_attempts Maximum number of retry attempts. If the function has been retried this many times or more when this policy is invoked the retry will be skipped, and instead the error will be escalated to the state's error handling logic. integer yes delay ISO8601 duration string giving a time delay between retry attempts. string no multiplier Value by which the delay is multiplied after each attempt. float no"},{"location":"spec/workflow-yaml/actions/#functionfiledefinition","title":"FunctionFileDefinition","text":"<pre><code>- id: a\n  type: action\n  action:\n    function: myfunc\n    input: 'jq(.x)'\n    files:\n    - key: VAR_A \n      scope: namespace\n      as: a\n</code></pre> <p>Some function types support loading variable directly from storage onto their file-systems. This object defines what variable to load and what to save it as.</p> Parameter Description Type Required <code>key</code> Identifies which variable to load into a file. string yes <code>scope</code> Specifies the scope from which to load the variable. VariableScopeDefinition no <code>as</code> Names the resulting file. If left unspecified, the <code>key</code> will be used instead. string no"},{"location":"spec/workflow-yaml/actions/#variablescopedefinition","title":"VariableScopeDefinition","text":"<p>Every variable exists within a single scope. The scope dictates what can access it and how persistent it is. There are four defined scopes:</p> <ul> <li><code>instance</code></li> <li><code>workflow</code></li> <li><code>namespace</code></li> <li><code>file</code></li> </ul>"},{"location":"spec/workflow-yaml/consume-event/","title":"ConsumeEvent State","text":"<pre><code>- id: a\n  type: consumeEvent\n  timeout: PT15M\n  event:\n    type: com.github.pull.create\n    context:\n      subject: '123'\n</code></pre>"},{"location":"spec/workflow-yaml/consume-event/#consumeeventstatedefinition","title":"ConsumeEventStateDefinition","text":"<p>To pause the workflow and wait until a CloudEvents event is received before proceeding, the <code>consumeEvent</code> is the simplest state that can be used. It is one of three states that can do so, along with <code>eventsAnd</code> and <code>eventsXor</code>.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>consumeEvent</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>timeout</code> An ISO8601 duration string. string no <code>event</code> Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes"},{"location":"spec/workflow-yaml/consume-event/#consumeeventdefinition","title":"ConsumeEventDefinition","text":"<p>The StartEventDefinition is a structure shared by various event-consuming states. </p> Parameter Description Type Required <code>type</code> Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own <code>type</code> context value. string yes <code>context</code> Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" must be strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. Structured JQ no <p>The received data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following data:</p> CloudEvents Event<pre><code>{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull.create\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n}\n</code></pre> Input Data<pre><code>{\n  \"com.github.pull.create\": {\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull.create\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n  }\n}\n</code></pre>"},{"location":"spec/workflow-yaml/delay/","title":"Delay State","text":"<pre><code>- id: a\n  type: delay\n  duration: PT10S\n</code></pre>"},{"location":"spec/workflow-yaml/delay/#delaystatedefinition","title":"DelayStateDefinition","text":"<p>If the workflow needs to pause for a specific length of time, the delay state is usually the simplest way to do that.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>delay</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>duration</code> An ISO8601 duration string. string yes"},{"location":"spec/workflow-yaml/error/","title":"Error State","text":"<pre><code>- id: a\n  type: error\n  error: badinput\n  message: 'Missing or invalid value for required input.'\n</code></pre>"},{"location":"spec/workflow-yaml/error/#errorstatedefinition","title":"ErrorStateDefinition","text":"<p>When workflow logic end up in a failure mode, the <code>error</code> state can be used to mark the instance as failed. This allows the instance to report what went wrong to the caller, which may then be handled or reported appropriately.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>error</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>error</code> A short descriptive error code that can be caught by a parent workflow. string yes <code>message</code> Generates a more detailed message or object that can contain instance data, to provide more information for users. Structured JQ yes"},{"location":"spec/workflow-yaml/errors/","title":"Errors","text":"<p>Errors can happen for many reasons. Direktiv allows you to catch and handle these errors using a common field 'catch'. This field takes an array of ErrorCatchDefinition objects, each specifying one or more errors that apply and where to transition to next in order to handle them. When an error is thrown, the list of error catchers is evaluated in order until a match is found. If no match is found, the instance fails. </p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: a\n  type: consumeEvent\n  timeout: PT5S\n  event:\n    type: com.github.pull.create\n  catch: \n  - error: \"direktiv.cancels.timeout.soft\"\n    transition: handle-error\n- id: handle-error\n  type: noop\n  log: handling error\n</code></pre>"},{"location":"spec/workflow-yaml/errors/#errorcatchdefinition","title":"ErrorCatchDefinition","text":"Parameter Description Type Required error Specified what error code(s) this catcher applies to. This should be a \"glob\" pattern that will be compared to catchable error codes to determine if this retry policy applies. string yes transition Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no"},{"location":"spec/workflow-yaml/events-and/","title":"EventsAnd State","text":"<pre><code>- id: a\n  type: eventsAnd\n  timeout: PT15M\n  events:\n  - type: com.github.pull.create\n    context:\n      subject: '123'\n  - type: com.github.pull.delete\n    context:\n      subject: '123'\n</code></pre>"},{"location":"spec/workflow-yaml/events-and/#eventsandstatedefinition","title":"EventsAndStateDefinition","text":"<p>To pause the workflow and wait until multiple CloudEvents events are received before proceeding, the <code>eventsAnd</code> is used. Every listed event must be received for the state to complete. If there are multiple events of the same type a index number will be added to the duplicate cloudevent types.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>eventsAnd</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>timeout</code> An ISO8601 duration string. string no <code>events</code> Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes"},{"location":"spec/workflow-yaml/events-xor/","title":"EventsXor State","text":"<pre><code>direktiv_api: workflow/v1\nstates:\n- id: a\n  type: eventsXor\n  timeout: PT15M\n  events:\n  - event:\n      type: com.github.pull.create\n      context:\n        subject: '123'\n    transition: received\n    transform:\n      hello: world\n  - event:\n      type: com.github.pull.delete\n      context:\n        subject: '123'\n    transition: received\n\n- id: received\n  type: noop\n</code></pre>"},{"location":"spec/workflow-yaml/events-xor/#eventsxorstatedefinition","title":"EventsXorStateDefinition","text":"<p>To pause the workflow and wait until one of multiple CloudEvents events is received before proceeding, the <code>eventsXor</code> state might be used. Any event match received will cause this state to complete.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>eventsXor</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>timeout</code> An ISO8601 duration string. string no <code>events</code> Defines the criteria by which incoming CloudEvents events are evaluated to find a match. ConsumeEventDefinition yes"},{"location":"spec/workflow-yaml/foreach/","title":"Foreach State","text":"<pre><code>- id: data\n  type: noop\n  transform:\n    names:\n    - hello\n    - world\n  transition: a\n- id: a\n  type: foreach\n  array: 'jq([.names[] | {name: .}])'\n  action:\n    function: echo\n    input: 'jq(.name)'\n</code></pre>"},{"location":"spec/workflow-yaml/foreach/#foreachstatedefinition","title":"ForeachStateDefinition","text":"<p>The <code>foreach</code> state is a convenient way to divide some data and then perform an action on each element in parallel. </p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>foreach</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>timeout</code> ISO8601 duration string to set a non-default timeout. string no <code>array</code> Selects or generates an array, from which each element will be separately acted upon. The <code>action.input</code> will be evaluated against each element in this array, rather than the usual instance data. Structured JQ yes <code>action</code> Defines the action to perform. ActionDefinition yes"},{"location":"spec/workflow-yaml/functions/","title":"Functions","text":""},{"location":"spec/workflow-yaml/functions/#functiondefinition","title":"FunctionDefinition","text":"<p>Functions refer to anything executable by Direktiv as a unit of logic within a subflow that isn't otherwise part of basic state functionality. Usually this means either a purpose-built container or another workflow executed as a subflow. In some cases functions can be extensively configured, and they are often reused repeatedly within a workflow. To manage the size of Direktiv workflow definitions functions are predefined as much as possible and referenced when called.</p> <p>These are the currently available function types:</p> <ul> <li>Functions</li> <li>FunctionDefinition<ul> <li>NamespacedKnativeFunctionDefinition</li> <li>WorkflowKnativeFunctionDefinition</li> <li>ContainerSizeDefinition</li> <li>SubflowFunctionDefinition</li> </ul> </li> </ul> <p>The following example demonstrate how to define and reference a function within a workflow:</p> Workflow<pre><code>direktiv_api: workflow/v1\ndescription: |\n  A basic demonstration of functions.\nfunctions:\n- type: knative-workflow\n  id: request\n  image: direktiv/request:latest\n  size: small\nstates:\n- id: getter\n  type: action\n  action:\n    function: request\n    input:\n      method: \"GET\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n</code></pre> Input<pre><code>{}\n</code></pre> Output<pre><code>{\n  \"return\": {\n    \"userId\": 1,\n    \"id\": 1,\n    \"title\": \"delectus aut autem\",\n    \"completed\": false\n  }\n}\n</code></pre>"},{"location":"spec/workflow-yaml/functions/#namespacedknativefunctiondefinition","title":"NamespacedKnativeFunctionDefinition","text":"<p>A <code>knative-namespace</code> refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service configured to be available on the namespace.</p> <p>This function type supports <code>files</code>.</p> Parameter Description Type Required <code>type</code> Identifies which kind of FunctionDefinition is being used. In this case it must be set to <code>knative-namespace</code>. string yes <code>id</code> A unique identifier for the function within the workflow definition. string yes <code>service</code> URI to a function on the namespace. string yes <code>cmd</code> Custom command to execute within the container. string no <code>envs</code> Environment variables EnvironmentVariablesDefinition no <code>patches</code> Patching the user container PatchDefinition no"},{"location":"spec/workflow-yaml/functions/#workflowknativefunctiondefinition","title":"WorkflowKnativeFunctionDefinition","text":"<p>A <code>knative-workflow</code> refers to a function that is implemented according to the requirements for a direktiv knative service. Specifically, in this case referring to a service that Direktiv can create on-demand for the exclusive use by this workflow.</p> <p>This function type supports <code>files</code>.</p> Parameter Description Type Required <code>type</code> Identifies which kind of FunctionDefinition is being used. In this case it must be set to <code>knative-workflow</code>. string yes <code>id</code> A unique identifier for the function within the workflow definition. string yes <code>image</code> URI to a <code>knative-workflow</code> compliant container. string yes <code>size</code> Specifies the container size. ContainerSizeDefinition no <code>cmd</code> Custom command to execute within the container. string no <code>envs</code> Environment variables EnvironmentVariablesDefinition no <code>patches</code> Patching the user container PatchDefinition no"},{"location":"spec/workflow-yaml/functions/#patchdefinition","title":"PatchDefinition","text":"<p>With <code>patches</code> the configuration of the function can be changed. Direktiv is using Kubernetes' patching feature to apply those. The following commands can be applied: <code>add</code>, <code>remove</code> and <code>replace</code>. Not all of the paths inside a function can be modified. To ensure that changes don't break Direktiv's functionality the following list of paths can be used within the <code>patch</code> section.</p> <ul> <li>/spec/template/metadata/labels</li> <li>/spec/template/metadata/annotations</li> <li>/spec/template/spec/affinity</li> <li>/spec/template/spec/securityContext</li> <li>/spec/template/spec/containers/0</li> </ul> Patching<pre><code>  patches:\n  - op: add\n    path: /spec/template/metadata/annotations\n    Value: { \"my\": \"annotation\" }\n  - op: add\n    path: /spec/template/spec/containers[0]/env\n    Value: { \"name\": \"hello\", \"value\": \"world\" }\n</code></pre>"},{"location":"spec/workflow-yaml/functions/#environmentvariablesdefinition","title":"EnvironmentVariablesDefinition","text":"<p>Environment variables can be defined for namespace services as well as in function definitions. It is an array with <code>name</code> and <code>value</code>. </p> Environment Variables<pre><code>envs:\n- name: key\n  value: value\n</code></pre> <p>Examples for environment variables can be found in the example section</p>"},{"location":"spec/workflow-yaml/functions/#containersizedefinition","title":"ContainerSizeDefinition","text":"<p>When functions use containers you may be able to specify what size the container should be. This is done using one of three keywords, each representing a different size preset defined in Direktiv's configuration files:</p> <ul> <li><code>small</code></li> <li><code>medium</code></li> <li><code>large</code></li> </ul>"},{"location":"spec/workflow-yaml/functions/#subflowfunctiondefinition","title":"SubflowFunctionDefinition","text":"<p>A <code>subflow</code> refers to a function that is actually another workflow. The other workflow is called with some input and its output is returned to this workflow.</p> <p>This function type does not support <code>files</code>.</p> Parameter Description Type Required <code>type</code> Identifies which kind of FunctionDefinition is being used. In this case it must be set to <code>subflow</code>. string yes <code>id</code> A unique identifier for the function within the workflow definition. string yes <code>workflow</code> URI to a workflow within the same namespace. string yes"},{"location":"spec/workflow-yaml/generate-event/","title":"GenerateEvent State","text":"<pre><code>- id: a\n  type: generateEvent\n  event:\n    type: myeventtype\n    source: myeventsource\n    data: \n      hello: world\n    datacontenttype: application/json\n</code></pre>"},{"location":"spec/workflow-yaml/generate-event/#generateeventstatedefinition","title":"GenerateEventStateDefinition","text":"Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>generateEvent</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>delay</code> ISO8601 duration string defining how long to hold the event before broadcasting it. string no <code>event</code> Defines the event to generate. GenerateEventDefinition yes"},{"location":"spec/workflow-yaml/generate-event/#generateeventdefinition","title":"GenerateEventDefinition","text":"Parameter Description Type Required <code>type</code> Sets the CloudEvents event type. string yes <code>source</code> Sets the CloudEvents event source. string yes <code>data</code> Defines the content of the payload for the CloudEvents event. Structured JQ no <code>datacontenttype</code> An RFC2046 string specifying the payload content type. string no <code>context</code> If defined, must evaluate to an object of key-value pairs. These will be used to define CloudEvents event context data. Structured JQ no"},{"location":"spec/workflow-yaml/getter/","title":"Getter State","text":"<pre><code>- id: a\n  type: setter\n  variables:\n  - key: x \n    scope: workflow\n    mimeType: application/json\n    value: Hello World\n  transition: b\n- id: b\n  type: getter\n  variables:\n  - key: x \n    scope: workflow\n</code></pre>"},{"location":"spec/workflow-yaml/getter/#getterstatedefinition","title":"GetterStateDefinition","text":"<p>To load variables, use the <code>getter</code> state. See Variables.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>getter</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>variables</code> Defines variables to load. []VariableGetterDefinition yes"},{"location":"spec/workflow-yaml/getter/#variablegetterdefinition","title":"VariableGetterDefinition","text":"Parameter Description Type Required <code>key</code> Variable name. Structured JQ yes <code>scope</code> Selects the scope to which the variable belongs. If undefined, defaults to <code>instance</code>. See Variables. yes no <code>as</code> Names the resulting data. If left unspecified, the <code>key</code> will be used instead. string no"},{"location":"spec/workflow-yaml/logging/","title":"Logging","text":"<p>All states can write to instance logs via a common field <code>log</code>. This field uses structured jx to support querying instance data and inserting it into the logs. </p> <pre><code>- id: a\n  type: noop\n  log: 'Hello, world!'\n</code></pre>"},{"location":"spec/workflow-yaml/metadata/","title":"Instance Metadata","text":"<p>Instance metadata is a way to monitor an instance. An instance can update its metadata at any time, replacing it with whatever information it needs to expose via the API.</p> <p>All states can write to instance metadata via a common field <code>metadata</code>. This field uses structured jx to support querying instance data and inserting it into the metadata. </p> <pre><code>direktiv_api: workflow/v1\nstates:\n- id: a\n  type: delay\n  duration: PT1M\n  metadata: \n    workflow-data: jq(.)\n</code></pre>"},{"location":"spec/workflow-yaml/noop/","title":"Noop State","text":"<pre><code>- id: a\n  type: noop\n</code></pre>"},{"location":"spec/workflow-yaml/noop/#noopstatedefinition","title":"NoopStateDefinition","text":"<p>Often workflows need to do something that can be achieved using logic built into most state types. For example, to log something, or to transform the instance data by running a <code>jq</code> command. In many cases this can be done by an existing state within the workflow, but sometimes it's necessary to split it out into a separate state. The <code>noop</code> state exists for this purpose.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>noop</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no"},{"location":"spec/workflow-yaml/parallel/","title":"Parallel State","text":"<pre><code>- id: a\n  type: parallel\n  mode: and\n  actions:\n  - function: myfunc\n    input: 'jq(.x)'\n  - function: myfunc\n    input: 'jq(.y)'\n</code></pre>"},{"location":"spec/workflow-yaml/parallel/#parallelstatedefinition","title":"ParallelStateDefinition","text":"<p>The <code>parallel</code> state is an alternative to the <code>action</code> state when a workflow can perform multiple threads of logic simultaneously. The values in <code>return</code> is an array of the returns of the individual actions. In mode <code>or</code> the first response is set in the array and the other actions are set to <code>null</code>.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>parallel</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>timeout</code> ISO8601 duration string to set a non-default timeout. string no <code>mode</code> If defined, must be either <code>and</code> or <code>or</code>. The default is <code>and</code>. This setting determines whether the state is considered successfully completed only if all threads have returned without error (<code>and</code>) or as soon as any single thread returns without error (<code>or</code>). string no <code>actions</code> Defines the action to perform. []ActionDefinition yes"},{"location":"spec/workflow-yaml/setter/","title":"Setter State","text":"<pre><code>- id: a\n  type: setter\n  variables:\n  - key: x \n    scope: workflow\n    mimeType: text/plain\n    value: 'jq(.x)'\n</code></pre>"},{"location":"spec/workflow-yaml/setter/#setterstatedefinition","title":"SetterStateDefinition","text":"<p>To create or change variables, use the <code>setter</code> state. See Variables.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>setter</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>variables</code> Defines variables to push. []VariableSetterDefinition yes"},{"location":"spec/workflow-yaml/setter/#variablesetterdefinition","title":"VariableSetterDefinition","text":"Parameter Description Type Required <code>key</code> Variable name. Structured JQ yes <code>scope</code> Selects the scope to which the variable belongs. If undefined, defaults to <code>instance</code>. See Variables. yes no <code>mimeType</code> Store a MIME type with the variable. If left undefined, it will default to <code>application/json</code>. Two specific MIME types cause this state to behave differently: <code>text/plain</code> and <code>application/octet-stream</code>. If the <code>value</code> evaluates to a JSON string the MIME type is <code>text/plain</code>, that string will be stored in plaintext (without JSON quotes and escapes). If if the <code>value</code> is a JSON string containing base64 encoded data and the MIME type is <code>application/octet-stream</code>, the base64 data will be decoded and stored as binary data. Structured JQ no <code>value</code> Select or generate the data to store. Structured JQ yes"},{"location":"spec/workflow-yaml/starts/","title":"Starts","text":""},{"location":"spec/workflow-yaml/starts/#startdefinition","title":"StartDefinition","text":"<p>A <code>StartDefinition</code> may be defined using one of the following, depending on the desired behaviour:</p> <ul> <li>Starts</li> <li>StartDefinition<ul> <li>DefaultStartDefinition</li> <li>ScheduledStartDefinition</li> <li>EventStartDefinition</li> <li>EventsXorStartDefinition</li> <li>EventsAndStartDefinition</li> <li>StartEventDefinition</li> </ul> </li> </ul> <p>If omitted from the workflow definition the DefaultStartDefinition will be used, which means the workflow will only be executed when called.</p>"},{"location":"spec/workflow-yaml/starts/#defaultstartdefinition","title":"DefaultStartDefinition","text":"<p>The default start definition is used for workflows that should only execute when called. This means subflows, workflows triggered by scripts, and workflows triggered manually by humans.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StartDefinition is being used. In this case it must be set to <code>default</code>. string yes <code>state</code> References a defined state's <code>id</code>. This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the <code>states</code> list. string no"},{"location":"spec/workflow-yaml/starts/#scheduledstartdefinition","title":"ScheduledStartDefinition","text":"<p>The scheduled start definition is used for workflows that should execute at regularly defined times. </p> <p>Scheduled workflow can be manually triggered for convenience and testing. They never have input data, so accurate testing should use <code>{}</code> as input. </p> Parameter Description Type Required <code>type</code> Identifies which kind of StartDefinition is being used. In this case it must be set to <code>scheduled</code>. string yes <code>state</code> References a defined state's <code>id</code>. This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the <code>states</code> list. string no <code>cron</code> Defines the time(s) when the workflow should execute using a CRON expression. string yes <p>Example (snippet) <pre><code>start:\n  type: scheduled\n  cron: '* * * * *' # Trigger a new instance every minute.\n</code></pre></p>"},{"location":"spec/workflow-yaml/starts/#eventstartdefinition","title":"EventStartDefinition","text":"<p>The event start definition is used for workflows that should be executed whenever a relevant CloudEvents event is received. </p> <p>See StartEventDefinition for an explanation of the input data of event-triggered workflows.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StartDefinition is being used. In this case it must be set to <code>event</code>. string yes <code>state</code> References a defined state's <code>id</code>. This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the <code>states</code> list. string no <code>event</code> Defines what events can trigger the workflow. StartEventDefinition yes"},{"location":"spec/workflow-yaml/starts/#eventsxorstartdefinition","title":"EventsXorStartDefinition","text":"<p>The event \"xor\" start definition is used for workflows that should be executed whenever one of multiple possible CloudEvents events is received. </p> <p>See StartEventDefinition for an explanation of the input data of event-triggered workflows.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StartDefinition is being used. In this case it must be set to <code>eventsXor</code>. string yes <code>state</code> References a defined state's <code>id</code>. This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the <code>states</code> list. string no <code>events</code> Defines what events can trigger the workflow. []StartEventDefinition yes"},{"location":"spec/workflow-yaml/starts/#eventsandstartdefinition","title":"EventsAndStartDefinition","text":"<p>The event \"and\" start definition is used for workflows that should be executed when multiple matching CloudEvents events are received. </p> <p>See StartEventDefinition for an explanation of the input data of event-triggered workflows.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StartDefinition is being used. In this case it must be set to <code>eventsAnd</code>. string yes <code>state</code> References a defined state's <code>id</code>. This state will be used as the entrypoint into the workflow. If left undefined, it defaults to the first state defined in the <code>states</code> list. string no <code>lifespan</code> An ISO8601 duration string. Sets the maximum duration an event can be stored before being discarded while waiting for other events. string no <code>events</code> Defines what events can trigger the workflow. []StartEventDefinition yes"},{"location":"spec/workflow-yaml/starts/#starteventdefinition","title":"StartEventDefinition","text":"<p>The StartEventDefinition is a structure shared by various start definitions involving events. </p> Parameter Description Type Required <code>type</code> Identifies which CloudEvents events can trigger the workflow by requiring an exact match to the event's own <code>type</code> context value. string yes <code>context</code> Optional key-value pairs to further restrict what events can trigger the workflow. For each pair, incoming CloudEvents context values will be checked for a match. All pairs must find a match for the event to be accepted. The \"keys\" are strings that match exactly to specific context keys, but the \"values\" can be \"glob\" patterns allowing them to match a range of possible context values. object no <p>The input data of an event-triggered workflow is a JSON representation of all the received events stored under keys matching the events' respective type. For example, this CloudEvents event will result in the following input data in a workflow triggered by a single event:</p> CloudEvents Event<pre><code>{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull.create\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n}\n</code></pre> Input Data<pre><code>{\n  \"com.github.pull.create\": {\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.github.pull.create\",\n    \"source\" : \"https://github.com/cloudevents/spec/pull\",\n    \"subject\" : \"123\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"text/xml\",\n    \"data\" : \"&lt;much wow=\\\"xml\\\"/&gt;\"\n  }\n}\n</code></pre>"},{"location":"spec/workflow-yaml/states/","title":"States","text":""},{"location":"spec/workflow-yaml/states/#statedefinition","title":"StateDefinition","text":"<p>A <code>StateDefinition</code> may be defined using one of the following, depending on the desired behaviour:</p> <ul> <li><code>action</code></li> <li><code>consumeEvent</code></li> <li><code>delay</code></li> <li><code>error</code></li> <li><code>eventsAnd</code></li> <li><code>eventsXor</code></li> <li><code>foreach</code></li> <li><code>generateEvent</code></li> <li><code>getter</code></li> <li><code>noop</code></li> <li><code>parallel</code></li> <li><code>setter</code></li> <li><code>switch</code></li> <li><code>validate</code></li> </ul>"},{"location":"spec/workflow-yaml/switch/","title":"Switch State","text":"<pre><code>- id: a\n  type: switch\n  defaultTransform: 'jq(del(.x))'\n  defaultTransition: b\n  conditions:\n  - condition: 'jq(.y == true)'\n    transform: 'jq(.x)'\n    transition: c\n  - condition: 'jq(.z == true)'\n    transform: 'jq(.x)'\n    transition: d\n</code></pre>"},{"location":"spec/workflow-yaml/switch/#switchstatedefinition","title":"SwitchStateDefinition","text":"<p>To change the behaviour of a workflow based on the instance data, use a <code>switch</code> state. This state does nothing except choose between any number of different possible state transitions.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>switch</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>defaultTransform</code> If defined, modifies the instance's data upon completing the state logic. But only if none of the <code>conditions</code> are met. See StateTransforms. Structured JQ no <code>defaultTransition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. But only if none of the <code>conditions</code> are met. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>conditions</code> List of conditions, which are evaluated in-order until a match is found. []SwitchConditionDefinition yes"},{"location":"spec/workflow-yaml/switch/#switchconditiondefinition","title":"SwitchConditionDefinition","text":"Parameter Description Type Required <code>condition</code> Selects or generates the data used to determine if condition is met. The condition is considered met if the result is anything other than <code>null</code>, <code>false</code>, <code>{}</code>, <code>[]</code>, <code>\"\"</code>, or <code>0</code>. Structured JQ yes <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, matching this condition terminates the workflow. string no"},{"location":"spec/workflow-yaml/timeouts/","title":"Timeouts","text":""},{"location":"spec/workflow-yaml/timeouts/#timeoutsdefinition","title":"TimeoutsDefinition","text":"<p>In addition to any timeouts applied on a state-by-state basis, every workflow has two global timeouts that begin ticking from the moment the workflow starts. This is where you can configure these timeouts differently to their defaults.</p> Parameter Description Type Required <code>interrupt</code> An ISO8601 duration string. Sets the time to wait before throwing a catchable <code>direktiv.cancels.timeout.soft</code> error. Consider this a soft timeout. string no <code>kill</code> An ISO8601 duration string. Sets the time to wait before throwing an uncatchable <code>direktiv.cancels.timeout.hard</code> error. This is a hard timeout. string no Workflow Timeout<pre><code>direktiv_api: workflow/v1\ntimeouts:\n  interrupt: PT60M\n  kill: PT30M\nfunctions:\n- type: knative-workflow\n  id: request\n  image: direktiv/request:latest\n  size: small\nstates:\n- id: getter\n  type: action\n  action:\n    function: request\n    input:\n      method: \"GET\"\n      url: \"https://jsonplaceholder.typicode.com/todos/1\"\n</code></pre>"},{"location":"spec/workflow-yaml/validate/","title":"Validate State","text":"<pre><code>- id: a\n  type: validate\n  schema:\n    title: Files\n    type: object\n    properties:\n      firstname:\n        type: string\n        description: Your first name\n        title: First Name\n</code></pre>"},{"location":"spec/workflow-yaml/validate/#validatestatedefinition","title":"ValidateStateDefinition","text":"<p>Since workflows receive external input it may be necessary to check that instance data is valid. The <code>validate</code> state exists for this purpose. If this state is the first state in the flow the UI will generate a input form based on the specification.</p> Parameter Description Type Required <code>type</code> Identifies which kind of StateDefinition is being used. In this case it must be set to <code>validate</code>. string yes <code>id</code> An identifier unique within the workflow to this one state. string yes <code>log</code> If defined, the workflow will generate a log when it commences this state. See StateLogging. Structured JQ no <code>metadata</code> If defined, updates the instance's metadata. See InstanceMetadata. Structured JQ no <code>transform</code> If defined, modifies the instance's data upon completing the state logic. See StateTransforms. Structured JQ no <code>transition</code> Identifies which state to transition to next, referring to the next state's unique <code>id</code>. If undefined, this state terminates the workflow. string no <code>catch</code> Defines behaviour for handling of catchable errors. []ErrorCatchDefinition no <code>subject</code> Selects or generates the data that will be compared to the <code>schema</code>. If undefined, it will be default to <code>'jq(.)'</code>. Structured JQ no <code>schema</code> A YAMLified representation of a JSON Schema that defines whether the <code>subject</code> is considered valid. object yes"},{"location":"spec/workflow-yaml/workflow/","title":"Workflow Definition","text":""},{"location":"spec/workflow-yaml/workflow/#direktiv-workflow-definition","title":"Direktiv Workflow Definition","text":"<p>This document describes the rules for Direktiv workflow definition files. These files are written in YAML and dictate the behaviour of a workflow running on Direktiv. </p> Workflow<pre><code>direktiv_api: workflow/v1\ndescription: |\n  A simple \"Hello, world\" demonstration.\nstates:\n- id: hello\n  type: noop\n  transform: 'jq({ msg: \"Hello, world!\" })'\n</code></pre> Input<pre><code>{}\n</code></pre> Output<pre><code>{\n  \"msg\": \"Hello, world!\"\n}\n</code></pre> <p>Workflows have inputs and outputs, usually in JSON. Where examples appear in this document they will often be accompanied by inputs and outputs as seen above.</p>"},{"location":"spec/workflow-yaml/workflow/#workflowdefinition","title":"WorkflowDefinition","text":"<p>This is the top-level structure of a Direktiv workflow definition. All workflows must have one.</p> Parameter Description Type Required <code>direktiv_api</code> Set it to 'workflow/v1' to reduce ambiguity, enabling tools to better identify this file as a workflow. string no <code>url</code> Link to further information. string no <code>description</code> Short description of the workflow. string no <code>functions</code> List of function definitions for use by function-based <code>states</code>. []FunctionDefinition no <code>start</code> Configuration for how the workflow should start. StartDefinition no <code>states</code> List of all possible workflow states. []StateDefinition yes <code>timeouts</code> Configuration of workflow-level timeouts. TimeoutsDefinition no"}]}