{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Test","title":"Test"},{"location":"#test","text":"","title":"Test"},{"location":"api/","text":"Direktiv API Direktiv Open API Specification Direktiv Documentation can be found at https://docs.direktiv.io/ Informations Version 1.0.0 Contact info@direktiv.io Content negotiation URI Schemes http https Consumes application/json text/plain Produces application/json text/event-stream Access control Security Schemes api_key (header: KEY) Type : apikey Security Requirements api_key All endpoints directory Method URI Name Summary PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory create directory Create a Directory global_services Method URI Name Summary POST /api/functions create global service Create Global Service DELETE /api/functions/{serviceName}/revisions/{revisionGeneration} delete global revision Delete Global Service Revision DELETE /api/functions/{serviceName} delete global service Delete Global Service GET /api/functions/{serviceName} get global service Get Global Service Details GET /api/functions get global service list Get Global Services List GET /api/functions/{serviceName}/revisions/{revisionGeneration}/pods list global service revision pods Get Global Service Revision Pods List POST /api/functions/{serviceName} update global service Create Global Service Revision PATCH /api/functions/{serviceName} update global service traffic Update Global Service Traffic GET /api/functions/{serviceName}/revisions/{revisionGeneration} watch global service revision Watch Global Service Revision GET /api/functions/{serviceName}/revisions watch global service revision list Watch Global Service Revision List instances Method URI Name Summary POST /api/namespaces/{namespace}/instances/{instance}/cancel cancel instance Cancel a Pending Instance GET /api/namespaces/{namespace}/instances/{instance} get instance Get a Instance GET /api/namespaces/{namespace}/instances/{instance}/input get instance input Get a Instance Input GET /api/namespaces/{namespace}/instances get instance list Get List Instances GET /api/namespaces/{namespace}/instances/{instance}/output get instance output Get a Instance Output logs Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=logs get workflow logs Get Workflow Level Logs GET /api/namespaces/{namespace}/instances/{instance}/logs instance logs Gets Instance Logs GET /api/namespaces/{namespace}/logs namespace logs Gets Namespace Level Logs GET /api/logs server logs Get Direktiv Server Logs metrics Method URI Name Summary GET /api/namespaces/{namespace}/metrics/failed namespace metrics failed Gets Namespace Failed Workflow Instances Metrics GET /api/namespaces/{namespace}/metrics/invoked namespace metrics invoked Gets Namespace Invoked Workflow Metrics GET /api/namespaces/{namespace}/metrics/milliseconds namespace metrics milliseconds Gets Namespace Workflow Timing Metrics GET /api/namespaces/{namespace}/metrics/successful namespace metrics successful Gets Namespace Successful Workflow Instances Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked workflow metrics invoked Gets Invoked Workflow Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed workflow metrics milliseconds Gets Workflow Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds workflow metrics state milliseconds Gets a Workflow State Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful workflow metrics successful Gets Successful Workflow Metrics namespace_services Method URI Name Summary POST /api/functions/namespaces/{namespace} create namespace service Create Namespace Service DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} delete namespace revision Delete Namespace Service Revision DELETE /api/functions/namespaces/{namespace}/function/{serviceName} delete namespace service Delete Namespace Service GET /api/functions/namespaces/{namespace}/function/{serviceName} get namespace service Get Namespace Service Details GET /api/functions/namespaces/{namespace} get namespace service list Get Namespace Services List GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods list namespace service revision pods Get Namespace Service Revision Pods List POST /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service Create Namespace Service Revision PATCH /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service traffic Update Namespace Service Traffic GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} watch namespace service revision Watch Namespace Service Revision GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions watch namespace service revision list Watch Namespace Service Revision List namespaces Method URI Name Summary PUT /api/namespaces/{namespace} create namespace Creates a namespace DELETE /api/namespaces/{namespace} delete namespace Delete a namespace GET /api/namespaces/{namespace}/config get namespace config Gets a namespace config GET /api/namespaces get namespaces Gets the list of namespaces PATCH /api/namespaces/{namespace}/config set namespace config Sets a namespace config node Method URI Name Summary DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node delete node Delete a node GET /api/namespaces/{namespace}/tree/{nodePath} get nodes Get List of Namespace Nodes other Method URI Name Summary POST /api/namespaces/{namespace}/broadcast broadcast cloudevent Broadcast Cloud Event POST /api/jq jq playground JQ Playground api to test jq queries GET /api/version version Returns version information for servers in the cluster. registries Method URI Name Summary POST /api/functions/registries/private create global private registry Create a Global Container Registry POST /api/functions/registries/global create global registry Create a Global Container Registry POST /api/functions/registries/namespaces/{namespace} create registry Create a Namespace Container Registry DELETE /api/functions/registries/private delete global private registry Delete a Global Container Registry DELETE /api/functions/registries/global delete global registry Delete a global Container Registry DELETE /api/functions/registries/namespaces/{namespace} delete registry Delete a Namespace Container Registry GET /api/functions/registries/private get global private registries Get List of Global Private Registries GET /api/functions/registries/global get global registries Get List of Global Registries GET /api/functions/registries/namespaces/{namespace} get registries Get List of Namespace Registries secrets Method URI Name Summary PUT /api/namespaces/{namespace}/secrets/{secret} create secret Create a Namespace Secret DELETE /api/namespaces/{namespace}/secrets/{secret} delete secret Delete a Namespace Secret GET /api/namespaces/{namespace}/secrets get secrets Get List of Namespace Secrets variables Method URI Name Summary DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} delete instance variable Delete a Instance Variable DELETE /api/namespaces/{namespace}/vars/{variable} delete namespace variable Delete a Namespace Variable DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var delete workflow variable Delete a Workflow Variable GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} get instance variable Get a Instance Variable GET /api/namespaces/{namespace}/instances/{instance}/vars get instance variables Get List of Instance Variable GET /api/namespaces/{namespace}/vars/{variable} get namespace variable Get a Namespace Variable GET /api/namespaces/{namespace}/vars get namespace variables Get Namespace Variable List GET /api/namespaces/{namespace}/tree/{workflow}?op=var get workflow variable Get a Workflow Variable GET /api/namespaces/{namespace}/tree/{workflow}?op=vars get workflow variables Get List of Workflow Variables PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} set instance variable Set a Instance Variable PUT /api/namespaces/{namespace}/vars/{variable} set namespace variable Set a Namespace Variable PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var set workflow variable Set a Workflow Variable workflow_services Method URI Name Summary GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function get workflow service Get Workflow Service Details GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision get workflow service revision Get Workflow Service Revision GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions get workflow service revision list Get Workflow Service Revision List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods list workflow service revision pods Get Workflow Service Revision Pods List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services list workflow services Get Workflow Services List workflows Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow Await Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow body Await Execute a Workflow With Body PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow create workflow Create a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=execute execute workflow Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging set workflow cloud event logs Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle toggle workflow Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow update workflow Update a Workflow Paths Await Execute a Workflow ( awaitExecuteWorkflow ) GET /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string. All responses Code Status Description Has headers Schema 200 OK successfully executed workflow schema Responses 200 - successfully executed workflow Status: OK Schema Await Execute a Workflow With Body ( awaitExecuteWorkflowBody ) POST /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow with optionally some input provided in the request body as json. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters. Input query parameters are only read if the request has no body. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string. Workflow Input body interface{} interface{} \u2713 The input of this workflow instance All responses Code Status Description Has headers Schema 200 OK successfully executed workflow schema Responses 200 - successfully executed workflow Status: OK Schema Broadcast Cloud Event ( broadcastCloudevent ) POST /api/namespaces/{namespace}/broadcast Broadcast a cloud event to a namespace. Cloud events posted to this api will be picked up by any workflows listening to the same event type on the namescape. The body of this request should follow the cloud event core specification defined at https://github.com/cloudevents/spec . Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace cloudevent body interface{} interface{} \u2713 Cloud Event request to be sent. All responses Code Status Description Has headers Schema 200 OK successfully sent cloud event schema Responses 200 - successfully sent cloud event Status: OK Schema Cancel a Pending Instance ( cancelInstance ) POST /api/namespaces/{namespace}/instances/{instance}/cancel Cancel a currently pending instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully cancelled instance schema Responses 200 - successfully cancelled instance Status: OK Schema Create a Directory ( createDirectory ) PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory Creates a directory at the target path. Parameters Name Source Type Go type Separator Required Default Description directory path string string \u2713 path to target directory namespace path string string \u2713 target namespace op query string string \u2713 \"create-directory\" the operation for the api All responses Code Status Description Has headers Schema 200 OK directory has been created schema default an error has occurred schema Responses 200 - directory has been created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create a Global Container Registry ( createGlobalPrivateRegistry ) POST /api/functions/registries/private Create a global container registry. Global Private registries are only available to global services. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN Parameters Name Source Type Go type Separator Required Default Description Registry Payload body CreateGlobalPrivateRegistryBody CreateGlobalPrivateRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully created global private registry schema Responses 200 - successfully created global private registry Status: OK Schema Inlined models CreateGlobalPrivateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL Create a Global Container Registry ( createGlobalRegistry ) POST /api/functions/registries/global Create a global container registry. Global registries are available to all services. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN Parameters Name Source Type Go type Separator Required Default Description Registry Payload body CreateGlobalRegistryBody CreateGlobalRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully created global registry schema Responses 200 - successfully created global registry Status: OK Schema Inlined models CreateGlobalRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL Create Global Service ( createGlobalService ) POST /api/functions Creates global scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/docs/walkthrough/using-functions.html Parameters Name Source Type Go type Separator Required Default Description Service body CreateGlobalServiceBody CreateGlobalServiceBody \u2713 Payload that contains information on new service All responses Code Status Description Has headers Schema 200 OK successfully created service schema Responses 200 - successfully created service Status: OK Schema Inlined models CreateGlobalServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size string string \u2713 Size of created service pods Creates a namespace ( createNamespace ) PUT /api/namespaces/{namespace} Creates a new namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to create All responses Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema Responses 200 - namespace has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create Namespace Service ( createNamespaceService ) POST /api/functions/namespaces/{namespace} Creates namespace scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/docs/walkthrough/using-functions.html Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Service body CreateNamespaceServiceBody CreateNamespaceServiceBody \u2713 Payload that contains information on new service All responses Code Status Description Has headers Schema 200 OK successfully created service schema Responses 200 - successfully created service Status: OK Schema Inlined models CreateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size string string \u2713 Size of created service pods Create a Namespace Container Registry ( createRegistry ) POST /api/functions/registries/namespaces/{namespace} Create a namespace container registry. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body CreateRegistryBody CreateRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully created namespace registry schema Responses 200 - successfully created namespace registry Status: OK Schema Inlined models CreateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL Create a Namespace Secret ( createSecret ) PUT /api/namespaces/{namespace}/secrets/{secret} Create a namespace secret. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret Secret Payload body string string \u2713 Payload that contains secret data. All responses Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema Responses 200 - namespace has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Create a Workflow ( createWorkflow ) PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow Creates a workflow at the target path. The body of this request should contain the workflow yaml. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"create-workflow\" the operation for the api workflow data body string string Payload that contains the direktiv workflow yaml to create. All responses Code Status Description Has headers Schema 200 OK successfully created workflow schema default an error has occurred schema Responses 200 - successfully created workflow Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete a Global Container Registry ( deleteGlobalPrivateRegistry ) DELETE /api/functions/registries/private Delete a global container registry. Global Private registries are only available to global services. Parameters Name Source Type Go type Separator Required Default Description Registry Payload body DeleteGlobalPrivateRegistryBody DeleteGlobalPrivateRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully delete global private registry schema Responses 200 - successfully delete global private registry Status: OK Schema Inlined models DeleteGlobalPrivateRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL Delete a global Container Registry ( deleteGlobalRegistry ) DELETE /api/functions/registries/global Delete a Global container registry Global registries are available to all services. Parameters Name Source Type Go type Separator Required Default Description Registry Payload body DeleteGlobalRegistryBody DeleteGlobalRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully delete global registry schema Responses 200 - successfully delete global registry Status: OK Schema Inlined models DeleteGlobalRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL Delete Global Service Revision ( deleteGlobalRevision ) DELETE /api/functions/{serviceName}/revisions/{revisionGeneration} Delete a global scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003'. Note: Revisions with traffic cannot be deleted. Parameters Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service revision schema Responses 200 - successfully deleted service revision Status: OK Schema Delete Global Service ( deleteGlobalService ) DELETE /api/functions/{serviceName} Deletes global scoped knative service and all its revisions. Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service schema Responses 200 - successfully deleted service Status: OK Schema Delete a Instance Variable ( deleteInstanceVariable ) DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Delete a instance variable. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted instance variable schema Responses 200 - successfully deleted instance variable Status: OK Schema Delete a namespace ( deleteNamespace ) DELETE /api/namespaces/{namespace} Delete a namespace. A namespace will not delete by default if it has any child resources (workflows, etc...). Deleting the namespace with all its children can be done using the recursive query parameter. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to delete recursive query boolean bool recursively deletes all child resources All responses Code Status Description Has headers Schema 200 OK namespace has been successfully deleted schema default an error has occurred schema Responses 200 - namespace has been successfully deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete Namespace Service Revision ( deleteNamespaceRevision ) DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Delete a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: Revisions with traffic cannot be deleted. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service revision schema Responses 200 - successfully deleted service revision Status: OK Schema Delete Namespace Service ( deleteNamespaceService ) DELETE /api/functions/namespaces/{namespace}/function/{serviceName} Deletes namespace scoped knative service and all its revisions. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully deleted service schema Responses 200 - successfully deleted service Status: OK Schema Delete a Namespace Variable ( deleteNamespaceVariable ) DELETE /api/namespaces/{namespace}/vars/{variable} Delete a namespace variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted namespace variable schema Responses 200 - successfully deleted namespace variable Status: OK Schema Delete a node ( deleteNode ) DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node Creates a directory at the target path. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace node path string string \u2713 path to target node op query string string \u2713 \"delete-node\" the operation for the api All responses Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema Responses 200 - node has been deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete a Namespace Container Registry ( deleteRegistry ) DELETE /api/functions/registries/namespaces/{namespace} Delete a namespace container registry Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body DeleteRegistryBody DeleteRegistryBody \u2713 Payload that contains registry data All responses Code Status Description Has headers Schema 200 OK successfully delete namespace registry schema Responses 200 - successfully delete namespace registry Status: OK Schema Inlined models DeleteRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL Delete a Namespace Secret ( deleteSecret ) DELETE /api/namespaces/{namespace}/secrets/{secret} Delete a namespace secret. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret All responses Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema Responses 200 - namespace has been successfully created Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Delete a Workflow Variable ( deleteWorkflowVariable ) DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var Delete a workflow variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully deleted workflow variable schema Responses 200 - successfully deleted workflow variable Status: OK Schema Execute a Workflow ( executeWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=execute Executes a workflow with optionally some input provided in the request body as json. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"execute\" the operation for the api Workflow Input body interface{} interface{} \u2713 The input of this workflow instance All responses Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema Responses 200 - node has been deleted Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get List of Global Private Registries ( getGlobalPrivateRegistries ) GET /api/functions/registries/private Gets the list of global private registries. Global Private registries are only available to global services. All responses Code Status Description Has headers Schema 200 OK successfully got global private registries schema Responses 200 - successfully got global private registries Status: OK Schema Get List of Global Registries ( getGlobalRegistries ) GET /api/functions/registries/global Gets the list of global registries. Global registries are available to all services. All responses Code Status Description Has headers Schema 200 OK successfully got global registries schema Responses 200 - successfully got global registries Status: OK Schema Get Global Service Details ( getGlobalService ) GET /api/functions/{serviceName} Get details of a global scoped knative service. Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got service details schema Responses 200 - successfully got service details Status: OK Schema Get Global Services List ( getGlobalServiceList ) GET /api/functions Gets a list of global knative services. All responses Code Status Description Has headers Schema 200 OK successfully got services list schema Responses 200 - successfully got services list Status: OK Schema Get a Instance ( getInstance ) GET /api/namespaces/{namespace}/instances/{instance} Gets the details of a executed workflow instance in this namespace. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance schema default an error has occurred schema Responses 200 - successfully got instance Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get a Instance Input ( getInstanceInput ) GET /api/namespaces/{namespace}/instances/{instance}/input Gets the input an instance was provided when executed. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance input schema Responses 200 - successfully got instance input Status: OK Schema Get List Instances ( getInstanceList ) GET /api/namespaces/{namespace}/instances Gets a list of instances in a namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace instances schema Responses 200 - successfully got namespace instances Status: OK Schema Get a Instance Output ( getInstanceOutput ) GET /api/namespaces/{namespace}/instances/{instance}/output Gets the output an instance was provided when executed. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance output schema Responses 200 - successfully got instance output Status: OK Schema Get a Instance Variable ( getInstanceVariable ) GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Get the value sorted in a instance variable. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got instance variable schema Responses 200 - successfully got instance variable Status: OK Schema Get List of Instance Variable ( getInstanceVariables ) GET /api/namespaces/{namespace}/instances/{instance}/vars Gets a list of variables in a instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance variables schema Responses 200 - successfully got instance variables Status: OK Schema Gets a namespace config ( getNamespaceConfig ) GET /api/namespaces/{namespace}/config Gets a namespace config. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update All responses Code Status Description Has headers Schema 200 OK successfully got namespace config schema Responses 200 - successfully got namespace config Status: OK Schema Get Namespace Service Details ( getNamespaceService ) GET /api/functions/namespaces/{namespace}/function/{serviceName} Get details of a namespace scoped knative service. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got service details schema Responses 200 - successfully got service details Status: OK Schema Get Namespace Services List ( getNamespaceServiceList ) GET /api/functions/namespaces/{namespace} Gets a list of namespace knative services. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got services list schema Responses 200 - successfully got services list Status: OK Schema Get a Namespace Variable ( getNamespaceVariable ) GET /api/namespaces/{namespace}/vars/{variable} Get the value sorted in a namespace variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got namespace variable schema Responses 200 - successfully got namespace variable Status: OK Schema Get Namespace Variable List ( getNamespaceVariables ) GET /api/namespaces/{namespace}/vars Gets a list of variables in a namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace variables schema Responses 200 - successfully got namespace variables Status: OK Schema Gets the list of namespaces ( getNamespaces ) GET /api/namespaces Gets the list of namespaces. All responses Code Status Description Has headers Schema 200 OK successfully got list of namespaces schema Responses 200 - successfully got list of namespaces Status: OK Schema Get List of Namespace Nodes ( getNodes ) GET /api/namespaces/{namespace}/tree/{nodePath} Gets Workflow and Directory Nodes at nodePath. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace nodePath path string string \u2713 target path in tree All responses Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema Responses 200 - successfully got namespace nodes Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get List of Namespace Registries ( getRegistries ) GET /api/functions/registries/namespaces/{namespace} Gets the list of namespace registries. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace registries schema Responses 200 - successfully got namespace registries Status: OK Schema Get List of Namespace Secrets ( getSecrets ) GET /api/namespaces/{namespace}/secrets Gets the list of namespace secrets. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema Responses 200 - successfully got namespace nodes Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Get Workflow Level Logs ( getWorkflowLogs ) GET /api/namespaces/{namespace}/tree/{workflow}?op=logs Get workflow level logs. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow logs schema Responses 200 - successfully got workflow logs Status: OK Schema Get Workflow Service Details ( getWorkflowService ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function Get a workflow scoped knative service details. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version All responses Code Status Description Has headers Schema 200 OK successfully got service details schema Responses 200 - successfully got service details Status: OK Schema Get Workflow Service Revision ( getWorkflowServiceRevision ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision Get a workflow scoped knative service revision. This will return details on a single revision. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got service revision details schema Responses 200 - successfully got service revision details Status: OK Schema Get Workflow Service Revision List ( getWorkflowServiceRevisionList ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions Get the revision list of a workflow scoped knative service. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version All responses Code Status Description Has headers Schema 200 OK successfully got service revisions schema Responses 200 - successfully got service revisions Status: OK Schema Get a Workflow Variable ( getWorkflowVariable ) GET /api/namespaces/{namespace}/tree/{workflow}?op=var Get the value sorted in a workflow variable. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable All responses Code Status Description Has headers Schema 200 OK successfully got workflow variable schema Responses 200 - successfully got workflow variable Status: OK Schema Get List of Workflow Variables ( getWorkflowVariables ) GET /api/namespaces/{namespace}/tree/{workflow}?op=vars Gets a list of variables in a workflow. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow variables schema Responses 200 - successfully got workflow variables Status: OK Schema Gets Instance Logs ( instanceLogs ) GET /api/namespaces/{namespace}/instances/{instance}/logs Gets the logs of an executed instance. Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance id namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got instance logs schema default an error has occurred schema Responses 200 - successfully got instance logs Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse JQ Playground api to test jq queries ( jqPlayground ) POST /api/jq JQ Playground is a sandbox where you can test jq queries with custom data. Parameters Name Source Type Go type Separator Required Default Description JQ payload body JqPlaygroundBody JqPlaygroundBody \u2713 Payload that contains both the JSON data to manipulate and jq query. All responses Code Status Description Has headers Schema 200 OK jq query was successful schema 400 Bad Request the request was invalid schema 500 Internal Server Error an unexpected internal error occurred schema Responses 200 - jq query was successful Status: OK Schema 400 - the request was invalid Status: Bad Request Schema 500 - an unexpected internal error occurred Status: Internal Server Error Schema Inlined models JqPlaygroundBody Properties Name Type Go type Required Default Description Example data string string \u2713 JSON data encoded in base64 query string string \u2713 jq query to manipulate JSON data Get Global Service Revision Pods List ( listGlobalServiceRevisionPods ) GET /api/functions/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a global scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003' . Parameters Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema Responses 200 - successfully got list of a service revision pods Status: OK Schema Get Namespace Service Revision Pods List ( listNamespaceServiceRevisionPods ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a namespace scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema Responses 200 - successfully got list of a service revision pods Status: OK Schema Get Workflow Service Revision Pods List ( listWorkflowServiceRevisionPods ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods List a revisions pods of a workflow scoped knative service. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema Responses 200 - successfully got list of a service revision pods Status: OK Schema Get Workflow Services List ( listWorkflowServices ) GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services Gets a list of workflow knative services. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got services list schema Responses 200 - successfully got services list Status: OK Schema Gets Namespace Level Logs ( namespaceLogs ) GET /api/namespaces/{namespace}/logs Gets Namespace Level Logs. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace logs schema Responses 200 - successfully got namespace logs Status: OK Schema Gets Namespace Failed Workflow Instances Metrics ( namespaceMetricsFailed ) GET /api/namespaces/{namespace}/metrics/failed Get metrics of failed workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Invoked Workflow Metrics ( namespaceMetricsInvoked ) GET /api/namespaces/{namespace}/metrics/invoked Get metrics of invoked workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Workflow Timing Metrics ( namespaceMetricsMilliseconds ) GET /api/namespaces/{namespace}/metrics/milliseconds Get timing metrics of workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Gets Namespace Successful Workflow Instances Metrics ( namespaceMetricsSuccessful ) GET /api/namespaces/{namespace}/metrics/successful Get metrics of successful workflows in the targeted namespace. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace All responses Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema Responses 200 - successfully got namespace metrics Status: OK Schema Get Direktiv Server Logs ( serverLogs ) GET /api/logs Gets Direktiv Server Logs. All responses Code Status Description Has headers Schema 200 OK successfully got server logs schema default an error has occurred schema Responses 200 - successfully got server logs Status: OK Schema OkBody Default Response an error has occurred Schema ErrorResponse Set a Instance Variable ( setInstanceVariable ) PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Set the value sorted in a instance variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set instance variable schema Responses 200 - successfully set instance variable Status: OK Schema Sets a namespace config ( setNamespaceConfig ) PATCH /api/namespaces/{namespace}/config Sets a namespace config. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update Config Payload body SetNamespaceConfigBody SetNamespaceConfigBody Payload that contains the config information to set. Note: This payload only need to contain the properities you wish to set. All responses Code Status Description Has headers Schema 200 OK namespace config has been successfully been updated schema Responses 200 - namespace config has been successfully been updated Status: OK Schema Inlined models SetNamespaceConfigBody Properties Name Type Go type Required Default Description Example broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace Set a Namespace Variable ( setNamespaceVariable ) PUT /api/namespaces/{namespace}/vars/{variable} Set the value sorted in a namespace variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set namespace variable schema Responses 200 - successfully set namespace variable Status: OK Schema Set Cloud Event for Workflow to Log to ( setWorkflowCloudEventLogs ) POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging Set Cloud Event for Workflow to Log to. When configured type direktiv.instanceLog cloud events will be generated with the logger parameter set to the configured value. Workflows can be configured to generate cloud events on their namespace anything the log parameter produces data. Please find more information on this topic here: https://docs.direktiv.io/docs/examples/logging.html Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Cloud Event Logger body SetWorkflowCloudEventLogsBody SetWorkflowCloudEventLogsBody \u2713 Cloud event logger to target All responses Code Status Description Has headers Schema 200 OK successfully update workflow schema Responses 200 - successfully update workflow Status: OK Schema Inlined models SetWorkflowCloudEventLogsBody Properties Name Type Go type Required Default Description Example logger string string \u2713 Target Cloud Event Set a Workflow Variable ( setWorkflowVariable ) PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var Set the value sorted in a workflow variable. If the target variable does not exists, it will be created. Variable data can be anything. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable data body string string \u2713 Payload that contains variable data. All responses Code Status Description Has headers Schema 200 OK successfully set workflow variable schema Responses 200 - successfully set workflow variable Status: OK Schema Set Cloud Event for Workflow to Log to ( toggleWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle Toggle's whether or not a workflow is active. Disabled workflows cannot be invoked. This includes start event and scheduled workflows. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Workflow Live Status body ToggleWorkflowBody ToggleWorkflowBody \u2713 Whether or not the workflow is alive or disabled All responses Code Status Description Has headers Schema 200 OK successfully updated workflow live status schema Responses 200 - successfully updated workflow live status Status: OK Schema Inlined models ToggleWorkflowBody Properties Name Type Go type Required Default Description Example live boolean bool \u2713 Workflow live status Create Global Service Revision ( updateGlobalService ) POST /api/functions/{serviceName} Creates a new global scoped knative service revision Revisions are created with a traffic percentage. This percentage controls how much traffic will be directed to this revision. Traffic can be set to 100 to direct all traffic. Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name Service body UpdateGlobalServiceBody UpdateGlobalServiceBody \u2713 Payload that contains information on service revision All responses Code Status Description Has headers Schema 200 OK successfully created service revision schema Responses 200 - successfully created service revision Status: OK Schema Inlined models UpdateGlobalServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods trafficPercent integer int64 \u2713 Traffic percentage new revision will use Update Global Service Traffic ( updateGlobalServiceTraffic ) PATCH /api/functions/{serviceName} Update Global Service traffic directed to each revision, traffic can only be configured between two revisions. All other revisions will bet set to 0 traffic. Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name Service Traffic body UpdateGlobalServiceTrafficBody UpdateGlobalServiceTrafficBody \u2713 Payload that contains information on service traffic All responses Code Status Description Has headers Schema 200 OK successfully updated service traffic schema Responses 200 - successfully updated service traffic Status: OK Schema Inlined models UpdateGlobalServiceTrafficBody Properties Name Type Go type Required Default Description Example values [] UpdateGlobalServiceTrafficParamsBodyValuesItems0 []*models.UpdateGlobalServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateGlobalServiceTrafficParamsBodyValuesItems0 Properties Name Type Go type Required Default Description Example percent integer int64 Target traffice percentage revision string string Target service revision Create Namespace Service Revision ( updateNamespaceService ) POST /api/functions/namespaces/{namespace}/function/{serviceName} Creates a new namespace scoped knative service revision. Revisions are created with a traffic percentage. This percentage controls how much traffic will be directed to this revision. Traffic can be set to 100 to direct all traffic. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service body UpdateNamespaceServiceBody UpdateNamespaceServiceBody \u2713 Payload that contains information on service revision All responses Code Status Description Has headers Schema 200 OK successfully created service revision schema Responses 200 - successfully created service revision Status: OK Schema Inlined models UpdateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods trafficPercent integer int64 \u2713 Traffic percentage new revision will use Update Namespace Service Traffic ( updateNamespaceServiceTraffic ) PATCH /api/functions/namespaces/{namespace}/function/{serviceName} Update Namespace Service traffic directed to each revision, traffic can only be configured between two revisions. All other revisions will bet set to 0 traffic. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service Traffic body UpdateNamespaceServiceTrafficBody UpdateNamespaceServiceTrafficBody \u2713 Payload that contains information on service traffic All responses Code Status Description Has headers Schema 200 OK successfully updated service traffic schema Responses 200 - successfully updated service traffic Status: OK Schema Inlined models UpdateNamespaceServiceTrafficBody Properties Name Type Go type Required Default Description Example values [] UpdateNamespaceServiceTrafficParamsBodyValuesItems0 []*models.UpdateNamespaceServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateNamespaceServiceTrafficParamsBodyValuesItems0 Properties Name Type Go type Required Default Description Example percent integer int64 Target traffice percentage revision string string Target service revision Update a Workflow ( updateWorkflow ) POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow Updates a workflow at the target path. The body of this request should contain the workflow yaml you want to update to. Consumes text/plain Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow workflow data body string string Payload that contains the updated direktiv workflow yaml. All responses Code Status Description Has headers Schema 200 OK successfully updated workflow schema Responses 200 - successfully updated workflow Status: OK Schema Returns version information for servers in the cluster. ( version ) GET /api/version Returns version information for servers in the cluster. All responses Code Status Description Has headers Schema 200 OK version query was successful schema Responses 200 - version query was successful Status: OK Schema Watch Global Service Revision ( watchGlobalServiceRevision ) GET /api/functions/{serviceName}/revisions/{revisionGeneration} Watch a global scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revision schema Responses 200 - successfully watching service revision Status: OK Schema Watch Global Service Revision List ( watchGlobalServiceRevisionList ) GET /api/functions/{serviceName}/revisions Watch the revision list of a global scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revisions schema Responses 200 - successfully watching service revisions Status: OK Schema Watch Namespace Service Revision ( watchNamespaceServiceRevision ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Watch a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revision schema Responses 200 - successfully watching service revision Status: OK Schema Watch Namespace Service Revision List ( watchNamespaceServiceRevisionList ) GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions Watch the revision list of a namespace scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client. Produces text/event-stream Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name All responses Code Status Description Has headers Schema 200 OK successfully watching service revisions schema Responses 200 - successfully watching service revisions Status: OK Schema Gets Invoked Workflow Metrics ( workflowMetricsInvoked ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked Get metrics of invoked workflow instances. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets Workflow Time Metrics ( workflowMetricsMilliseconds ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed Get the timing metrics of a workflow's instance. This returns a total sum of the milliseconds a workflow has been executed for. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets a Workflow State Time Metrics ( workflowMetricsStateMilliseconds ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds Get the state timing metrics of a workflow's instance. This returns the timing of individual states in a workflow. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Gets Successful Workflow Metrics ( workflowMetricsSuccessful ) GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful Get metrics of a workflow, where the instance was successful. Parameters Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow All responses Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema Responses 200 - successfully got workflow metrics Status: OK Schema Models CreateGlobalPrivateRegistryBody CreateGlobalPrivateRegistryBody create global private registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL CreateGlobalRegistryBody CreateGlobalRegistryBody create global registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL CreateGlobalServiceBody CreateGlobalServiceBody create global service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v12\",\"minScale\":\"1\",\"name\":\"fast-request\",\"size\":\"small\"} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Name string string \u2713 Name of new service Size string string \u2713 Size of created service pods CreateNamespaceServiceBody CreateNamespaceServiceBody create namespace service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v12\",\"minScale\":\"1\",\"name\":\"fast-request\",\"size\":\"small\"} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Name string string \u2713 Name of new service Size string string \u2713 Size of created service pods CreateRegistryBody CreateRegistryBody create registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL DeleteGlobalPrivateRegistryBody DeleteGlobalPrivateRegistryBody delete global private registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL DeleteGlobalRegistryBody DeleteGlobalRegistryBody delete global registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL DeleteRegistryBody DeleteRegistryBody delete registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL ErrorResponse Properties Name Type Go type Required Default Description Example Error string string StatusCode int64 (formatted integer) int64 JqPlaygroundBody JqPlaygroundBody jq playground body Example {\"data\":\"eyJhIjogMSwgImIiOiAyLCAiYyI6IDQsICJkIjogN30=\",\"query\":\"map(select(. \\u003e= 2))\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 JSON data encoded in base64 Query string string \u2713 jq query to manipulate JSON data OkBody OkBody OkBody is an arbitrary placeholder response that represents an ok response body OkBody SetNamespaceConfigBody SetNamespaceConfigBody set namespace config body Example {\"broadcast\":{\"directory.create\":false,\"directory.delete\":false,\"instance.failed\":false,\"instance.started\":false,\"instance.success\":false,\"instance.variable.create\":false,\"instance.variable.delete\":false,\"instance.variable.update\":false,\"namespace.variable.create\":false,\"namespace.variable.delete\":false,\"namespace.variable.update\":false,\"workflow.create\":false,\"workflow.delete\":false,\"workflow.update\":false,\"workflow.variable.create\":false,\"workflow.variable.delete\":false,\"workflow.variable.update\":false}} Properties Name Type Go type Required Default Description Example Broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace SetWorkflowCloudEventLogsBody SetWorkflowCloudEventLogsBody set workflow cloud event logs body Example {\"logger\":\"mylog\"} Properties Name Type Go type Required Default Description Example Logger string string \u2713 Target Cloud Event ToggleWorkflowBody ToggleWorkflowBody toggle workflow body Example {\"live\":false} Properties Name Type Go type Required Default Description Example Live boolean bool \u2713 Workflow live status UpdateGlobalServiceBody UpdateGlobalServiceBody update global service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v10\",\"minScale\":\"1\",\"size\":\"small\",\"trafficPercent\":50} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Size string string \u2713 Size of created service pods TrafficPercent int64 (formatted integer) int64 \u2713 Traffic percentage new revision will use UpdateGlobalServiceTrafficBody UpdateGlobalServiceTrafficBody update global service traffic body Example {\"values\":[{\"percent\":60,\"revision\":\"global-fast-request-00002\"},{\"percent\":40,\"revision\":\"global-fast-request-00001\"}]} Properties Name Type Go type Required Default Description Example Values [] UpdateGlobalServiceTrafficParamsBodyValuesItems0 []*UpdateGlobalServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateGlobalServiceTrafficParamsBodyValuesItems0 UpdateGlobalServiceTrafficParamsBodyValuesItems0 update global service traffic params body values items0 Properties Name Type Go type Required Default Description Example Percent int64 (formatted integer) int64 Target traffice percentage Revision string string Target service revision UpdateNamespaceServiceBody UpdateNamespaceServiceBody update namespace service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v10\",\"minScale\":\"1\",\"size\":\"small\",\"trafficPercent\":50} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Size string string \u2713 Size of created service pods TrafficPercent int64 (formatted integer) int64 \u2713 Traffic percentage new revision will use UpdateNamespaceServiceTrafficBody UpdateNamespaceServiceTrafficBody update namespace service traffic body Example {\"values\":[{\"percent\":60,\"revision\":\"namespace-direktiv-fast-request-00002\"},{\"percent\":40,\"revision\":\"namespace-direktiv-fast-request-00001\"}]} Properties Name Type Go type Required Default Description Example Values [] UpdateNamespaceServiceTrafficParamsBodyValuesItems0 []*UpdateNamespaceServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateNamespaceServiceTrafficParamsBodyValuesItems0 UpdateNamespaceServiceTrafficParamsBodyValuesItems0 update namespace service traffic params body values items0 Properties Name Type Go type Required Default Description Example Percent int64 (formatted integer) int64 Target traffice percentage Revision string string Target service revision UpdateServiceRequest UpdateServiceRequest UpdateServiceRequest UpdateServiceRequest update service request Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 image MinScale int32 (formatted integer) int32 \u2713 minScale Size int32 (formatted integer) int32 \u2713 size TrafficPercent int64 (formatted integer) int64 \u2713 trafficPercent updateServiceRequest UpdateServiceRequest UpdateServiceRequest update service request Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 image MinScale int32 (formatted integer) int32 \u2713 minScale Size int32 (formatted integer) int32 \u2713 size TrafficPercent int64 (formatted integer) int64 \u2713 trafficPercent","title":"API"},{"location":"api/#direktiv-api","text":"Direktiv Open API Specification Direktiv Documentation can be found at https://docs.direktiv.io/","title":"Direktiv API"},{"location":"api/#informations","text":"","title":"Informations"},{"location":"api/#version","text":"1.0.0","title":"Version"},{"location":"api/#contact","text":"info@direktiv.io","title":"Contact"},{"location":"api/#content-negotiation","text":"","title":"Content negotiation"},{"location":"api/#uri-schemes","text":"http https","title":"URI Schemes"},{"location":"api/#consumes","text":"application/json text/plain","title":"Consumes"},{"location":"api/#produces","text":"application/json text/event-stream","title":"Produces"},{"location":"api/#access-control","text":"","title":"Access control"},{"location":"api/#security-schemes","text":"","title":"Security Schemes"},{"location":"api/#api_key-header-key","text":"Type : apikey","title":"api_key (header: KEY)"},{"location":"api/#security-requirements","text":"api_key","title":"Security Requirements"},{"location":"api/#all-endpoints","text":"","title":"All endpoints"},{"location":"api/#directory","text":"Method URI Name Summary PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory create directory Create a Directory","title":"directory"},{"location":"api/#global_services","text":"Method URI Name Summary POST /api/functions create global service Create Global Service DELETE /api/functions/{serviceName}/revisions/{revisionGeneration} delete global revision Delete Global Service Revision DELETE /api/functions/{serviceName} delete global service Delete Global Service GET /api/functions/{serviceName} get global service Get Global Service Details GET /api/functions get global service list Get Global Services List GET /api/functions/{serviceName}/revisions/{revisionGeneration}/pods list global service revision pods Get Global Service Revision Pods List POST /api/functions/{serviceName} update global service Create Global Service Revision PATCH /api/functions/{serviceName} update global service traffic Update Global Service Traffic GET /api/functions/{serviceName}/revisions/{revisionGeneration} watch global service revision Watch Global Service Revision GET /api/functions/{serviceName}/revisions watch global service revision list Watch Global Service Revision List","title":"global_services"},{"location":"api/#instances","text":"Method URI Name Summary POST /api/namespaces/{namespace}/instances/{instance}/cancel cancel instance Cancel a Pending Instance GET /api/namespaces/{namespace}/instances/{instance} get instance Get a Instance GET /api/namespaces/{namespace}/instances/{instance}/input get instance input Get a Instance Input GET /api/namespaces/{namespace}/instances get instance list Get List Instances GET /api/namespaces/{namespace}/instances/{instance}/output get instance output Get a Instance Output","title":"instances"},{"location":"api/#logs","text":"Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=logs get workflow logs Get Workflow Level Logs GET /api/namespaces/{namespace}/instances/{instance}/logs instance logs Gets Instance Logs GET /api/namespaces/{namespace}/logs namespace logs Gets Namespace Level Logs GET /api/logs server logs Get Direktiv Server Logs","title":"logs"},{"location":"api/#metrics","text":"Method URI Name Summary GET /api/namespaces/{namespace}/metrics/failed namespace metrics failed Gets Namespace Failed Workflow Instances Metrics GET /api/namespaces/{namespace}/metrics/invoked namespace metrics invoked Gets Namespace Invoked Workflow Metrics GET /api/namespaces/{namespace}/metrics/milliseconds namespace metrics milliseconds Gets Namespace Workflow Timing Metrics GET /api/namespaces/{namespace}/metrics/successful namespace metrics successful Gets Namespace Successful Workflow Instances Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked workflow metrics invoked Gets Invoked Workflow Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed workflow metrics milliseconds Gets Workflow Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds workflow metrics state milliseconds Gets a Workflow State Time Metrics GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful workflow metrics successful Gets Successful Workflow Metrics","title":"metrics"},{"location":"api/#namespace_services","text":"Method URI Name Summary POST /api/functions/namespaces/{namespace} create namespace service Create Namespace Service DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} delete namespace revision Delete Namespace Service Revision DELETE /api/functions/namespaces/{namespace}/function/{serviceName} delete namespace service Delete Namespace Service GET /api/functions/namespaces/{namespace}/function/{serviceName} get namespace service Get Namespace Service Details GET /api/functions/namespaces/{namespace} get namespace service list Get Namespace Services List GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods list namespace service revision pods Get Namespace Service Revision Pods List POST /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service Create Namespace Service Revision PATCH /api/functions/namespaces/{namespace}/function/{serviceName} update namespace service traffic Update Namespace Service Traffic GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} watch namespace service revision Watch Namespace Service Revision GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions watch namespace service revision list Watch Namespace Service Revision List","title":"namespace_services"},{"location":"api/#namespaces","text":"Method URI Name Summary PUT /api/namespaces/{namespace} create namespace Creates a namespace DELETE /api/namespaces/{namespace} delete namespace Delete a namespace GET /api/namespaces/{namespace}/config get namespace config Gets a namespace config GET /api/namespaces get namespaces Gets the list of namespaces PATCH /api/namespaces/{namespace}/config set namespace config Sets a namespace config","title":"namespaces"},{"location":"api/#node","text":"Method URI Name Summary DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node delete node Delete a node GET /api/namespaces/{namespace}/tree/{nodePath} get nodes Get List of Namespace Nodes","title":"node"},{"location":"api/#other","text":"Method URI Name Summary POST /api/namespaces/{namespace}/broadcast broadcast cloudevent Broadcast Cloud Event POST /api/jq jq playground JQ Playground api to test jq queries GET /api/version version Returns version information for servers in the cluster.","title":"other"},{"location":"api/#registries","text":"Method URI Name Summary POST /api/functions/registries/private create global private registry Create a Global Container Registry POST /api/functions/registries/global create global registry Create a Global Container Registry POST /api/functions/registries/namespaces/{namespace} create registry Create a Namespace Container Registry DELETE /api/functions/registries/private delete global private registry Delete a Global Container Registry DELETE /api/functions/registries/global delete global registry Delete a global Container Registry DELETE /api/functions/registries/namespaces/{namespace} delete registry Delete a Namespace Container Registry GET /api/functions/registries/private get global private registries Get List of Global Private Registries GET /api/functions/registries/global get global registries Get List of Global Registries GET /api/functions/registries/namespaces/{namespace} get registries Get List of Namespace Registries","title":"registries"},{"location":"api/#secrets","text":"Method URI Name Summary PUT /api/namespaces/{namespace}/secrets/{secret} create secret Create a Namespace Secret DELETE /api/namespaces/{namespace}/secrets/{secret} delete secret Delete a Namespace Secret GET /api/namespaces/{namespace}/secrets get secrets Get List of Namespace Secrets","title":"secrets"},{"location":"api/#variables","text":"Method URI Name Summary DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} delete instance variable Delete a Instance Variable DELETE /api/namespaces/{namespace}/vars/{variable} delete namespace variable Delete a Namespace Variable DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var delete workflow variable Delete a Workflow Variable GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} get instance variable Get a Instance Variable GET /api/namespaces/{namespace}/instances/{instance}/vars get instance variables Get List of Instance Variable GET /api/namespaces/{namespace}/vars/{variable} get namespace variable Get a Namespace Variable GET /api/namespaces/{namespace}/vars get namespace variables Get Namespace Variable List GET /api/namespaces/{namespace}/tree/{workflow}?op=var get workflow variable Get a Workflow Variable GET /api/namespaces/{namespace}/tree/{workflow}?op=vars get workflow variables Get List of Workflow Variables PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} set instance variable Set a Instance Variable PUT /api/namespaces/{namespace}/vars/{variable} set namespace variable Set a Namespace Variable PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var set workflow variable Set a Workflow Variable","title":"variables"},{"location":"api/#workflow_services","text":"Method URI Name Summary GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function get workflow service Get Workflow Service Details GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision get workflow service revision Get Workflow Service Revision GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions get workflow service revision list Get Workflow Service Revision List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods list workflow service revision pods Get Workflow Service Revision Pods List GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services list workflow services Get Workflow Services List","title":"workflow_services"},{"location":"api/#workflows","text":"Method URI Name Summary GET /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow Await Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=wait await execute workflow body Await Execute a Workflow With Body PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow create workflow Create a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=execute execute workflow Execute a Workflow POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging set workflow cloud event logs Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle toggle workflow Set Cloud Event for Workflow to Log to POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow update workflow Update a Workflow","title":"workflows"},{"location":"api/#paths","text":"","title":"Paths"},{"location":"api/#await-execute-a-workflow-awaitexecuteworkflow","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters.","title":" Await Execute a Workflow (awaitExecuteWorkflow)"},{"location":"api/#parameters","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string.","title":"Parameters"},{"location":"api/#all-responses","text":"Code Status Description Has headers Schema 200 OK successfully executed workflow schema","title":"All responses"},{"location":"api/#responses","text":"","title":"Responses"},{"location":"api/#200-successfully-executed-workflow","text":"Status: OK","title":" 200 - successfully executed workflow"},{"location":"api/#schema","text":"","title":" Schema"},{"location":"api/#await-execute-a-workflow-with-body-awaitexecuteworkflowbody","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=wait Executes a workflow with optionally some input provided in the request body as json. This path will wait until the workflow execution has completed and return the instance output. NOTE: Input can also be provided with the input.X query parameters; Where X is the json key. Only top level json keys are supported when providing input with query parameters. Input query parameters are only read if the request has no body.","title":" Await Execute a Workflow With Body (awaitExecuteWorkflowBody)"},{"location":"api/#parameters_1","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow ctype query string string Manually set the Content-Type response header instead of auto-detected. This doesn't change the body of the response in any way. field query string string If provided, instead of returning the entire output json the response body will contain the single top-level json field raw-output query boolean bool If set to true, will return an empty output as null, encoded base64 data as decoded binary data, and quoted json strings as a escaped string. Workflow Input body interface{} interface{} \u2713 The input of this workflow instance","title":"Parameters"},{"location":"api/#all-responses_1","text":"Code Status Description Has headers Schema 200 OK successfully executed workflow schema","title":"All responses"},{"location":"api/#responses_1","text":"","title":"Responses"},{"location":"api/#200-successfully-executed-workflow_1","text":"Status: OK","title":" 200 - successfully executed workflow"},{"location":"api/#schema_1","text":"","title":" Schema"},{"location":"api/#broadcast-cloud-event-broadcastcloudevent","text":"POST /api/namespaces/{namespace}/broadcast Broadcast a cloud event to a namespace. Cloud events posted to this api will be picked up by any workflows listening to the same event type on the namescape. The body of this request should follow the cloud event core specification defined at https://github.com/cloudevents/spec .","title":" Broadcast Cloud Event (broadcastCloudevent)"},{"location":"api/#parameters_2","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace cloudevent body interface{} interface{} \u2713 Cloud Event request to be sent.","title":"Parameters"},{"location":"api/#all-responses_2","text":"Code Status Description Has headers Schema 200 OK successfully sent cloud event schema","title":"All responses"},{"location":"api/#responses_2","text":"","title":"Responses"},{"location":"api/#200-successfully-sent-cloud-event","text":"Status: OK","title":" 200 - successfully sent cloud event"},{"location":"api/#schema_2","text":"","title":" Schema"},{"location":"api/#cancel-a-pending-instance-cancelinstance","text":"POST /api/namespaces/{namespace}/instances/{instance}/cancel Cancel a currently pending instance.","title":" Cancel a Pending Instance (cancelInstance)"},{"location":"api/#parameters_3","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_3","text":"Code Status Description Has headers Schema 200 OK successfully cancelled instance schema","title":"All responses"},{"location":"api/#responses_3","text":"","title":"Responses"},{"location":"api/#200-successfully-cancelled-instance","text":"Status: OK","title":" 200 - successfully cancelled instance"},{"location":"api/#schema_3","text":"","title":" Schema"},{"location":"api/#create-a-directory-createdirectory","text":"PUT /api/namespaces/{namespace}/tree/{directory}?op=create-directory Creates a directory at the target path.","title":" Create a Directory (createDirectory)"},{"location":"api/#parameters_4","text":"Name Source Type Go type Separator Required Default Description directory path string string \u2713 path to target directory namespace path string string \u2713 target namespace op query string string \u2713 \"create-directory\" the operation for the api","title":"Parameters"},{"location":"api/#all-responses_4","text":"Code Status Description Has headers Schema 200 OK directory has been created schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_4","text":"","title":"Responses"},{"location":"api/#200-directory-has-been-created","text":"Status: OK","title":" 200 - directory has been created"},{"location":"api/#schema_4","text":"OkBody","title":" Schema"},{"location":"api/#default-response","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_5","text":"ErrorResponse","title":" Schema"},{"location":"api/#create-a-global-container-registry-createglobalprivateregistry","text":"POST /api/functions/registries/private Create a global container registry. Global Private registries are only available to global services. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN","title":" Create a Global Container Registry (createGlobalPrivateRegistry)"},{"location":"api/#parameters_5","text":"Name Source Type Go type Separator Required Default Description Registry Payload body CreateGlobalPrivateRegistryBody CreateGlobalPrivateRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_5","text":"Code Status Description Has headers Schema 200 OK successfully created global private registry schema","title":"All responses"},{"location":"api/#responses_5","text":"","title":"Responses"},{"location":"api/#200-successfully-created-global-private-registry","text":"Status: OK","title":" 200 - successfully created global private registry"},{"location":"api/#schema_6","text":"","title":" Schema"},{"location":"api/#inlined-models","text":"CreateGlobalPrivateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#create-a-global-container-registry-createglobalregistry","text":"POST /api/functions/registries/global Create a global container registry. Global registries are available to all services. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN","title":" Create a Global Container Registry (createGlobalRegistry)"},{"location":"api/#parameters_6","text":"Name Source Type Go type Separator Required Default Description Registry Payload body CreateGlobalRegistryBody CreateGlobalRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_6","text":"Code Status Description Has headers Schema 200 OK successfully created global registry schema","title":"All responses"},{"location":"api/#responses_6","text":"","title":"Responses"},{"location":"api/#200-successfully-created-global-registry","text":"Status: OK","title":" 200 - successfully created global registry"},{"location":"api/#schema_7","text":"","title":" Schema"},{"location":"api/#inlined-models_1","text":"CreateGlobalRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#create-global-service-createglobalservice","text":"POST /api/functions Creates global scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/docs/walkthrough/using-functions.html","title":" Create Global Service (createGlobalService)"},{"location":"api/#parameters_7","text":"Name Source Type Go type Separator Required Default Description Service body CreateGlobalServiceBody CreateGlobalServiceBody \u2713 Payload that contains information on new service","title":"Parameters"},{"location":"api/#all-responses_7","text":"Code Status Description Has headers Schema 200 OK successfully created service schema","title":"All responses"},{"location":"api/#responses_7","text":"","title":"Responses"},{"location":"api/#200-successfully-created-service","text":"Status: OK","title":" 200 - successfully created service"},{"location":"api/#schema_8","text":"","title":" Schema"},{"location":"api/#inlined-models_2","text":"CreateGlobalServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size string string \u2713 Size of created service pods","title":"Inlined models"},{"location":"api/#creates-a-namespace-createnamespace","text":"PUT /api/namespaces/{namespace} Creates a new namespace.","title":" Creates a namespace (createNamespace)"},{"location":"api/#parameters_8","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to create","title":"Parameters"},{"location":"api/#all-responses_8","text":"Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_8","text":"","title":"Responses"},{"location":"api/#200-namespace-has-been-successfully-created","text":"Status: OK","title":" 200 - namespace has been successfully created"},{"location":"api/#schema_9","text":"OkBody","title":" Schema"},{"location":"api/#default-response_1","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_10","text":"ErrorResponse","title":" Schema"},{"location":"api/#create-namespace-service-createnamespaceservice","text":"POST /api/functions/namespaces/{namespace} Creates namespace scoped knative service. Service Names are unique on a scope level. These services can be used as functions in workflows, more about this can be read here: https://docs.direktiv.io/docs/walkthrough/using-functions.html","title":" Create Namespace Service (createNamespaceService)"},{"location":"api/#parameters_9","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Service body CreateNamespaceServiceBody CreateNamespaceServiceBody \u2713 Payload that contains information on new service","title":"Parameters"},{"location":"api/#all-responses_9","text":"Code Status Description Has headers Schema 200 OK successfully created service schema","title":"All responses"},{"location":"api/#responses_9","text":"","title":"Responses"},{"location":"api/#200-successfully-created-service_1","text":"Status: OK","title":" 200 - successfully created service"},{"location":"api/#schema_11","text":"","title":" Schema"},{"location":"api/#inlined-models_3","text":"CreateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live name string string \u2713 Name of new service size string string \u2713 Size of created service pods","title":"Inlined models"},{"location":"api/#create-a-namespace-container-registry-createregistry","text":"POST /api/functions/registries/namespaces/{namespace} Create a namespace container registry. This can be used to connect your workflows to private container registries that require tokens. The data property in the body is made up from the registry user and token. It follows the pattern : data=USER:TOKEN","title":" Create a Namespace Container Registry (createRegistry)"},{"location":"api/#parameters_10","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body CreateRegistryBody CreateRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_10","text":"Code Status Description Has headers Schema 200 OK successfully created namespace registry schema","title":"All responses"},{"location":"api/#responses_10","text":"","title":"Responses"},{"location":"api/#200-successfully-created-namespace-registry","text":"Status: OK","title":" 200 - successfully created namespace registry"},{"location":"api/#schema_12","text":"","title":" Schema"},{"location":"api/#inlined-models_4","text":"CreateRegistryBody Properties Name Type Go type Required Default Description Example data string string \u2713 Target registry connection data containing the user and token. reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#create-a-namespace-secret-createsecret","text":"PUT /api/namespaces/{namespace}/secrets/{secret} Create a namespace secret.","title":" Create a Namespace Secret (createSecret)"},{"location":"api/#consumes_1","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_11","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret Secret Payload body string string \u2713 Payload that contains secret data.","title":"Parameters"},{"location":"api/#all-responses_11","text":"Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_11","text":"","title":"Responses"},{"location":"api/#200-namespace-has-been-successfully-created_1","text":"Status: OK","title":" 200 - namespace has been successfully created"},{"location":"api/#schema_13","text":"OkBody","title":" Schema"},{"location":"api/#default-response_2","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_14","text":"ErrorResponse","title":" Schema"},{"location":"api/#create-a-workflow-createworkflow","text":"PUT /api/namespaces/{namespace}/tree/{workflow}?op=create-workflow Creates a workflow at the target path. The body of this request should contain the workflow yaml.","title":" Create a Workflow (createWorkflow)"},{"location":"api/#consumes_2","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_12","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"create-workflow\" the operation for the api workflow data body string string Payload that contains the direktiv workflow yaml to create.","title":"Parameters"},{"location":"api/#all-responses_12","text":"Code Status Description Has headers Schema 200 OK successfully created workflow schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_12","text":"","title":"Responses"},{"location":"api/#200-successfully-created-workflow","text":"Status: OK","title":" 200 - successfully created workflow"},{"location":"api/#schema_15","text":"OkBody","title":" Schema"},{"location":"api/#default-response_3","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_16","text":"ErrorResponse","title":" Schema"},{"location":"api/#delete-a-global-container-registry-deleteglobalprivateregistry","text":"DELETE /api/functions/registries/private Delete a global container registry. Global Private registries are only available to global services.","title":" Delete a Global Container Registry (deleteGlobalPrivateRegistry)"},{"location":"api/#parameters_13","text":"Name Source Type Go type Separator Required Default Description Registry Payload body DeleteGlobalPrivateRegistryBody DeleteGlobalPrivateRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_13","text":"Code Status Description Has headers Schema 200 OK successfully delete global private registry schema","title":"All responses"},{"location":"api/#responses_13","text":"","title":"Responses"},{"location":"api/#200-successfully-delete-global-private-registry","text":"Status: OK","title":" 200 - successfully delete global private registry"},{"location":"api/#schema_17","text":"","title":" Schema"},{"location":"api/#inlined-models_5","text":"DeleteGlobalPrivateRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#delete-a-global-container-registry-deleteglobalregistry","text":"DELETE /api/functions/registries/global Delete a Global container registry Global registries are available to all services.","title":" Delete a global Container Registry (deleteGlobalRegistry)"},{"location":"api/#parameters_14","text":"Name Source Type Go type Separator Required Default Description Registry Payload body DeleteGlobalRegistryBody DeleteGlobalRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_14","text":"Code Status Description Has headers Schema 200 OK successfully delete global registry schema","title":"All responses"},{"location":"api/#responses_14","text":"","title":"Responses"},{"location":"api/#200-successfully-delete-global-registry","text":"Status: OK","title":" 200 - successfully delete global registry"},{"location":"api/#schema_18","text":"","title":" Schema"},{"location":"api/#inlined-models_6","text":"DeleteGlobalRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#delete-global-service-revision-deleteglobalrevision","text":"DELETE /api/functions/{serviceName}/revisions/{revisionGeneration} Delete a global scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003'. Note: Revisions with traffic cannot be deleted.","title":" Delete Global Service Revision (deleteGlobalRevision)"},{"location":"api/#parameters_15","text":"Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_15","text":"Code Status Description Has headers Schema 200 OK successfully deleted service revision schema","title":"All responses"},{"location":"api/#responses_15","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-service-revision","text":"Status: OK","title":" 200 - successfully deleted service revision"},{"location":"api/#schema_19","text":"","title":" Schema"},{"location":"api/#delete-global-service-deleteglobalservice","text":"DELETE /api/functions/{serviceName} Deletes global scoped knative service and all its revisions.","title":" Delete Global Service (deleteGlobalService)"},{"location":"api/#parameters_16","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_16","text":"Code Status Description Has headers Schema 200 OK successfully deleted service schema","title":"All responses"},{"location":"api/#responses_16","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-service","text":"Status: OK","title":" 200 - successfully deleted service"},{"location":"api/#schema_20","text":"","title":" Schema"},{"location":"api/#delete-a-instance-variable-deleteinstancevariable","text":"DELETE /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Delete a instance variable.","title":" Delete a Instance Variable (deleteInstanceVariable)"},{"location":"api/#parameters_17","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_17","text":"Code Status Description Has headers Schema 200 OK successfully deleted instance variable schema","title":"All responses"},{"location":"api/#responses_17","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-instance-variable","text":"Status: OK","title":" 200 - successfully deleted instance variable"},{"location":"api/#schema_21","text":"","title":" Schema"},{"location":"api/#delete-a-namespace-deletenamespace","text":"DELETE /api/namespaces/{namespace} Delete a namespace. A namespace will not delete by default if it has any child resources (workflows, etc...). Deleting the namespace with all its children can be done using the recursive query parameter.","title":" Delete a namespace (deleteNamespace)"},{"location":"api/#parameters_18","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to delete recursive query boolean bool recursively deletes all child resources","title":"Parameters"},{"location":"api/#all-responses_18","text":"Code Status Description Has headers Schema 200 OK namespace has been successfully deleted schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_18","text":"","title":"Responses"},{"location":"api/#200-namespace-has-been-successfully-deleted","text":"Status: OK","title":" 200 - namespace has been successfully deleted"},{"location":"api/#schema_22","text":"OkBody","title":" Schema"},{"location":"api/#default-response_4","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_23","text":"ErrorResponse","title":" Schema"},{"location":"api/#delete-namespace-service-revision-deletenamespacerevision","text":"DELETE /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Delete a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: Revisions with traffic cannot be deleted.","title":" Delete Namespace Service Revision (deleteNamespaceRevision)"},{"location":"api/#parameters_19","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_19","text":"Code Status Description Has headers Schema 200 OK successfully deleted service revision schema","title":"All responses"},{"location":"api/#responses_19","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-service-revision_1","text":"Status: OK","title":" 200 - successfully deleted service revision"},{"location":"api/#schema_24","text":"","title":" Schema"},{"location":"api/#delete-namespace-service-deletenamespaceservice","text":"DELETE /api/functions/namespaces/{namespace}/function/{serviceName} Deletes namespace scoped knative service and all its revisions.","title":" Delete Namespace Service (deleteNamespaceService)"},{"location":"api/#parameters_20","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_20","text":"Code Status Description Has headers Schema 200 OK successfully deleted service schema","title":"All responses"},{"location":"api/#responses_20","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-service_1","text":"Status: OK","title":" 200 - successfully deleted service"},{"location":"api/#schema_25","text":"","title":" Schema"},{"location":"api/#delete-a-namespace-variable-deletenamespacevariable","text":"DELETE /api/namespaces/{namespace}/vars/{variable} Delete a namespace variable.","title":" Delete a Namespace Variable (deleteNamespaceVariable)"},{"location":"api/#parameters_21","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_21","text":"Code Status Description Has headers Schema 200 OK successfully deleted namespace variable schema","title":"All responses"},{"location":"api/#responses_21","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-namespace-variable","text":"Status: OK","title":" 200 - successfully deleted namespace variable"},{"location":"api/#schema_26","text":"","title":" Schema"},{"location":"api/#delete-a-node-deletenode","text":"DELETE /api/namespaces/{namespace}/tree/{node}?op=delete-node Creates a directory at the target path.","title":" Delete a node (deleteNode)"},{"location":"api/#parameters_22","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace node path string string \u2713 path to target node op query string string \u2713 \"delete-node\" the operation for the api","title":"Parameters"},{"location":"api/#all-responses_22","text":"Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_22","text":"","title":"Responses"},{"location":"api/#200-node-has-been-deleted","text":"Status: OK","title":" 200 - node has been deleted"},{"location":"api/#schema_27","text":"OkBody","title":" Schema"},{"location":"api/#default-response_5","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_28","text":"ErrorResponse","title":" Schema"},{"location":"api/#delete-a-namespace-container-registry-deleteregistry","text":"DELETE /api/functions/registries/namespaces/{namespace} Delete a namespace container registry","title":" Delete a Namespace Container Registry (deleteRegistry)"},{"location":"api/#parameters_23","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace Registry Payload body DeleteRegistryBody DeleteRegistryBody \u2713 Payload that contains registry data","title":"Parameters"},{"location":"api/#all-responses_23","text":"Code Status Description Has headers Schema 200 OK successfully delete namespace registry schema","title":"All responses"},{"location":"api/#responses_23","text":"","title":"Responses"},{"location":"api/#200-successfully-delete-namespace-registry","text":"Status: OK","title":" 200 - successfully delete namespace registry"},{"location":"api/#schema_29","text":"","title":" Schema"},{"location":"api/#inlined-models_7","text":"DeleteRegistryBody Properties Name Type Go type Required Default Description Example reg string string \u2713 Target registry URL","title":"Inlined models"},{"location":"api/#delete-a-namespace-secret-deletesecret","text":"DELETE /api/namespaces/{namespace}/secrets/{secret} Delete a namespace secret.","title":" Delete a Namespace Secret (deleteSecret)"},{"location":"api/#parameters_24","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace secret path string string \u2713 target secret","title":"Parameters"},{"location":"api/#all-responses_24","text":"Code Status Description Has headers Schema 200 OK namespace has been successfully created schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_24","text":"","title":"Responses"},{"location":"api/#200-namespace-has-been-successfully-created_2","text":"Status: OK","title":" 200 - namespace has been successfully created"},{"location":"api/#schema_30","text":"OkBody","title":" Schema"},{"location":"api/#default-response_6","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_31","text":"ErrorResponse","title":" Schema"},{"location":"api/#delete-a-workflow-variable-deleteworkflowvariable","text":"DELETE /api/namespaces/{namespace}/tree/{workflow}?op=delete-var Delete a workflow variable.","title":" Delete a Workflow Variable (deleteWorkflowVariable)"},{"location":"api/#parameters_25","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_25","text":"Code Status Description Has headers Schema 200 OK successfully deleted workflow variable schema","title":"All responses"},{"location":"api/#responses_25","text":"","title":"Responses"},{"location":"api/#200-successfully-deleted-workflow-variable","text":"Status: OK","title":" 200 - successfully deleted workflow variable"},{"location":"api/#schema_32","text":"","title":" Schema"},{"location":"api/#execute-a-workflow-executeworkflow","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=execute Executes a workflow with optionally some input provided in the request body as json.","title":" Execute a Workflow (executeWorkflow)"},{"location":"api/#parameters_26","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow op query string string \u2713 \"execute\" the operation for the api Workflow Input body interface{} interface{} \u2713 The input of this workflow instance","title":"Parameters"},{"location":"api/#all-responses_26","text":"Code Status Description Has headers Schema 200 OK node has been deleted schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_26","text":"","title":"Responses"},{"location":"api/#200-node-has-been-deleted_1","text":"Status: OK","title":" 200 - node has been deleted"},{"location":"api/#schema_33","text":"OkBody","title":" Schema"},{"location":"api/#default-response_7","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_34","text":"ErrorResponse","title":" Schema"},{"location":"api/#get-list-of-global-private-registries-getglobalprivateregistries","text":"GET /api/functions/registries/private Gets the list of global private registries. Global Private registries are only available to global services.","title":" Get List of Global Private Registries (getGlobalPrivateRegistries)"},{"location":"api/#all-responses_27","text":"Code Status Description Has headers Schema 200 OK successfully got global private registries schema","title":"All responses"},{"location":"api/#responses_27","text":"","title":"Responses"},{"location":"api/#200-successfully-got-global-private-registries","text":"Status: OK","title":" 200 - successfully got global private registries"},{"location":"api/#schema_35","text":"","title":" Schema"},{"location":"api/#get-list-of-global-registries-getglobalregistries","text":"GET /api/functions/registries/global Gets the list of global registries. Global registries are available to all services.","title":" Get List of Global Registries (getGlobalRegistries)"},{"location":"api/#all-responses_28","text":"Code Status Description Has headers Schema 200 OK successfully got global registries schema","title":"All responses"},{"location":"api/#responses_28","text":"","title":"Responses"},{"location":"api/#200-successfully-got-global-registries","text":"Status: OK","title":" 200 - successfully got global registries"},{"location":"api/#schema_36","text":"","title":" Schema"},{"location":"api/#get-global-service-details-getglobalservice","text":"GET /api/functions/{serviceName} Get details of a global scoped knative service.","title":" Get Global Service Details (getGlobalService)"},{"location":"api/#parameters_27","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_29","text":"Code Status Description Has headers Schema 200 OK successfully got service details schema","title":"All responses"},{"location":"api/#responses_29","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-details","text":"Status: OK","title":" 200 - successfully got service details"},{"location":"api/#schema_37","text":"","title":" Schema"},{"location":"api/#get-global-services-list-getglobalservicelist","text":"GET /api/functions Gets a list of global knative services.","title":" Get Global Services List (getGlobalServiceList)"},{"location":"api/#all-responses_30","text":"Code Status Description Has headers Schema 200 OK successfully got services list schema","title":"All responses"},{"location":"api/#responses_30","text":"","title":"Responses"},{"location":"api/#200-successfully-got-services-list","text":"Status: OK","title":" 200 - successfully got services list"},{"location":"api/#schema_38","text":"","title":" Schema"},{"location":"api/#get-a-instance-getinstance","text":"GET /api/namespaces/{namespace}/instances/{instance} Gets the details of a executed workflow instance in this namespace.","title":" Get a Instance (getInstance)"},{"location":"api/#parameters_28","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_31","text":"Code Status Description Has headers Schema 200 OK successfully got instance schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_31","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance","text":"Status: OK","title":" 200 - successfully got instance"},{"location":"api/#schema_39","text":"OkBody","title":" Schema"},{"location":"api/#default-response_8","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_40","text":"ErrorResponse","title":" Schema"},{"location":"api/#get-a-instance-input-getinstanceinput","text":"GET /api/namespaces/{namespace}/instances/{instance}/input Gets the input an instance was provided when executed.","title":" Get a Instance Input (getInstanceInput)"},{"location":"api/#parameters_29","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_32","text":"Code Status Description Has headers Schema 200 OK successfully got instance input schema","title":"All responses"},{"location":"api/#responses_32","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-input","text":"Status: OK","title":" 200 - successfully got instance input"},{"location":"api/#schema_41","text":"","title":" Schema"},{"location":"api/#get-list-instances-getinstancelist","text":"GET /api/namespaces/{namespace}/instances Gets a list of instances in a namespace.","title":" Get List Instances (getInstanceList)"},{"location":"api/#parameters_30","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_33","text":"Code Status Description Has headers Schema 200 OK successfully got namespace instances schema","title":"All responses"},{"location":"api/#responses_33","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-instances","text":"Status: OK","title":" 200 - successfully got namespace instances"},{"location":"api/#schema_42","text":"","title":" Schema"},{"location":"api/#get-a-instance-output-getinstanceoutput","text":"GET /api/namespaces/{namespace}/instances/{instance}/output Gets the output an instance was provided when executed.","title":" Get a Instance Output (getInstanceOutput)"},{"location":"api/#parameters_31","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_34","text":"Code Status Description Has headers Schema 200 OK successfully got instance output schema","title":"All responses"},{"location":"api/#responses_34","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-output","text":"Status: OK","title":" 200 - successfully got instance output"},{"location":"api/#schema_43","text":"","title":" Schema"},{"location":"api/#get-a-instance-variable-getinstancevariable","text":"GET /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Get the value sorted in a instance variable.","title":" Get a Instance Variable (getInstanceVariable)"},{"location":"api/#parameters_32","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_35","text":"Code Status Description Has headers Schema 200 OK successfully got instance variable schema","title":"All responses"},{"location":"api/#responses_35","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-variable","text":"Status: OK","title":" 200 - successfully got instance variable"},{"location":"api/#schema_44","text":"","title":" Schema"},{"location":"api/#get-list-of-instance-variable-getinstancevariables","text":"GET /api/namespaces/{namespace}/instances/{instance}/vars Gets a list of variables in a instance.","title":" Get List of Instance Variable (getInstanceVariables)"},{"location":"api/#parameters_33","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_36","text":"Code Status Description Has headers Schema 200 OK successfully got instance variables schema","title":"All responses"},{"location":"api/#responses_36","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-variables","text":"Status: OK","title":" 200 - successfully got instance variables"},{"location":"api/#schema_45","text":"","title":" Schema"},{"location":"api/#gets-a-namespace-config-getnamespaceconfig","text":"GET /api/namespaces/{namespace}/config Gets a namespace config.","title":" Gets a namespace config (getNamespaceConfig)"},{"location":"api/#parameters_34","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update","title":"Parameters"},{"location":"api/#all-responses_37","text":"Code Status Description Has headers Schema 200 OK successfully got namespace config schema","title":"All responses"},{"location":"api/#responses_37","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-config","text":"Status: OK","title":" 200 - successfully got namespace config"},{"location":"api/#schema_46","text":"","title":" Schema"},{"location":"api/#get-namespace-service-details-getnamespaceservice","text":"GET /api/functions/namespaces/{namespace}/function/{serviceName} Get details of a namespace scoped knative service.","title":" Get Namespace Service Details (getNamespaceService)"},{"location":"api/#parameters_35","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_38","text":"Code Status Description Has headers Schema 200 OK successfully got service details schema","title":"All responses"},{"location":"api/#responses_38","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-details_1","text":"Status: OK","title":" 200 - successfully got service details"},{"location":"api/#schema_47","text":"","title":" Schema"},{"location":"api/#get-namespace-services-list-getnamespaceservicelist","text":"GET /api/functions/namespaces/{namespace} Gets a list of namespace knative services.","title":" Get Namespace Services List (getNamespaceServiceList)"},{"location":"api/#parameters_36","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_39","text":"Code Status Description Has headers Schema 200 OK successfully got services list schema","title":"All responses"},{"location":"api/#responses_39","text":"","title":"Responses"},{"location":"api/#200-successfully-got-services-list_1","text":"Status: OK","title":" 200 - successfully got services list"},{"location":"api/#schema_48","text":"","title":" Schema"},{"location":"api/#get-a-namespace-variable-getnamespacevariable","text":"GET /api/namespaces/{namespace}/vars/{variable} Get the value sorted in a namespace variable.","title":" Get a Namespace Variable (getNamespaceVariable)"},{"location":"api/#parameters_37","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_40","text":"Code Status Description Has headers Schema 200 OK successfully got namespace variable schema","title":"All responses"},{"location":"api/#responses_40","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-variable","text":"Status: OK","title":" 200 - successfully got namespace variable"},{"location":"api/#schema_49","text":"","title":" Schema"},{"location":"api/#get-namespace-variable-list-getnamespacevariables","text":"GET /api/namespaces/{namespace}/vars Gets a list of variables in a namespace.","title":" Get Namespace Variable List (getNamespaceVariables)"},{"location":"api/#parameters_38","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_41","text":"Code Status Description Has headers Schema 200 OK successfully got namespace variables schema","title":"All responses"},{"location":"api/#responses_41","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-variables","text":"Status: OK","title":" 200 - successfully got namespace variables"},{"location":"api/#schema_50","text":"","title":" Schema"},{"location":"api/#gets-the-list-of-namespaces-getnamespaces","text":"GET /api/namespaces Gets the list of namespaces.","title":" Gets the list of namespaces (getNamespaces)"},{"location":"api/#all-responses_42","text":"Code Status Description Has headers Schema 200 OK successfully got list of namespaces schema","title":"All responses"},{"location":"api/#responses_42","text":"","title":"Responses"},{"location":"api/#200-successfully-got-list-of-namespaces","text":"Status: OK","title":" 200 - successfully got list of namespaces"},{"location":"api/#schema_51","text":"","title":" Schema"},{"location":"api/#get-list-of-namespace-nodes-getnodes","text":"GET /api/namespaces/{namespace}/tree/{nodePath} Gets Workflow and Directory Nodes at nodePath.","title":" Get List of Namespace Nodes (getNodes)"},{"location":"api/#parameters_39","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace nodePath path string string \u2713 target path in tree","title":"Parameters"},{"location":"api/#all-responses_43","text":"Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_43","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-nodes","text":"Status: OK","title":" 200 - successfully got namespace nodes"},{"location":"api/#schema_52","text":"OkBody","title":" Schema"},{"location":"api/#default-response_9","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_53","text":"ErrorResponse","title":" Schema"},{"location":"api/#get-list-of-namespace-registries-getregistries","text":"GET /api/functions/registries/namespaces/{namespace} Gets the list of namespace registries.","title":" Get List of Namespace Registries (getRegistries)"},{"location":"api/#parameters_40","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_44","text":"Code Status Description Has headers Schema 200 OK successfully got namespace registries schema","title":"All responses"},{"location":"api/#responses_44","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-registries","text":"Status: OK","title":" 200 - successfully got namespace registries"},{"location":"api/#schema_54","text":"","title":" Schema"},{"location":"api/#get-list-of-namespace-secrets-getsecrets","text":"GET /api/namespaces/{namespace}/secrets Gets the list of namespace secrets.","title":" Get List of Namespace Secrets (getSecrets)"},{"location":"api/#parameters_41","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_45","text":"Code Status Description Has headers Schema 200 OK successfully got namespace nodes schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_45","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-nodes_1","text":"Status: OK","title":" 200 - successfully got namespace nodes"},{"location":"api/#schema_55","text":"OkBody","title":" Schema"},{"location":"api/#default-response_10","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_56","text":"ErrorResponse","title":" Schema"},{"location":"api/#get-workflow-level-logs-getworkflowlogs","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=logs Get workflow level logs.","title":" Get Workflow Level Logs (getWorkflowLogs)"},{"location":"api/#parameters_42","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_46","text":"Code Status Description Has headers Schema 200 OK successfully got workflow logs schema","title":"All responses"},{"location":"api/#responses_46","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-logs","text":"Status: OK","title":" 200 - successfully got workflow logs"},{"location":"api/#schema_57","text":"","title":" Schema"},{"location":"api/#get-workflow-service-details-getworkflowservice","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function Get a workflow scoped knative service details. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Get Workflow Service Details (getWorkflowService)"},{"location":"api/#parameters_43","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version","title":"Parameters"},{"location":"api/#all-responses_47","text":"Code Status Description Has headers Schema 200 OK successfully got service details schema","title":"All responses"},{"location":"api/#responses_47","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-details_2","text":"Status: OK","title":" 200 - successfully got service details"},{"location":"api/#schema_58","text":"","title":" Schema"},{"location":"api/#get-workflow-service-revision-getworkflowservicerevision","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revision Get a workflow scoped knative service revision. This will return details on a single revision. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'.","title":" Get Workflow Service Revision (getWorkflowServiceRevision)"},{"location":"api/#parameters_44","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_48","text":"Code Status Description Has headers Schema 200 OK successfully got service revision details schema","title":"All responses"},{"location":"api/#responses_48","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-revision-details","text":"Status: OK","title":" 200 - successfully got service revision details"},{"location":"api/#schema_59","text":"","title":" Schema"},{"location":"api/#get-workflow-service-revision-list-getworkflowservicerevisionlist","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=function-revisions Get the revision list of a workflow scoped knative service.","title":" Get Workflow Service Revision List (getWorkflowServiceRevisionList)"},{"location":"api/#parameters_45","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow svn query string string \u2713 target service name version query string string \u2713 target service version","title":"Parameters"},{"location":"api/#all-responses_49","text":"Code Status Description Has headers Schema 200 OK successfully got service revisions schema","title":"All responses"},{"location":"api/#responses_49","text":"","title":"Responses"},{"location":"api/#200-successfully-got-service-revisions","text":"Status: OK","title":" 200 - successfully got service revisions"},{"location":"api/#schema_60","text":"","title":" Schema"},{"location":"api/#get-a-workflow-variable-getworkflowvariable","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=var Get the value sorted in a workflow variable.","title":" Get a Workflow Variable (getWorkflowVariable)"},{"location":"api/#parameters_46","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable","title":"Parameters"},{"location":"api/#all-responses_50","text":"Code Status Description Has headers Schema 200 OK successfully got workflow variable schema","title":"All responses"},{"location":"api/#responses_50","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-variable","text":"Status: OK","title":" 200 - successfully got workflow variable"},{"location":"api/#schema_61","text":"","title":" Schema"},{"location":"api/#get-list-of-workflow-variables-getworkflowvariables","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=vars Gets a list of variables in a workflow.","title":" Get List of Workflow Variables (getWorkflowVariables)"},{"location":"api/#parameters_47","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_51","text":"Code Status Description Has headers Schema 200 OK successfully got workflow variables schema","title":"All responses"},{"location":"api/#responses_51","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-variables","text":"Status: OK","title":" 200 - successfully got workflow variables"},{"location":"api/#schema_62","text":"","title":" Schema"},{"location":"api/#gets-instance-logs-instancelogs","text":"GET /api/namespaces/{namespace}/instances/{instance}/logs Gets the logs of an executed instance.","title":" Gets Instance Logs (instanceLogs)"},{"location":"api/#parameters_48","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance id namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_52","text":"Code Status Description Has headers Schema 200 OK successfully got instance logs schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_52","text":"","title":"Responses"},{"location":"api/#200-successfully-got-instance-logs","text":"Status: OK","title":" 200 - successfully got instance logs"},{"location":"api/#schema_63","text":"OkBody","title":" Schema"},{"location":"api/#default-response_11","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_64","text":"ErrorResponse","title":" Schema"},{"location":"api/#jq-playground-api-to-test-jq-queries-jqplayground","text":"POST /api/jq JQ Playground is a sandbox where you can test jq queries with custom data.","title":" JQ Playground api to test jq queries (jqPlayground)"},{"location":"api/#parameters_49","text":"Name Source Type Go type Separator Required Default Description JQ payload body JqPlaygroundBody JqPlaygroundBody \u2713 Payload that contains both the JSON data to manipulate and jq query.","title":"Parameters"},{"location":"api/#all-responses_53","text":"Code Status Description Has headers Schema 200 OK jq query was successful schema 400 Bad Request the request was invalid schema 500 Internal Server Error an unexpected internal error occurred schema","title":"All responses"},{"location":"api/#responses_53","text":"","title":"Responses"},{"location":"api/#200-jq-query-was-successful","text":"Status: OK","title":" 200 - jq query was successful"},{"location":"api/#schema_65","text":"","title":" Schema"},{"location":"api/#400-the-request-was-invalid","text":"Status: Bad Request","title":" 400 - the request was invalid"},{"location":"api/#schema_66","text":"","title":" Schema"},{"location":"api/#500-an-unexpected-internal-error-occurred","text":"Status: Internal Server Error","title":" 500 - an unexpected internal error occurred"},{"location":"api/#schema_67","text":"","title":" Schema"},{"location":"api/#inlined-models_8","text":"JqPlaygroundBody Properties Name Type Go type Required Default Description Example data string string \u2713 JSON data encoded in base64 query string string \u2713 jq query to manipulate JSON data","title":"Inlined models"},{"location":"api/#get-global-service-revision-pods-list-listglobalservicerevisionpods","text":"GET /api/functions/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a global scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003' .","title":" Get Global Service Revision Pods List (listGlobalServiceRevisionPods)"},{"location":"api/#parameters_50","text":"Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_54","text":"Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema","title":"All responses"},{"location":"api/#responses_54","text":"","title":"Responses"},{"location":"api/#200-successfully-got-list-of-a-service-revision-pods","text":"Status: OK","title":" 200 - successfully got list of a service revision pods"},{"location":"api/#schema_68","text":"","title":" Schema"},{"location":"api/#get-namespace-service-revision-pods-list-listnamespaceservicerevisionpods","text":"GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration}/pods List a revisions pods of a namespace scoped knative service. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'.","title":" Get Namespace Service Revision Pods List (listNamespaceServiceRevisionPods)"},{"location":"api/#parameters_51","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_55","text":"Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema","title":"All responses"},{"location":"api/#responses_55","text":"","title":"Responses"},{"location":"api/#200-successfully-got-list-of-a-service-revision-pods_1","text":"Status: OK","title":" 200 - successfully got list of a service revision pods"},{"location":"api/#schema_69","text":"","title":" Schema"},{"location":"api/#get-workflow-service-revision-pods-list-listworkflowservicerevisionpods","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=pods List a revisions pods of a workflow scoped knative service. The target revision generation (rev query) is the number suffix on a revision. Example: A revision named 'workflow-10640097968065193909-get-00001' would have the revisionGeneration '00001'.","title":" Get Workflow Service Revision Pods List (listWorkflowServiceRevisionPods)"},{"location":"api/#parameters_52","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow rev query string string \u2713 target service revison svn query string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_56","text":"Code Status Description Has headers Schema 200 OK successfully got list of a service revision pods schema","title":"All responses"},{"location":"api/#responses_56","text":"","title":"Responses"},{"location":"api/#200-successfully-got-list-of-a-service-revision-pods_2","text":"Status: OK","title":" 200 - successfully got list of a service revision pods"},{"location":"api/#schema_70","text":"","title":" Schema"},{"location":"api/#get-workflow-services-list-listworkflowservices","text":"GET /api/functions/namespaces/{namespace}/tree/{workflow}?op=services Gets a list of workflow knative services.","title":" Get Workflow Services List (listWorkflowServices)"},{"location":"api/#parameters_53","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_57","text":"Code Status Description Has headers Schema 200 OK successfully got services list schema","title":"All responses"},{"location":"api/#responses_57","text":"","title":"Responses"},{"location":"api/#200-successfully-got-services-list_2","text":"Status: OK","title":" 200 - successfully got services list"},{"location":"api/#schema_71","text":"","title":" Schema"},{"location":"api/#gets-namespace-level-logs-namespacelogs","text":"GET /api/namespaces/{namespace}/logs Gets Namespace Level Logs.","title":" Gets Namespace Level Logs (namespaceLogs)"},{"location":"api/#parameters_54","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_58","text":"Code Status Description Has headers Schema 200 OK successfully got namespace logs schema","title":"All responses"},{"location":"api/#responses_58","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-logs","text":"Status: OK","title":" 200 - successfully got namespace logs"},{"location":"api/#schema_72","text":"","title":" Schema"},{"location":"api/#gets-namespace-failed-workflow-instances-metrics-namespacemetricsfailed","text":"GET /api/namespaces/{namespace}/metrics/failed Get metrics of failed workflows in the targeted namespace.","title":" Gets Namespace Failed Workflow Instances Metrics (namespaceMetricsFailed)"},{"location":"api/#parameters_55","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_59","text":"Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema","title":"All responses"},{"location":"api/#responses_59","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-metrics","text":"Status: OK","title":" 200 - successfully got namespace metrics"},{"location":"api/#schema_73","text":"","title":" Schema"},{"location":"api/#gets-namespace-invoked-workflow-metrics-namespacemetricsinvoked","text":"GET /api/namespaces/{namespace}/metrics/invoked Get metrics of invoked workflows in the targeted namespace.","title":" Gets Namespace Invoked Workflow Metrics (namespaceMetricsInvoked)"},{"location":"api/#parameters_56","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_60","text":"Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema","title":"All responses"},{"location":"api/#responses_60","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-metrics_1","text":"Status: OK","title":" 200 - successfully got namespace metrics"},{"location":"api/#schema_74","text":"","title":" Schema"},{"location":"api/#gets-namespace-workflow-timing-metrics-namespacemetricsmilliseconds","text":"GET /api/namespaces/{namespace}/metrics/milliseconds Get timing metrics of workflows in the targeted namespace.","title":" Gets Namespace Workflow Timing Metrics (namespaceMetricsMilliseconds)"},{"location":"api/#parameters_57","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_61","text":"Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema","title":"All responses"},{"location":"api/#responses_61","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-metrics_2","text":"Status: OK","title":" 200 - successfully got namespace metrics"},{"location":"api/#schema_75","text":"","title":" Schema"},{"location":"api/#gets-namespace-successful-workflow-instances-metrics-namespacemetricssuccessful","text":"GET /api/namespaces/{namespace}/metrics/successful Get metrics of successful workflows in the targeted namespace.","title":" Gets Namespace Successful Workflow Instances Metrics (namespaceMetricsSuccessful)"},{"location":"api/#parameters_58","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace","title":"Parameters"},{"location":"api/#all-responses_62","text":"Code Status Description Has headers Schema 200 OK successfully got namespace metrics schema","title":"All responses"},{"location":"api/#responses_62","text":"","title":"Responses"},{"location":"api/#200-successfully-got-namespace-metrics_3","text":"Status: OK","title":" 200 - successfully got namespace metrics"},{"location":"api/#schema_76","text":"","title":" Schema"},{"location":"api/#get-direktiv-server-logs-serverlogs","text":"GET /api/logs Gets Direktiv Server Logs.","title":" Get Direktiv Server Logs (serverLogs)"},{"location":"api/#all-responses_63","text":"Code Status Description Has headers Schema 200 OK successfully got server logs schema default an error has occurred schema","title":"All responses"},{"location":"api/#responses_63","text":"","title":"Responses"},{"location":"api/#200-successfully-got-server-logs","text":"Status: OK","title":" 200 - successfully got server logs"},{"location":"api/#schema_77","text":"OkBody","title":" Schema"},{"location":"api/#default-response_12","text":"an error has occurred","title":" Default Response"},{"location":"api/#schema_78","text":"ErrorResponse","title":" Schema"},{"location":"api/#set-a-instance-variable-setinstancevariable","text":"PUT /api/namespaces/{namespace}/instances/{instance}/vars/{variable} Set the value sorted in a instance variable. If the target variable does not exists, it will be created. Variable data can be anything.","title":" Set a Instance Variable (setInstanceVariable)"},{"location":"api/#consumes_3","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_59","text":"Name Source Type Go type Separator Required Default Description instance path string string \u2713 target instance namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data.","title":"Parameters"},{"location":"api/#all-responses_64","text":"Code Status Description Has headers Schema 200 OK successfully set instance variable schema","title":"All responses"},{"location":"api/#responses_64","text":"","title":"Responses"},{"location":"api/#200-successfully-set-instance-variable","text":"Status: OK","title":" 200 - successfully set instance variable"},{"location":"api/#schema_79","text":"","title":" Schema"},{"location":"api/#sets-a-namespace-config-setnamespaceconfig","text":"PATCH /api/namespaces/{namespace}/config Sets a namespace config.","title":" Sets a namespace config (setNamespaceConfig)"},{"location":"api/#parameters_60","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace to update Config Payload body SetNamespaceConfigBody SetNamespaceConfigBody Payload that contains the config information to set. Note: This payload only need to contain the properities you wish to set.","title":"Parameters"},{"location":"api/#all-responses_65","text":"Code Status Description Has headers Schema 200 OK namespace config has been successfully been updated schema","title":"All responses"},{"location":"api/#responses_65","text":"","title":"Responses"},{"location":"api/#200-namespace-config-has-been-successfully-been-updated","text":"Status: OK","title":" 200 - namespace config has been successfully been updated"},{"location":"api/#schema_80","text":"","title":" Schema"},{"location":"api/#inlined-models_9","text":"SetNamespaceConfigBody Properties Name Type Go type Required Default Description Example broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace","title":"Inlined models"},{"location":"api/#set-a-namespace-variable-setnamespacevariable","text":"PUT /api/namespaces/{namespace}/vars/{variable} Set the value sorted in a namespace variable. If the target variable does not exists, it will be created. Variable data can be anything.","title":" Set a Namespace Variable (setNamespaceVariable)"},{"location":"api/#consumes_4","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_61","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace variable path string string \u2713 target variable data body string string \u2713 Payload that contains variable data.","title":"Parameters"},{"location":"api/#all-responses_66","text":"Code Status Description Has headers Schema 200 OK successfully set namespace variable schema","title":"All responses"},{"location":"api/#responses_66","text":"","title":"Responses"},{"location":"api/#200-successfully-set-namespace-variable","text":"Status: OK","title":" 200 - successfully set namespace variable"},{"location":"api/#schema_81","text":"","title":" Schema"},{"location":"api/#set-cloud-event-for-workflow-to-log-to-setworkflowcloudeventlogs","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=set-workflow-event-logging Set Cloud Event for Workflow to Log to. When configured type direktiv.instanceLog cloud events will be generated with the logger parameter set to the configured value. Workflows can be configured to generate cloud events on their namespace anything the log parameter produces data. Please find more information on this topic here: https://docs.direktiv.io/docs/examples/logging.html","title":" Set Cloud Event for Workflow to Log to (setWorkflowCloudEventLogs)"},{"location":"api/#parameters_62","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Cloud Event Logger body SetWorkflowCloudEventLogsBody SetWorkflowCloudEventLogsBody \u2713 Cloud event logger to target","title":"Parameters"},{"location":"api/#all-responses_67","text":"Code Status Description Has headers Schema 200 OK successfully update workflow schema","title":"All responses"},{"location":"api/#responses_67","text":"","title":"Responses"},{"location":"api/#200-successfully-update-workflow","text":"Status: OK","title":" 200 - successfully update workflow"},{"location":"api/#schema_82","text":"","title":" Schema"},{"location":"api/#inlined-models_10","text":"SetWorkflowCloudEventLogsBody Properties Name Type Go type Required Default Description Example logger string string \u2713 Target Cloud Event","title":"Inlined models"},{"location":"api/#set-a-workflow-variable-setworkflowvariable","text":"PUT /api/namespaces/{namespace}/tree/{workflow}?op=set-var Set the value sorted in a workflow variable. If the target variable does not exists, it will be created. Variable data can be anything.","title":" Set a Workflow Variable (setWorkflowVariable)"},{"location":"api/#consumes_5","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_63","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow var query string string \u2713 target variable data body string string \u2713 Payload that contains variable data.","title":"Parameters"},{"location":"api/#all-responses_68","text":"Code Status Description Has headers Schema 200 OK successfully set workflow variable schema","title":"All responses"},{"location":"api/#responses_68","text":"","title":"Responses"},{"location":"api/#200-successfully-set-workflow-variable","text":"Status: OK","title":" 200 - successfully set workflow variable"},{"location":"api/#schema_83","text":"","title":" Schema"},{"location":"api/#set-cloud-event-for-workflow-to-log-to-toggleworkflow","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=toggle Toggle's whether or not a workflow is active. Disabled workflows cannot be invoked. This includes start event and scheduled workflows.","title":" Set Cloud Event for Workflow to Log to (toggleWorkflow)"},{"location":"api/#parameters_64","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow Workflow Live Status body ToggleWorkflowBody ToggleWorkflowBody \u2713 Whether or not the workflow is alive or disabled","title":"Parameters"},{"location":"api/#all-responses_69","text":"Code Status Description Has headers Schema 200 OK successfully updated workflow live status schema","title":"All responses"},{"location":"api/#responses_69","text":"","title":"Responses"},{"location":"api/#200-successfully-updated-workflow-live-status","text":"Status: OK","title":" 200 - successfully updated workflow live status"},{"location":"api/#schema_84","text":"","title":" Schema"},{"location":"api/#inlined-models_11","text":"ToggleWorkflowBody Properties Name Type Go type Required Default Description Example live boolean bool \u2713 Workflow live status","title":"Inlined models"},{"location":"api/#create-global-service-revision-updateglobalservice","text":"POST /api/functions/{serviceName} Creates a new global scoped knative service revision Revisions are created with a traffic percentage. This percentage controls how much traffic will be directed to this revision. Traffic can be set to 100 to direct all traffic.","title":" Create Global Service Revision (updateGlobalService)"},{"location":"api/#parameters_65","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name Service body UpdateGlobalServiceBody UpdateGlobalServiceBody \u2713 Payload that contains information on service revision","title":"Parameters"},{"location":"api/#all-responses_70","text":"Code Status Description Has headers Schema 200 OK successfully created service revision schema","title":"All responses"},{"location":"api/#responses_70","text":"","title":"Responses"},{"location":"api/#200-successfully-created-service-revision","text":"Status: OK","title":" 200 - successfully created service revision"},{"location":"api/#schema_85","text":"","title":" Schema"},{"location":"api/#inlined-models_12","text":"UpdateGlobalServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods trafficPercent integer int64 \u2713 Traffic percentage new revision will use","title":"Inlined models"},{"location":"api/#update-global-service-traffic-updateglobalservicetraffic","text":"PATCH /api/functions/{serviceName} Update Global Service traffic directed to each revision, traffic can only be configured between two revisions. All other revisions will bet set to 0 traffic.","title":" Update Global Service Traffic (updateGlobalServiceTraffic)"},{"location":"api/#parameters_66","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name Service Traffic body UpdateGlobalServiceTrafficBody UpdateGlobalServiceTrafficBody \u2713 Payload that contains information on service traffic","title":"Parameters"},{"location":"api/#all-responses_71","text":"Code Status Description Has headers Schema 200 OK successfully updated service traffic schema","title":"All responses"},{"location":"api/#responses_71","text":"","title":"Responses"},{"location":"api/#200-successfully-updated-service-traffic","text":"Status: OK","title":" 200 - successfully updated service traffic"},{"location":"api/#schema_86","text":"","title":" Schema"},{"location":"api/#inlined-models_13","text":"UpdateGlobalServiceTrafficBody Properties Name Type Go type Required Default Description Example values [] UpdateGlobalServiceTrafficParamsBodyValuesItems0 []*models.UpdateGlobalServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateGlobalServiceTrafficParamsBodyValuesItems0 Properties Name Type Go type Required Default Description Example percent integer int64 Target traffice percentage revision string string Target service revision","title":"Inlined models"},{"location":"api/#create-namespace-service-revision-updatenamespaceservice","text":"POST /api/functions/namespaces/{namespace}/function/{serviceName} Creates a new namespace scoped knative service revision. Revisions are created with a traffic percentage. This percentage controls how much traffic will be directed to this revision. Traffic can be set to 100 to direct all traffic.","title":" Create Namespace Service Revision (updateNamespaceService)"},{"location":"api/#parameters_67","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service body UpdateNamespaceServiceBody UpdateNamespaceServiceBody \u2713 Payload that contains information on service revision","title":"Parameters"},{"location":"api/#all-responses_72","text":"Code Status Description Has headers Schema 200 OK successfully created service revision schema","title":"All responses"},{"location":"api/#responses_72","text":"","title":"Responses"},{"location":"api/#200-successfully-created-service-revision_1","text":"Status: OK","title":" 200 - successfully created service revision"},{"location":"api/#schema_87","text":"","title":" Schema"},{"location":"api/#inlined-models_14","text":"UpdateNamespaceServiceBody Properties Name Type Go type Required Default Description Example cmd string string \u2713 image string string \u2713 Target image a service will use minScale integer int64 \u2713 Minimum amount of service pods to be live size string string \u2713 Size of created service pods trafficPercent integer int64 \u2713 Traffic percentage new revision will use","title":"Inlined models"},{"location":"api/#update-namespace-service-traffic-updatenamespaceservicetraffic","text":"PATCH /api/functions/namespaces/{namespace}/function/{serviceName} Update Namespace Service traffic directed to each revision, traffic can only be configured between two revisions. All other revisions will bet set to 0 traffic.","title":" Update Namespace Service Traffic (updateNamespaceServiceTraffic)"},{"location":"api/#parameters_68","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name Service Traffic body UpdateNamespaceServiceTrafficBody UpdateNamespaceServiceTrafficBody \u2713 Payload that contains information on service traffic","title":"Parameters"},{"location":"api/#all-responses_73","text":"Code Status Description Has headers Schema 200 OK successfully updated service traffic schema","title":"All responses"},{"location":"api/#responses_73","text":"","title":"Responses"},{"location":"api/#200-successfully-updated-service-traffic_1","text":"Status: OK","title":" 200 - successfully updated service traffic"},{"location":"api/#schema_88","text":"","title":" Schema"},{"location":"api/#inlined-models_15","text":"UpdateNamespaceServiceTrafficBody Properties Name Type Go type Required Default Description Example values [] UpdateNamespaceServiceTrafficParamsBodyValuesItems0 []*models.UpdateNamespaceServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets UpdateNamespaceServiceTrafficParamsBodyValuesItems0 Properties Name Type Go type Required Default Description Example percent integer int64 Target traffice percentage revision string string Target service revision","title":"Inlined models"},{"location":"api/#update-a-workflow-updateworkflow","text":"POST /api/namespaces/{namespace}/tree/{workflow}?op=update-workflow Updates a workflow at the target path. The body of this request should contain the workflow yaml you want to update to.","title":" Update a Workflow (updateWorkflow)"},{"location":"api/#consumes_6","text":"text/plain","title":"Consumes"},{"location":"api/#parameters_69","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow workflow data body string string Payload that contains the updated direktiv workflow yaml.","title":"Parameters"},{"location":"api/#all-responses_74","text":"Code Status Description Has headers Schema 200 OK successfully updated workflow schema","title":"All responses"},{"location":"api/#responses_74","text":"","title":"Responses"},{"location":"api/#200-successfully-updated-workflow","text":"Status: OK","title":" 200 - successfully updated workflow"},{"location":"api/#schema_89","text":"","title":" Schema"},{"location":"api/#returns-version-information-for-servers-in-the-cluster-version","text":"GET /api/version Returns version information for servers in the cluster.","title":" Returns version information for servers in the cluster. (version)"},{"location":"api/#all-responses_75","text":"Code Status Description Has headers Schema 200 OK version query was successful schema","title":"All responses"},{"location":"api/#responses_75","text":"","title":"Responses"},{"location":"api/#200-version-query-was-successful","text":"Status: OK","title":" 200 - version query was successful"},{"location":"api/#schema_90","text":"","title":" Schema"},{"location":"api/#watch-global-service-revision-watchglobalservicerevision","text":"GET /api/functions/{serviceName}/revisions/{revisionGeneration} Watch a global scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'global-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Watch Global Service Revision (watchGlobalServiceRevision)"},{"location":"api/#produces_1","text":"text/event-stream","title":"Produces"},{"location":"api/#parameters_70","text":"Name Source Type Go type Separator Required Default Description revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_76","text":"Code Status Description Has headers Schema 200 OK successfully watching service revision schema","title":"All responses"},{"location":"api/#responses_76","text":"","title":"Responses"},{"location":"api/#200-successfully-watching-service-revision","text":"Status: OK","title":" 200 - successfully watching service revision"},{"location":"api/#schema_91","text":"","title":" Schema"},{"location":"api/#watch-global-service-revision-list-watchglobalservicerevisionlist","text":"GET /api/functions/{serviceName}/revisions Watch the revision list of a global scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Watch Global Service Revision List (watchGlobalServiceRevisionList)"},{"location":"api/#produces_2","text":"text/event-stream","title":"Produces"},{"location":"api/#parameters_71","text":"Name Source Type Go type Separator Required Default Description serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_77","text":"Code Status Description Has headers Schema 200 OK successfully watching service revisions schema","title":"All responses"},{"location":"api/#responses_77","text":"","title":"Responses"},{"location":"api/#200-successfully-watching-service-revisions","text":"Status: OK","title":" 200 - successfully watching service revisions"},{"location":"api/#schema_92","text":"","title":" Schema"},{"location":"api/#watch-namespace-service-revision-watchnamespaceservicerevision","text":"GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions/{revisionGeneration} Watch a namespace scoped knative service revision. The target revision generation is the number suffix on a revision. Example: A revision named 'namespace-direktiv-fast-request-00003' would have the revisionGeneration '00003'. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Watch Namespace Service Revision (watchNamespaceServiceRevision)"},{"location":"api/#produces_3","text":"text/event-stream","title":"Produces"},{"location":"api/#parameters_72","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace revisionGeneration path string string \u2713 target revision generation serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_78","text":"Code Status Description Has headers Schema 200 OK successfully watching service revision schema","title":"All responses"},{"location":"api/#responses_78","text":"","title":"Responses"},{"location":"api/#200-successfully-watching-service-revision_1","text":"Status: OK","title":" 200 - successfully watching service revision"},{"location":"api/#schema_93","text":"","title":" Schema"},{"location":"api/#watch-namespace-service-revision-list-watchnamespaceservicerevisionlist","text":"GET /api/functions/namespaces/{namespace}/function/{serviceName}/revisions Watch the revision list of a namespace scoped knative service. Note: This is a Server-Sent-Event endpoint, and will not work with the default swagger client.","title":" Watch Namespace Service Revision List (watchNamespaceServiceRevisionList)"},{"location":"api/#produces_4","text":"text/event-stream","title":"Produces"},{"location":"api/#parameters_73","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace serviceName path string string \u2713 target service name","title":"Parameters"},{"location":"api/#all-responses_79","text":"Code Status Description Has headers Schema 200 OK successfully watching service revisions schema","title":"All responses"},{"location":"api/#responses_79","text":"","title":"Responses"},{"location":"api/#200-successfully-watching-service-revisions_1","text":"Status: OK","title":" 200 - successfully watching service revisions"},{"location":"api/#schema_94","text":"","title":" Schema"},{"location":"api/#gets-invoked-workflow-metrics-workflowmetricsinvoked","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-invoked Get metrics of invoked workflow instances.","title":" Gets Invoked Workflow Metrics (workflowMetricsInvoked)"},{"location":"api/#parameters_74","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_80","text":"Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema","title":"All responses"},{"location":"api/#responses_80","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-metrics","text":"Status: OK","title":" 200 - successfully got workflow metrics"},{"location":"api/#schema_95","text":"","title":" Schema"},{"location":"api/#gets-workflow-time-metrics-workflowmetricsmilliseconds","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-failed Get the timing metrics of a workflow's instance. This returns a total sum of the milliseconds a workflow has been executed for.","title":" Gets Workflow Time Metrics (workflowMetricsMilliseconds)"},{"location":"api/#parameters_75","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_81","text":"Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema","title":"All responses"},{"location":"api/#responses_81","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-metrics_1","text":"Status: OK","title":" 200 - successfully got workflow metrics"},{"location":"api/#schema_96","text":"","title":" Schema"},{"location":"api/#gets-a-workflow-state-time-metrics-workflowmetricsstatemilliseconds","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-state-milliseconds Get the state timing metrics of a workflow's instance. This returns the timing of individual states in a workflow.","title":" Gets a Workflow State Time Metrics (workflowMetricsStateMilliseconds)"},{"location":"api/#parameters_76","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_82","text":"Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema","title":"All responses"},{"location":"api/#responses_82","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-metrics_2","text":"Status: OK","title":" 200 - successfully got workflow metrics"},{"location":"api/#schema_97","text":"","title":" Schema"},{"location":"api/#gets-successful-workflow-metrics-workflowmetricssuccessful","text":"GET /api/namespaces/{namespace}/tree/{workflow}?op=metrics-successful Get metrics of a workflow, where the instance was successful.","title":" Gets Successful Workflow Metrics (workflowMetricsSuccessful)"},{"location":"api/#parameters_77","text":"Name Source Type Go type Separator Required Default Description namespace path string string \u2713 target namespace workflow path string string \u2713 path to target workflow","title":"Parameters"},{"location":"api/#all-responses_83","text":"Code Status Description Has headers Schema 200 OK successfully got workflow metrics schema","title":"All responses"},{"location":"api/#responses_83","text":"","title":"Responses"},{"location":"api/#200-successfully-got-workflow-metrics_3","text":"Status: OK","title":" 200 - successfully got workflow metrics"},{"location":"api/#schema_98","text":"","title":" Schema"},{"location":"api/#models","text":"","title":"Models"},{"location":"api/#createglobalprivateregistrybody","text":"CreateGlobalPrivateRegistryBody create global private registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL","title":" CreateGlobalPrivateRegistryBody"},{"location":"api/#createglobalregistrybody","text":"CreateGlobalRegistryBody create global registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL","title":" CreateGlobalRegistryBody"},{"location":"api/#createglobalservicebody","text":"CreateGlobalServiceBody create global service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v12\",\"minScale\":\"1\",\"name\":\"fast-request\",\"size\":\"small\"} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Name string string \u2713 Name of new service Size string string \u2713 Size of created service pods","title":" CreateGlobalServiceBody"},{"location":"api/#createnamespaceservicebody","text":"CreateNamespaceServiceBody create namespace service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v12\",\"minScale\":\"1\",\"name\":\"fast-request\",\"size\":\"small\"} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Name string string \u2713 Name of new service Size string string \u2713 Size of created service pods","title":" CreateNamespaceServiceBody"},{"location":"api/#createregistrybody","text":"CreateRegistryBody create registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 Target registry connection data containing the user and token. Reg string string \u2713 Target registry URL","title":" CreateRegistryBody"},{"location":"api/#deleteglobalprivateregistrybody","text":"DeleteGlobalPrivateRegistryBody delete global private registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL","title":" DeleteGlobalPrivateRegistryBody"},{"location":"api/#deleteglobalregistrybody","text":"DeleteGlobalRegistryBody delete global registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL","title":" DeleteGlobalRegistryBody"},{"location":"api/#deleteregistrybody","text":"DeleteRegistryBody delete registry body Example {\"data\":\"admin:8QwFLg%D$qg*\",\"reg\":\"https://prod.customreg.io\"} Properties Name Type Go type Required Default Description Example Reg string string \u2713 Target registry URL","title":" DeleteRegistryBody"},{"location":"api/#errorresponse","text":"Properties Name Type Go type Required Default Description Example Error string string StatusCode int64 (formatted integer) int64","title":" ErrorResponse"},{"location":"api/#jqplaygroundbody","text":"JqPlaygroundBody jq playground body Example {\"data\":\"eyJhIjogMSwgImIiOiAyLCAiYyI6IDQsICJkIjogN30=\",\"query\":\"map(select(. \\u003e= 2))\"} Properties Name Type Go type Required Default Description Example Data string string \u2713 JSON data encoded in base64 Query string string \u2713 jq query to manipulate JSON data","title":" JqPlaygroundBody"},{"location":"api/#okbody","text":"OkBody OkBody is an arbitrary placeholder response that represents an ok response body OkBody","title":" OkBody"},{"location":"api/#setnamespaceconfigbody","text":"SetNamespaceConfigBody set namespace config body Example {\"broadcast\":{\"directory.create\":false,\"directory.delete\":false,\"instance.failed\":false,\"instance.started\":false,\"instance.success\":false,\"instance.variable.create\":false,\"instance.variable.delete\":false,\"instance.variable.update\":false,\"namespace.variable.create\":false,\"namespace.variable.delete\":false,\"namespace.variable.update\":false,\"workflow.create\":false,\"workflow.delete\":false,\"workflow.update\":false,\"workflow.variable.create\":false,\"workflow.variable.delete\":false,\"workflow.variable.update\":false}} Properties Name Type Go type Required Default Description Example Broadcast interface{} interface{} Configuration on which direktiv operations will trigger coud events on the namespace","title":" SetNamespaceConfigBody"},{"location":"api/#setworkflowcloudeventlogsbody","text":"SetWorkflowCloudEventLogsBody set workflow cloud event logs body Example {\"logger\":\"mylog\"} Properties Name Type Go type Required Default Description Example Logger string string \u2713 Target Cloud Event","title":" SetWorkflowCloudEventLogsBody"},{"location":"api/#toggleworkflowbody","text":"ToggleWorkflowBody toggle workflow body Example {\"live\":false} Properties Name Type Go type Required Default Description Example Live boolean bool \u2713 Workflow live status","title":" ToggleWorkflowBody"},{"location":"api/#updateglobalservicebody","text":"UpdateGlobalServiceBody update global service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v10\",\"minScale\":\"1\",\"size\":\"small\",\"trafficPercent\":50} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Size string string \u2713 Size of created service pods TrafficPercent int64 (formatted integer) int64 \u2713 Traffic percentage new revision will use","title":" UpdateGlobalServiceBody"},{"location":"api/#updateglobalservicetrafficbody","text":"UpdateGlobalServiceTrafficBody update global service traffic body Example {\"values\":[{\"percent\":60,\"revision\":\"global-fast-request-00002\"},{\"percent\":40,\"revision\":\"global-fast-request-00001\"}]} Properties Name Type Go type Required Default Description Example Values [] UpdateGlobalServiceTrafficParamsBodyValuesItems0 []*UpdateGlobalServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets","title":" UpdateGlobalServiceTrafficBody"},{"location":"api/#updateglobalservicetrafficparamsbodyvaluesitems0","text":"UpdateGlobalServiceTrafficParamsBodyValuesItems0 update global service traffic params body values items0 Properties Name Type Go type Required Default Description Example Percent int64 (formatted integer) int64 Target traffice percentage Revision string string Target service revision","title":" UpdateGlobalServiceTrafficParamsBodyValuesItems0"},{"location":"api/#updatenamespaceservicebody","text":"UpdateNamespaceServiceBody update namespace service body Example {\"cmd\":\"\",\"image\":\"direktiv/request:v10\",\"minScale\":\"1\",\"size\":\"small\",\"trafficPercent\":50} Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 Target image a service will use MinScale int64 (formatted integer) int64 \u2713 Minimum amount of service pods to be live Size string string \u2713 Size of created service pods TrafficPercent int64 (formatted integer) int64 \u2713 Traffic percentage new revision will use","title":" UpdateNamespaceServiceBody"},{"location":"api/#updatenamespaceservicetrafficbody","text":"UpdateNamespaceServiceTrafficBody update namespace service traffic body Example {\"values\":[{\"percent\":60,\"revision\":\"namespace-direktiv-fast-request-00002\"},{\"percent\":40,\"revision\":\"namespace-direktiv-fast-request-00001\"}]} Properties Name Type Go type Required Default Description Example Values [] UpdateNamespaceServiceTrafficParamsBodyValuesItems0 []*UpdateNamespaceServiceTrafficParamsBodyValuesItems0 \u2713 List of revision traffic targets","title":" UpdateNamespaceServiceTrafficBody"},{"location":"api/#updatenamespaceservicetrafficparamsbodyvaluesitems0","text":"UpdateNamespaceServiceTrafficParamsBodyValuesItems0 update namespace service traffic params body values items0 Properties Name Type Go type Required Default Description Example Percent int64 (formatted integer) int64 Target traffice percentage Revision string string Target service revision","title":" UpdateNamespaceServiceTrafficParamsBodyValuesItems0"},{"location":"api/#updateservicerequest","text":"UpdateServiceRequest UpdateServiceRequest UpdateServiceRequest update service request Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 image MinScale int32 (formatted integer) int32 \u2713 minScale Size int32 (formatted integer) int32 \u2713 size TrafficPercent int64 (formatted integer) int64 \u2713 trafficPercent","title":" UpdateServiceRequest"},{"location":"api/#updateservicerequest_1","text":"UpdateServiceRequest UpdateServiceRequest update service request Properties Name Type Go type Required Default Description Example Cmd string string \u2713 cmd Image string string \u2713 image MinScale int32 (formatted integer) int32 \u2713 minScale Size int32 (formatted integer) int32 \u2713 size TrafficPercent int64 (formatted integer) int64 \u2713 trafficPercent","title":" updateServiceRequest"},{"location":"quickstart/","text":"Quickstart Starting the Server Getting a local playground environment can be easily done with Docker. The following command starts a docker container with kubernetes. On startup it can take a few minutes to download all images. When the installation is done all pods should show \"Running\" or \"Completed\". docker run --privileged -p 8080:80 -ti direktiv/direktiv-kube For proxy usage: docker run --privileged -p 8080 :80 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \".default,10.0.0.0/8,172.0.0.0/8,localhost\" direktiv/direktiv-kube Testing Direktiv : Download the direkcli command-line tool from the releases page (contained in the ZIP file) and create your first namespace by running: direkcli namespaces create demo $ direkcli namespaces create demo Created namespace: demo $ direkcli namespaces list +------+ | NAME | +------+ | demo | +------+ Kubernetes installation For instructions on how to install in a pre-existing Kubernetes environment, following the installation instructions . Workflow specification The below example is the minimal configuration needed for a workflow, following the workflow language specification : id : helloworld states : - id : hello type : noop transform : msg : \"Hello jq(.name)!\" Creating and Running a Workflow The following script does everything required to run the first workflow. This includes creating a namespace & workflow and running the workflow the first time. $ curl -X PUT \"http://localhost:8080/api/namespaces/demo\" { \"namespace\" : { \"createdAt\" : \"2021-10-06T00:03:22.444884147Z\" , \"updatedAt\" : \"2021-10-06T00:03:22.444884447Z\" , \"name\" : \"demo\" , \"oid\" : \"\" } } $ cat > helloworld.yml <<- EOF states: - id: hello type: noop transform: msg: \"Hello, jq(.name)!\" EOF $ curl -vv -X PUT --data-binary \"@helloworld.yml\" \"http://localhost:8080/api/namespaces/demo/tree/helloworld?op=create-workflow\" { \"namespace\" : \"demo\" , \"node\" : { ... } , \"revision\" : { ... } } $ cat > input.json <<- EOF { \"name\": \"Alan\" } EOF $ curl -vv -X POST --data-binary \"@input.json\" \"http://localhost:8080/api/namespaces/demo/tree/helloworld?op=wait\" { \"msg\" : \"Hello, Alan!\" } Next steps For more complex examples review the Getting Started section of the documentation. See Also The direktiv.io website. The direktiv.io repository. The Godoc library documentation.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#starting-the-server","text":"Getting a local playground environment can be easily done with Docker. The following command starts a docker container with kubernetes. On startup it can take a few minutes to download all images. When the installation is done all pods should show \"Running\" or \"Completed\". docker run --privileged -p 8080:80 -ti direktiv/direktiv-kube For proxy usage: docker run --privileged -p 8080 :80 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \".default,10.0.0.0/8,172.0.0.0/8,localhost\" direktiv/direktiv-kube Testing Direktiv : Download the direkcli command-line tool from the releases page (contained in the ZIP file) and create your first namespace by running: direkcli namespaces create demo $ direkcli namespaces create demo Created namespace: demo $ direkcli namespaces list +------+ | NAME | +------+ | demo | +------+","title":"Starting the Server"},{"location":"quickstart/#kubernetes-installation","text":"For instructions on how to install in a pre-existing Kubernetes environment, following the installation instructions .","title":"Kubernetes installation"},{"location":"quickstart/#workflow-specification","text":"The below example is the minimal configuration needed for a workflow, following the workflow language specification : id : helloworld states : - id : hello type : noop transform : msg : \"Hello jq(.name)!\"","title":"Workflow specification"},{"location":"quickstart/#creating-and-running-a-workflow","text":"The following script does everything required to run the first workflow. This includes creating a namespace & workflow and running the workflow the first time. $ curl -X PUT \"http://localhost:8080/api/namespaces/demo\" { \"namespace\" : { \"createdAt\" : \"2021-10-06T00:03:22.444884147Z\" , \"updatedAt\" : \"2021-10-06T00:03:22.444884447Z\" , \"name\" : \"demo\" , \"oid\" : \"\" } } $ cat > helloworld.yml <<- EOF states: - id: hello type: noop transform: msg: \"Hello, jq(.name)!\" EOF $ curl -vv -X PUT --data-binary \"@helloworld.yml\" \"http://localhost:8080/api/namespaces/demo/tree/helloworld?op=create-workflow\" { \"namespace\" : \"demo\" , \"node\" : { ... } , \"revision\" : { ... } } $ cat > input.json <<- EOF { \"name\": \"Alan\" } EOF $ curl -vv -X POST --data-binary \"@input.json\" \"http://localhost:8080/api/namespaces/demo/tree/helloworld?op=wait\" { \"msg\" : \"Hello, Alan!\" }","title":"Creating and Running a Workflow"},{"location":"quickstart/#next-steps","text":"For more complex examples review the Getting Started section of the documentation.","title":"Next steps"},{"location":"quickstart/#see-also","text":"The direktiv.io website. The direktiv.io repository. The Godoc library documentation.","title":"See Also"},{"location":"specification/","text":"Specification Workflow Overview Diagram The diagram below captures the workflow definition specification. This is to be used as a reference only, the full specification is described in detail in the sections below. Workflow Definition Parameter Description Type Required id Workflow unique identifier. string yes name Workflow name (metadata). string no description Workflow description (metadata). string no version Version information. string no singular Attempts to invoke this workflow will fail when an instance is running. no bool functions Workflow function definitions. []FunctionDefinition no schemas Workflow schema definitions. []SchemaDefinition no states Workflow states. []StateDefinition no timeouts Workflow global timeouts. TimeoutDefinition no start Workflow start configuration. Start no FunctionDefinition GlobalFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"knative-global\"). string yes service The service being referenced. string yes files Workflow file definition. []FunctionFileDefinition no NamespacedFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"knative-namespace\"). string yes service The service being referenced. string yes files Workflow file definition. []FunctionFileDefinition no ReusableFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"reusable\"). string yes image Image URI. string yes files Workflow file definition. []FunctionFileDefinition no cmd Command to run in container string no size Size of virtual machine enum no scale Minimum number of instances int no A reusable function can be defined in three different sizes: \" small \"(default), \" medium \", and \" large \". These sizes control how much cpu, memory and storage a virtual machine is given for a function when their virtual machine is created. The default value for \" scale \" is 0 which means the service will be removed after a ceratin amount of time. It defines the minimum number of containers to run for this services if it is greater than 0. Size CPU Memory Storage small 1 256 MB +64 MB medium 1 512 MB +64 MB large 2 1024 MB +64 MB SubflowFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"subflow\"). enum yes workflow ID of workflow within the same namespace. string yes IsolatedFunctionDefinition Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"isolated\"). string yes image Image URI. string yes files Workflow file definition. []FunctionFileDefinition no cmd Command to run in container string no size Size of virtual machine enum no scale Minimum number of instances int no An isolated function can be defined in three different sizes: \" small \"(default), \" medium \", and \" large \". These sizes control how much cpu, memory and storage a virtual machine is given for a function when their virtual machine is created. The default value for \" scale \" is 0 which means the service will be removed after a ceratin amount of time. It defines the minimum number of containers to run for this services if it is greater than 0. Size CPU Memory Storage small 1 256 MB +64 MB medium 1 512 MB +64 MB large 2 1024 MB +64 MB FunctionFileDefinition Parameter Description Type Required key Key used to select variable. string yes scope Scope used to select variable. Defaults to 'instance', but can be 'workflow' or 'namespace'. string no as Set the filename of the file. The default is the same as the key. string no type How to treat the file. Options include 'plain', 'base64', 'tar', 'tar.gz'. string no SchemaDefinition Parameter Description Type Required id Schema definition unique identifier. string yes schema Schema (based on JSON Schema). object yes States Common Fields Parameter Description Type Required id State unique identifier. string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no log jq command to generate data for instance-logging. string no catch Error handling. []ErrorDefinition no The id field must be unique amongst all states in the workflow, and may consist of only alphanumeric characters as well as periods, dashes, and underscores. The transform field can be a jq command string applied to the state information in order to enrich, filter, or change it. Whatever the command resolves to will completely replace the state's information. The transform will be applied immediately before the transition , so it won't change the state information before the main function of the state is performed. The transition , if provided, must be set to the id of a state within the workflow. If left unspecified, reaching this transition will end the workflow without raising an error. ErrorDefinition Parameter Description Type Required error A glob pattern to test error codes for a match. string yes transition State to transition to next. string no The error parameter can be a glob pattern to match multiple types of errors. When an error is thrown it will be compared against each ErrorDefinition in order until it finds a match. If no matches are found the workflow will immediately abort and escalate the error to any caller, unless the retry policy is ready to take over. ActionState Parameter Description Type Required id State unique identifier. string yes type State type (\"action\"). string yes action Action to perform. ActionDefinition yes async If workflow execution can continue without waiting for the action to return. boolean no timeout Duration to wait for action to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no ActionDefinition Parameter Description Type Required function Name of the referenced function. string yes input jq command to generate the input for the action. string no secrets List of secrets to temporarily add to the state data under .secrets before running the input jq command. []string no retries Retry policy. RetryDefinition no RetryDefinition Parameter Description Type Required max_attempts Maximum number of retry attempts. int yes delay Time delay between retry attempts (ISO8601). string no multiplier Value by which the delay is multiplied after each attempt. float no codes Regex patterns to specify which error codes to catch. []string yes If a retry strategy is defined, the action will be retried on any failures that statify any of the regex codes . If the retry fails max_attempts times a direktiv.retries.exceeded error will be thrown. An example definition - id : insert-into-database type : action action : function : insert-into-database-function input : customer : jq(.customer) The Action State runs another workflow as a subflow, or a function as defined in the functions section of the workflow definition. Functions may include things such as containers or direktiv virtual-machines. The input for the action is determined by an optional jq command in the input field. If unspecified, the default command is \".\" , which duplicates the entire state data. After the action has returned, whatever the results were will be stored in the state information under return . If an error occurred, it will be automatically raised, and can be handled using catch , or ignored if the desired behaviour is to abort the workflow. If async is true , the workflow will not wait for it to return before transitioning to the next state. The action will be fire-and-forget, and considered completely detached from the calling workflow. In this case, the Action State will not set the return value. ConsumeEventState Parameter Description Type Required id State unique identifier. string yes type State type (\"consumeEvent\"). string yes event Event to consume. ConsumeEventDefinition yes timeout Duration to wait to receive event (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no ConsumeEventDefinition Parameter Description Type Required type CloudEvent type. string yes context Key-value pairs for CloudEvent context values that must match. object no An example definition - id : wait-for-booking type : consumeEvent event : type : guestbooking context : source : 'bookings.*' customerId : jq(.customerId) venue : Sydney timeout : PT1H transform : jq(.customer) transition : add-booking-to-database The ConsumeEvent State is the simplest state you can use to listen for CloudEvents in the middle of a workflow (for triggering a workflow when receiving an event, see Start ). More complex event consumers include EventXor State and the EventAnd State . When a workflow reaches a ConsumeEvent State it will halt its execution until it receives a matching event, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving a matching event a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The event payload will stored at a variable with the same name as the event's type . If the payload is not valid JSON it will be base64 encoded as a string first. DelayState Parameter Description Type Required id State unique identifier. string yes type State type (\"delay\"). string yes duration Duration to delay (ISO8601). string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : sleep type : delay duration : PT1H transition : fetch-data The Delay State pauses execution of the workflow for a predefined length of time. ErrorState Parameter Description Type Required id State unique identifier. string yes type State type (\"error\"). string yes error Error code, catchable on a calling workflow. string yes message Format string to provide more context to the error. string yes args A list of jq commands to generate arguments for substitution in the message format string. []string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : error-out-of-date type : error error : validation.outOfDate message : \"food item %s is out of date\" args : - jq(.item.name) The Error State allows a subflow to throw an error, catchable by the calling workflow. The first transition to an Error State anywhere within the workflow means that a waiting caller -- if one exists -- will receive that error after this subflow returns. This doesn't prevent the Error State from transitioning to other states, which might be necessary to clean up or undo actions performed by the workflow. Subsequent transitions into Error States after the first have no effect. An error consists of two parts: an error code, and an error message. The code should be a short string can can contain alphanumeric characters, periods, dashes, and underscores. It is good practice to structure error codes similar to domain names, to make them easier to handle. The message allows you to provide extra context, and can be formatted like a printf string where each entry in args will be substituted. The args must be jq commands, allowing the state to insert state information into the error message. EventAndState Parameter Description Type Required id State unique identifier. string yes type State type (\"eventAnd\"). string yes events Events to consume. []ConsumeEventDefinition yes timeout Duration to wait to receive all events (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : event-and type : eventAnd timeout : PT1H transition : done events : - event : type : purchasePaid context : source : 'purchase.*' customerId : jq(.customerId) country : Australia - event : type : purchaseSent context : source : 'purchase.*' customerId : jq(.customerId) country : Australia When a workflow reaches an EventAnd State it will halt its execution until it receives a matching event for every event in its events list, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving matches for all required events a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The event payloads will stored in variables with the same names as each event's type . If a payload is not valid JSON it will be base64 encoded as a string first. EventXorState Parameter Description Type Required id State unique identifier. string yes type State type (\"eventXor\"). string yes events Events to consume, and what to do based on which event was received. []EventConditionDefinition yes timeout Duration to wait to receive event (ISO8601). string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no EventConditionDefinition Parameter Description Type Required event Event to consume. ConsumeEventDefinition yes transition State to transition to if this branch is selected. string no transform jq command to transform the state's data output. string no An example definition - id : event-xor type : eventXor timeout : PT1H events : - transition : \"reservation-accept\" event : type : reservationAccept context : source : \"reservation.*\" guestName : jq(.guestName) venue : \"Compu Global HMN\" - transition : \"reservation-decline\" event : type : reservationDecline context : source : \"reservation.*\" guestName : jq(.guestName) venue : \"Compu Global HMN\" When a workflow reaches an EventXor State it will halt its execution until it receives any matching event in its events list, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving matches for any required event a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The received event payload will stored in a variable with the same name as its event type . If a payload is not valid JSON it will be base64 encoded as a string first. ForeachState Parameter Description Type Required id State unique identifier. string yes type State type (\"foreach\"). string yes array jq command to produce an array of objects to loop through. string yes action Action to perform. ActionDefinition yes timeout Duration to wait for all actions to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no The ForeachState can be used to split up state data into an array and then perform an action on each element in parallel. The jq command provided in the array must produce an array or a direktiv.foreachInput error will be thrown. The jq command used to generate the input for the action will be applied to a single element from that array. The return values of each action will be included in an array stored at .return at the same index from which its input was generated. GenerateEventState Parameter Description Type Required id State unique identifier. string yes type State type (\"generateEvent\"). string yes event Event to generate. GenerateEventDefinition yes transform jq command to transform the state's data output. string no delay Duration to wait before publishing event (ISO8601). string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no GenerateEventDefinition Parameter Description Type Required type CloudEvent type. string yes source CloudEvent source. string yes data A jq command to generate the data (payload) for the produced event. string no datacontenttype An RFC 2046 string specifying the payload content type. string no context Add additional event extension context attributes (key-value). object no The GenerateEvent State will produce an event that other workflows could listen for. If the optional datacontenttype is defined and set to something other than application/json , and the jq command defined in data produces a base64 encoded string, it will be decoded before being used as the event payload. GetterState Parameter Description Type Required id State unique identifier. string yes type State type (\"getter\"). string yes variables Variables to fetch. []VariableGetterDefinition yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no VariableGetterDefinition Parameter Description Type Required key Variable name. string yes scope Variable scope (\"instance\", \"workflow\", or \"namespace\"). string yes The getter state is used to retrieve persistent data. NoopState Parameter Description Type Required id State unique identifier. string yes type State type (\"noop\"). string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : hello type : noop transform : message : \"Hello\" transition : world The No-op State exists for when nothing more than generic state functionality is required. A common use-case would be to perform a jq operation on the state data without performing another operation. ParallelState Parameter Description Type Required id State unique identifier. string yes type State type (\"parallel\"). string yes actions Actions to perform. []ActionDefinition yes mode Option types on how to complete branch execution: \"and\" (default), or \"or\". enum no timeout Duration to wait for all actions to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no catch Error handling. []ErrorDefinition no The Parallel State is an expansion on the Action State , used for running multiple actions in parallel. The state can operate in two different modes: and and or . In and mode all actions must return successfully before completing. In or mode the state can complete as soon as any one action returns without error. Return values from each of the actions will be stored in an array at .return in the order that each action is defined. If an action doesn't return but the state can still complete without errors any missing return values will be null in the array. If the timeout is reached before the state can transition a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . Any actions still running when the state transitions will be cancelled with \"best effort\" attempts. SetterState Parameter Description Type Required id State unique identifier. string yes type State type (\"setter\"). string yes variables Variables to push. []VariableSetterDefinition yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no VariableSetterDefinition Parameter Description Type Required key Variable name. string yes scope Variable scope (\"instance\", \"workflow\", or \"namespace\"). string yes value jq command to generate variable value. string yes mimeType MimeType to store variable value as. string no The setter state is used to store persistent data. A mimeType type can be provided to specify the type of content a variable is. If mimeType is not provided it will default to application/json . There are three mimeType's that are specifcically handled: * application/json - Default behaviour, value is treated as a json object. * text/plain - Value is treated as a plaintext string, no json marshalling is done. * application/octet-stream - Value is expected to be a base64 string and is stored as its decoded binary value. Read more about mimeTyps in the Examples . SwitchState Parameter Description Type Required id State unique identifier. string yes type State type (\"switch\"). string yes conditions Conditions to evaluate and determine which state to transition to next. []SwitchConditionDefinition yes defaultTransition State to transition to next if no conditions are matched. string no defaultTransform jq command to transform the state's data output. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no SwitchConditionDefinition Parameter Description Type Required condition jq command evaluated against state data. True if results are not empty. string yes transition State to transition to if this branch is selected. string no transform jq command to transform the state's data output. string no An example definition - id : decision type : switch conditions : - condition : jq(.patient.contactInfo.mobile) transition : sms transform : 'jq(. + { phone: .contact.mobile)' - condition : jq(.patient.contactInfo.landline) transition : call transform : 'jq(. + { phone: .contact.landline })' defaultTransition : email The Switch State is used to perform conditional transitions based on the current state information. A condition can be any jq command. The command will be run on the current state information and a result of anything other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a match. The list of conditions is evaluated in-order and the first match determines what happens next. If no conditions are matched the defaultTransition will be used. ValidateState Parameter Description Type Required id State unique identifier. string yes type State type (\"validate\"). string yes subject jq command to select the subject of the schema validation. Defaults to '.' if unspecified. no string schema Name of the referenced state data schema. string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no An example definition - id : validate-input type : validate schema : type : object required : - name properties : name : type : string additionalProperties : false transition : process-request This schema is based off the following JSON Schema: { \"type\" : \"object\" , \"required\" :[ \"name\" ], \"properties\" :{ \"name\" :{ \"type\" : \"string\" } }, \"additionalProperties\" : false } The Validate State can be used to validate the structure of the state's data. The schema field takes a yaml-ified representation of a JSON Schema document. TimeoutDefinition Parameter Description Type Required interrupt Duration to wait before triggering a timeout error in the workflow (ISO8601). string no kill Duration to wait before killing the workflow (ISO8601). string no Start ScheduledStartDefinition Parameter Description Type Required type Start type (\"scheduled\"). string yes state ID of the state to use as the start state. string no cron Cron expression to schedule workflow. string no EventStartDefinition Parameter Description Type Required type Start type (\"event\"). string yes state ID of the state to use as the start state. string no event Event to listen for, which can trigger the workflow. StartEventDefinition yes StartEventDefinition Parameter Description Type Required type CloudEvent type. string yes filters Key-value regex pairs for CloudEvent context values that must match. object no EventsXorStartDefinition Parameter Description Type Required type Start type (\"eventsXor\"). string yes state ID of the state to use as the start state. string no events Event to listen for, which can trigger the workflow. []StartEventDefinition yes EventsAndStartDefinition Parameter Description Type Required type Start type (\"eventsAnd\"). string yes state ID of the state to use as the start state. string no events Event to listen for, which can trigger the workflow. []StartEventDefinition yes lifespan Maximum duration an event can be stored before being discarded while waiting for other events (ISO8601). string no correlate Context keys that must exist on every event and have matching values to be grouped together. []string no","title":"Specification"},{"location":"specification/#specification","text":"","title":"Specification"},{"location":"specification/#workflow-overview-diagram","text":"The diagram below captures the workflow definition specification. This is to be used as a reference only, the full specification is described in detail in the sections below.","title":"Workflow Overview Diagram"},{"location":"specification/#workflow-definition","text":"Parameter Description Type Required id Workflow unique identifier. string yes name Workflow name (metadata). string no description Workflow description (metadata). string no version Version information. string no singular Attempts to invoke this workflow will fail when an instance is running. no bool functions Workflow function definitions. []FunctionDefinition no schemas Workflow schema definitions. []SchemaDefinition no states Workflow states. []StateDefinition no timeouts Workflow global timeouts. TimeoutDefinition no start Workflow start configuration. Start no","title":"Workflow Definition"},{"location":"specification/#functiondefinition","text":"","title":"FunctionDefinition"},{"location":"specification/#globalfunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"knative-global\"). string yes service The service being referenced. string yes files Workflow file definition. []FunctionFileDefinition no","title":"GlobalFunctionDefinition"},{"location":"specification/#namespacedfunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"knative-namespace\"). string yes service The service being referenced. string yes files Workflow file definition. []FunctionFileDefinition no","title":"NamespacedFunctionDefinition"},{"location":"specification/#reusablefunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"reusable\"). string yes image Image URI. string yes files Workflow file definition. []FunctionFileDefinition no cmd Command to run in container string no size Size of virtual machine enum no scale Minimum number of instances int no A reusable function can be defined in three different sizes: \" small \"(default), \" medium \", and \" large \". These sizes control how much cpu, memory and storage a virtual machine is given for a function when their virtual machine is created. The default value for \" scale \" is 0 which means the service will be removed after a ceratin amount of time. It defines the minimum number of containers to run for this services if it is greater than 0. Size CPU Memory Storage small 1 256 MB +64 MB medium 1 512 MB +64 MB large 2 1024 MB +64 MB","title":"ReusableFunctionDefinition"},{"location":"specification/#subflowfunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"subflow\"). enum yes workflow ID of workflow within the same namespace. string yes","title":"SubflowFunctionDefinition"},{"location":"specification/#isolatedfunctiondefinition","text":"Parameter Description Type Required id Function definition unique identifier. string yes type Type of function (\"isolated\"). string yes image Image URI. string yes files Workflow file definition. []FunctionFileDefinition no cmd Command to run in container string no size Size of virtual machine enum no scale Minimum number of instances int no An isolated function can be defined in three different sizes: \" small \"(default), \" medium \", and \" large \". These sizes control how much cpu, memory and storage a virtual machine is given for a function when their virtual machine is created. The default value for \" scale \" is 0 which means the service will be removed after a ceratin amount of time. It defines the minimum number of containers to run for this services if it is greater than 0. Size CPU Memory Storage small 1 256 MB +64 MB medium 1 512 MB +64 MB large 2 1024 MB +64 MB","title":"IsolatedFunctionDefinition"},{"location":"specification/#functionfiledefinition","text":"Parameter Description Type Required key Key used to select variable. string yes scope Scope used to select variable. Defaults to 'instance', but can be 'workflow' or 'namespace'. string no as Set the filename of the file. The default is the same as the key. string no type How to treat the file. Options include 'plain', 'base64', 'tar', 'tar.gz'. string no","title":"FunctionFileDefinition"},{"location":"specification/#schemadefinition","text":"Parameter Description Type Required id Schema definition unique identifier. string yes schema Schema (based on JSON Schema). object yes","title":"SchemaDefinition"},{"location":"specification/#states","text":"","title":"States"},{"location":"specification/#common-fields","text":"Parameter Description Type Required id State unique identifier. string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no log jq command to generate data for instance-logging. string no catch Error handling. []ErrorDefinition no The id field must be unique amongst all states in the workflow, and may consist of only alphanumeric characters as well as periods, dashes, and underscores. The transform field can be a jq command string applied to the state information in order to enrich, filter, or change it. Whatever the command resolves to will completely replace the state's information. The transform will be applied immediately before the transition , so it won't change the state information before the main function of the state is performed. The transition , if provided, must be set to the id of a state within the workflow. If left unspecified, reaching this transition will end the workflow without raising an error.","title":"Common Fields"},{"location":"specification/#errordefinition","text":"Parameter Description Type Required error A glob pattern to test error codes for a match. string yes transition State to transition to next. string no The error parameter can be a glob pattern to match multiple types of errors. When an error is thrown it will be compared against each ErrorDefinition in order until it finds a match. If no matches are found the workflow will immediately abort and escalate the error to any caller, unless the retry policy is ready to take over.","title":"ErrorDefinition"},{"location":"specification/#actionstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"action\"). string yes action Action to perform. ActionDefinition yes async If workflow execution can continue without waiting for the action to return. boolean no timeout Duration to wait for action to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"ActionState"},{"location":"specification/#actiondefinition","text":"Parameter Description Type Required function Name of the referenced function. string yes input jq command to generate the input for the action. string no secrets List of secrets to temporarily add to the state data under .secrets before running the input jq command. []string no retries Retry policy. RetryDefinition no","title":"ActionDefinition"},{"location":"specification/#retrydefinition","text":"Parameter Description Type Required max_attempts Maximum number of retry attempts. int yes delay Time delay between retry attempts (ISO8601). string no multiplier Value by which the delay is multiplied after each attempt. float no codes Regex patterns to specify which error codes to catch. []string yes If a retry strategy is defined, the action will be retried on any failures that statify any of the regex codes . If the retry fails max_attempts times a direktiv.retries.exceeded error will be thrown.","title":"RetryDefinition"},{"location":"specification/#an-example-definition","text":"- id : insert-into-database type : action action : function : insert-into-database-function input : customer : jq(.customer) The Action State runs another workflow as a subflow, or a function as defined in the functions section of the workflow definition. Functions may include things such as containers or direktiv virtual-machines. The input for the action is determined by an optional jq command in the input field. If unspecified, the default command is \".\" , which duplicates the entire state data. After the action has returned, whatever the results were will be stored in the state information under return . If an error occurred, it will be automatically raised, and can be handled using catch , or ignored if the desired behaviour is to abort the workflow. If async is true , the workflow will not wait for it to return before transitioning to the next state. The action will be fire-and-forget, and considered completely detached from the calling workflow. In this case, the Action State will not set the return value.","title":"An example definition"},{"location":"specification/#consumeeventstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"consumeEvent\"). string yes event Event to consume. ConsumeEventDefinition yes timeout Duration to wait to receive event (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"ConsumeEventState"},{"location":"specification/#consumeeventdefinition","text":"Parameter Description Type Required type CloudEvent type. string yes context Key-value pairs for CloudEvent context values that must match. object no","title":"ConsumeEventDefinition"},{"location":"specification/#an-example-definition_1","text":"- id : wait-for-booking type : consumeEvent event : type : guestbooking context : source : 'bookings.*' customerId : jq(.customerId) venue : Sydney timeout : PT1H transform : jq(.customer) transition : add-booking-to-database The ConsumeEvent State is the simplest state you can use to listen for CloudEvents in the middle of a workflow (for triggering a workflow when receiving an event, see Start ). More complex event consumers include EventXor State and the EventAnd State . When a workflow reaches a ConsumeEvent State it will halt its execution until it receives a matching event, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving a matching event a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The event payload will stored at a variable with the same name as the event's type . If the payload is not valid JSON it will be base64 encoded as a string first.","title":"An example definition"},{"location":"specification/#delaystate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"delay\"). string yes duration Duration to delay (ISO8601). string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"DelayState"},{"location":"specification/#an-example-definition_2","text":"- id : sleep type : delay duration : PT1H transition : fetch-data The Delay State pauses execution of the workflow for a predefined length of time.","title":"An example definition"},{"location":"specification/#errorstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"error\"). string yes error Error code, catchable on a calling workflow. string yes message Format string to provide more context to the error. string yes args A list of jq commands to generate arguments for substitution in the message format string. []string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"ErrorState"},{"location":"specification/#an-example-definition_3","text":"- id : error-out-of-date type : error error : validation.outOfDate message : \"food item %s is out of date\" args : - jq(.item.name) The Error State allows a subflow to throw an error, catchable by the calling workflow. The first transition to an Error State anywhere within the workflow means that a waiting caller -- if one exists -- will receive that error after this subflow returns. This doesn't prevent the Error State from transitioning to other states, which might be necessary to clean up or undo actions performed by the workflow. Subsequent transitions into Error States after the first have no effect. An error consists of two parts: an error code, and an error message. The code should be a short string can can contain alphanumeric characters, periods, dashes, and underscores. It is good practice to structure error codes similar to domain names, to make them easier to handle. The message allows you to provide extra context, and can be formatted like a printf string where each entry in args will be substituted. The args must be jq commands, allowing the state to insert state information into the error message.","title":"An example definition"},{"location":"specification/#eventandstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"eventAnd\"). string yes events Events to consume. []ConsumeEventDefinition yes timeout Duration to wait to receive all events (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"EventAndState"},{"location":"specification/#an-example-definition_4","text":"- id : event-and type : eventAnd timeout : PT1H transition : done events : - event : type : purchasePaid context : source : 'purchase.*' customerId : jq(.customerId) country : Australia - event : type : purchaseSent context : source : 'purchase.*' customerId : jq(.customerId) country : Australia When a workflow reaches an EventAnd State it will halt its execution until it receives a matching event for every event in its events list, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving matches for all required events a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The event payloads will stored in variables with the same names as each event's type . If a payload is not valid JSON it will be base64 encoded as a string first.","title":"An example definition"},{"location":"specification/#eventxorstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"eventXor\"). string yes events Events to consume, and what to do based on which event was received. []EventConditionDefinition yes timeout Duration to wait to receive event (ISO8601). string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"EventXorState"},{"location":"specification/#eventconditiondefinition","text":"Parameter Description Type Required event Event to consume. ConsumeEventDefinition yes transition State to transition to if this branch is selected. string no transform jq command to transform the state's data output. string no","title":"EventConditionDefinition"},{"location":"specification/#an-example-definition_5","text":"- id : event-xor type : eventXor timeout : PT1H events : - transition : \"reservation-accept\" event : type : reservationAccept context : source : \"reservation.*\" guestName : jq(.guestName) venue : \"Compu Global HMN\" - transition : \"reservation-decline\" event : type : reservationDecline context : source : \"reservation.*\" guestName : jq(.guestName) venue : \"Compu Global HMN\" When a workflow reaches an EventXor State it will halt its execution until it receives any matching event in its events list, where matches are determined according to the type and context parameters. While type is a required string constant, context can include any number of key-value pairs that will be used to filter for a match. The keys for this context field will be checked within the CloudEvent's Context metadata fields for matches. By default, any context value will be treated as a standard Javascript Regex pattern, but jq queries can be performed within a string literal with the jq() funciton. For example: \"Hello jq(.name)!\" If the timeout is reached without receiving matches for any required event a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . The received event payload will stored in a variable with the same name as its event type . If a payload is not valid JSON it will be base64 encoded as a string first.","title":"An example definition"},{"location":"specification/#foreachstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"foreach\"). string yes array jq command to produce an array of objects to loop through. string yes action Action to perform. ActionDefinition yes timeout Duration to wait for all actions to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no The ForeachState can be used to split up state data into an array and then perform an action on each element in parallel. The jq command provided in the array must produce an array or a direktiv.foreachInput error will be thrown. The jq command used to generate the input for the action will be applied to a single element from that array. The return values of each action will be included in an array stored at .return at the same index from which its input was generated.","title":"ForeachState"},{"location":"specification/#generateeventstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"generateEvent\"). string yes event Event to generate. GenerateEventDefinition yes transform jq command to transform the state's data output. string no delay Duration to wait before publishing event (ISO8601). string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"GenerateEventState"},{"location":"specification/#generateeventdefinition","text":"Parameter Description Type Required type CloudEvent type. string yes source CloudEvent source. string yes data A jq command to generate the data (payload) for the produced event. string no datacontenttype An RFC 2046 string specifying the payload content type. string no context Add additional event extension context attributes (key-value). object no The GenerateEvent State will produce an event that other workflows could listen for. If the optional datacontenttype is defined and set to something other than application/json , and the jq command defined in data produces a base64 encoded string, it will be decoded before being used as the event payload.","title":"GenerateEventDefinition"},{"location":"specification/#getterstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"getter\"). string yes variables Variables to fetch. []VariableGetterDefinition yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"GetterState"},{"location":"specification/#variablegetterdefinition","text":"Parameter Description Type Required key Variable name. string yes scope Variable scope (\"instance\", \"workflow\", or \"namespace\"). string yes The getter state is used to retrieve persistent data.","title":"VariableGetterDefinition"},{"location":"specification/#noopstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"noop\"). string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"NoopState"},{"location":"specification/#an-example-definition_6","text":"- id : hello type : noop transform : message : \"Hello\" transition : world The No-op State exists for when nothing more than generic state functionality is required. A common use-case would be to perform a jq operation on the state data without performing another operation.","title":"An example definition"},{"location":"specification/#parallelstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"parallel\"). string yes actions Actions to perform. []ActionDefinition yes mode Option types on how to complete branch execution: \"and\" (default), or \"or\". enum no timeout Duration to wait for all actions to complete (ISO8601). string no transform jq command to transform the state's data output. string no transition State to transition to next. string no catch Error handling. []ErrorDefinition no The Parallel State is an expansion on the Action State , used for running multiple actions in parallel. The state can operate in two different modes: and and or . In and mode all actions must return successfully before completing. In or mode the state can complete as soon as any one action returns without error. Return values from each of the actions will be stored in an array at .return in the order that each action is defined. If an action doesn't return but the state can still complete without errors any missing return values will be null in the array. If the timeout is reached before the state can transition a direktiv.stateTimeout error will be thrown, which may be caught and handled via catch . Any actions still running when the state transitions will be cancelled with \"best effort\" attempts.","title":"ParallelState"},{"location":"specification/#setterstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"setter\"). string yes variables Variables to push. []VariableSetterDefinition yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"SetterState"},{"location":"specification/#variablesetterdefinition","text":"Parameter Description Type Required key Variable name. string yes scope Variable scope (\"instance\", \"workflow\", or \"namespace\"). string yes value jq command to generate variable value. string yes mimeType MimeType to store variable value as. string no The setter state is used to store persistent data. A mimeType type can be provided to specify the type of content a variable is. If mimeType is not provided it will default to application/json . There are three mimeType's that are specifcically handled: * application/json - Default behaviour, value is treated as a json object. * text/plain - Value is treated as a plaintext string, no json marshalling is done. * application/octet-stream - Value is expected to be a base64 string and is stored as its decoded binary value. Read more about mimeTyps in the Examples .","title":"VariableSetterDefinition"},{"location":"specification/#switchstate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"switch\"). string yes conditions Conditions to evaluate and determine which state to transition to next. []SwitchConditionDefinition yes defaultTransition State to transition to next if no conditions are matched. string no defaultTransform jq command to transform the state's data output. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"SwitchState"},{"location":"specification/#switchconditiondefinition","text":"Parameter Description Type Required condition jq command evaluated against state data. True if results are not empty. string yes transition State to transition to if this branch is selected. string no transform jq command to transform the state's data output. string no","title":"SwitchConditionDefinition"},{"location":"specification/#an-example-definition_7","text":"- id : decision type : switch conditions : - condition : jq(.patient.contactInfo.mobile) transition : sms transform : 'jq(. + { phone: .contact.mobile)' - condition : jq(.patient.contactInfo.landline) transition : call transform : 'jq(. + { phone: .contact.landline })' defaultTransition : email The Switch State is used to perform conditional transitions based on the current state information. A condition can be any jq command. The command will be run on the current state information and a result of anything other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a match. The list of conditions is evaluated in-order and the first match determines what happens next. If no conditions are matched the defaultTransition will be used.","title":"An example definition"},{"location":"specification/#validatestate","text":"Parameter Description Type Required id State unique identifier. string yes type State type (\"validate\"). string yes subject jq command to select the subject of the schema validation. Defaults to '.' if unspecified. no string schema Name of the referenced state data schema. string yes transform jq command to transform the state's data output. string no transition State to transition to next. string no retries Retry policy. RetryDefinition no catch Error handling. []ErrorDefinition no","title":"ValidateState"},{"location":"specification/#an-example-definition_8","text":"- id : validate-input type : validate schema : type : object required : - name properties : name : type : string additionalProperties : false transition : process-request This schema is based off the following JSON Schema: { \"type\" : \"object\" , \"required\" :[ \"name\" ], \"properties\" :{ \"name\" :{ \"type\" : \"string\" } }, \"additionalProperties\" : false } The Validate State can be used to validate the structure of the state's data. The schema field takes a yaml-ified representation of a JSON Schema document.","title":"An example definition"},{"location":"specification/#timeoutdefinition","text":"Parameter Description Type Required interrupt Duration to wait before triggering a timeout error in the workflow (ISO8601). string no kill Duration to wait before killing the workflow (ISO8601). string no","title":"TimeoutDefinition"},{"location":"specification/#start","text":"","title":"Start"},{"location":"specification/#scheduledstartdefinition","text":"Parameter Description Type Required type Start type (\"scheduled\"). string yes state ID of the state to use as the start state. string no cron Cron expression to schedule workflow. string no","title":"ScheduledStartDefinition"},{"location":"specification/#eventstartdefinition","text":"Parameter Description Type Required type Start type (\"event\"). string yes state ID of the state to use as the start state. string no event Event to listen for, which can trigger the workflow. StartEventDefinition yes","title":"EventStartDefinition"},{"location":"specification/#starteventdefinition","text":"Parameter Description Type Required type CloudEvent type. string yes filters Key-value regex pairs for CloudEvent context values that must match. object no","title":"StartEventDefinition"},{"location":"specification/#eventsxorstartdefinition","text":"Parameter Description Type Required type Start type (\"eventsXor\"). string yes state ID of the state to use as the start state. string no events Event to listen for, which can trigger the workflow. []StartEventDefinition yes","title":"EventsXorStartDefinition"},{"location":"specification/#eventsandstartdefinition","text":"Parameter Description Type Required type Start type (\"eventsAnd\"). string yes state ID of the state to use as the start state. string no events Event to listen for, which can trigger the workflow. []StartEventDefinition yes lifespan Maximum duration an event can be stored before being discarded while waiting for other events (ISO8601). string no correlate Context keys that must exist on every event and have matching values to be grouped together. []string no","title":"EventsAndStartDefinition"},{"location":"environment/direktiv-development-environment/","text":"Development Standalone environment To improve function and workflows development it is recommended to setup a local development environment. This section explains how to setup the development environment. Details about developing custom functions is described in this section . Running direktiv Setting up a development direktiv instance on a local machine is pretty simple. Assuming docker is installed, run the folowing command: docker run --privileged -p 8080 :80 -p 31212 :31212 -d --name direktiv direktiv/direktiv-kube This command starts direktiv as container 'direktiv'. The initial boot-time will take a few minutes. The progress can be followed with: docker logs direktiv -f Once all pods reach 'running' status, direktiv is ready and the URL http://localhost:8080/api/namespaces is accessible. The database uses a persitent volume so the data stored should survive restarts with 'docker stop/start' . Running with proxy Running direktiv with a proxy configuration, the following settings can be passed as environmental variables: docker run --privileged -p 8080 :80 -p 31212 :31212 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \".default,10.0.0.0/8,172.0.0.0/8,localhost\" --env PERSIST = true -ti -v /tmp/pg:/tmp/pg direktiv/direktiv-kube Docker registry Direktiv pulls containers from a registry and runs them as functions. For development purposes the direktiv docker container comes with a registry installed. It is accessible on localhost:31212. To test the local repository the golang example from direktiv-apps can be used: git clone https://github.com/direktiv/direktiv-apps.git docker build direktiv-apps/examples/golang/ -t localhost:31212/myapp docker push localhost:31212/myapp # confirm upload curl http://localhost:31212/v2/_catalog To use it we need to create a namespace and a workflow. # create namespace 'test' curl -X PUT http://localhost:8080/api/namespaces/test # create the workflow file cat > helloworld.yml <<- EOF description: A simple 'action' state that sends a get request\" functions: - id: get type: reusable image: localhost:31212/myapp states: - id: getter type: action action: function: get input: name: John EOF # upload workflow curl -X PUT --data-binary @helloworld.yml \"http://localhost:8080/api/namespaces/test/tree/test?op=create-workflow\" # execute workflow (initial call will be slightly slower than subsequent calls) curl \"http://localhost:8080/api/namespaces/test/tree/test?op=wait\"","title":"Standalone Environment"},{"location":"environment/direktiv-development-environment/#development-standalone-environment","text":"To improve function and workflows development it is recommended to setup a local development environment. This section explains how to setup the development environment. Details about developing custom functions is described in this section .","title":"Development Standalone environment"},{"location":"environment/direktiv-development-environment/#running-direktiv","text":"Setting up a development direktiv instance on a local machine is pretty simple. Assuming docker is installed, run the folowing command: docker run --privileged -p 8080 :80 -p 31212 :31212 -d --name direktiv direktiv/direktiv-kube This command starts direktiv as container 'direktiv'. The initial boot-time will take a few minutes. The progress can be followed with: docker logs direktiv -f Once all pods reach 'running' status, direktiv is ready and the URL http://localhost:8080/api/namespaces is accessible. The database uses a persitent volume so the data stored should survive restarts with 'docker stop/start' .","title":"Running direktiv"},{"location":"environment/direktiv-development-environment/#running-with-proxy","text":"Running direktiv with a proxy configuration, the following settings can be passed as environmental variables: docker run --privileged -p 8080 :80 -p 31212 :31212 --env HTTPS_PROXY = \"http://<proxy-address>:443\" --env NO_PROXY = \".default,10.0.0.0/8,172.0.0.0/8,localhost\" --env PERSIST = true -ti -v /tmp/pg:/tmp/pg direktiv/direktiv-kube","title":"Running with proxy"},{"location":"environment/direktiv-development-environment/#docker-registry","text":"Direktiv pulls containers from a registry and runs them as functions. For development purposes the direktiv docker container comes with a registry installed. It is accessible on localhost:31212. To test the local repository the golang example from direktiv-apps can be used: git clone https://github.com/direktiv/direktiv-apps.git docker build direktiv-apps/examples/golang/ -t localhost:31212/myapp docker push localhost:31212/myapp # confirm upload curl http://localhost:31212/v2/_catalog To use it we need to create a namespace and a workflow. # create namespace 'test' curl -X PUT http://localhost:8080/api/namespaces/test # create the workflow file cat > helloworld.yml <<- EOF description: A simple 'action' state that sends a get request\" functions: - id: get type: reusable image: localhost:31212/myapp states: - id: getter type: action action: function: get input: name: John EOF # upload workflow curl -X PUT --data-binary @helloworld.yml \"http://localhost:8080/api/namespaces/test/tree/test?op=create-workflow\" # execute workflow (initial call will be slightly slower than subsequent calls) curl \"http://localhost:8080/api/namespaces/test/tree/test?op=wait\"","title":"Docker registry"},{"location":"environment/direktiv-vscode/","text":"Visual Studio Code (IDE) Although developing workflows with the web UI is easy, direktiv's Visual Studio Code extension can be used to make local workflow development faster and more convenient. Install Extension The direktiv extension is published on the marketplace and a simple search for 'direktiv' in the marketplace should return one result. After pressing 'Install' the extension is ready. The direktiv extension is available on the Visual Studio Code extension marketplace. Simply searching for 'direktiv' should return the correct result. After clicking on 'Install' the extension is ready for use. Adding workspace To connect a local folder in your workspace to direktiv, right click a folder and click the 'Download Workflows' button in the context menu. When prompted, enter http://localhost:8080 as the Connection URL and test as a namespace. The selected folder should now contain the workflow created previously. Running workflows It is now possible to edit workflows through Visual Studio Code (eg. changing the value of the name field to Hello ). When the file is saved, only the local copy of the workflow is affected. The direktiv extension provides a few keyboard shorcuts for common actions: Save and Upload (Ctrl+Alt+S) Save, Upload and Execute (Ctrl+Alt+X) Execute (Shift+Alt+X) For example, pressing Ctrl-Alt-X will save the local copy of the workflow, update the remote version of the workflow, and execute it. An 'instance' tab is then opened inside of Visual Studio Code that presents the instance logs in real-time. To manually locate instance logs, click on the direktiv logo on the sidebar panel. Click on the ... icon in the top-right corner of the DIREKTIV: INSTANCES panel, and enter http://localhost:8080 . When prompted for a namespace, enter test . Press enter one more time to skip past the final prompt. To view instance output, right-click on one of the listed instances and select Get Instance Output from the context menu.","title":"VSCode Extension"},{"location":"environment/direktiv-vscode/#visual-studio-code-ide","text":"Although developing workflows with the web UI is easy, direktiv's Visual Studio Code extension can be used to make local workflow development faster and more convenient.","title":"Visual Studio Code (IDE)"},{"location":"environment/direktiv-vscode/#install-extension","text":"The direktiv extension is published on the marketplace and a simple search for 'direktiv' in the marketplace should return one result. After pressing 'Install' the extension is ready. The direktiv extension is available on the Visual Studio Code extension marketplace. Simply searching for 'direktiv' should return the correct result. After clicking on 'Install' the extension is ready for use.","title":"Install Extension"},{"location":"environment/direktiv-vscode/#adding-workspace","text":"To connect a local folder in your workspace to direktiv, right click a folder and click the 'Download Workflows' button in the context menu. When prompted, enter http://localhost:8080 as the Connection URL and test as a namespace. The selected folder should now contain the workflow created previously.","title":"Adding workspace"},{"location":"environment/direktiv-vscode/#running-workflows","text":"It is now possible to edit workflows through Visual Studio Code (eg. changing the value of the name field to Hello ). When the file is saved, only the local copy of the workflow is affected. The direktiv extension provides a few keyboard shorcuts for common actions: Save and Upload (Ctrl+Alt+S) Save, Upload and Execute (Ctrl+Alt+X) Execute (Shift+Alt+X) For example, pressing Ctrl-Alt-X will save the local copy of the workflow, update the remote version of the workflow, and execute it. An 'instance' tab is then opened inside of Visual Studio Code that presents the instance logs in real-time. To manually locate instance logs, click on the direktiv logo on the sidebar panel. Click on the ... icon in the top-right corner of the DIREKTIV: INSTANCES panel, and enter http://localhost:8080 . When prompted for a namespace, enter test . Press enter one more time to skip past the final prompt. To view instance output, right-click on one of the listed instances and select Get Instance Output from the context menu.","title":"Running workflows"},{"location":"events/","text":"Events To trigger a cloud event simple send a post request to '/api/namespaces/{namespace}/event'. With the following request body. { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } Authentication Use Bearer Token Apply this header \"Authorization\" : \"Bearer ACCESS_TOKEN\" Use ApiKey Apply this header \"apiKey\" : \"API_KEY\"","title":"General"},{"location":"events/#events","text":"To trigger a cloud event simple send a post request to '/api/namespaces/{namespace}/event'. With the following request body. { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull_request.opened\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" }","title":"Events"},{"location":"events/#authentication","text":"","title":"Authentication"},{"location":"events/#use-bearer-token","text":"Apply this header \"Authorization\" : \"Bearer ACCESS_TOKEN\"","title":"Use Bearer Token"},{"location":"events/#use-apikey","text":"Apply this header \"apiKey\" : \"API_KEY\"","title":"Use ApiKey"},{"location":"events/cloud/","text":"Cloud A simple introduction to using events provided from Google, Amazon and Azure to send to Direktiv.","title":"General"},{"location":"events/cloud/#cloud","text":"A simple introduction to using events provided from Google, Amazon and Azure to send to Direktiv.","title":"Cloud"},{"location":"events/cloud/amazon/","text":"Amazon EventBridge We're going to go through the process of setting up a rule for 'ec2' to send events to our Direktiv service. This explains how to create an api destination and transform the aws event input to cloud event format. Note: the below tutorial assumes that the user has already created the IAM role for the EventBridge API integration as described in Amazon EventBridge User Guide From the Role create above - keep the Role Arn details as it is needed in the final step. A screenshot is shown below: Create a rule aws events put-rule --name \"direktiv-rule\" --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"]}\" The following output should appear (make sure you hold onto the ARN as it is used further down to attach a target to the rule): { \"RuleArn\" : \"<RULE_ARN>\" } Create a connection After creating an Authorization token from the Direktiv interface, create the connection using the token as follow: aws events create-connection --name direktiv-connection --authorization-type API_KEY --auth-parameters \"{\\\"ApiKeyAuthParameters\\\": {\\\"ApiKeyName\\\":\\\"Authorization\\\", \\\"ApiKeyValue\\\":\\\"Bearer <DIREKTIV_TOKEN>\\\"}}\" Upon creating the connection the following output from the CLI should appear. { \"ConnectionArn\" : \"<CONNECTION_ARN>\" , \"ConnectionState\" : \"AUTHORIZED\" , \"CreationTime\" : \"2021-08-04T05:28:24+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:28:24+00:00\" } We will need to use the connection arn in the next command. Create an Api-Destination aws events create-api-destination --name direktiv-api --connection-arn \"<CONNECTION_ARN>\" --invocation-endpoint https://run.direktiv.io/api/namespaces/complex-workflows/event --http-method POST The output should resemble this: { \"ApiDestinationArn\" : \"<API_ARN>\" , \"ApiDestinationState\" : \"ACTIVE\" , \"CreationTime\" : \"2021-08-04T05:30:50+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:30:50+00:00\" } Put Targets to the AWS EventBridge Rule Adding the targets to the EventBridge rule also requires us to define an Input Path and Input Template. aws events put-targets --rule direktiv-rule --targets '[ { \"Id\": \"direktiv-api\", \"RoleArn\": \"<ROLE_ARN>\", \"Arn\": \"<API_ARN>\", \"InputTransformer\": { \"InputPathsMap\": { \"data\":\"$\", \"id\":\"$.id\", \"source\":\"$.source\", \"state\":\"$.detail.state\", \"subject\":\"$.source\", \"time\":\"$.time\", \"type\":\"$.detail-type\" }, \"InputTemplate\": \" {\\\"specversion\\\":\\\"1.0\\\", \\\"id\\\":<id>, \\\"source\\\":<source>, \\\"type\\\":<type>, \\\"subject\\\":<subject>, \\\"time\\\":<time>, \\\"data\\\":<data>}\" } } ]' The output (if successful) below: { \"FailedEntryCount\" : 0 , \"FailedEntries\" : [] } Input Path Map Example Input Path Map captures the EventBridge event so we can easily filter into a cloud event to send to Direktiv { \"data\" : \"$\" , \"id\" : \"$.id\" , \"source\" : \"$.source\" , \"state\" : \"$.detail.state\" , \"subject\" : \"$.source\" , \"time\" : \"$.time\" , \"type\" : \"$.detail-type\" } Input Template Example The Input Template allows you to spec out what you want the JSON to look like parsing the values from the input path. { \"specversion\" : \"1.0\" , \"id\" : <id> , \"source\" : <source> , \"type\" : < t ype> , \"subject\" : <subjec t > , \"time\" : < t ime> , \"data\" : <da ta > } So now when you change the state of an instance on EC2 a workflow will be triggered on Direktiv if it is listening to 'aws.ec2'. For reference, when an AWS event is generated, the default event structure (for an EC2 status change as an example) is shown below: { \"version\" : \"0\" , \"id\" : \"a0bca05d-2cd7-2044-04a6-ce94a6271d10\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"338328518639\" , \"time\" : \"2021-08-04T04:17:23Z\" , \"region\" : \"ap-southeast-2\" , \"resources\" : [], \"detail\" : { \"instance-id\" : \"i-07dc0a80689d48dab\" , \"state\" : \"running\" } } The CloudEvent received by Direktiv after the transformation is shown below: { \"specversion\" : \"1.0\" , \"id\" : \"3156cfde-9e3d-570f-b1f6-ca4358472fe0\" , \"source\" : \"aws.ec2\" , \"type\" : \"EC2 Instance State-change Notification\" , \"subject\" : \"aws.ec2\" , \"time\" : \"2021-08-04T04:17:23Z\" , \"data\" : { \"version\" : \"0\" , \"id\" : \"3156cfde-9e3d-570f-b1f6-ca4358472fe0\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"338328518639\" , \"time\" : \"2021-08-04T04:17:23Z\" , \"region\" : \"ap-southeast-2\" , \"resources\" : [], \"detail\" : { \"instance-id\" : \"i-07dc0a80689d48dab\" , \"state\" : \"running\" } } } Testing Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : \"EC2 Instance State-change Notification\" states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Amazon"},{"location":"events/cloud/amazon/#amazon-eventbridge","text":"We're going to go through the process of setting up a rule for 'ec2' to send events to our Direktiv service. This explains how to create an api destination and transform the aws event input to cloud event format. Note: the below tutorial assumes that the user has already created the IAM role for the EventBridge API integration as described in Amazon EventBridge User Guide From the Role create above - keep the Role Arn details as it is needed in the final step. A screenshot is shown below:","title":"Amazon EventBridge"},{"location":"events/cloud/amazon/#create-a-rule","text":"aws events put-rule --name \"direktiv-rule\" --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"]}\" The following output should appear (make sure you hold onto the ARN as it is used further down to attach a target to the rule): { \"RuleArn\" : \"<RULE_ARN>\" }","title":"Create a rule"},{"location":"events/cloud/amazon/#create-a-connection","text":"After creating an Authorization token from the Direktiv interface, create the connection using the token as follow: aws events create-connection --name direktiv-connection --authorization-type API_KEY --auth-parameters \"{\\\"ApiKeyAuthParameters\\\": {\\\"ApiKeyName\\\":\\\"Authorization\\\", \\\"ApiKeyValue\\\":\\\"Bearer <DIREKTIV_TOKEN>\\\"}}\" Upon creating the connection the following output from the CLI should appear. { \"ConnectionArn\" : \"<CONNECTION_ARN>\" , \"ConnectionState\" : \"AUTHORIZED\" , \"CreationTime\" : \"2021-08-04T05:28:24+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:28:24+00:00\" } We will need to use the connection arn in the next command.","title":"Create a connection"},{"location":"events/cloud/amazon/#create-an-api-destination","text":"aws events create-api-destination --name direktiv-api --connection-arn \"<CONNECTION_ARN>\" --invocation-endpoint https://run.direktiv.io/api/namespaces/complex-workflows/event --http-method POST The output should resemble this: { \"ApiDestinationArn\" : \"<API_ARN>\" , \"ApiDestinationState\" : \"ACTIVE\" , \"CreationTime\" : \"2021-08-04T05:30:50+00:00\" , \"LastModifiedTime\" : \"2021-08-04T05:30:50+00:00\" }","title":"Create an Api-Destination"},{"location":"events/cloud/amazon/#put-targets-to-the-aws-eventbridge-rule","text":"Adding the targets to the EventBridge rule also requires us to define an Input Path and Input Template. aws events put-targets --rule direktiv-rule --targets '[ { \"Id\": \"direktiv-api\", \"RoleArn\": \"<ROLE_ARN>\", \"Arn\": \"<API_ARN>\", \"InputTransformer\": { \"InputPathsMap\": { \"data\":\"$\", \"id\":\"$.id\", \"source\":\"$.source\", \"state\":\"$.detail.state\", \"subject\":\"$.source\", \"time\":\"$.time\", \"type\":\"$.detail-type\" }, \"InputTemplate\": \" {\\\"specversion\\\":\\\"1.0\\\", \\\"id\\\":<id>, \\\"source\\\":<source>, \\\"type\\\":<type>, \\\"subject\\\":<subject>, \\\"time\\\":<time>, \\\"data\\\":<data>}\" } } ]' The output (if successful) below: { \"FailedEntryCount\" : 0 , \"FailedEntries\" : [] }","title":"Put Targets to the AWS EventBridge Rule"},{"location":"events/cloud/amazon/#input-path-map-example","text":"Input Path Map captures the EventBridge event so we can easily filter into a cloud event to send to Direktiv { \"data\" : \"$\" , \"id\" : \"$.id\" , \"source\" : \"$.source\" , \"state\" : \"$.detail.state\" , \"subject\" : \"$.source\" , \"time\" : \"$.time\" , \"type\" : \"$.detail-type\" }","title":"Input Path Map Example"},{"location":"events/cloud/amazon/#input-template-example","text":"The Input Template allows you to spec out what you want the JSON to look like parsing the values from the input path. { \"specversion\" : \"1.0\" , \"id\" : <id> , \"source\" : <source> , \"type\" : < t ype> , \"subject\" : <subjec t > , \"time\" : < t ime> , \"data\" : <da ta > } So now when you change the state of an instance on EC2 a workflow will be triggered on Direktiv if it is listening to 'aws.ec2'. For reference, when an AWS event is generated, the default event structure (for an EC2 status change as an example) is shown below: { \"version\" : \"0\" , \"id\" : \"a0bca05d-2cd7-2044-04a6-ce94a6271d10\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"338328518639\" , \"time\" : \"2021-08-04T04:17:23Z\" , \"region\" : \"ap-southeast-2\" , \"resources\" : [], \"detail\" : { \"instance-id\" : \"i-07dc0a80689d48dab\" , \"state\" : \"running\" } } The CloudEvent received by Direktiv after the transformation is shown below: { \"specversion\" : \"1.0\" , \"id\" : \"3156cfde-9e3d-570f-b1f6-ca4358472fe0\" , \"source\" : \"aws.ec2\" , \"type\" : \"EC2 Instance State-change Notification\" , \"subject\" : \"aws.ec2\" , \"time\" : \"2021-08-04T04:17:23Z\" , \"data\" : { \"version\" : \"0\" , \"id\" : \"3156cfde-9e3d-570f-b1f6-ca4358472fe0\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"338328518639\" , \"time\" : \"2021-08-04T04:17:23Z\" , \"region\" : \"ap-southeast-2\" , \"resources\" : [], \"detail\" : { \"instance-id\" : \"i-07dc0a80689d48dab\" , \"state\" : \"running\" } } }","title":"Input Template Example"},{"location":"events/cloud/amazon/#testing","text":"Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : \"EC2 Instance State-change Notification\" states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/cloud/azure/","text":"Azure EventGrid Goes through the process of setting up a storage account that listens for events on upload. Being that Azure uses native cloud events we won't need to run anything apart from the initial setup. Setup To follow along you will need access to the resource group you wish to setup in. Create a Storage Account & Container Create a storage account under a resource group az storage account create --name direktivstoragetest --resource-group trentis-direktiv-apps-test Create a container under that storage account. You can get the --account-key by doing the following az storage account keys list --account-name direktivstoragetest az storage container create --name direktiv-container --account-name direktivstorage100 --account-key ACCOUNT-KEY Create an Event Subscription Create an event subscription attached to the storage account. az eventgrid event-subscription create \\ --name direktiv-event \\ --source-resource-id = $( az storage account show --name direktivstoragetest --resource-group trentis-direktiv-apps-test --query id --output tsv ) \\ --endpoint = https://playground.direktiv.io/api/namespaces/trent/event \\ --endpoint-type = webhook --event-delivery-schema cloudeventschemav1_0 \\ --delivery-attribute-mapping Authorization Static \"Bearer ACCESS_TOKEN\" true Testing id : listen-for-azure-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : Microsoft.Storage.BlobCreated states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Azure"},{"location":"events/cloud/azure/#azure-eventgrid","text":"Goes through the process of setting up a storage account that listens for events on upload. Being that Azure uses native cloud events we won't need to run anything apart from the initial setup.","title":"Azure EventGrid"},{"location":"events/cloud/azure/#setup","text":"To follow along you will need access to the resource group you wish to setup in.","title":"Setup"},{"location":"events/cloud/azure/#create-a-storage-account-container","text":"Create a storage account under a resource group az storage account create --name direktivstoragetest --resource-group trentis-direktiv-apps-test Create a container under that storage account. You can get the --account-key by doing the following az storage account keys list --account-name direktivstoragetest az storage container create --name direktiv-container --account-name direktivstorage100 --account-key ACCOUNT-KEY","title":"Create a Storage Account &amp; Container"},{"location":"events/cloud/azure/#create-an-event-subscription","text":"Create an event subscription attached to the storage account. az eventgrid event-subscription create \\ --name direktiv-event \\ --source-resource-id = $( az storage account show --name direktivstoragetest --resource-group trentis-direktiv-apps-test --query id --output tsv ) \\ --endpoint = https://playground.direktiv.io/api/namespaces/trent/event \\ --endpoint-type = webhook --event-delivery-schema cloudeventschemav1_0 \\ --delivery-attribute-mapping Authorization Static \"Bearer ACCESS_TOKEN\" true","title":"Create an Event Subscription"},{"location":"events/cloud/azure/#testing","text":"id : listen-for-azure-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : Microsoft.Storage.BlobCreated states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/cloud/gcp/","text":"Google Cloud EventArc To send Google Cloud Audit log events to EventArc you will need a container service running on Cloud Run. We provide you a container located at 'gcr.io/direktiv/event-arc-listener'. That container's job is to read the cloud event it receives and relays it back to a Direktiv service. Setup Setup Audit Logs to be managed Read policy file to /tmp/policy.yaml gcloud projects get-iam-policy PROJECT_ID > /tmp/policy.yaml Add the follow section above 'bindings:' auditConfigs : - auditLogConfigs : - logType : ADMIN_READ - logType : DATA_WRITE - logType : DATA_READ service : storage.googleapis.com Set the new policy gcloud projects set-iam-policy PROJECT_ID /tmp/policy.yaml Setup Configs for Gcloud to run properly gcloud config set project PROJECT_ID gcloud config set run/region us-central1 gcloud config set run/platform managed gcloud config set eventarc/location us-central1 Configure the Cloud Run Service Using Authentication Create a secret to use as the DIREKTIV_TOKEN gcloud secrets create DIREKTIV_TOKEN \\ --replication-policy = \"automatic\" Create a file that contains the ACCESS_TOKEN generated from Direktiv that has 'namespaceEvent' privilege. I chose to create the file as '/tmp/ac'. Add the secret data to the secret gcloud secrets versions add DIREKTIV_TOKEN --data-file = /tmp/ac Create a Cloud Run Service Deploy the container to your environment gcloud beta run deploy event-arc-listener --image gcr.io/direktiv/event-arc-listener \\ --update-secrets = DIREKTIV_TOKEN = DIREKTIV_TOKEN:1 \\ --set-env-vars \"DIREKTIV_NAMESPACE=trent\" \\ --set-env-vars \"DIREKTIV_ENDPOINT=https://playground.direktiv.io\" \\ --allow-unauthenticated Create a Trigger for the Cloud Run Service Create a new trigger to listen for storage events on this project. gcloud eventarc triggers create storage-upload-trigger \\ --destination-run-service = event-arc-listener \\ --destination-run-region = us-central1 \\ --event-filters = \"type=google.cloud.audit.log.v1.written\" \\ --event-filters = \"serviceName=storage.googleapis.com\" \\ --event-filters = \"methodName=storage.objects.create\" \\ --service-account = SERVICE_ACCOUNT_ADDRESS Note: Keep in mind this trigger will take 10 minutes to work Testing Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : google.cloud.audit.log.v1.written states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Google Cloud Platform"},{"location":"events/cloud/gcp/#google-cloud-eventarc","text":"To send Google Cloud Audit log events to EventArc you will need a container service running on Cloud Run. We provide you a container located at 'gcr.io/direktiv/event-arc-listener'. That container's job is to read the cloud event it receives and relays it back to a Direktiv service.","title":"Google Cloud EventArc"},{"location":"events/cloud/gcp/#setup","text":"","title":"Setup"},{"location":"events/cloud/gcp/#setup-audit-logs-to-be-managed","text":"Read policy file to /tmp/policy.yaml gcloud projects get-iam-policy PROJECT_ID > /tmp/policy.yaml Add the follow section above 'bindings:' auditConfigs : - auditLogConfigs : - logType : ADMIN_READ - logType : DATA_WRITE - logType : DATA_READ service : storage.googleapis.com Set the new policy gcloud projects set-iam-policy PROJECT_ID /tmp/policy.yaml","title":"Setup Audit Logs to be managed"},{"location":"events/cloud/gcp/#setup-configs-for-gcloud-to-run-properly","text":"gcloud config set project PROJECT_ID gcloud config set run/region us-central1 gcloud config set run/platform managed gcloud config set eventarc/location us-central1","title":"Setup Configs for Gcloud to run properly"},{"location":"events/cloud/gcp/#configure-the-cloud-run-service","text":"","title":"Configure the Cloud Run Service"},{"location":"events/cloud/gcp/#using-authentication","text":"Create a secret to use as the DIREKTIV_TOKEN gcloud secrets create DIREKTIV_TOKEN \\ --replication-policy = \"automatic\" Create a file that contains the ACCESS_TOKEN generated from Direktiv that has 'namespaceEvent' privilege. I chose to create the file as '/tmp/ac'. Add the secret data to the secret gcloud secrets versions add DIREKTIV_TOKEN --data-file = /tmp/ac","title":"Using Authentication"},{"location":"events/cloud/gcp/#create-a-cloud-run-service","text":"Deploy the container to your environment gcloud beta run deploy event-arc-listener --image gcr.io/direktiv/event-arc-listener \\ --update-secrets = DIREKTIV_TOKEN = DIREKTIV_TOKEN:1 \\ --set-env-vars \"DIREKTIV_NAMESPACE=trent\" \\ --set-env-vars \"DIREKTIV_ENDPOINT=https://playground.direktiv.io\" \\ --allow-unauthenticated","title":"Create a Cloud Run Service"},{"location":"events/cloud/gcp/#create-a-trigger-for-the-cloud-run-service","text":"Create a new trigger to listen for storage events on this project. gcloud eventarc triggers create storage-upload-trigger \\ --destination-run-service = event-arc-listener \\ --destination-run-region = us-central1 \\ --event-filters = \"type=google.cloud.audit.log.v1.written\" \\ --event-filters = \"serviceName=storage.googleapis.com\" \\ --event-filters = \"methodName=storage.objects.create\" \\ --service-account = SERVICE_ACCOUNT_ADDRESS Note: Keep in mind this trigger will take 10 minutes to work","title":"Create a Trigger for the Cloud Run Service"},{"location":"events/cloud/gcp/#testing","text":"Create this simple workflow that gets executed when it receives a cloud-event of a specific type. id : listen-for-event description : Listen to a custom cloud event start : type : event state : helloworld event : type : google.cloud.audit.log.v1.written states : - id : helloworld type : noop transform : 'jq({ result: . })'","title":"Testing"},{"location":"events/knative/example/","text":"Kafka, Knative and Direktiv In this example we will generate an event in Kafka, consume it in Direktiv via Knative and publish a new event back to Knative. Install Knative The knative installation is simply applying two yaml files to the cluster. Install Knative kubectl apply -f https://github.com/knative/eventing/releases/download/v0.26.1/eventing-crds.yaml kubectl apply -f https://github.com/knative/eventing/releases/download/v0.26.1/eventing-core.yaml After successful installation there are two pods runnning: kubectl get pods -n knative-eventing NAME READY STATUS RESTARTS AGE eventing-controller-7b466b585f-76fd4 1/1 Running 0 7s eventing-webhook-5c8886cb56-q2h7r 1/1 Running 0 7s Kafka Setup If there is no Kafka instance available in the environment, it is easy to setup and configure Kafka with Strimzi . This is a two step process: installing the kafka operator and creating the Kafka cluster itself. Install Operator kubectl create namespace kafka kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka Create Kafka Cluster kubectl apply -f https://strimzi.io/examples/latest/kafka/kafka-persistent-single.yaml -n kafka # wait for cluster to be ready kubectl wait kafka/my-cluster --for=condition=Ready --timeout=300s -n kafka Create Kafka Topics In this example we will consume an event from one channel and publish to a second channel. Therefore we need two new topics to subscribe and publish to. cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-direktiv-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : receiver-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF # test if topics have been created kubectl get kafkatopics.kafka.strimzi.io -n kafka Create Broker In Knative there are different broker and channel combinations available. Here we will use Kafka as a Broker. To install Kafka as Knative broker there needs to be a Kafka broker controller and the implementation itself. Install Kafka Broker Controller kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-controller.yaml kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-broker.yaml With these installed the actual broker can be created. It requires a configmap to nominate the Kafka bootstrap server(s) and the broker yaml file to create the actual broker. Kafka Broker Configuration cat <<-EOF | kubectl apply -f - --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : default.topic.partitions : \"10\" default.topic.replication.factor : \"1\" bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" EOF Kafka Broker cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing EOF An successful installation creates a broker in ready state. kubectl get brokers.eventing.knative.dev NAME URL AGE READY REASON default http://kafka-broker-ingress.knative-eventing.svc.cluster.local/default/default 3s True Kafka Source The basic Knative and Kafka setup is finished and the sources and sinks to integrate the components are the last components missing. Install Kafka Source Implementation kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/source.yaml Creating a Kafka source requires to provide the bootstrap server and the topic to subscribe to. The topic knative-direktiv-topic was created at the beginning of this example and will be the channel to ingest events into Direktiv. The sink value configures this source to send all events coming from this channel to the Kafka broker. Create Kafka Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : direktiv-kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-direktiv-topic sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF This yaml creates the source and it is ready to consume events. kubectl get kafkasources.sources.knative.dev NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE direktiv-kafka-source [\"knative-direktiv-topic\"] [\"my-cluster-kafka-bootstrap.kafka:9092\"] True 10s With this source Knative can receive events but it requires a trigger to have another system consume the event. The setup of source, broker and trigger decouples the systems involved in this architecture. The folllwing yaml creates such a trigger. Because there is a trigger filter defined this trigger consumes all events of type dev.knative.kafka.event and forwards it to Direktiv's direktiv-eventing service. The uri value specifies the target namespace in Direktiv. cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-in namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /direktiv EOF This setup will already send events to a namespace called direktiv if data arrives at the knative-direktiv-topic topic in Kafka. This can be easily tested but the direktiv namespace has to exist in Direktiv. To test it we start a pod which connects to the topic. kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:0.26.0-kafka-3.0.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic knative-direktiv-topic After running the pod add JSON into the command prompt, e.g. {} . This sends the JSON object to Kafka. Knative's broker will pick it up and executes the trigger for direktiv. The event will appear on the direktiv namespace dashboard. Direktiv Source To connect Direktiv back to Knative again we need to install direktiv-knative-source . This source listens to events generated in Direktiv and pushes them to Knative. In this example the message is pushed back to the broker. The required argument for this source is the direktiv URI within the cluster, e.g. direktiv-flow.default:3333. Direktiv Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF Kafka Sink The last task left is installing a Kafka sink and trigger to send events coming from Direktiv to the broker. Installing Kafka Sink Implementation kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-sink.yaml Creating Kafka Sink cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : direktiv-kafka-sink namespace : default spec : topic : receiver-topic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 EOF The following yaml configures the trigger for Kafka. It is important to add a filter for this trigger. In this case the trigger fires if the type of the cloudevent is myevent . Creating Kafka Trigger cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-receive namespace : default spec : broker : default filter : attributes : type : myevent subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : direktiv-kafka-sink EOF Workflow After all these components are installed and connected we need to create a workflow in the direktiv namespace. To see the full configuration we will create flow which listens to Knative events, extracts the data and sends it back to Knative and eventually Kafka. start : type : event state : tellme event : type : dev.knative.kafka.event states : - id : tellme type : generateEvent event : type : myevent source : Direktiv data : x : jq(.\"dev.knative.kafka.event\".data) A second receiver pod is needed to see the events coming from Direktiv . It listens to topic receiver-topic which was created at the beginning of this tutorial. If data is put on the knative-direktiv-topic it will appear in this receiver topic. kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.26.0-kafka-3.0.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic receiver-topic --from-beginning","title":"Kafka Example"},{"location":"events/knative/example/#kafka-knative-and-direktiv","text":"In this example we will generate an event in Kafka, consume it in Direktiv via Knative and publish a new event back to Knative.","title":"Kafka, Knative and Direktiv"},{"location":"events/knative/example/#install-knative","text":"The knative installation is simply applying two yaml files to the cluster. Install Knative kubectl apply -f https://github.com/knative/eventing/releases/download/v0.26.1/eventing-crds.yaml kubectl apply -f https://github.com/knative/eventing/releases/download/v0.26.1/eventing-core.yaml After successful installation there are two pods runnning: kubectl get pods -n knative-eventing NAME READY STATUS RESTARTS AGE eventing-controller-7b466b585f-76fd4 1/1 Running 0 7s eventing-webhook-5c8886cb56-q2h7r 1/1 Running 0 7s","title":"Install Knative"},{"location":"events/knative/example/#kafka-setup","text":"If there is no Kafka instance available in the environment, it is easy to setup and configure Kafka with Strimzi . This is a two step process: installing the kafka operator and creating the Kafka cluster itself. Install Operator kubectl create namespace kafka kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka Create Kafka Cluster kubectl apply -f https://strimzi.io/examples/latest/kafka/kafka-persistent-single.yaml -n kafka # wait for cluster to be ready kubectl wait kafka/my-cluster --for=condition=Ready --timeout=300s -n kafka","title":"Kafka Setup"},{"location":"events/knative/example/#create-kafka-topics","text":"In this example we will consume an event from one channel and publish to a second channel. Therefore we need two new topics to subscribe and publish to. cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-direktiv-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF cat <<-EOF | kubectl apply -f - --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : receiver-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 EOF # test if topics have been created kubectl get kafkatopics.kafka.strimzi.io -n kafka","title":"Create Kafka Topics"},{"location":"events/knative/example/#create-broker","text":"In Knative there are different broker and channel combinations available. Here we will use Kafka as a Broker. To install Kafka as Knative broker there needs to be a Kafka broker controller and the implementation itself. Install Kafka Broker Controller kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-controller.yaml kubectl apply --filename https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-broker.yaml With these installed the actual broker can be created. It requires a configmap to nominate the Kafka bootstrap server(s) and the broker yaml file to create the actual broker. Kafka Broker Configuration cat <<-EOF | kubectl apply -f - --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : default.topic.partitions : \"10\" default.topic.replication.factor : \"1\" bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" EOF Kafka Broker cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing EOF An successful installation creates a broker in ready state. kubectl get brokers.eventing.knative.dev NAME URL AGE READY REASON default http://kafka-broker-ingress.knative-eventing.svc.cluster.local/default/default 3s True","title":"Create Broker"},{"location":"events/knative/example/#kafka-source","text":"The basic Knative and Kafka setup is finished and the sources and sinks to integrate the components are the last components missing. Install Kafka Source Implementation kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/source.yaml Creating a Kafka source requires to provide the bootstrap server and the topic to subscribe to. The topic knative-direktiv-topic was created at the beginning of this example and will be the channel to ingest events into Direktiv. The sink value configures this source to send all events coming from this channel to the Kafka broker. Create Kafka Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : direktiv-kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-direktiv-topic sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF This yaml creates the source and it is ready to consume events. kubectl get kafkasources.sources.knative.dev NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE direktiv-kafka-source [\"knative-direktiv-topic\"] [\"my-cluster-kafka-bootstrap.kafka:9092\"] True 10s With this source Knative can receive events but it requires a trigger to have another system consume the event. The setup of source, broker and trigger decouples the systems involved in this architecture. The folllwing yaml creates such a trigger. Because there is a trigger filter defined this trigger consumes all events of type dev.knative.kafka.event and forwards it to Direktiv's direktiv-eventing service. The uri value specifies the target namespace in Direktiv. cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-in namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /direktiv EOF This setup will already send events to a namespace called direktiv if data arrives at the knative-direktiv-topic topic in Kafka. This can be easily tested but the direktiv namespace has to exist in Direktiv. To test it we start a pod which connects to the topic. kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:0.26.0-kafka-3.0.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap.kafka:9092 --topic knative-direktiv-topic After running the pod add JSON into the command prompt, e.g. {} . This sends the JSON object to Kafka. Knative's broker will pick it up and executes the trigger for direktiv. The event will appear on the direktiv namespace dashboard.","title":"Kafka Source"},{"location":"events/knative/example/#direktiv-source","text":"To connect Direktiv back to Knative again we need to install direktiv-knative-source . This source listens to events generated in Direktiv and pushes them to Knative. In this example the message is pushed back to the broker. The required argument for this source is the direktiv URI within the cluster, e.g. direktiv-flow.default:3333. Direktiv Source cat <<-EOF | kubectl apply -f - --- apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default EOF","title":"Direktiv Source"},{"location":"events/knative/example/#kafka-sink","text":"The last task left is installing a Kafka sink and trigger to send events coming from Direktiv to the broker. Installing Kafka Sink Implementation kubectl apply -f https://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.26.0/eventing-kafka-sink.yaml Creating Kafka Sink cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : direktiv-kafka-sink namespace : default spec : topic : receiver-topic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 EOF The following yaml configures the trigger for Kafka. It is important to add a filter for this trigger. In this case the trigger fires if the type of the cloudevent is myevent . Creating Kafka Trigger cat <<-EOF | kubectl apply -f - --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : direktiv-receive namespace : default spec : broker : default filter : attributes : type : myevent subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : direktiv-kafka-sink EOF","title":"Kafka Sink"},{"location":"events/knative/example/#workflow","text":"After all these components are installed and connected we need to create a workflow in the direktiv namespace. To see the full configuration we will create flow which listens to Knative events, extracts the data and sends it back to Knative and eventually Kafka. start : type : event state : tellme event : type : dev.knative.kafka.event states : - id : tellme type : generateEvent event : type : myevent source : Direktiv data : x : jq(.\"dev.knative.kafka.event\".data) A second receiver pod is needed to see the events coming from Direktiv . It listens to topic receiver-topic which was created at the beginning of this tutorial. If data is put on the knative-direktiv-topic it will appear in this receiver topic. kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.26.0-kafka-3.0.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic receiver-topic --from-beginning","title":"Workflow"},{"location":"events/knative/knative/","text":"Direktiv provides a sink and a source for integration into Knative Eventing . This Kafka example provides a test configuration with Knative Eventing, Kafka and Direktiv. Sink If eventing is enabled in Direktiv's helm chart an additional service is available in the namespace called direktiv-eventing . Knative triggers can be used to subscribe to events from configured Knative sources and executes flows in Direktiv. Triggers can target namespaces with the uri parameter in the YAML configuration. It is also possible to send to all namespaces if the uri is set to '/'. Sending events to all namespaces is a costly operation and should not be used if not absolutely necessary. Knative trigger for namespace apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-namespace-trigger namespace : default spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /mynamespace Source Direktiv provides a source for integrating Direktiv events into Knative as well. To use the source eventing needs to be enabled via helm. Helm Configuration eventing : enabled : true The image to use as a source is vorteil/direktiv-knative-source which establishes a GRPC stream to Direktiv to fetch the generated events. The required arg provides the Direktiv connection URL. Direktiv Knative Source apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default","title":"Eventing"},{"location":"events/knative/knative/#sink","text":"If eventing is enabled in Direktiv's helm chart an additional service is available in the namespace called direktiv-eventing . Knative triggers can be used to subscribe to events from configured Knative sources and executes flows in Direktiv. Triggers can target namespaces with the uri parameter in the YAML configuration. It is also possible to send to all namespaces if the uri is set to '/'. Sending events to all namespaces is a costly operation and should not be used if not absolutely necessary. Knative trigger for namespace apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-namespace-trigger namespace : default spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : direktiv-eventing uri : /mynamespace","title":"Sink"},{"location":"events/knative/knative/#source","text":"Direktiv provides a source for integrating Direktiv events into Knative as well. To use the source eventing needs to be enabled via helm. Helm Configuration eventing : enabled : true The image to use as a source is vorteil/direktiv-knative-source which establishes a GRPC stream to Direktiv to fetch the generated events. The required arg provides the Direktiv connection URL. Direktiv Knative Source apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : direktiv-source spec : template : spec : containers : - image : vorteil/direktiv-knative-source name : direktiv-source args : - --direktiv=direktiv-flow.default:3333 sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default","title":"Source"},{"location":"examples/automatically-blur-nsfw-images/","text":"Introduction We're going to be creating a workflow that takes an image via a URL and checks if it is safe for work using Googles Vision api. The response of this workflow will either be the image unaltered or blurred (if the contents are explicit). This workflow requires three functions: The imagerecognition container to determine if the image at a URL is safe for work. The blur container to fetch and apply a blur filter to the image at the URL. The request container to fetch the unaltered image at the URL. id : check-image functions : - id : check image : direktiv/imagerecognition:v1 type : reusable - id : blur image : direktiv/blur:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : \"Evaluates an image using Google Vision API\" states : # continued in next code block Google Vision First we need to define a state that fetches the image from a url input required and then it uses the Google Vision AI to determine whether it is safe for work. - id : check_image type : action action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : check input : url : jq(.image) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check_val Switch Next with the results of the Google Vision AI we'll go into a switch state to determine if we need transition to the fetch_image or blur_image state - id : check_val type : switch conditions : - condition : jq(.return.safeForWork == true) transition : fetch_image defaultTransition : blur_image Blur Image A simple state that uses the blur container to the fetch an image from a URL and apply a blur filter. - id : blur_image type : action action : function : blur input : image : jq(.image) Fetch Image A simple state that uses the request container to the fetch an image from a URL. - id : fetch_image type : action action : function : request input : url : jq(.image) method : \"GET\" transform : return : jq(.return.data) Full Example Joining every part above we end up with the following workflow. The input required for this workflow is a json field 'image' which contains a url to an image. We can either send this via rest client using 'POST' and adding a JSON body or we could type it directly into the browser like so by filling in the NAMESPACE and WORKFLOW_NAME fields. http://localhost/api/namespaces/{NAMESPACE}/workflows/{WORKFLOW_NAME}/execute?wait=true&field=.return&image=https://images2.minutemediacdn.com/image/fetch/w_736,h_485,c_fill,g_auto,f_auto/https%3A%2F%2Fundeadwalking.com%2Ffiles%2Fimage-exchange%2F2018%2F08%2Fie_58809-850x560.jpeg wait tells the request to return the result instead of an instance id field tells the request what to return image is the input im providing take the json example below { \"image\" : \"https://images2.minutemediacdn.com/image/fetch/w_736,h_485,c_fill,g_auto,f_auto/https%3A%2F%2Fundeadwalking.com%2Ffiles%2Fimage-exchange%2F2018%2F08%2Fie_58809-850x560.jpeg\" } id : check-image functions : - id : check image : direktiv/imagerecognition:v1 type : reusable - id : blur image : direktiv/blur:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : \"Evaluates an image using Google Vision API\" states : - id : check_image type : action action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : check input : url : jq(.image) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check_val - id : check_val type : switch conditions : - condition : jq(.return.safeForWork == true) transition : fetch_image defaultTransition : blur_image - id : blur_image type : action action : function : blur input : image : jq(.image) - id : fetch_image type : action action : function : request input : url : jq(.image) method : \"GET\" transform : return : jq(.return.data)","title":"Blur a NSFW Image"},{"location":"examples/automatically-blur-nsfw-images/#introduction","text":"We're going to be creating a workflow that takes an image via a URL and checks if it is safe for work using Googles Vision api. The response of this workflow will either be the image unaltered or blurred (if the contents are explicit). This workflow requires three functions: The imagerecognition container to determine if the image at a URL is safe for work. The blur container to fetch and apply a blur filter to the image at the URL. The request container to fetch the unaltered image at the URL. id : check-image functions : - id : check image : direktiv/imagerecognition:v1 type : reusable - id : blur image : direktiv/blur:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : \"Evaluates an image using Google Vision API\" states : # continued in next code block","title":"Introduction"},{"location":"examples/automatically-blur-nsfw-images/#google-vision","text":"First we need to define a state that fetches the image from a url input required and then it uses the Google Vision AI to determine whether it is safe for work. - id : check_image type : action action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : check input : url : jq(.image) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check_val","title":"Google Vision"},{"location":"examples/automatically-blur-nsfw-images/#switch","text":"Next with the results of the Google Vision AI we'll go into a switch state to determine if we need transition to the fetch_image or blur_image state - id : check_val type : switch conditions : - condition : jq(.return.safeForWork == true) transition : fetch_image defaultTransition : blur_image","title":"Switch"},{"location":"examples/automatically-blur-nsfw-images/#blur-image","text":"A simple state that uses the blur container to the fetch an image from a URL and apply a blur filter. - id : blur_image type : action action : function : blur input : image : jq(.image)","title":"Blur Image"},{"location":"examples/automatically-blur-nsfw-images/#fetch-image","text":"A simple state that uses the request container to the fetch an image from a URL. - id : fetch_image type : action action : function : request input : url : jq(.image) method : \"GET\" transform : return : jq(.return.data)","title":"Fetch Image"},{"location":"examples/automatically-blur-nsfw-images/#full-example","text":"Joining every part above we end up with the following workflow. The input required for this workflow is a json field 'image' which contains a url to an image. We can either send this via rest client using 'POST' and adding a JSON body or we could type it directly into the browser like so by filling in the NAMESPACE and WORKFLOW_NAME fields. http://localhost/api/namespaces/{NAMESPACE}/workflows/{WORKFLOW_NAME}/execute?wait=true&field=.return&image=https://images2.minutemediacdn.com/image/fetch/w_736,h_485,c_fill,g_auto,f_auto/https%3A%2F%2Fundeadwalking.com%2Ffiles%2Fimage-exchange%2F2018%2F08%2Fie_58809-850x560.jpeg wait tells the request to return the result instead of an instance id field tells the request what to return image is the input im providing take the json example below { \"image\" : \"https://images2.minutemediacdn.com/image/fetch/w_736,h_485,c_fill,g_auto,f_auto/https%3A%2F%2Fundeadwalking.com%2Ffiles%2Fimage-exchange%2F2018%2F08%2Fie_58809-850x560.jpeg\" } id : check-image functions : - id : check image : direktiv/imagerecognition:v1 type : reusable - id : blur image : direktiv/blur:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : \"Evaluates an image using Google Vision API\" states : - id : check_image type : action action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : check input : url : jq(.image) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check_val - id : check_val type : switch conditions : - condition : jq(.return.safeForWork == true) transition : fetch_image defaultTransition : blur_image - id : blur_image type : action action : function : blur input : image : jq(.image) - id : fetch_image type : action action : function : request input : url : jq(.image) method : \"GET\" transform : return : jq(.return.data)","title":"Full Example"},{"location":"examples/check-email-for-intent/","text":"Introduction In this example, we will create two workflows; one will send and email and generate a cloud event, and the other will trigger upon receiving the cloud event, check the contents of an email, use AI to discern the 'intent' of the message, and respond if the intent is deemed to be negative. Send an Email and Trigger an Event id : send-email-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : # continued in next code block Send Email This state uses the direktiv/smtp:v1 container to send an email. - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"You are very bad at doing work.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD))\" server : smtp.gmail.com port : 587 transition : sendcloudevent Trigger Event This generateEvent state sends a cloud event to a namespace. - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp Workflow Send Email id : send-email-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : - id : sendemail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"You are very bad at doing work.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp Read the Email and Check its Intent id : listen-for-email description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : smtp image : direktiv/smtp:v1 type : reusable - id : sentiment image : direktiv/google-sentiment-check:v1 type : reusable start : type : event state : read-email event : type : smtp states : # continued in next code block NOTE: This workflow will be triggered when the 'direktiv' namespace receives a cloud event of type smtp. Read Email This state takes the first message from the email 'INBOX', reads & outputs the contents of the message, and transitions to the sentiment-check state. - id : read-email type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : sentiment-check Check Intent - id : sentiment-check type : action action : function : sentiment secrets : [ \"SERVICE_ACCOUNT_KEY\" ] input : message : jq(.return.msg) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check-feeling Check Intent If the message 'intent' is determined to be negative, send an automated response to the sender. - id : check-feeling type : switch conditions : - condition : jq(.return.feeling == \"Negative\") transition : send-response Reply to Email This state uses the direktiv/smtp:v2 isolate to send an email to the sender of the negative email. - id : send-response type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"I dont appreciate your message\" message : \"\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 Workflow Listen For Email id : listen-for-email description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : smtp image : direktiv/smtp:v1 type : reusable - id : sentiment image : direktiv/google-sentiment-check:v1 type : reusable start : type : event state : read-email event : type : smtp states : - id : read-email type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : sentiment-check - id : sentiment-check type : action action : function : sentiment secrets : [ \"SERVICE_ACCOUNT_KEY\" ] input : message : jq(.return.msg) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check-feeling - id : check-feeling type : switch conditions : - condition : jq(.return.feeling == \"Negative\") transition : send-response - id : send-response type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"I dont appreciate your message\" message : \"\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 Although this particular example is quite simple, the logic used could be modified for something far more 'in-depth'. For example, it could be extended to run an 'IMAP' listener that generates a cloud event, triggering the 'intent' checker on Direktiv. Depending on the determined intent of each email received, actions such as email deletion could be performed.","title":"Check Email for Intent"},{"location":"examples/check-email-for-intent/#introduction","text":"In this example, we will create two workflows; one will send and email and generate a cloud event, and the other will trigger upon receiving the cloud event, check the contents of an email, use AI to discern the 'intent' of the message, and respond if the intent is deemed to be negative.","title":"Introduction"},{"location":"examples/check-email-for-intent/#send-an-email-and-trigger-an-event","text":"id : send-email-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : # continued in next code block","title":"Send an Email and Trigger an Event"},{"location":"examples/check-email-for-intent/#send-email","text":"This state uses the direktiv/smtp:v1 container to send an email. - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"You are very bad at doing work.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD))\" server : smtp.gmail.com port : 587 transition : sendcloudevent","title":"Send Email"},{"location":"examples/check-email-for-intent/#trigger-event","text":"This generateEvent state sends a cloud event to a namespace. - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp","title":"Trigger Event"},{"location":"examples/check-email-for-intent/#workflow-send-email","text":"id : send-email-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : - id : sendemail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"You are very bad at doing work.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp","title":"Workflow Send Email"},{"location":"examples/check-email-for-intent/#read-the-email-and-check-its-intent","text":"id : listen-for-email description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : smtp image : direktiv/smtp:v1 type : reusable - id : sentiment image : direktiv/google-sentiment-check:v1 type : reusable start : type : event state : read-email event : type : smtp states : # continued in next code block NOTE: This workflow will be triggered when the 'direktiv' namespace receives a cloud event of type smtp.","title":"Read the Email and Check its Intent"},{"location":"examples/check-email-for-intent/#read-email","text":"This state takes the first message from the email 'INBOX', reads & outputs the contents of the message, and transitions to the sentiment-check state. - id : read-email type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : sentiment-check","title":"Read Email"},{"location":"examples/check-email-for-intent/#check-intent","text":"- id : sentiment-check type : action action : function : sentiment secrets : [ \"SERVICE_ACCOUNT_KEY\" ] input : message : jq(.return.msg) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check-feeling","title":"Check Intent"},{"location":"examples/check-email-for-intent/#check-intent_1","text":"If the message 'intent' is determined to be negative, send an automated response to the sender. - id : check-feeling type : switch conditions : - condition : jq(.return.feeling == \"Negative\") transition : send-response","title":"Check Intent"},{"location":"examples/check-email-for-intent/#reply-to-email","text":"This state uses the direktiv/smtp:v2 isolate to send an email to the sender of the negative email. - id : send-response type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"I dont appreciate your message\" message : \"\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587","title":"Reply to Email"},{"location":"examples/check-email-for-intent/#workflow-listen-for-email","text":"id : listen-for-email description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : smtp image : direktiv/smtp:v1 type : reusable - id : sentiment image : direktiv/google-sentiment-check:v1 type : reusable start : type : event state : read-email event : type : smtp states : - id : read-email type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : sentiment-check - id : sentiment-check type : action action : function : sentiment secrets : [ \"SERVICE_ACCOUNT_KEY\" ] input : message : jq(.return.msg) serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) transition : check-feeling - id : check-feeling type : switch conditions : - condition : jq(.return.feeling == \"Negative\") transition : send-response - id : send-response type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : smtp input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"I dont appreciate your message\" message : \"\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 Although this particular example is quite simple, the logic used could be modified for something far more 'in-depth'. For example, it could be extended to run an 'IMAP' listener that generates a cloud event, triggering the 'intent' checker on Direktiv. Depending on the determined intent of each email received, actions such as email deletion could be performed.","title":"Workflow Listen For Email"},{"location":"examples/create-vm-set-dns/","text":"Creating a VM and a DNS Record There are a number of pre-made 'isolate' containers designed specifically to bootstrap workflow development with direktiv. In this article, the following four isolates will be used: direktiv/aws-ec2-create:v1 Creates an instance on Amazon Web Services EC2 direktiv/awsgo:v1 Wraps the AWS CLI, enabling the use of any existing AWS CLI command in a workflow direktiv/request:v1 Sends a custom HTTP request direktiv/smtp:v1 Sends an email To keep everything clean, this workflow will actually be split up in to 1 'main' workflow and 2 'subflows' that the main workflow calls. The following 'secrets' must be configured, in order to authenticate with various services: AWS_KEY AWS access key AWS_SECRET AWS access key secret GODADDY_KEY GoDaddy API Key GODADDY_SECRET GoDaddy API Key Secret SMTP_USER The 'sender' address, used to authenticate with the SMTP server. SMTP_PASSWORD Password for the SMTP user. Workflow #1 - Main This workflow creates an AWS EC2 instance, retrieves it's public IP address, and invokes the add-dns-record and send-email subflows. The first state, set-input , is included purely as a matter of convenience, as it allows us to execute the workflow without needing to remember to provide it with an input struct. id : create-vm-with-dns description : Creates an instance on AWS EC2, add a DNS record to GoDaddy, and posts a CloudEvent on completion. functions : - id : create-vm image : direktiv/aws-ec2-create:v1 type : reusable size : medium - id : get-vm image : direktiv/awsgo:v1 type : reusable - id : add-dns-record type : subflow workflow : add-dns-record - id : send-email type : subflow workflow : send-email states : # Set data (skips need for providing an input object on each invocation) - id : set-input type : noop transform : jq(.ami = \"ami-093d266a\" | .region = \"ap-southeast-2\" | .instanceType = \"t1.micro\" | .recipient = \"john.doe@example.com\" | .subdomain = \"direktiv\" | .domain = \"mydomain.com\") transition : create-instance # Create the AWS EC2 Instance - id : create-instance log : jq(.) type : action action : function : create-vm secrets : [ 'AWS_KEY' , 'AWS_SECRET' ] input : access-key : 'jq( .secrets.AWS_KEY )' access-secret : 'jq( .secrets.AWS_SECRET )' image-id : 'jq( .ami )' region : 'jq( .region )' instance-type : 'jq( .instanceType )' transition : get-instance-ip transform : jq(.) # Query AWS for the public IP address of the instance - id : get-instance-ip log : jq(.) type : action action : function : get-vm secrets : [ 'AWS_KEY' , 'AWS_SECRET' ] input : access-key : jq( .secrets.AWS_KEY ) access-secret : jq( .secrets.AWS_SECRET ) command : - '--region' - jq( .region ) - 'ec2' - 'describe-instances' - '--filters' - 'Name=instance-state-name,Values=running' - jq( \"Name=instance-id,Values=\" + .return.Instances[0].InstanceId ) - '--query' - 'Reservations[*].Instances[*].[PublicIpAddress]' - '--output' - 'json' transform : jq(.address = .return[0][0][0]) transition : add-dns-record # Add an 'A' DNS record - id : add-dns-record type : action log : jq(.) action : function : add-dns-record input : domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) transition : send-email # Send a 'success' email - id : send-email type : action action : function : send-email input : recipient : jq(.recipient) domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) Workflow #2 - Add DNS Record The add-dns-record contains 2 states. The first, validate , simply ensures that the provided input matches the expected schema, and will cause the workflow to fail otherwise. The second state, api-request , sends a custom HTTP PATCH request to GoDaddy, instructing it to create a new DNS record pointing to the IP address of the newly created VM. id : add-dns-record description : Add an A DNS record to the specified domain on GoDaddy. functions : - id : req image : direktiv/request:v1 type : reusable states : # Validate input - id : validate type : validate log : '.' transition : api-request schema : type : object required : - domain - subdomain - address additionalProperties : false properties : domain : type : string subdomain : type : string address : type : string # Send GoDaddy API request - id : api-request type : action action : secrets : [ \"GODADDY_KEY\" , \"GODADDY_SECRET\" ] function : req input : method : \"PATCH\" url : jq( \"https://api.godaddy.com/v1/domains/\" + .domain + \"/records\" ) headers : \"Content-Type\" : \"application/json\" \"Authorization\" : jq( \"sso-key \" + .secrets.GODADDY_KEY + \":\" + .secrets.GODADDY_SECRET ) body : - data : jq( .address ) name : jq( .subdomain ) ttl : 3600 type : \"A\" Workflow #3 - Send Email This workflow is only called once the instance is successfully created and the DNS record is set. It contains only 2 states. The validate state operates in the same way as the validate state of the add-dns-record workflow. The send-email state uses the direktiv/smtp:v1 isolate to generate and send and email to the specified email address. id : send-email description : Sends an email to the specified user informing them of successful VM / DNS setup. functions : - id : send-email image : direktiv/smtp:v1 type : reusable states : # Validate input - id : validate type : validate schema : type : object required : - recipient - domain - subdomain - address additionalProperties : false properties : recipient : type : string domain : type : string subdomain : type : string address : type : string transition : send-email # Send email - id : send-email type : action action : function : send-email secrets : [ \"SMTP_USER\" , \"SMTP_PASSWORD\" ] input : to : jq( .recipient ) subject : 'Success!' message : jq( \"Instance creation successful. Created DNS record pointing \" + .subdomain + \".\" + .domain + \" to \" + .address + \"!\" ) from : jq( .secrets.SMTP_USER ) password : jq( .secrets.SMTP_PASSWORD ) server : \"smtp.gmail.com\" port : 587 Finishing up Hopefully this article has illustrated how to use pre-existing direktiv isolates to bootstrap workflow development! It should also serve as a reminder that, by making a workflow 'modular' through the use of subflows, a complicated workflow can be made to appear quite straightforward. If you're interested in seeing what other isolates already exist, check out the direktiv-apps GitHub page . To learn how to write your own custom isolates, click here","title":"Create VM and DNS Record"},{"location":"examples/create-vm-set-dns/#creating-a-vm-and-a-dns-record","text":"There are a number of pre-made 'isolate' containers designed specifically to bootstrap workflow development with direktiv. In this article, the following four isolates will be used: direktiv/aws-ec2-create:v1 Creates an instance on Amazon Web Services EC2 direktiv/awsgo:v1 Wraps the AWS CLI, enabling the use of any existing AWS CLI command in a workflow direktiv/request:v1 Sends a custom HTTP request direktiv/smtp:v1 Sends an email To keep everything clean, this workflow will actually be split up in to 1 'main' workflow and 2 'subflows' that the main workflow calls. The following 'secrets' must be configured, in order to authenticate with various services: AWS_KEY AWS access key AWS_SECRET AWS access key secret GODADDY_KEY GoDaddy API Key GODADDY_SECRET GoDaddy API Key Secret SMTP_USER The 'sender' address, used to authenticate with the SMTP server. SMTP_PASSWORD Password for the SMTP user.","title":"Creating a VM and a DNS Record"},{"location":"examples/create-vm-set-dns/#workflow-1-main","text":"This workflow creates an AWS EC2 instance, retrieves it's public IP address, and invokes the add-dns-record and send-email subflows. The first state, set-input , is included purely as a matter of convenience, as it allows us to execute the workflow without needing to remember to provide it with an input struct. id : create-vm-with-dns description : Creates an instance on AWS EC2, add a DNS record to GoDaddy, and posts a CloudEvent on completion. functions : - id : create-vm image : direktiv/aws-ec2-create:v1 type : reusable size : medium - id : get-vm image : direktiv/awsgo:v1 type : reusable - id : add-dns-record type : subflow workflow : add-dns-record - id : send-email type : subflow workflow : send-email states : # Set data (skips need for providing an input object on each invocation) - id : set-input type : noop transform : jq(.ami = \"ami-093d266a\" | .region = \"ap-southeast-2\" | .instanceType = \"t1.micro\" | .recipient = \"john.doe@example.com\" | .subdomain = \"direktiv\" | .domain = \"mydomain.com\") transition : create-instance # Create the AWS EC2 Instance - id : create-instance log : jq(.) type : action action : function : create-vm secrets : [ 'AWS_KEY' , 'AWS_SECRET' ] input : access-key : 'jq( .secrets.AWS_KEY )' access-secret : 'jq( .secrets.AWS_SECRET )' image-id : 'jq( .ami )' region : 'jq( .region )' instance-type : 'jq( .instanceType )' transition : get-instance-ip transform : jq(.) # Query AWS for the public IP address of the instance - id : get-instance-ip log : jq(.) type : action action : function : get-vm secrets : [ 'AWS_KEY' , 'AWS_SECRET' ] input : access-key : jq( .secrets.AWS_KEY ) access-secret : jq( .secrets.AWS_SECRET ) command : - '--region' - jq( .region ) - 'ec2' - 'describe-instances' - '--filters' - 'Name=instance-state-name,Values=running' - jq( \"Name=instance-id,Values=\" + .return.Instances[0].InstanceId ) - '--query' - 'Reservations[*].Instances[*].[PublicIpAddress]' - '--output' - 'json' transform : jq(.address = .return[0][0][0]) transition : add-dns-record # Add an 'A' DNS record - id : add-dns-record type : action log : jq(.) action : function : add-dns-record input : domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) transition : send-email # Send a 'success' email - id : send-email type : action action : function : send-email input : recipient : jq(.recipient) domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address)","title":"Workflow #1 - Main"},{"location":"examples/create-vm-set-dns/#workflow-2-add-dns-record","text":"The add-dns-record contains 2 states. The first, validate , simply ensures that the provided input matches the expected schema, and will cause the workflow to fail otherwise. The second state, api-request , sends a custom HTTP PATCH request to GoDaddy, instructing it to create a new DNS record pointing to the IP address of the newly created VM. id : add-dns-record description : Add an A DNS record to the specified domain on GoDaddy. functions : - id : req image : direktiv/request:v1 type : reusable states : # Validate input - id : validate type : validate log : '.' transition : api-request schema : type : object required : - domain - subdomain - address additionalProperties : false properties : domain : type : string subdomain : type : string address : type : string # Send GoDaddy API request - id : api-request type : action action : secrets : [ \"GODADDY_KEY\" , \"GODADDY_SECRET\" ] function : req input : method : \"PATCH\" url : jq( \"https://api.godaddy.com/v1/domains/\" + .domain + \"/records\" ) headers : \"Content-Type\" : \"application/json\" \"Authorization\" : jq( \"sso-key \" + .secrets.GODADDY_KEY + \":\" + .secrets.GODADDY_SECRET ) body : - data : jq( .address ) name : jq( .subdomain ) ttl : 3600 type : \"A\"","title":"Workflow #2 - Add DNS Record"},{"location":"examples/create-vm-set-dns/#workflow-3-send-email","text":"This workflow is only called once the instance is successfully created and the DNS record is set. It contains only 2 states. The validate state operates in the same way as the validate state of the add-dns-record workflow. The send-email state uses the direktiv/smtp:v1 isolate to generate and send and email to the specified email address. id : send-email description : Sends an email to the specified user informing them of successful VM / DNS setup. functions : - id : send-email image : direktiv/smtp:v1 type : reusable states : # Validate input - id : validate type : validate schema : type : object required : - recipient - domain - subdomain - address additionalProperties : false properties : recipient : type : string domain : type : string subdomain : type : string address : type : string transition : send-email # Send email - id : send-email type : action action : function : send-email secrets : [ \"SMTP_USER\" , \"SMTP_PASSWORD\" ] input : to : jq( .recipient ) subject : 'Success!' message : jq( \"Instance creation successful. Created DNS record pointing \" + .subdomain + \".\" + .domain + \" to \" + .address + \"!\" ) from : jq( .secrets.SMTP_USER ) password : jq( .secrets.SMTP_PASSWORD ) server : \"smtp.gmail.com\" port : 587","title":"Workflow #3 - Send Email"},{"location":"examples/create-vm-set-dns/#finishing-up","text":"Hopefully this article has illustrated how to use pre-existing direktiv isolates to bootstrap workflow development! It should also serve as a reminder that, by making a workflow 'modular' through the use of subflows, a complicated workflow can be made to appear quite straightforward. If you're interested in seeing what other isolates already exist, check out the direktiv-apps GitHub page . To learn how to write your own custom isolates, click here","title":"Finishing up"},{"location":"examples/event-based-greeting/","text":"Event-based Greeting Example This example demonstrates a workflow that waits for a greetingcloudevent event. When the event is received, a state will be triggered using the data provided by the event. The generate-greeting workflow generates the greetingcloudevent that the eventbased-greeting workflow is waiting for. Event Listener Workflow YAML id : eventbased-greeting functions : - id : greeter image : direktiv/greeting:v1 type : reusable start : type : event state : greeter event : type : greetingcloudevent description : \"A simple action that greets you\" states : - id : greeter type : action action : function : greeter input : jq(.greetingcloudevent) transform : 'jq({ \"greeting\": .return.greeting })' GenerateGreeting Workflow YAML id : generate-greeting description : \"Generate greeting event\" states : - id : gen type : generateEvent event : type : greetingcloudevent source : Direktiv data : name : \"Trent\"","title":"Event-based Greeting (StartEvent)"},{"location":"examples/event-based-greeting/#event-based-greeting-example","text":"This example demonstrates a workflow that waits for a greetingcloudevent event. When the event is received, a state will be triggered using the data provided by the event. The generate-greeting workflow generates the greetingcloudevent that the eventbased-greeting workflow is waiting for.","title":"Event-based Greeting Example"},{"location":"examples/event-based-greeting/#event-listener-workflow-yaml","text":"id : eventbased-greeting functions : - id : greeter image : direktiv/greeting:v1 type : reusable start : type : event state : greeter event : type : greetingcloudevent description : \"A simple action that greets you\" states : - id : greeter type : action action : function : greeter input : jq(.greetingcloudevent) transform : 'jq({ \"greeting\": .return.greeting })'","title":"Event Listener Workflow YAML"},{"location":"examples/event-based-greeting/#generategreeting-workflow-yaml","text":"id : generate-greeting description : \"Generate greeting event\" states : - id : gen type : generateEvent event : type : greetingcloudevent source : Direktiv data : name : \"Trent\"","title":"GenerateGreeting Workflow YAML"},{"location":"examples/event-based-transition/","text":"Check Credit Score This example demonstrates the use of a switch state in an event-based workflow. The state waits for the arrival of a checkcredit event, and conditionally 'approves' or 'rejects' a hypothetical loan request based on data included in the checkcredit event using a state. check-credit Workflow YAML id : check-credit start : type : event state : check-credit event : type : checkcredit states : - id : check-credit type : switch conditions : - condition : jq(.checkcredit.value > 500) transition : approve-loan defaultTransition : reject-loan - id : reject-loan type : noop transform : 'jq({ \"msg\": \"You have been rejected for this loan\" })' - id : approve-loan type : noop transform : 'jq({ \"msg\": \"You have been approved for this loan\" })' gen-credit Workflow YAML id : generate-credit description : \"Generate credit score event\" states : - id : gen type : generateEvent event : type : checkcredit source : Direktiv data : value : 501","title":"Check Credit Score (ConsumeEvent)"},{"location":"examples/event-based-transition/#check-credit-score","text":"This example demonstrates the use of a switch state in an event-based workflow. The state waits for the arrival of a checkcredit event, and conditionally 'approves' or 'rejects' a hypothetical loan request based on data included in the checkcredit event using a state.","title":"Check Credit Score"},{"location":"examples/event-based-transition/#check-credit-workflow-yaml","text":"id : check-credit start : type : event state : check-credit event : type : checkcredit states : - id : check-credit type : switch conditions : - condition : jq(.checkcredit.value > 500) transition : approve-loan defaultTransition : reject-loan - id : reject-loan type : noop transform : 'jq({ \"msg\": \"You have been rejected for this loan\" })' - id : approve-loan type : noop transform : 'jq({ \"msg\": \"You have been approved for this loan\" })'","title":"check-credit Workflow YAML"},{"location":"examples/event-based-transition/#gen-credit-workflow-yaml","text":"id : generate-credit description : \"Generate credit score event\" states : - id : gen type : generateEvent event : type : checkcredit source : Direktiv data : value : 501","title":"gen-credit Workflow YAML"},{"location":"examples/git-clone-go-build/","text":"Introduction This article seeks to demonstrate how Direktiv workflows can be used to clone a git repository, build a binary from the code contained within the repository, and upload it to Amazon S3. The following snippet is the start of the workflow definition, and details each of the functions that will be required within the workflow. id : build-go-binary functions : - id : go image : direktiv/go:v1 type : reusable files : - key : helloworld scope : instance type : tar.gz - id : git image : direktiv/git:v1 type : reusable - id : upload image : direktiv/amazon-upload:v1 type : reusable files : - key : helloworldserver scope : instance description : \"Clones a repository and builds a Go server.\" states : # continued in the next code block The git function will clone the entire repository and save it as an instance-scope variable called helloworld, which is then referenced by the go function (note that the go function definition references the helloworld variable in its files section). The upload function takes the output from the go function and uploads it to Amazon S3. Git Clone For the purposes of this demonstration, I've created a git repository that provides the code required to build the Go binary. The contents of the repository will be saved to an instance variable, to be accessed by subsequent functions/states. - id : clone-repo type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/helloworld.git $out/instance/helloworld\" ] transition : build-server Go Build This state runs the go isolate, which is currently only capable of running go build commands. Additionally functionality may be added to this isolate in the future. - id : build-server type : action action : function : go input : args : [ \"build\" , \"-o\" , \"helloworldserver\" ] execution-folder : helloworld variable : helloworldserver variable-type : instance transition : upload-binary Upload to S3 This container reads the contents of the helloworldserver instance-scope variable and uploads it to Amazon S3. - id : upload-binary type : action action : function : upload secrets : [ \"AWS_ACCESS_KEY\" , \"AWS_SECRET_KEY\" ] input : filename : helloworldserver bucket : direktiv region : us-east-1 upload-name : helloworldserver key : jq(.secrets.AWS_ACCESS_KEY) secret : jq(.secrets.AWS_SECRET_KEY) Final Workflow Putting all of the pieces together; this workflow clones a git repository, builds a go binary, and uploads the results to Amazon S3: id : build-go-binary functions : - id : go image : direktiv/go:v1 type : reusable files : - key : helloworld scope : instance type : tar.gz - id : git image : direktiv/git:v1 type : reusable - id : upload image : direktiv/amazon-upload:v1 type : reusable files : - key : helloworldserver scope : instance description : \"Clones a repository and builds a Go server.\" states : - id : clone-repo type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/helloworld.git $out/instance/helloworld\" ] transition : build-server - id : build-server type : action action : function : go input : args : [ \"build\" , \"-o\" , \"helloworldserver\" ] execution-folder : helloworld variable : helloworldserver variable-type : instance transition : upload-binary - id : upload-binary type : action action : function : upload secrets : [ \"AWS_ACCESS_KEY\" , \"AWS_SECRET_KEY\" ] input : filename : helloworldserver bucket : direktiv region : us-east-1 upload-name : helloworldserver key : \"jq(.secrets.AWS_ACCESS_KEY)\" secret : \"jq(.secrets.AWS_SECRET_KEY)\"","title":"Git Clone and Go Build"},{"location":"examples/git-clone-go-build/#introduction","text":"This article seeks to demonstrate how Direktiv workflows can be used to clone a git repository, build a binary from the code contained within the repository, and upload it to Amazon S3. The following snippet is the start of the workflow definition, and details each of the functions that will be required within the workflow. id : build-go-binary functions : - id : go image : direktiv/go:v1 type : reusable files : - key : helloworld scope : instance type : tar.gz - id : git image : direktiv/git:v1 type : reusable - id : upload image : direktiv/amazon-upload:v1 type : reusable files : - key : helloworldserver scope : instance description : \"Clones a repository and builds a Go server.\" states : # continued in the next code block The git function will clone the entire repository and save it as an instance-scope variable called helloworld, which is then referenced by the go function (note that the go function definition references the helloworld variable in its files section). The upload function takes the output from the go function and uploads it to Amazon S3.","title":"Introduction"},{"location":"examples/git-clone-go-build/#git-clone","text":"For the purposes of this demonstration, I've created a git repository that provides the code required to build the Go binary. The contents of the repository will be saved to an instance variable, to be accessed by subsequent functions/states. - id : clone-repo type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/helloworld.git $out/instance/helloworld\" ] transition : build-server","title":"Git Clone"},{"location":"examples/git-clone-go-build/#go-build","text":"This state runs the go isolate, which is currently only capable of running go build commands. Additionally functionality may be added to this isolate in the future. - id : build-server type : action action : function : go input : args : [ \"build\" , \"-o\" , \"helloworldserver\" ] execution-folder : helloworld variable : helloworldserver variable-type : instance transition : upload-binary","title":"Go Build"},{"location":"examples/git-clone-go-build/#upload-to-s3","text":"This container reads the contents of the helloworldserver instance-scope variable and uploads it to Amazon S3. - id : upload-binary type : action action : function : upload secrets : [ \"AWS_ACCESS_KEY\" , \"AWS_SECRET_KEY\" ] input : filename : helloworldserver bucket : direktiv region : us-east-1 upload-name : helloworldserver key : jq(.secrets.AWS_ACCESS_KEY) secret : jq(.secrets.AWS_SECRET_KEY)","title":"Upload to S3"},{"location":"examples/git-clone-go-build/#final-workflow","text":"Putting all of the pieces together; this workflow clones a git repository, builds a go binary, and uploads the results to Amazon S3: id : build-go-binary functions : - id : go image : direktiv/go:v1 type : reusable files : - key : helloworld scope : instance type : tar.gz - id : git image : direktiv/git:v1 type : reusable - id : upload image : direktiv/amazon-upload:v1 type : reusable files : - key : helloworldserver scope : instance description : \"Clones a repository and builds a Go server.\" states : - id : clone-repo type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/helloworld.git $out/instance/helloworld\" ] transition : build-server - id : build-server type : action action : function : go input : args : [ \"build\" , \"-o\" , \"helloworldserver\" ] execution-folder : helloworld variable : helloworldserver variable-type : instance transition : upload-binary - id : upload-binary type : action action : function : upload secrets : [ \"AWS_ACCESS_KEY\" , \"AWS_SECRET_KEY\" ] input : filename : helloworldserver bucket : direktiv region : us-east-1 upload-name : helloworldserver key : \"jq(.secrets.AWS_ACCESS_KEY)\" secret : \"jq(.secrets.AWS_SECRET_KEY)\"","title":"Final Workflow"},{"location":"examples/greeting/","text":"Greeting Example This simple example workflow uses a single action state to call the direktiv/greeting action, which 'greets' the user specified in the \"name\" field of the input provided to the workflow. Workflow YAML id : greeting functions : - id : greeter image : direktiv/greeting:v1 type : reusable description : \"A simple action that greets you\" states : - id : greeter type : action action : function : greeter input : jq(.) transform : 'jq({ \"greeting\": .return.greeting })' Input { \"name\" : \"Trent\" } Output The results of this action will contain a greeting addressed to the provided name. { \"return\" : { \"greeting\" : \"Welcome to Direktiv, Trent!\" } }","title":"Greeting (ActionState)"},{"location":"examples/greeting/#greeting-example","text":"This simple example workflow uses a single action state to call the direktiv/greeting action, which 'greets' the user specified in the \"name\" field of the input provided to the workflow.","title":"Greeting Example"},{"location":"examples/greeting/#workflow-yaml","text":"id : greeting functions : - id : greeter image : direktiv/greeting:v1 type : reusable description : \"A simple action that greets you\" states : - id : greeter type : action action : function : greeter input : jq(.) transform : 'jq({ \"greeting\": .return.greeting })'","title":"Workflow YAML"},{"location":"examples/greeting/#input","text":"{ \"name\" : \"Trent\" }","title":"Input"},{"location":"examples/greeting/#output","text":"The results of this action will contain a greeting addressed to the provided name. { \"return\" : { \"greeting\" : \"Welcome to Direktiv, Trent!\" } }","title":"Output"},{"location":"examples/logging/","text":"Logging Though Direktiv provides instance logs viewable on the instance's information page, it won't keep them forever. Sending logs to a third-party logging service can be useful for taking control of your data. Here's how we can do it. The 'log' Parameter Every state in a workflow definition supports an optional log parameter. If this parameter is defined, Direktiv will evaluate it as a jq query against the instance data before executing the state's logic, and send the results to the instance logs. CloudEvents From Logs Workflows can be configured to generate CloudEvents on their namespace anytime the log parameter produces data. Look for a field called \"Log To Event\" on the workflow definition (YAML) UI page. If this field is set to anything other than an empty string CloudEvents will be generated with an additional extension logger set to the value saved here. Using this it's easy to capture instance logs and do whatever you want with them. Create another workflow that is triggered by matching CloudEvents, then send them to your own Logstash server, for example. id : helloworld states : - id : hello type : noop transform : 'jq({ result: \"Hello World!\" })' log : '\"Hello, logger!\"' This workflow, configured to Log To Event to mylog , will produce a CloudEvent like the following: { \"specversion\" : \"1.0\" , \"type\" : \"direktiv.instanceLog\" , \"source\" : \"4e455e04-dfcc-4d7d-90b3-37e00988335d\" , \"id\" : \"a9a07797-fe2a-490e-ba61-61b86e61d6c0\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"logger\" : \"mylog\" , \"datacontenttype\" : \"text/json\" , \"data\" : \"Hello, logger!\" } Google Cloud Platform Stackdriver Example This workflow listens for the logger gcpLogger and sends it to gcp: id : gcp-logger functions : - id : send-log image : direktiv/gcplog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : gcpLogger states : - id : log type : action action : function : send-log secrets : [ GCP_SERVICEACCOUNTKEY ] input : serviceAccountKey : jq(.secrets.GCP_SERVICEACCOUNTKEY) \"project-id\" : \"direktiv\" \"log-name\" : \"direktiv-log\" message : jq(.\"direktiv.instanceLog\") AWS Cloudwatch Example This workflow listens for the logger awsLogger and sends it to aws: id : aws-logger functions : - id : send-log image : direktiv/awslog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : awsLogger states : - id : log type : action action : function : send-log secrets : [ AWS_KEY , AWS_SECRET ] input : key : jq(.secrets.AWS_KEY) secret : jq(.secrets.AWS_SECRET) region : \"us-east-2\" \"log-group\" : \"direktiv\" \"log-stream\" : \"direktiv\" message : jq(.\"direktiv.instanceLog\") Azure Log Analytics Example This workflow listens for the logger azureLogger and sends it to azure: id : azure-logger functions : - id : send-log image : direktiv/azlog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : azure-logger states : - id : log type : action action : function : send-log secrets : [ AZURE_WORKSPACE_ID , AZURE_WORKSPACE_KEY ] input : \"workspace-id\" : jq(.secrets.AZURE_WORKSPACE_ID) key : jq(.secrets.AZURE_WORKSPACE_KEY) type : \"direktiv-log\" message : jq(.\"direktiv.instanceLog\") Other Providers These three cloud examples serve to demonstrate how this all works, but they're not the only options. Following this pattern you can log any way you like.","title":"Logging"},{"location":"examples/logging/#logging","text":"Though Direktiv provides instance logs viewable on the instance's information page, it won't keep them forever. Sending logs to a third-party logging service can be useful for taking control of your data. Here's how we can do it.","title":"Logging"},{"location":"examples/logging/#the-log-parameter","text":"Every state in a workflow definition supports an optional log parameter. If this parameter is defined, Direktiv will evaluate it as a jq query against the instance data before executing the state's logic, and send the results to the instance logs.","title":"The 'log' Parameter"},{"location":"examples/logging/#cloudevents-from-logs","text":"Workflows can be configured to generate CloudEvents on their namespace anytime the log parameter produces data. Look for a field called \"Log To Event\" on the workflow definition (YAML) UI page. If this field is set to anything other than an empty string CloudEvents will be generated with an additional extension logger set to the value saved here. Using this it's easy to capture instance logs and do whatever you want with them. Create another workflow that is triggered by matching CloudEvents, then send them to your own Logstash server, for example. id : helloworld states : - id : hello type : noop transform : 'jq({ result: \"Hello World!\" })' log : '\"Hello, logger!\"' This workflow, configured to Log To Event to mylog , will produce a CloudEvent like the following: { \"specversion\" : \"1.0\" , \"type\" : \"direktiv.instanceLog\" , \"source\" : \"4e455e04-dfcc-4d7d-90b3-37e00988335d\" , \"id\" : \"a9a07797-fe2a-490e-ba61-61b86e61d6c0\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"logger\" : \"mylog\" , \"datacontenttype\" : \"text/json\" , \"data\" : \"Hello, logger!\" }","title":"CloudEvents From Logs"},{"location":"examples/logging/#google-cloud-platform-stackdriver-example","text":"This workflow listens for the logger gcpLogger and sends it to gcp: id : gcp-logger functions : - id : send-log image : direktiv/gcplog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : gcpLogger states : - id : log type : action action : function : send-log secrets : [ GCP_SERVICEACCOUNTKEY ] input : serviceAccountKey : jq(.secrets.GCP_SERVICEACCOUNTKEY) \"project-id\" : \"direktiv\" \"log-name\" : \"direktiv-log\" message : jq(.\"direktiv.instanceLog\")","title":"Google Cloud Platform Stackdriver Example"},{"location":"examples/logging/#aws-cloudwatch-example","text":"This workflow listens for the logger awsLogger and sends it to aws: id : aws-logger functions : - id : send-log image : direktiv/awslog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : awsLogger states : - id : log type : action action : function : send-log secrets : [ AWS_KEY , AWS_SECRET ] input : key : jq(.secrets.AWS_KEY) secret : jq(.secrets.AWS_SECRET) region : \"us-east-2\" \"log-group\" : \"direktiv\" \"log-stream\" : \"direktiv\" message : jq(.\"direktiv.instanceLog\")","title":"AWS Cloudwatch Example"},{"location":"examples/logging/#azure-log-analytics-example","text":"This workflow listens for the logger azureLogger and sends it to azure: id : azure-logger functions : - id : send-log image : direktiv/azlog:v1 type : reusable start : type : event event : type : direktiv.instanceLog filters : logger : azure-logger states : - id : log type : action action : function : send-log secrets : [ AZURE_WORKSPACE_ID , AZURE_WORKSPACE_KEY ] input : \"workspace-id\" : jq(.secrets.AZURE_WORKSPACE_ID) key : jq(.secrets.AZURE_WORKSPACE_KEY) type : \"direktiv-log\" message : jq(.\"direktiv.instanceLog\")","title":"Azure Log Analytics Example"},{"location":"examples/logging/#other-providers","text":"These three cloud examples serve to demonstrate how this all works, but they're not the only options. Following this pattern you can log any way you like.","title":"Other Providers"},{"location":"examples/md-translation/","text":"Introduction Today we will be creating a workflow that takes a string of markdown and convert it to Spanish and German. It will showcase how to use a foreach state to run the workflow. First we will need the 'google-translator' container to convert what is being passed to a different language. id : translate-md description : Translates a string into different languages functions : - id : translate image : direktiv/google-translator:v1 type : reusable states : # continued in next code block Google Translate Next we'll define a state that gets passed an array of strings. Where we pass each element in the string array as an object with the property 'id' so JQ can interpret it. - id : translate-markdown type : foreach array : \"jq(.langs[] | {id: .})\" action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : translate input : serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) target-language : jq(.id) message : \"# Hello\\n\\n ## World! \\n\\n This is a test message that will get converted to a different language.\" Full Example Joining every part above we end up with the following workflow which takes the input as below. { \"langs\" : [ \"es\" , \"de\" ] } For a reference to what you can translate check out this page id : translate-md description : Translates a string into different languages functions : - id : translate image : direktiv/google-translator:v1 type : reusable states : - id : translate-markdown type : foreach array : \"jq(.langs[] | {id: .})\" action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : translate input : serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) target-language : jq(.id) message : \"# Hello\\n\\n ## World! \\n\\n This is a test message that will get converted to a different language.\"","title":"Convert Markdown to Multiple Languages"},{"location":"examples/md-translation/#introduction","text":"Today we will be creating a workflow that takes a string of markdown and convert it to Spanish and German. It will showcase how to use a foreach state to run the workflow. First we will need the 'google-translator' container to convert what is being passed to a different language. id : translate-md description : Translates a string into different languages functions : - id : translate image : direktiv/google-translator:v1 type : reusable states : # continued in next code block","title":"Introduction"},{"location":"examples/md-translation/#google-translate","text":"Next we'll define a state that gets passed an array of strings. Where we pass each element in the string array as an object with the property 'id' so JQ can interpret it. - id : translate-markdown type : foreach array : \"jq(.langs[] | {id: .})\" action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : translate input : serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) target-language : jq(.id) message : \"# Hello\\n\\n ## World! \\n\\n This is a test message that will get converted to a different language.\"","title":"Google Translate"},{"location":"examples/md-translation/#full-example","text":"Joining every part above we end up with the following workflow which takes the input as below. { \"langs\" : [ \"es\" , \"de\" ] } For a reference to what you can translate check out this page id : translate-md description : Translates a string into different languages functions : - id : translate image : direktiv/google-translator:v1 type : reusable states : - id : translate-markdown type : foreach array : \"jq(.langs[] | {id: .})\" action : secrets : [ \"SERVICE_ACCOUNT_KEY\" ] function : translate input : serviceAccountKey : jq(.secrets.SERVICE_ACCOUNT_KEY) target-language : jq(.id) message : \"# Hello\\n\\n ## World! \\n\\n This is a test message that will get converted to a different language.\"","title":"Full Example"},{"location":"examples/non-json-input/","text":"Handling non-JSON input data Workflows can be invoked with input data that will be available as instance data. The format of the data being provided as input data dictates how it will be available as instance data. JSON Input Data If the input is provided as a JSON object, it will be unchanged when converted to instance data. Example Input Data { \"key\" : \"value\" } Instance Data { \"key\" : \"value\" } Non-JSON Input Data If the input is provided in a format that is not JSON, it will be base64 encoded into a string and stored as the value for the \"input\" key of the resulting JSON object. This will happen if, for example, the input data provided is a binary file. Example Input Data Hello , world! Instance Data { \"input\" : \"SGVsbG8sIHdvcmxkIQ==\" }","title":"Handling Non-JSON Input"},{"location":"examples/non-json-input/#handling-non-json-input-data","text":"Workflows can be invoked with input data that will be available as instance data. The format of the data being provided as input data dictates how it will be available as instance data.","title":"Handling non-JSON input data"},{"location":"examples/non-json-input/#json-input-data","text":"If the input is provided as a JSON object, it will be unchanged when converted to instance data.","title":"JSON Input Data"},{"location":"examples/non-json-input/#example","text":"","title":"Example"},{"location":"examples/non-json-input/#input-data","text":"{ \"key\" : \"value\" }","title":"Input Data"},{"location":"examples/non-json-input/#instance-data","text":"{ \"key\" : \"value\" }","title":"Instance Data"},{"location":"examples/non-json-input/#non-json-input-data","text":"If the input is provided in a format that is not JSON, it will be base64 encoded into a string and stored as the value for the \"input\" key of the resulting JSON object. This will happen if, for example, the input data provided is a binary file.","title":"Non-JSON Input Data"},{"location":"examples/non-json-input/#example_1","text":"","title":"Example"},{"location":"examples/non-json-input/#input-data_1","text":"Hello , world!","title":"Input Data"},{"location":"examples/non-json-input/#instance-data_1","text":"{ \"input\" : \"SGVsbG8sIHdvcmxkIQ==\" }","title":"Instance Data"},{"location":"examples/parallel-execution/","text":"Parallel Execution and Wait Example This example demonstrates the use of parallel subflows that must all complete before the run state will succeed. A hypothetical scenario where this approach may be used could involve a CI/CD process for which 3 different binaries are built (one each on Windows, Linux, and Mac) before creating a new product release. The run workflow will wait until all three subflows have received an event before proceeding. waiting Workflow YAML id : waiting functions : - id : wait-for-windows type : subflow workflow : wait-for-windows - id : wait-for-linux type : subflow workflow : wait-for-linux - id : wait-for-mac type : subflow workflow : wait-for-mac states : - id : run type : parallel actions : - function : wait-for-windows - function : wait-for-linux - function : wait-for-mac mode : and wait-for Workflow YAML Replace {OS} with windows , mac , and linux , to create the 3 subflows referenced by the run state. id : wait-for-{OS} states : - id : wait-for-event type : consumeEvent event : type : gen-event-{OS} generateEvent Workflow YAML Replace {OS} with windows , mac and linux to create workflows that will generate the events that the previous three subflows are waiting to receive. id : send-event-for-{OS} states : - id : send-event type : generateEvent event : type : gen-event-{OS} source : direktiv This example defines 7 workflows: * waiting * wait-for-linux * wait-for-windows * wait-for-mac * send-event-for-linux * send-event-for-windows * send-event-for-mac Executing the waiting workflow will begin the three wait-for-{OS} workflows. The waiting instance will not continue past its run state until all three send-event-for-{OS} workflows are executed.","title":"Parallel Execution (Parallel)"},{"location":"examples/parallel-execution/#parallel-execution-and-wait-example","text":"This example demonstrates the use of parallel subflows that must all complete before the run state will succeed. A hypothetical scenario where this approach may be used could involve a CI/CD process for which 3 different binaries are built (one each on Windows, Linux, and Mac) before creating a new product release. The run workflow will wait until all three subflows have received an event before proceeding.","title":"Parallel Execution and Wait Example"},{"location":"examples/parallel-execution/#waiting-workflow-yaml","text":"id : waiting functions : - id : wait-for-windows type : subflow workflow : wait-for-windows - id : wait-for-linux type : subflow workflow : wait-for-linux - id : wait-for-mac type : subflow workflow : wait-for-mac states : - id : run type : parallel actions : - function : wait-for-windows - function : wait-for-linux - function : wait-for-mac mode : and","title":"waiting Workflow YAML"},{"location":"examples/parallel-execution/#wait-for-workflow-yaml","text":"Replace {OS} with windows , mac , and linux , to create the 3 subflows referenced by the run state. id : wait-for-{OS} states : - id : wait-for-event type : consumeEvent event : type : gen-event-{OS}","title":"wait-for Workflow YAML"},{"location":"examples/parallel-execution/#generateevent-workflow-yaml","text":"Replace {OS} with windows , mac and linux to create workflows that will generate the events that the previous three subflows are waiting to receive. id : send-event-for-{OS} states : - id : send-event type : generateEvent event : type : gen-event-{OS} source : direktiv This example defines 7 workflows: * waiting * wait-for-linux * wait-for-windows * wait-for-mac * send-event-for-linux * send-event-for-windows * send-event-for-mac Executing the waiting workflow will begin the three wait-for-{OS} workflows. The waiting instance will not continue past its run state until all three send-event-for-{OS} workflows are executed.","title":"generateEvent Workflow YAML"},{"location":"examples/regex-check-for-mobile/","text":"Introduction We're going to be creating two workflows. Send an email and Trigger a cloud event Read the email and use regex to workout the mobile number provided. Which will then send an SMS to the number saying we are out of the office at the moment. Send Email and Trigger Event To execute this workflow we need to define some functions the following are defined. smtp sends an email request send a http request id : send-mobile-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : This workflow sends an email and triggers an event. states : # continued in next code block Send Email This action state uses the smtp container to send an email. For example purposes you will notice that I am sendin the email to myself. - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"Hello my name is Trent Hilliam please msg me on +INSERT_MOBILE_NUMBER.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent Trigger Event This generateEvent state sends a cloud event to a namespace. - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp-mobile Workflow Send Email Full Example id : send-mobile-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"Hello my name is Trent Hilliam please msg me on +61430545789.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp-mobile Read Mail and Send SMS To execute this workflow we need to define some functions the following are defined. imap reads the first email message received regex takes a regex string and a string and returns the values the regex matches twilio sends an sms or an email id : listen-for-email-mobile description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : regex image : direktiv/regex:v1 type : reusable - id : twilio image : direktiv/twilio:v1 type : reusable start : type : event state : read-mail event : type : smtp-mobile states : # continued in next code block Read Email This takes the first message from your \"INBOX\" email and reads the body and outputs it. - id : read-mail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : regex-check Check Regex The following state uses the regex container we're provided the regex of '+[0-9]{1,2}[0-9]{9}' and it should return the results from the msg being provided. - id : regex-check type : action action : function : regex input : msg : jq(.return.msg) regex : \"\\\\+[0-9]{1,2}[0-9]{9}\" transition : send-sms transform : number : jq(.return.results[0]) Send SMS This uses a 'twilio' container to send a message to the emailee. - id : send-sms type : action action : secrets : [ \"TWILIO_SID\" , \"TWILIO_TOKEN\" , \"TWILIO_PROVIDED_NUMBER\" ] function : twilio input : typeof : sms sid : jq(.secrets.TWILIO_SID) token : jq(.secrets.TWILIO_TOKEN) message : \"Hey you just emailed but I am currently out of office.\" from : jq(.secrets.TWILIO_PROVIDED_NUMBER) to : jq(.number) Workflow Read Email and Send SMS id : listen-for-email-mobile description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : regex image : direktiv/regex:v1 type : reusable - id : twilio image : direktiv/twilio:v1 type : reusable start : type : event state : read-mail event : type : smtp-mobile states : - id : read-mail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : regex-check - id : regex-check type : action action : function : regex input : msg : jq(.return.msg) regex : \"\\\\+[0-9]{1,2}[0-9]{9}\" transition : send-sms transform : number : jq(.return.results[0]) - id : send-sms type : action action : secrets : [ \"TWILIO_SID\" , \"TWILIO_TOKEN\" , \"TWILIO_PROVIDED_NUMBER\" ] function : twilio input : typeof : sms sid : jq(.secrets.TWILIO_SID) token : jq(.secrets.TWILIO_TOKEN) message : \"Hey you just emailed but I am currently out of office.\" from : jq(.secrets.TWILIO_PROVIDED_NUMBER) to : jq(.number)","title":"Check Email for Mobile Number"},{"location":"examples/regex-check-for-mobile/#introduction","text":"We're going to be creating two workflows. Send an email and Trigger a cloud event Read the email and use regex to workout the mobile number provided. Which will then send an SMS to the number saying we are out of the office at the moment.","title":"Introduction"},{"location":"examples/regex-check-for-mobile/#send-email-and-trigger-event","text":"To execute this workflow we need to define some functions the following are defined. smtp sends an email request send a http request id : send-mobile-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable - id : request image : direktiv/request:v1 type : reusable description : This workflow sends an email and triggers an event. states : # continued in next code block","title":"Send Email and Trigger Event"},{"location":"examples/regex-check-for-mobile/#send-email","text":"This action state uses the smtp container to send an email. For example purposes you will notice that I am sendin the email to myself. - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"Hello my name is Trent Hilliam please msg me on +INSERT_MOBILE_NUMBER.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent","title":"Send Email"},{"location":"examples/regex-check-for-mobile/#trigger-event","text":"This generateEvent state sends a cloud event to a namespace. - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp-mobile","title":"Trigger Event"},{"location":"examples/regex-check-for-mobile/#workflow-send-email-full-example","text":"id : send-mobile-trigger-event functions : - id : smtp image : direktiv/smtp:v1 type : reusable description : This workflow sends an email and triggers an event. states : - id : sendemail type : action action : function : smtp secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] input : to : jq(.secrets.EMAIL_ADDRESS) subject : \"Your Review\" message : \"Hello my name is Trent Hilliam please msg me on +61430545789.\" from : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) server : smtp.gmail.com port : 587 transition : sendcloudevent - id : sendcloudevent type : generateEvent event : source : direktiv type : smtp-mobile","title":"Workflow Send Email Full Example"},{"location":"examples/regex-check-for-mobile/#read-mail-and-send-sms","text":"To execute this workflow we need to define some functions the following are defined. imap reads the first email message received regex takes a regex string and a string and returns the values the regex matches twilio sends an sms or an email id : listen-for-email-mobile description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : regex image : direktiv/regex:v1 type : reusable - id : twilio image : direktiv/twilio:v1 type : reusable start : type : event state : read-mail event : type : smtp-mobile states : # continued in next code block","title":"Read Mail and Send SMS"},{"location":"examples/regex-check-for-mobile/#read-email","text":"This takes the first message from your \"INBOX\" email and reads the body and outputs it. - id : read-mail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : regex-check","title":"Read Email"},{"location":"examples/regex-check-for-mobile/#check-regex","text":"The following state uses the regex container we're provided the regex of '+[0-9]{1,2}[0-9]{9}' and it should return the results from the msg being provided. - id : regex-check type : action action : function : regex input : msg : jq(.return.msg) regex : \"\\\\+[0-9]{1,2}[0-9]{9}\" transition : send-sms transform : number : jq(.return.results[0])","title":"Check Regex"},{"location":"examples/regex-check-for-mobile/#send-sms","text":"This uses a 'twilio' container to send a message to the emailee. - id : send-sms type : action action : secrets : [ \"TWILIO_SID\" , \"TWILIO_TOKEN\" , \"TWILIO_PROVIDED_NUMBER\" ] function : twilio input : typeof : sms sid : jq(.secrets.TWILIO_SID) token : jq(.secrets.TWILIO_TOKEN) message : \"Hey you just emailed but I am currently out of office.\" from : jq(.secrets.TWILIO_PROVIDED_NUMBER) to : jq(.number)","title":"Send SMS"},{"location":"examples/regex-check-for-mobile/#workflow-read-email-and-send-sms","text":"id : listen-for-email-mobile description : This workflow reads an email when a cloud event is received. functions : - id : imap image : direktiv/imap:v1 type : reusable - id : regex image : direktiv/regex:v1 type : reusable - id : twilio image : direktiv/twilio:v1 type : reusable start : type : event state : read-mail event : type : smtp-mobile states : - id : read-mail type : action action : secrets : [ \"EMAIL_ADDRESS\" , \"EMAIL_PASSWORD\" ] function : imap input : email : jq(.secrets.EMAIL_ADDRESS) password : jq(.secrets.EMAIL_PASSWORD) imap-address : \"imap.gmail.com:993\" transition : regex-check - id : regex-check type : action action : function : regex input : msg : jq(.return.msg) regex : \"\\\\+[0-9]{1,2}[0-9]{9}\" transition : send-sms transform : number : jq(.return.results[0]) - id : send-sms type : action action : secrets : [ \"TWILIO_SID\" , \"TWILIO_TOKEN\" , \"TWILIO_PROVIDED_NUMBER\" ] function : twilio input : typeof : sms sid : jq(.secrets.TWILIO_SID) token : jq(.secrets.TWILIO_TOKEN) message : \"Hey you just emailed but I am currently out of office.\" from : jq(.secrets.TWILIO_PROVIDED_NUMBER) to : jq(.number)","title":"Workflow Read Email and Send SMS"},{"location":"examples/respond-to-events/","text":"Reacting to (\"consuming\") Cloud Events Workflows can be triggered in a number of ways; by default they must be manually triggered, but with the correct configuration a workflow will start each time a cloud event reaches the parent namespace that satisfies the constraints detailed in the workflow definition. To demonstrate this, let's modify the 'main' workflow from this article , removing the send-email state and replacing it with a state that will generate an event. ... # Add an 'A' DNS record - id : add-dns-record type : action log : jq(.) action : function : add-dns-record input : domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) transition : generate-event # Send a custom event that will trigger the next workflow - id : generate-event type : generateEvent event : type : example.vm.created source : exampleWorkflow data : address : jq(.address) host : \"jq(.subdomain).jq(.domain)\" recipient : jq(.recipient) domain : jq(.domain) subdomain : jq(.subdomain) region : jq(.region) Now that the main workflow will generate a cloud event on completion, we require a workflow that will react to it. This workflow will extract information from the body of the caught event and submit a new record to a 'FreshService' service before sending the 'success' email to the specified user. id : consume-new-vm-event # Start workflow when correct event occurs start : type : event event : type : example.vm.created filters : source : exampleWorkflow functions : - id : query-fresh-service-cmdb image : direktiv/request:v1 type : reusable - id : send-email type : subflow workflow : send-email states : # Submit new record to FreshService - id : push-instance-2-cmdb type : action action : function : query-fresh-service-cmdb input : url : \"https://direktiv.freshservice.com/cmdb/items.json\" method : \"POST\" body : cmdb_config_item : name : jq(.\"example.vm.created\".host) ci_type_id : \"75000270995\" level_field_attributes : aws_region_75000270981 : jq(.\"example.vm.created\".region) availability_zone_75000270981 : jq(.\"example.vm.created\".region) instance_id_75000270995 : jq(.\"example.vm.created\".host) public_ip_75000270995 : jq(.\"example.vm.created\".address) public_dns_75000270995 : jq(.\"example.vm.created\".host) instance_state_75000270995 : \"created\" headers : \"Content-Type\" : \"application/json\" Authorization : \"Basic <EXAMPLE_AUTHORISATION>\" transform : jq(.msg = .return.item.config_item | del(.return)) transition : send-email # Send a 'success' email - id : send-email log : jq(.) type : action action : function : send-email input : recipient : jq(.\"example.vm.created\".recipient) domain : jq(.\"example.vm.created\".domain) subdomain : jq(.\"example.vm.created\".subdomain) address : jq(.\"example.vm.created\".address) Note: The value of the type and source fields defined in the start configuration of this workflow must match the corresponding fields of an incoming cloud event. After making these changes, trigger the main workflow ( create-vm-with-dns ). When complete, an instance will be created for the consume-new-vm-event workflow. All of the data provided to the data field of the generated event is accessible to the receiving workflow, as children of the example.vm.created field (the name of the field corresponds to the event type ).","title":"Reacting to Consuming Cloud Events"},{"location":"examples/respond-to-events/#reacting-to-consuming-cloud-events","text":"Workflows can be triggered in a number of ways; by default they must be manually triggered, but with the correct configuration a workflow will start each time a cloud event reaches the parent namespace that satisfies the constraints detailed in the workflow definition. To demonstrate this, let's modify the 'main' workflow from this article , removing the send-email state and replacing it with a state that will generate an event. ... # Add an 'A' DNS record - id : add-dns-record type : action log : jq(.) action : function : add-dns-record input : domain : jq(.domain) subdomain : jq(.subdomain) address : jq(.address) transition : generate-event # Send a custom event that will trigger the next workflow - id : generate-event type : generateEvent event : type : example.vm.created source : exampleWorkflow data : address : jq(.address) host : \"jq(.subdomain).jq(.domain)\" recipient : jq(.recipient) domain : jq(.domain) subdomain : jq(.subdomain) region : jq(.region) Now that the main workflow will generate a cloud event on completion, we require a workflow that will react to it. This workflow will extract information from the body of the caught event and submit a new record to a 'FreshService' service before sending the 'success' email to the specified user. id : consume-new-vm-event # Start workflow when correct event occurs start : type : event event : type : example.vm.created filters : source : exampleWorkflow functions : - id : query-fresh-service-cmdb image : direktiv/request:v1 type : reusable - id : send-email type : subflow workflow : send-email states : # Submit new record to FreshService - id : push-instance-2-cmdb type : action action : function : query-fresh-service-cmdb input : url : \"https://direktiv.freshservice.com/cmdb/items.json\" method : \"POST\" body : cmdb_config_item : name : jq(.\"example.vm.created\".host) ci_type_id : \"75000270995\" level_field_attributes : aws_region_75000270981 : jq(.\"example.vm.created\".region) availability_zone_75000270981 : jq(.\"example.vm.created\".region) instance_id_75000270995 : jq(.\"example.vm.created\".host) public_ip_75000270995 : jq(.\"example.vm.created\".address) public_dns_75000270995 : jq(.\"example.vm.created\".host) instance_state_75000270995 : \"created\" headers : \"Content-Type\" : \"application/json\" Authorization : \"Basic <EXAMPLE_AUTHORISATION>\" transform : jq(.msg = .return.item.config_item | del(.return)) transition : send-email # Send a 'success' email - id : send-email log : jq(.) type : action action : function : send-email input : recipient : jq(.\"example.vm.created\".recipient) domain : jq(.\"example.vm.created\".domain) subdomain : jq(.\"example.vm.created\".subdomain) address : jq(.\"example.vm.created\".address) Note: The value of the type and source fields defined in the start configuration of this workflow must match the corresponding fields of an incoming cloud event. After making these changes, trigger the main workflow ( create-vm-with-dns ). When complete, an instance will be created for the consume-new-vm-event workflow. All of the data provided to the data field of the generated event is accessible to the receiving workflow, as children of the example.vm.created field (the name of the field corresponds to the event type ).","title":"Reacting to (\"consuming\") Cloud Events"},{"location":"examples/scheduled-example/","text":"Scheduled Cronjob Workflow This example demonstrates a workflow that triggers every minute via a cron setup. You can setup the cron to trigger at certain periods of time via the start field. * * * * * means it will trigger each minute not every minute. Scheduled Cronjob Workflow YAML start : type : scheduled cron : '* * * * *' description : A simple 'no-op' state that returns 'Hello world!' states : - id : helloworld type : noop transform : result : Hello world!","title":"Scheduling (ScheduledStartType)"},{"location":"examples/scheduled-example/#scheduled-cronjob-workflow","text":"This example demonstrates a workflow that triggers every minute via a cron setup. You can setup the cron to trigger at certain periods of time via the start field. * * * * * means it will trigger each minute not every minute.","title":"Scheduled Cronjob Workflow"},{"location":"examples/scheduled-example/#scheduled-cronjob-workflow-yaml","text":"start : type : scheduled cron : '* * * * *' description : A simple 'no-op' state that returns 'Hello world!' states : - id : helloworld type : noop transform : result : Hello world!","title":"Scheduled Cronjob Workflow YAML"},{"location":"examples/solving-math-expressions/","text":"Solving Math Expressions Example This example shows how we can iterate over data using the ForEach state. Which executes an action that solves a math expression. The workflow data input are the expressions you want to solve as a string array. The example demonstrates the use of an action isolate to solve a number of mathematical expressions using a foreach state. For each expression in the input array, the isolate will be run once. Solver Workflow YAML id : solver description : \"Solves a string array of expressions\" functions : - id : solve-math-expression image : direktiv/solve:v1 type : reusable states : - id : solve type : foreach array : 'jq(.expressions[] | { expression: . })' action : function : solve-math-expression input : 'jq({ x: .expression })' transform : 'jq({ solved: .return })' Input { \"expressions\" : [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] } Output The results of this foreach loop will be a json array of strings that have the solved answers. { \"solved\" : [ \"14\" , \"1\" , \"300\" , \"100\" ] } Note: The array for a foreach state must be passed as an array of objects. This is why to iterate over the expressions string array, we must pipe it and construct a new array of objects using .expressions[] | { expression: . } . jq: .expressions [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] jq: .expressions[] | { expression: . } { \"expression\" : \"4+10\" } { \"expression\" : \"15-14\" } { \"expression\" : \"100*3\" } { \"expression\" : \"200/2\" }","title":"Solving Math Expressions (Foreach)"},{"location":"examples/solving-math-expressions/#solving-math-expressions-example","text":"This example shows how we can iterate over data using the ForEach state. Which executes an action that solves a math expression. The workflow data input are the expressions you want to solve as a string array. The example demonstrates the use of an action isolate to solve a number of mathematical expressions using a foreach state. For each expression in the input array, the isolate will be run once.","title":"Solving Math Expressions Example"},{"location":"examples/solving-math-expressions/#solver-workflow-yaml","text":"id : solver description : \"Solves a string array of expressions\" functions : - id : solve-math-expression image : direktiv/solve:v1 type : reusable states : - id : solve type : foreach array : 'jq(.expressions[] | { expression: . })' action : function : solve-math-expression input : 'jq({ x: .expression })' transform : 'jq({ solved: .return })'","title":"Solver Workflow YAML"},{"location":"examples/solving-math-expressions/#input","text":"{ \"expressions\" : [ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ] }","title":"Input"},{"location":"examples/solving-math-expressions/#output","text":"The results of this foreach loop will be a json array of strings that have the solved answers. { \"solved\" : [ \"14\" , \"1\" , \"300\" , \"100\" ] } Note: The array for a foreach state must be passed as an array of objects. This is why to iterate over the expressions string array, we must pipe it and construct a new array of objects using .expressions[] | { expression: . } .","title":"Output"},{"location":"examples/solving-math-expressions/#jq-expressions","text":"[ \"4+10\" , \"15-14\" , \"100*3\" , \"200/2\" ]","title":"jq: .expressions"},{"location":"examples/solving-math-expressions/#jq-expressions-expression","text":"{ \"expression\" : \"4+10\" } { \"expression\" : \"15-14\" } { \"expression\" : \"100*3\" } { \"expression\" : \"200/2\" }","title":"jq: .expressions[] | { expression: . }"},{"location":"examples/start-event-and-example/","text":"Start EventAnd Workflow This example demonstrates a workflow that triggers whenever the cloud event type greeting and now was received. Start EventAnd Workflow YAML start : type : eventsAnd events : - type : greeting - type : now state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use the following two workflows to trigger the above workflow. Generate Greeting Event Workflow YAML description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv Generate Now Event Workflow YAML description : A simple 'generateEvent' state that triggers a now listener. states : - id : generate type : generateEvent event : type : now source : direktiv","title":"EventAnd Listener (EventAndStartType)"},{"location":"examples/start-event-and-example/#start-eventand-workflow","text":"This example demonstrates a workflow that triggers whenever the cloud event type greeting and now was received.","title":"Start EventAnd Workflow"},{"location":"examples/start-event-and-example/#start-eventand-workflow-yaml","text":"start : type : eventsAnd events : - type : greeting - type : now state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use the following two workflows to trigger the above workflow.","title":"Start EventAnd Workflow YAML"},{"location":"examples/start-event-and-example/#generate-greeting-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv","title":"Generate Greeting Event Workflow YAML"},{"location":"examples/start-event-and-example/#generate-now-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a now listener. states : - id : generate type : generateEvent event : type : now source : direktiv","title":"Generate Now Event Workflow YAML"},{"location":"examples/start-event-xor-example/","text":"Start EventXor Workflow This example demonstrates a workflow that triggers whenever the cloud event type greeting or now was received. Start EventsXor Workflow YAML start : type : eventsXor events : - type : greeting - type : now state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use one of the following workflows to trigger the above workflow. Generate Greeting Event Workflow YAML description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv Generate Now Event Workflow YAML description : A simple 'generateEvent' state that triggers a now listener. states : - id : generate type : generateEvent event : type : now source : direktiv","title":"EventXor Listener (EventXorStartType)"},{"location":"examples/start-event-xor-example/#start-eventxor-workflow","text":"This example demonstrates a workflow that triggers whenever the cloud event type greeting or now was received.","title":"Start EventXor Workflow"},{"location":"examples/start-event-xor-example/#start-eventsxor-workflow-yaml","text":"start : type : eventsXor events : - type : greeting - type : now state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use one of the following workflows to trigger the above workflow.","title":"Start EventsXor Workflow YAML"},{"location":"examples/start-event-xor-example/#generate-greeting-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv","title":"Generate Greeting Event Workflow YAML"},{"location":"examples/start-event-xor-example/#generate-now-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a now listener. states : - id : generate type : generateEvent event : type : now source : direktiv","title":"Generate Now Event Workflow YAML"},{"location":"examples/start-example/","text":"Start Event Workflow This example demonstrates a workflow that triggers whenever the cloud event type greeting was received. Start Event Workflow YAML start : type : event event : type : greeting state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use the following workflow to generate the event yourself. Generate Greeting Event Workflow YAML description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv","title":"Event Listener (EventStartType)"},{"location":"examples/start-example/#start-event-workflow","text":"This example demonstrates a workflow that triggers whenever the cloud event type greeting was received.","title":"Start Event Workflow"},{"location":"examples/start-example/#start-event-workflow-yaml","text":"start : type : event event : type : greeting state : helloworld states : - id : helloworld type : noop transform : result : Hello world! You can use the following workflow to generate the event yourself.","title":"Start Event Workflow YAML"},{"location":"examples/start-example/#generate-greeting-event-workflow-yaml","text":"description : A simple 'generateEvent' state that triggers a greeting listener. states : - id : generate type : generateEvent event : type : greeting source : direktiv","title":"Generate Greeting Event Workflow YAML"},{"location":"examples/terraform-and-ansible/","text":"Using Terraform & Ansible in a Workflow Terraform and Ansible are both widely used technologies in the area of automating infrastructure deployments, configurations, and more. At first glance, it might seem daunting to incorporate these technologies into a Direktiv workflow; but it's actually fairly straightforward. This article seeks to explain how to structure potentially 'complex' workflows in order to support technologies that may require data that persists across multiple workflow executions. The Workflow This workflow consists of only 2 states. The first state runs the terraform isolate, and requires access to a variable that contains the contents of a main.tf file. This main.tf file includes all of the configuration and authorisation details required to create a new virtual machine on Google Cloud Platform. The state is configured to assign the public IP address of the resulting virtual machine as .addr The second state uses the ansible isolate, and requires access to two different variables: playbook.yml Ansible playbook that will be executed to connect to the remote machine and print a 'Hello, world!' message. pk.pem Private key included in the main.tf variable of the previous state, used by Ansible to securely connect to the remote virtual machine. Examples of each variable are included at the end of this article. # This workflow uses Terraform to create an instance on # Google Cloud Platform, and connects to it with Ansible. id : terraform-and-ansible description : \"This workflow uses Terraform to create an instance on Google Cloud Platform, and connects to it with Ansible.\" functions : # Calls the standard 'terraform' isolate, providing # the `main.tf` file from an existing workflow variable. - id : terraform image : direktiv/terraform:v1 files : - key : main.tf scope : workflow type : plain # Runs ansible - id : ansible image : direktiv/ansible:v1 files : - key : playbook.yml scope : workflow type : plain - key : pk.pem scope : workflow states : # Create a GCP instance using Terraform - id : run-terraform type : action action : secrets : [ \"GCP_PROJECT\" ] function : terraform input : action : \"apply\" \"args-on-init\" : - \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" variables : \"state-name\" : \"terraform-gcp-instance\" project_id : jq(.secrets.GCP_PROJECT) transform : jq(.addr = .return.output[\"ip-address\"].value | del(.return)) transition : run-ansible # Use Ansible to connect to the instance using provided private key file - id : run-ansible type : action action : function : ansible input : playbook : playbook.yml privateKey : pk.pem args : - \"-i\" - \"jq(.addr),\" Variables main.tf The main.tf file used by the terraform isolate ensures the creation of a Virtual Machine on Google Cloud Platform with a startup script that will save a private/public key pair to the system and ensure that it is authorised for access to the machine via SSH. To change the name of the resulting virtual machine, modify the value of the name field. terraform { backend \"http\" { } } provider \"google\" { credentials = var.service_account_key } resource \"google_compute_instance\" \"default\" { project = var.project_id name = \"direktiv-terraform-ansible\" machine_type = \"n1-standard-1\" zone = \"australia-southeast1-a\" tags = [ \"direktiv\", \"direktiv\" ] boot_disk { initialize_params { image = \"ubuntu-os-cloud/ubuntu-2004-lts\" } } network_interface { network = \"default\" access_config { // Ephemeral IP } } metadata_startup_script = <<EOT echo <base 64 -encoded contents of a pem privat key> > / pk.b64 base 64 -d / pk.b64 > / pk.pem echo <base 64 -encoded contents of an rsa public key> > / ssh.b64 base 64 -d / ssh.b64 > / ssh.key chmod 600 / pk.pem eval `ssh-agent -s` ssh-add / pk.pem cat / ssh.key >> ~/ . ssh/authorized_keys EOT } variable \"service_account_key\" { description = \"the entire contents of a service account key\" default = << EOT { \"type\" : \"service_account\" , \"project_id\" : \"*****\" , \"private_key_id\" : \"*****\" , \"private_key\" : \"*****\" , \"client_email\" : \"*****\" , \"client_id\" : \"*****\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://oauth2.googleapis.com/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/*****\" } EOT } variable \"project_id\" { description = \"project_id to spawn the virtual machine on\" } output \"ip-address\" { value = google_compute_instance.default.network_interface[0].access_config[0].nat_ip } playbook.yml This is an incredibly basic Ansible playbook. After connecting to the remote machine, it simply prints Hello, world! and returns. --- - hosts: all name: helloworld playbook gather_facts: yes remote_user: root connection: ssh tasks: - name: run helloworld logic ansible.builtin.debug: msg: - \"Hello, world!\" pk.pem This is a PEM formatted private key file, provided to both the created virtual machine (via main.tf ) and the ansible isolate. It provides a way for the ansible isolate to securely connect to the virtual machine via SSH.","title":"Terraform or Ansible"},{"location":"examples/terraform-and-ansible/#using-terraform-ansible-in-a-workflow","text":"Terraform and Ansible are both widely used technologies in the area of automating infrastructure deployments, configurations, and more. At first glance, it might seem daunting to incorporate these technologies into a Direktiv workflow; but it's actually fairly straightforward. This article seeks to explain how to structure potentially 'complex' workflows in order to support technologies that may require data that persists across multiple workflow executions.","title":"Using Terraform &amp; Ansible in a Workflow"},{"location":"examples/terraform-and-ansible/#the-workflow","text":"This workflow consists of only 2 states. The first state runs the terraform isolate, and requires access to a variable that contains the contents of a main.tf file. This main.tf file includes all of the configuration and authorisation details required to create a new virtual machine on Google Cloud Platform. The state is configured to assign the public IP address of the resulting virtual machine as .addr The second state uses the ansible isolate, and requires access to two different variables: playbook.yml Ansible playbook that will be executed to connect to the remote machine and print a 'Hello, world!' message. pk.pem Private key included in the main.tf variable of the previous state, used by Ansible to securely connect to the remote virtual machine. Examples of each variable are included at the end of this article. # This workflow uses Terraform to create an instance on # Google Cloud Platform, and connects to it with Ansible. id : terraform-and-ansible description : \"This workflow uses Terraform to create an instance on Google Cloud Platform, and connects to it with Ansible.\" functions : # Calls the standard 'terraform' isolate, providing # the `main.tf` file from an existing workflow variable. - id : terraform image : direktiv/terraform:v1 files : - key : main.tf scope : workflow type : plain # Runs ansible - id : ansible image : direktiv/ansible:v1 files : - key : playbook.yml scope : workflow type : plain - key : pk.pem scope : workflow states : # Create a GCP instance using Terraform - id : run-terraform type : action action : secrets : [ \"GCP_PROJECT\" ] function : terraform input : action : \"apply\" \"args-on-init\" : - \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" variables : \"state-name\" : \"terraform-gcp-instance\" project_id : jq(.secrets.GCP_PROJECT) transform : jq(.addr = .return.output[\"ip-address\"].value | del(.return)) transition : run-ansible # Use Ansible to connect to the instance using provided private key file - id : run-ansible type : action action : function : ansible input : playbook : playbook.yml privateKey : pk.pem args : - \"-i\" - \"jq(.addr),\"","title":"The Workflow"},{"location":"examples/terraform-and-ansible/#variables","text":"","title":"Variables"},{"location":"examples/terraform-and-ansible/#maintf","text":"The main.tf file used by the terraform isolate ensures the creation of a Virtual Machine on Google Cloud Platform with a startup script that will save a private/public key pair to the system and ensure that it is authorised for access to the machine via SSH. To change the name of the resulting virtual machine, modify the value of the name field. terraform { backend \"http\" { } } provider \"google\" { credentials = var.service_account_key } resource \"google_compute_instance\" \"default\" { project = var.project_id name = \"direktiv-terraform-ansible\" machine_type = \"n1-standard-1\" zone = \"australia-southeast1-a\" tags = [ \"direktiv\", \"direktiv\" ] boot_disk { initialize_params { image = \"ubuntu-os-cloud/ubuntu-2004-lts\" } } network_interface { network = \"default\" access_config { // Ephemeral IP } } metadata_startup_script = <<EOT echo <base 64 -encoded contents of a pem privat key> > / pk.b64 base 64 -d / pk.b64 > / pk.pem echo <base 64 -encoded contents of an rsa public key> > / ssh.b64 base 64 -d / ssh.b64 > / ssh.key chmod 600 / pk.pem eval `ssh-agent -s` ssh-add / pk.pem cat / ssh.key >> ~/ . ssh/authorized_keys EOT } variable \"service_account_key\" { description = \"the entire contents of a service account key\" default = << EOT { \"type\" : \"service_account\" , \"project_id\" : \"*****\" , \"private_key_id\" : \"*****\" , \"private_key\" : \"*****\" , \"client_email\" : \"*****\" , \"client_id\" : \"*****\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://oauth2.googleapis.com/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/*****\" } EOT } variable \"project_id\" { description = \"project_id to spawn the virtual machine on\" } output \"ip-address\" { value = google_compute_instance.default.network_interface[0].access_config[0].nat_ip }","title":"main.tf"},{"location":"examples/terraform-and-ansible/#playbookyml","text":"This is an incredibly basic Ansible playbook. After connecting to the remote machine, it simply prints Hello, world! and returns. --- - hosts: all name: helloworld playbook gather_facts: yes remote_user: root connection: ssh tasks: - name: run helloworld logic ansible.builtin.debug: msg: - \"Hello, world!\"","title":"playbook.yml"},{"location":"examples/terraform-and-ansible/#pkpem","text":"This is a PEM formatted private key file, provided to both the created virtual machine (via main.tf ) and the ansible isolate. It provides a way for the ansible isolate to securely connect to the virtual machine via SSH.","title":"pk.pem"},{"location":"examples/using-terraform-plugin/","text":"Introduction This example will detail how to user Direktiv with Terraform to create a virtual machine. To do this, we will use examples listed on the public git repository . After creating the virtual machine, a message will be sent to a Discord webhook, resulting in the message being posted to a Discord server text channel. id : spawn-instance functions : - id : git image : direktiv/git:v1 type : reusable - id : discordmsg image : direktiv/discordmsg:v1 type : reusable - id : tfrun image : direktiv/terraform:v1 type : reusable files : - key : terraform-examples scope : instance type : tar.gz description : Clones a repository and uses terraform to deploy an instance states : # continued in next code block NOTE: The files attribute is empty and will be populated using the git state. Git The following state fetches the repository and clones it into an instance variable to be used in the terraform container. - id : cloning type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/terraform-examples.git $out/instance/terraform-examples\" ] transition : deploy_{CLOUD} log : . Terraform The files terraform need are provided from the git clone state that happened before which saves the variable as tar.gz file. When it is imported into the terraform container it ends up being a folder on the temp directory. The execution-folder in the input directs terraform to where the apply will be executed from. NOTE: Terraform container has its own http backend located at 'http://localhost:8001/{state-name}'. If provided args-on-init and state-name we will write the tfstate to a workflow variable. Azure The only secrets required to run this workflow with Azure are: subscription_id client_id client_secret tenant_id - id : deploy_azure type : action log : . action : secrets : [ \"CLIENT_ID\" , \"TENANT_ID\" , \"SUBSCRIPTION_ID\" , \"CLIENT_SECRET\" ] function : tfrun input : action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-azure-instance\" ] execution-folder : terraform-examples/azure variables : state-name : terraform-azure-instance subscription_id : jq(.secrets.SUBSCRIPTION_ID) client_id : jq(.secrets.CLIENT_ID) client_secret : jq(.secrets.CLIENT_SECRET) tenant_id : jq(.secrets.TENANT_ID) transform : ip : jq(.ip) transition : send_message Google Cloud Platform The only secrets required to run this workflow with Google Cloud Platform are: service_account_key (plain contents of a service account key) project_id - id : deploy_gcp type : action log : . action : secrets : [ \"SERVICE_ACCOUNT_KEY\" , \"PROJECT_ID\" ] function : tfrun input : execution-folder : terraform-examples/google action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" ] variables : state-name : terraform-gcp-instance project_id : jq(.secrets.PROJECT_ID) service_account_key : jq(.secrets.SERVICE_ACCOUNT_KEY) transform : ip : jq(.ip) transition : send_message Amazon Web Services The only secrets required to run this workflow with Amazon Web Services are: access_key secret_key - id : deploy_amazon type : action log : . action : secrets : [ \"AMAZON_KEY\" , \"AMAZON_SECRET\" ] function : tfrun input : execution-folder : terraform-examples/amazon action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-amazon-instance\" ] variables : region : us-east-2 state-name : terraform-amazon-instance amazon_key : jq(.secrets.AMAZON_KEY) amazon_secret : jq(.secrets.AMAZON_SECRET) transition : send_message transform : ip : jq(.ip) NOTE: Each terraform state uses the transform field to pluck the IP address of the created virtual machine, to be sent to the Discord webhook. Discord Message A simple action that sends a request to a Discord Webhook to post a message. - id : send_message type : action action : secrets : [ \"WEBHOOK_URL\" ] function : discordmsg input : tts : false url : jq(.secrets.WEBHOOK_URL) message : The ip address of your machine is jq(.ip). Full Example Let's bring all of the states together to create a workflow that creates a virtual machine on Google Cloud Platform, Amazon Web Services, and Azure. The following input is required: { \"action\" : \"'apply' or 'destroy'\" } We've also included a new switch state to facilitate not sending anything to the Discord webhook on destroy actions. id : spawn-instance functions : - id : git image : direktiv/git:v1 type : reusable - id : discordmsg image : direktiv/discordmsg:v1 type : reusable - id : tfrun image : direktiv/terraform:v1 type : reusable files : - key : terraform-examples scope : instance type : tar.gz description : Clones a repository and uses terraform to deploy an instance states : - id : cloning type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/terraform-examples.git $out/instance/terraform-examples\" ] transition : deploy_azure log : . - id : deploy_azure type : action log : . action : secrets : [ \"CLIENT_ID\" , \"TENANT_ID\" , \"SUBSCRIPTION_ID\" , \"CLIENT_SECRET\" ] function : tfrun input : action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-azure-instance\" ] execution-folder : terraform-examples/azure variables : state-name : terraform-azure-instance subscription_id : jq(.secrets.SUBSCRIPTION_ID) client_id : jq(.secrets.CLIENT_ID) client_secret : jq(.secrets.CLIENT_SECRET) tenant_id : jq(.secrets.TENANT_ID) transform : action : jq(.action) azure_ip : jq(.return.output.ip_address.value) transition : deploy_gcp - id : deploy_gcp type : action log : . action : secrets : [ \"SERVICE_ACCOUNT_KEY\" , \"PROJECT_ID\" ] function : tfrun input : execution-folder : terraform-examples/google action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" ] variables : state-name : terraform-gcp-instance project_id : jq(.secrets.PROJECT_ID) service_account_key : jq(.secrets.SERVICE_ACCOUNT_KEY) transform : action : jq(.action) azure_ip : jq(.azure_ip) google_ip : jq(.return.output.\"ip-address\".value) transition : deploy_amazon - id : deploy_amazon type : action log : . action : secrets : [ \"AMAZON_KEY\" , \"AMAZON_SECRET\" ] function : tfrun input : execution-folder : terraform-examples/amazon action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-amazon-instance\" ] variables : region : us-east-2 state-name : terraform-amazon-instance amazon_key : jq(.secrets.AMAZON_KEY) amazon_secret : jq(.secrets.AMAZON_SECRET) transform : action : jq(.action) azure_ip : jq(.azure_ip) google_ip : jq(.google_ip) amazon_ip : jq(.return.output.\"ip-address\".value) transition : check_apply_or_destroy - id : check_apply_or_destroy type : switch conditions : - condition : jq(.action == \"apply\") transition : send_message - id : send_message type : action log : . action : secrets : [ \"WEBHOOK_URL\" ] function : discordmsg input : tts : false url : jq(.secrets.WEBHOOK_URL) message : The ip address of your Azure machine is jq(.azure_ip). The ip address of your Google machine is jq(.google_ip). The ip address of your Amazon machine is jq(.amazon_ip).","title":"Running Terraform Scripts"},{"location":"examples/using-terraform-plugin/#introduction","text":"This example will detail how to user Direktiv with Terraform to create a virtual machine. To do this, we will use examples listed on the public git repository . After creating the virtual machine, a message will be sent to a Discord webhook, resulting in the message being posted to a Discord server text channel. id : spawn-instance functions : - id : git image : direktiv/git:v1 type : reusable - id : discordmsg image : direktiv/discordmsg:v1 type : reusable - id : tfrun image : direktiv/terraform:v1 type : reusable files : - key : terraform-examples scope : instance type : tar.gz description : Clones a repository and uses terraform to deploy an instance states : # continued in next code block NOTE: The files attribute is empty and will be populated using the git state.","title":"Introduction"},{"location":"examples/using-terraform-plugin/#git","text":"The following state fetches the repository and clones it into an instance variable to be used in the terraform container. - id : cloning type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/terraform-examples.git $out/instance/terraform-examples\" ] transition : deploy_{CLOUD} log : .","title":"Git"},{"location":"examples/using-terraform-plugin/#terraform","text":"The files terraform need are provided from the git clone state that happened before which saves the variable as tar.gz file. When it is imported into the terraform container it ends up being a folder on the temp directory. The execution-folder in the input directs terraform to where the apply will be executed from. NOTE: Terraform container has its own http backend located at 'http://localhost:8001/{state-name}'. If provided args-on-init and state-name we will write the tfstate to a workflow variable.","title":"Terraform"},{"location":"examples/using-terraform-plugin/#azure","text":"The only secrets required to run this workflow with Azure are: subscription_id client_id client_secret tenant_id - id : deploy_azure type : action log : . action : secrets : [ \"CLIENT_ID\" , \"TENANT_ID\" , \"SUBSCRIPTION_ID\" , \"CLIENT_SECRET\" ] function : tfrun input : action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-azure-instance\" ] execution-folder : terraform-examples/azure variables : state-name : terraform-azure-instance subscription_id : jq(.secrets.SUBSCRIPTION_ID) client_id : jq(.secrets.CLIENT_ID) client_secret : jq(.secrets.CLIENT_SECRET) tenant_id : jq(.secrets.TENANT_ID) transform : ip : jq(.ip) transition : send_message","title":"Azure"},{"location":"examples/using-terraform-plugin/#google-cloud-platform","text":"The only secrets required to run this workflow with Google Cloud Platform are: service_account_key (plain contents of a service account key) project_id - id : deploy_gcp type : action log : . action : secrets : [ \"SERVICE_ACCOUNT_KEY\" , \"PROJECT_ID\" ] function : tfrun input : execution-folder : terraform-examples/google action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" ] variables : state-name : terraform-gcp-instance project_id : jq(.secrets.PROJECT_ID) service_account_key : jq(.secrets.SERVICE_ACCOUNT_KEY) transform : ip : jq(.ip) transition : send_message","title":"Google Cloud Platform"},{"location":"examples/using-terraform-plugin/#amazon-web-services","text":"The only secrets required to run this workflow with Amazon Web Services are: access_key secret_key - id : deploy_amazon type : action log : . action : secrets : [ \"AMAZON_KEY\" , \"AMAZON_SECRET\" ] function : tfrun input : execution-folder : terraform-examples/amazon action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-amazon-instance\" ] variables : region : us-east-2 state-name : terraform-amazon-instance amazon_key : jq(.secrets.AMAZON_KEY) amazon_secret : jq(.secrets.AMAZON_SECRET) transition : send_message transform : ip : jq(.ip) NOTE: Each terraform state uses the transform field to pluck the IP address of the created virtual machine, to be sent to the Discord webhook.","title":"Amazon Web Services"},{"location":"examples/using-terraform-plugin/#discord-message","text":"A simple action that sends a request to a Discord Webhook to post a message. - id : send_message type : action action : secrets : [ \"WEBHOOK_URL\" ] function : discordmsg input : tts : false url : jq(.secrets.WEBHOOK_URL) message : The ip address of your machine is jq(.ip).","title":"Discord Message"},{"location":"examples/using-terraform-plugin/#full-example","text":"Let's bring all of the states together to create a workflow that creates a virtual machine on Google Cloud Platform, Amazon Web Services, and Azure. The following input is required: { \"action\" : \"'apply' or 'destroy'\" } We've also included a new switch state to facilitate not sending anything to the Discord webhook on destroy actions. id : spawn-instance functions : - id : git image : direktiv/git:v1 type : reusable - id : discordmsg image : direktiv/discordmsg:v1 type : reusable - id : tfrun image : direktiv/terraform:v1 type : reusable files : - key : terraform-examples scope : instance type : tar.gz description : Clones a repository and uses terraform to deploy an instance states : - id : cloning type : action action : function : git input : cmds : [ \"clone https://github.com/direktiv/terraform-examples.git $out/instance/terraform-examples\" ] transition : deploy_azure log : . - id : deploy_azure type : action log : . action : secrets : [ \"CLIENT_ID\" , \"TENANT_ID\" , \"SUBSCRIPTION_ID\" , \"CLIENT_SECRET\" ] function : tfrun input : action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-azure-instance\" ] execution-folder : terraform-examples/azure variables : state-name : terraform-azure-instance subscription_id : jq(.secrets.SUBSCRIPTION_ID) client_id : jq(.secrets.CLIENT_ID) client_secret : jq(.secrets.CLIENT_SECRET) tenant_id : jq(.secrets.TENANT_ID) transform : action : jq(.action) azure_ip : jq(.return.output.ip_address.value) transition : deploy_gcp - id : deploy_gcp type : action log : . action : secrets : [ \"SERVICE_ACCOUNT_KEY\" , \"PROJECT_ID\" ] function : tfrun input : execution-folder : terraform-examples/google action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-gcp-instance\" ] variables : state-name : terraform-gcp-instance project_id : jq(.secrets.PROJECT_ID) service_account_key : jq(.secrets.SERVICE_ACCOUNT_KEY) transform : action : jq(.action) azure_ip : jq(.azure_ip) google_ip : jq(.return.output.\"ip-address\".value) transition : deploy_amazon - id : deploy_amazon type : action log : . action : secrets : [ \"AMAZON_KEY\" , \"AMAZON_SECRET\" ] function : tfrun input : execution-folder : terraform-examples/amazon action : jq(.action) args-on-init : [ \"-backend-config=address=http://localhost:8001/terraform-amazon-instance\" ] variables : region : us-east-2 state-name : terraform-amazon-instance amazon_key : jq(.secrets.AMAZON_KEY) amazon_secret : jq(.secrets.AMAZON_SECRET) transform : action : jq(.action) azure_ip : jq(.azure_ip) google_ip : jq(.google_ip) amazon_ip : jq(.return.output.\"ip-address\".value) transition : check_apply_or_destroy - id : check_apply_or_destroy type : switch conditions : - condition : jq(.action == \"apply\") transition : send_message - id : send_message type : action log : . action : secrets : [ \"WEBHOOK_URL\" ] function : discordmsg input : tts : false url : jq(.secrets.WEBHOOK_URL) message : The ip address of your Azure machine is jq(.azure_ip). The ip address of your Google machine is jq(.google_ip). The ip address of your Amazon machine is jq(.amazon_ip).","title":"Full Example"},{"location":"examples/variable-mime-types/","text":"Variable Mime Type Example All variables have an associated mime type to distinguish the content type of its value. This example will show two examples, and the special behaviour that happens when mimeType is text/plain or application/octet-stream . Example 1: Storing a string as a raw plaintext variable. By default (mimeType=application/json) all variables are treated as JSON values. So this means even if you store a string in a variable, it's value is stored with quotes wrapped around it. Workflow - Example JSON String Var states : - id : set-var type : setter variables : - key : StringVar scope : workflow value : | hello world Variable - StringVar Value \"hello\\nworld\" There are certain scenarios where you would not want to store the variable with its quotes. To do this all need to do is simply set the mimeType to text/plain or text/plain; charset=utf-8 . This will store the variable as a raw string without quotes. Workflow - Example Plaintext String Var states : - id : set-var type : setter variables : - key : StringVar scope : workflow mimeType : 'text/plain' value : | hello world Variable - StringVar Value hello world Example 2: Auto Decoding Base64 string Another special behaviour is that it's also possible to auto decode a base64 string by setting the mimeType to application/octet-stream . Workflow - Example Auto Decode Base64 String states : - id : set-var type : setter variables : - key : MessageVar scope : workflow value : 'aGVsbG8gZnJvbSBkaXJla3Rpdg==' mimeType : 'application/octet-stream' Variable - MessageVar Value hello from direktiv These are the only two mime types with special behaviour. Any other mimeType will be treated internally by the default JSON behaviour. The default value for mimeType is application/json","title":"Variable Mimetypes"},{"location":"examples/variable-mime-types/#variable-mime-type-example","text":"All variables have an associated mime type to distinguish the content type of its value. This example will show two examples, and the special behaviour that happens when mimeType is text/plain or application/octet-stream .","title":"Variable Mime Type Example"},{"location":"examples/variable-mime-types/#example-1-storing-a-string-as-a-raw-plaintext-variable","text":"By default (mimeType=application/json) all variables are treated as JSON values. So this means even if you store a string in a variable, it's value is stored with quotes wrapped around it.","title":"Example 1: Storing a string as a raw plaintext variable."},{"location":"examples/variable-mime-types/#workflow-example-json-string-var","text":"states : - id : set-var type : setter variables : - key : StringVar scope : workflow value : | hello world","title":"Workflow - Example JSON String Var"},{"location":"examples/variable-mime-types/#variable-stringvar-value","text":"\"hello\\nworld\" There are certain scenarios where you would not want to store the variable with its quotes. To do this all need to do is simply set the mimeType to text/plain or text/plain; charset=utf-8 . This will store the variable as a raw string without quotes.","title":"Variable - StringVar Value"},{"location":"examples/variable-mime-types/#workflow-example-plaintext-string-var","text":"states : - id : set-var type : setter variables : - key : StringVar scope : workflow mimeType : 'text/plain' value : | hello world","title":"Workflow - Example Plaintext String Var"},{"location":"examples/variable-mime-types/#variable-stringvar-value_1","text":"hello world","title":"Variable - StringVar Value"},{"location":"examples/variable-mime-types/#example-2-auto-decoding-base64-string","text":"Another special behaviour is that it's also possible to auto decode a base64 string by setting the mimeType to application/octet-stream .","title":"Example 2: Auto Decoding Base64 string"},{"location":"examples/variable-mime-types/#workflow-example-auto-decode-base64-string","text":"states : - id : set-var type : setter variables : - key : MessageVar scope : workflow value : 'aGVsbG8gZnJvbSBkaXJla3Rpdg==' mimeType : 'application/octet-stream'","title":"Workflow - Example Auto Decode Base64 String"},{"location":"examples/variable-mime-types/#variable-messagevar-value","text":"hello from direktiv These are the only two mime types with special behaviour. Any other mimeType will be treated internally by the default JSON behaviour. The default value for mimeType is application/json","title":"Variable - MessageVar Value"},{"location":"examples/website-change/","text":"Check if Website has changed Example This example demonstrates a use case for a hypothetical scenario when you need to periodically check if a website has changed. It makes use of workflow variables to keep persistent data to test against. Most websites would likely have minor changes if you were to just fetch their HTML. These changes are usually stuff like time and session properties generated on each request. We don't want to categorize these changes as true as these are false positives. Luckily many websites have one or both header values: Etag and Last-Modified . If either one of these headers changes it is safe to assume that the website has been updated. Note: Although many websites have adopted this standard, there are still websites that have not. These websites are not supported. This example is fairly simple and can be broken up into four steps: 1. Fetch the current headers of the target website. 2. Get the saved headers variables from the previous execution of this workflow. 3. Compare the current and previous headers to determine if the target website has changed. 4. Save current headers to workflow variables. Below we'll explain each step in more detail. For this example we've chosen https://docs.direktiv.io as the target website. Fetch Website Headers Here we are performing a HTTP POST request on the target website using our direktiv/request:v1 direktiv app as a function. Since we only care about the Last-Modified and Etag headers we then extract those values and store them in the lastModified and etag properties using the transform field in the fetch-site-headers state. Now that we have fetched the current header values required, the state will transition to the get-old-headers state. id : check-website-change description : \"A simple workflow that fetches current headers from a website and compares them to the previously stored headers to determine if it has changed.\" functions : - id : get image : direktiv/request:v1 type : reusable states : - id : fetch-site-headers type : action transform : 'jq({lastModified: .return.headers.[\"Last-Modified\"][0], etag: .return.headers.[\"Etag\"][0]})' transition : get-old-headers action : function : get input : method : \"HEAD\" url : \"https://docs.direktiv.io\" Get Old Headers Direktiv has variables scoped to namespaces, workflows and instance. In this state ( get-old-headers ) we'll get the variables lastModified and etag in the workflow scope. These variables are set to whatever the header values were last time when this workflow was executed. If this is the first time this workflow is executed these value will be null . This is fine, but it will cause the results to be siteChanged: true because the current headers can never equal null . - id : get-old-headers type : getter transition : check-site variables : - key : lastModified scope : workflow - key : etag scope : workflow Compare Values Now that we have both the current and previous header values we can make a comparison and check whether the website has changed using the switch state check-site . The switch state below has three possible conditions. The first condition is used for validation and will transition to the error state unsupported-site if neither etag or lastModified was fetched from the current headers. The last two are to check if either of the etag or lastModified values have changed between the previous and current headers. If either one of these headers has changed it means that the website has changed and the property siteChanged is set to true. If none of these conditions are satisfied, the siteChanged property is set to false because we can assume that no errors/changes have occurred. - id : check-site type : switch defaultTransition : save-values defaultTransform : 'jq(. += {siteChanged: false})' conditions : - condition : 'jq(.etag == null and .lastModified == null)' transition : unsupported-site - condition : 'jq(.etag != .var.etag)' transition : save-values transform : 'jq(. += {siteChanged: true})' - condition : 'jq(.lastModified != .var.lastModified)' transition : save-values transform : 'jq(. += {siteChanged: true})' - id : unsupported-site type : error error : unsupported.site message : \"https://docs.direktiv.io is not supported: site must respond with atleast one of these headers: ['Etag', 'Last-Modified']\" Save current values Finally we save the current headers to the lastModified and etag workflow variables, so next time this workflow is executed they can be retrieved in the get-old-headers state. - id : save-values type : setter variables : - key : lastModified scope : workflow value : 'jq(.lastModified)' - key : etag scope : workflow value : 'jq(.etag)' Sample Output Note: the getter state will place variables into the var property. So the var.etag and var.lastModified values are the old headers. { \"etag\" : \"\\\"60d55d9b-54b1\\\"\" , \"lastModified\" : \"Fri, 25 Jun 2021 04:37:47 GMT\" , \"siteChanged\" : false , \"var\" : { \"etag\" : \"\\\"60d55d9b-54b1\\\"\" , \"lastModified\" : \"Fri, 25 Jun 2021 04:37:47 GMT\" } } Extra - Converting to a Cron Job This workflow can currently run as is, and be manually executed. However this example is more than likely to be used as a cron job . To convert this workflow all you need to do is add the start block to the top of the workflow. Below is an example that, if added to the workflow, will run this workflow every once every two hours. start : type : scheduled cron : \"0 */2 * * *\" Full Workflow id : a-cron-example description : A simple 'action' state that sends a get request\" functions : - id : get image : direktiv/request:v1 type : reusable states : - id : fetch-site-headers type : action transform : 'jq({lastModified: .return.headers.[\"Last-Modified\"][0], etag: .return.headers.[\"Etag\"][0]})' transition : get-old-headers action : function : get input : method : \"HEAD\" url : \"https://docs.direktiv.io\" - id : get-old-headers type : getter transition : check-site variables : - key : lastModified scope : workflow - key : etag scope : workflow - id : check-site type : switch defaultTransition : save-values defaultTransform : 'jq(. += {siteChanged: false})' conditions : - condition : 'jq(.etag == null and .lastModified == null)' transition : unsupported-site - condition : 'jq(.etag != .var.etag)' transition : save-values transform : 'jq(. += {siteChanged: true})' - condition : 'jq(.lastModified != .var.lastModified)' transition : save-values transform : 'jq(. += {siteChanged: true})' - id : unsupported-site type : error error : unsupported.site message : \"https://docs.direktiv.io is not supported: site must respond with atleast one of these headers: ['Etag', 'Last-Modified']\" - id : save-values type : setter variables : - key : lastModified scope : workflow value : 'jq(.lastModified)' - key : etag scope : workflow value : 'jq(.etag)'","title":"Check for Website Change"},{"location":"examples/website-change/#check-if-website-has-changed-example","text":"This example demonstrates a use case for a hypothetical scenario when you need to periodically check if a website has changed. It makes use of workflow variables to keep persistent data to test against. Most websites would likely have minor changes if you were to just fetch their HTML. These changes are usually stuff like time and session properties generated on each request. We don't want to categorize these changes as true as these are false positives. Luckily many websites have one or both header values: Etag and Last-Modified . If either one of these headers changes it is safe to assume that the website has been updated. Note: Although many websites have adopted this standard, there are still websites that have not. These websites are not supported. This example is fairly simple and can be broken up into four steps: 1. Fetch the current headers of the target website. 2. Get the saved headers variables from the previous execution of this workflow. 3. Compare the current and previous headers to determine if the target website has changed. 4. Save current headers to workflow variables. Below we'll explain each step in more detail. For this example we've chosen https://docs.direktiv.io as the target website.","title":"Check if Website has changed Example"},{"location":"examples/website-change/#fetch-website-headers","text":"Here we are performing a HTTP POST request on the target website using our direktiv/request:v1 direktiv app as a function. Since we only care about the Last-Modified and Etag headers we then extract those values and store them in the lastModified and etag properties using the transform field in the fetch-site-headers state. Now that we have fetched the current header values required, the state will transition to the get-old-headers state. id : check-website-change description : \"A simple workflow that fetches current headers from a website and compares them to the previously stored headers to determine if it has changed.\" functions : - id : get image : direktiv/request:v1 type : reusable states : - id : fetch-site-headers type : action transform : 'jq({lastModified: .return.headers.[\"Last-Modified\"][0], etag: .return.headers.[\"Etag\"][0]})' transition : get-old-headers action : function : get input : method : \"HEAD\" url : \"https://docs.direktiv.io\"","title":"Fetch Website Headers"},{"location":"examples/website-change/#get-old-headers","text":"Direktiv has variables scoped to namespaces, workflows and instance. In this state ( get-old-headers ) we'll get the variables lastModified and etag in the workflow scope. These variables are set to whatever the header values were last time when this workflow was executed. If this is the first time this workflow is executed these value will be null . This is fine, but it will cause the results to be siteChanged: true because the current headers can never equal null . - id : get-old-headers type : getter transition : check-site variables : - key : lastModified scope : workflow - key : etag scope : workflow","title":"Get Old Headers"},{"location":"examples/website-change/#compare-values","text":"Now that we have both the current and previous header values we can make a comparison and check whether the website has changed using the switch state check-site . The switch state below has three possible conditions. The first condition is used for validation and will transition to the error state unsupported-site if neither etag or lastModified was fetched from the current headers. The last two are to check if either of the etag or lastModified values have changed between the previous and current headers. If either one of these headers has changed it means that the website has changed and the property siteChanged is set to true. If none of these conditions are satisfied, the siteChanged property is set to false because we can assume that no errors/changes have occurred. - id : check-site type : switch defaultTransition : save-values defaultTransform : 'jq(. += {siteChanged: false})' conditions : - condition : 'jq(.etag == null and .lastModified == null)' transition : unsupported-site - condition : 'jq(.etag != .var.etag)' transition : save-values transform : 'jq(. += {siteChanged: true})' - condition : 'jq(.lastModified != .var.lastModified)' transition : save-values transform : 'jq(. += {siteChanged: true})' - id : unsupported-site type : error error : unsupported.site message : \"https://docs.direktiv.io is not supported: site must respond with atleast one of these headers: ['Etag', 'Last-Modified']\"","title":"Compare Values"},{"location":"examples/website-change/#save-current-values","text":"Finally we save the current headers to the lastModified and etag workflow variables, so next time this workflow is executed they can be retrieved in the get-old-headers state. - id : save-values type : setter variables : - key : lastModified scope : workflow value : 'jq(.lastModified)' - key : etag scope : workflow value : 'jq(.etag)'","title":"Save current values"},{"location":"examples/website-change/#sample-output","text":"Note: the getter state will place variables into the var property. So the var.etag and var.lastModified values are the old headers. { \"etag\" : \"\\\"60d55d9b-54b1\\\"\" , \"lastModified\" : \"Fri, 25 Jun 2021 04:37:47 GMT\" , \"siteChanged\" : false , \"var\" : { \"etag\" : \"\\\"60d55d9b-54b1\\\"\" , \"lastModified\" : \"Fri, 25 Jun 2021 04:37:47 GMT\" } }","title":"Sample Output"},{"location":"examples/website-change/#extra-converting-to-a-cron-job","text":"This workflow can currently run as is, and be manually executed. However this example is more than likely to be used as a cron job . To convert this workflow all you need to do is add the start block to the top of the workflow. Below is an example that, if added to the workflow, will run this workflow every once every two hours. start : type : scheduled cron : \"0 */2 * * *\"","title":"Extra - Converting to a Cron Job"},{"location":"examples/website-change/#full-workflow","text":"id : a-cron-example description : A simple 'action' state that sends a get request\" functions : - id : get image : direktiv/request:v1 type : reusable states : - id : fetch-site-headers type : action transform : 'jq({lastModified: .return.headers.[\"Last-Modified\"][0], etag: .return.headers.[\"Etag\"][0]})' transition : get-old-headers action : function : get input : method : \"HEAD\" url : \"https://docs.direktiv.io\" - id : get-old-headers type : getter transition : check-site variables : - key : lastModified scope : workflow - key : etag scope : workflow - id : check-site type : switch defaultTransition : save-values defaultTransform : 'jq(. += {siteChanged: false})' conditions : - condition : 'jq(.etag == null and .lastModified == null)' transition : unsupported-site - condition : 'jq(.etag != .var.etag)' transition : save-values transform : 'jq(. += {siteChanged: true})' - condition : 'jq(.lastModified != .var.lastModified)' transition : save-values transform : 'jq(. += {siteChanged: true})' - id : unsupported-site type : error error : unsupported.site message : \"https://docs.direktiv.io is not supported: site must respond with atleast one of these headers: ['Etag', 'Last-Modified']\" - id : save-values type : setter variables : - key : lastModified scope : workflow value : 'jq(.lastModified)' - key : etag scope : workflow value : 'jq(.etag)'","title":"Full Workflow"},{"location":"getting_started/conditional-transitions/","text":"Conditional Transitions Oftentimes a workflow needs to be a little bit smarter than an immutable sequence of states. That's when you might need conditional transitions. In this article you'll learn about instance input data, the Switch State, and how to define loops. Demo id : multiposter functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : ifelse type : switch conditions : - condition : jq(.names) transition : poster - id : poster type : action action : function : httprequest input : '{ \"method\": \"POST\", \"url\": \"https://jsonplaceholder.typicode.com/posts\", \"body\": { \"name\": .names[0] } }' transform : jq(del(.names[0])) transition : ifelse Input input : names : - \"Alan\" - \"Jon\" - \"Trent\" Output { \"names\" : [], \"return\" : { \"id\" : 101 } } Instance Input Workflows can be invoked with input data that will be available as instance data. There are a few ifs-and-buts that apply to input data, but it's not that complicated. Input JSON Object If the input is a JSON object it will become the instance data. Input Data { \"key\" : \"value\" } Instance Data { \"key\" : \"value\" } Input JSON Non-Object If the input is valid JSON but not an object it will be stored under \"input\" . Input Data [ 1 , 2 , 3 ] Instance Data { \"input\" : [ 1 , 2 , 3 ] } Input Non-JSON Finally, if the input is not valid JSON it will be base64 encoded into a string and then stored under \"input\" . Hello, world! Instance Data { \"input\" : \"SGVsbG8sIHdvcmxkIQ==\" } Switch State The Switch State can make decisions about where to transition to next based on the instance data by evaluating a number of jq expressions and checking the results. Here's an example switch state definition: - id : ifelse type : switch conditions : - condition : 'jq(.person.age > 18)' transition : accept #transform: - condition : 'jq(.person.age != nil)' transition : reject #transform: defaultTransition : failure #defaultTransform: Each of the conditions will be evaluated in the order it appears by running the jq command in condition . Any result other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a successful match. If no conditions match the default transition will be used. In the demo example the switch state will transition to poster until the list of names is empty, at which point the workflow will end. Other Conditional Transitions The Switch State is not the only way to do conditional transitions. The EventsXor state also transitions conditionally based on which CloudEvent was received. All states can also define handlers for catching various types of errors. Both of these will be discussed in a later article. Loops By transitioning to a state that has already happened it's possible to create loops in workflow instances. In this demo we've got a type of range loop, iterating over the contents of an array. Direktiv sets limits for the number of transitions an instance can make in order to protect itself from infinitely-looping workflows. Consider some of the following alternatives if you want to have a loop: Foreach State For range loops like the one in this demo there's another state called a Foreach State that simplifies the logic and splits up a data set to run many actions in parallel without doing lots of transitions. A fuller explanation of the Foreach State will be discussed in another article , but here's an equivalent workflow definition to the demo if you're curious: id : multiposter functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : poster type : foreach array : 'jq(.names[] | { name: . })' action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/posts\" body : name : 'jq(.)' transform : 'jq(del(.names) | .names = [])' Retries One obvious use for loops is to retry some logic if an error occurs, but there's no need to design looping workflow because Direktiv has configurable error catching & retrying available on every action-based state. This will be discussed in a later article. Isolates For large data sets or logic that could needs to loop many times it's generally better to custom-write an Isolate function that performs all of the computation. Writing custom functions is discussed in another article.","title":"Conditional Transitions"},{"location":"getting_started/conditional-transitions/#conditional-transitions","text":"Oftentimes a workflow needs to be a little bit smarter than an immutable sequence of states. That's when you might need conditional transitions. In this article you'll learn about instance input data, the Switch State, and how to define loops.","title":"Conditional Transitions"},{"location":"getting_started/conditional-transitions/#demo","text":"id : multiposter functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : ifelse type : switch conditions : - condition : jq(.names) transition : poster - id : poster type : action action : function : httprequest input : '{ \"method\": \"POST\", \"url\": \"https://jsonplaceholder.typicode.com/posts\", \"body\": { \"name\": .names[0] } }' transform : jq(del(.names[0])) transition : ifelse","title":"Demo"},{"location":"getting_started/conditional-transitions/#input","text":"input : names : - \"Alan\" - \"Jon\" - \"Trent\"","title":"Input"},{"location":"getting_started/conditional-transitions/#output","text":"{ \"names\" : [], \"return\" : { \"id\" : 101 } }","title":"Output"},{"location":"getting_started/conditional-transitions/#instance-input","text":"Workflows can be invoked with input data that will be available as instance data. There are a few ifs-and-buts that apply to input data, but it's not that complicated.","title":"Instance Input"},{"location":"getting_started/conditional-transitions/#input-json-object","text":"If the input is a JSON object it will become the instance data. Input Data { \"key\" : \"value\" } Instance Data { \"key\" : \"value\" }","title":"Input JSON Object"},{"location":"getting_started/conditional-transitions/#input-json-non-object","text":"If the input is valid JSON but not an object it will be stored under \"input\" . Input Data [ 1 , 2 , 3 ] Instance Data { \"input\" : [ 1 , 2 , 3 ] }","title":"Input JSON Non-Object"},{"location":"getting_started/conditional-transitions/#input-non-json","text":"Finally, if the input is not valid JSON it will be base64 encoded into a string and then stored under \"input\" . Hello, world! Instance Data { \"input\" : \"SGVsbG8sIHdvcmxkIQ==\" }","title":"Input Non-JSON"},{"location":"getting_started/conditional-transitions/#switch-state","text":"The Switch State can make decisions about where to transition to next based on the instance data by evaluating a number of jq expressions and checking the results. Here's an example switch state definition: - id : ifelse type : switch conditions : - condition : 'jq(.person.age > 18)' transition : accept #transform: - condition : 'jq(.person.age != nil)' transition : reject #transform: defaultTransition : failure #defaultTransform: Each of the conditions will be evaluated in the order it appears by running the jq command in condition . Any result other than null , false , {} , [] , \"\" , or 0 will cause the condition to be considered a successful match. If no conditions match the default transition will be used. In the demo example the switch state will transition to poster until the list of names is empty, at which point the workflow will end.","title":"Switch State"},{"location":"getting_started/conditional-transitions/#other-conditional-transitions","text":"The Switch State is not the only way to do conditional transitions. The EventsXor state also transitions conditionally based on which CloudEvent was received. All states can also define handlers for catching various types of errors. Both of these will be discussed in a later article.","title":"Other Conditional Transitions"},{"location":"getting_started/conditional-transitions/#loops","text":"By transitioning to a state that has already happened it's possible to create loops in workflow instances. In this demo we've got a type of range loop, iterating over the contents of an array. Direktiv sets limits for the number of transitions an instance can make in order to protect itself from infinitely-looping workflows. Consider some of the following alternatives if you want to have a loop:","title":"Loops"},{"location":"getting_started/conditional-transitions/#foreach-state","text":"For range loops like the one in this demo there's another state called a Foreach State that simplifies the logic and splits up a data set to run many actions in parallel without doing lots of transitions. A fuller explanation of the Foreach State will be discussed in another article , but here's an equivalent workflow definition to the demo if you're curious: id : multiposter functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : poster type : foreach array : 'jq(.names[] | { name: . })' action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/posts\" body : name : 'jq(.)' transform : 'jq(del(.names) | .names = [])'","title":"Foreach State"},{"location":"getting_started/conditional-transitions/#retries","text":"One obvious use for loops is to retry some logic if an error occurs, but there's no need to design looping workflow because Direktiv has configurable error catching & retrying available on every action-based state. This will be discussed in a later article.","title":"Retries"},{"location":"getting_started/conditional-transitions/#isolates","text":"For large data sets or logic that could needs to loop many times it's generally better to custom-write an Isolate function that performs all of the computation. Writing custom functions is discussed in another article.","title":"Isolates"},{"location":"getting_started/error-handling/","text":"Error Handling Handling errors can be an important part of a workflow. In this article you'll learn about timeouts, how to catch errors, retries, and recovery. Demo id : unreliable start : type : scheduled cron : \"0 * * * *\" functions : - id : select image : direktiv/select:v1 type : reusable - id : insert image : direktiv/insert:v1 type : reusable - id : delete image : direktiv/delete:v1 type : reusable - id : cruncher image : direktiv/cruncher:v1 type : reusable - id : notify image : direktiv/notifier:v1 type : reusable states : - id : select-rows type : action action : function : select retries : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] transform : 'jq(.return)' transition : crunch-numbers catch : - error : \"*\" - id : crunch-numbers type : action action : function : cruncher transform : 'jq(.return)' transition : store-some-results - id : store-some-results type : action action : function : insert input : 'jq(.someResults)' transition : store-other-results catch : - error : \"*\" transition : report-failure - id : store-other-results type : action action : function : insert input : 'jq(.otherResults)' catch : - error : \"*\" transition : revert-store-some-results - id : revert-store-some-results type : action action : function : delete input : 'jq(.someResults)' transition : report-failure - id : report-failure type : action action : function : notifier In this demo the select , delete , and insert functions are hypothetical containers that interact with a database, and the crunch function is a hypothetical container that produces some output from an input. The notifier function is a stand-in for whatever method you want to report an error: email, SMS, etc. This demo simulates some sort of database transaction through a workflow in order to demonstrate error handling. In reality you should write a custom Isolate to actually perform a real database transaction if possible. Catchable Errors Errors that occur during instance execution usually are considered \"catchable\". Any workflow state may optionally define error catchers, and if a catchable error is raised Direktiv will check to see if any catchers can handle it. Errors have a \"code\", which is a string formatted in a style similar to a domain name. Error catchers can explicitly catch a single error code or they can use * wildcards in their error codes to catch ranges of errors. Setting the error catcher to just \" * \" means it will handle any error, so long as no catcher defined higher up in the list has already caught it. If no catcher is able to handle an error, the workflow will fail immediately. Uncatchable Errors Rarely, some errors are considered \"uncatchable\", but generally an uncatchable error becomes catchable if escalated to a calling workflow. One example of this is the error triggered by Direktiv if a workflow fails to complete within its maximum timeout. If a workflow fails to complete within its maximum timeout it will not be given an opportunity to catch the error and continue running. But if that workflow is running as a subflow its parent workflow will be able to detect and handle that error. Retries Action definitions may optionally define a retry strategy. If a retry strategy is defined the catcher's transition won't be used and no error will be escalated for retryable errors until all retries have failed. A retry strategy might look like the following: retry : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] In this example you can see that a maximum number of attempts is defined, alongside an initial delay between attempts and a multiplication factor to apply to the delay between subsequent attempts. Recovery Workflows sometimes perform actions which may need to be reverted or undone if the workflow as a whole cannot complete successfully. Solving these problems requires careful use of error catchers and transitions.","title":"Error Handling"},{"location":"getting_started/error-handling/#error-handling","text":"Handling errors can be an important part of a workflow. In this article you'll learn about timeouts, how to catch errors, retries, and recovery.","title":"Error Handling"},{"location":"getting_started/error-handling/#demo","text":"id : unreliable start : type : scheduled cron : \"0 * * * *\" functions : - id : select image : direktiv/select:v1 type : reusable - id : insert image : direktiv/insert:v1 type : reusable - id : delete image : direktiv/delete:v1 type : reusable - id : cruncher image : direktiv/cruncher:v1 type : reusable - id : notify image : direktiv/notifier:v1 type : reusable states : - id : select-rows type : action action : function : select retries : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] transform : 'jq(.return)' transition : crunch-numbers catch : - error : \"*\" - id : crunch-numbers type : action action : function : cruncher transform : 'jq(.return)' transition : store-some-results - id : store-some-results type : action action : function : insert input : 'jq(.someResults)' transition : store-other-results catch : - error : \"*\" transition : report-failure - id : store-other-results type : action action : function : insert input : 'jq(.otherResults)' catch : - error : \"*\" transition : revert-store-some-results - id : revert-store-some-results type : action action : function : delete input : 'jq(.someResults)' transition : report-failure - id : report-failure type : action action : function : notifier In this demo the select , delete , and insert functions are hypothetical containers that interact with a database, and the crunch function is a hypothetical container that produces some output from an input. The notifier function is a stand-in for whatever method you want to report an error: email, SMS, etc. This demo simulates some sort of database transaction through a workflow in order to demonstrate error handling. In reality you should write a custom Isolate to actually perform a real database transaction if possible.","title":"Demo"},{"location":"getting_started/error-handling/#catchable-errors","text":"Errors that occur during instance execution usually are considered \"catchable\". Any workflow state may optionally define error catchers, and if a catchable error is raised Direktiv will check to see if any catchers can handle it. Errors have a \"code\", which is a string formatted in a style similar to a domain name. Error catchers can explicitly catch a single error code or they can use * wildcards in their error codes to catch ranges of errors. Setting the error catcher to just \" * \" means it will handle any error, so long as no catcher defined higher up in the list has already caught it. If no catcher is able to handle an error, the workflow will fail immediately.","title":"Catchable Errors"},{"location":"getting_started/error-handling/#uncatchable-errors","text":"Rarely, some errors are considered \"uncatchable\", but generally an uncatchable error becomes catchable if escalated to a calling workflow. One example of this is the error triggered by Direktiv if a workflow fails to complete within its maximum timeout. If a workflow fails to complete within its maximum timeout it will not be given an opportunity to catch the error and continue running. But if that workflow is running as a subflow its parent workflow will be able to detect and handle that error.","title":"Uncatchable Errors"},{"location":"getting_started/error-handling/#retries","text":"Action definitions may optionally define a retry strategy. If a retry strategy is defined the catcher's transition won't be used and no error will be escalated for retryable errors until all retries have failed. A retry strategy might look like the following: retry : max_attempts : 3 delay : PT30S multiplier : 2.0 codes : [ \".*\" ] In this example you can see that a maximum number of attempts is defined, alongside an initial delay between attempts and a multiplication factor to apply to the delay between subsequent attempts.","title":"Retries"},{"location":"getting_started/error-handling/#recovery","text":"Workflows sometimes perform actions which may need to be reverted or undone if the workflow as a whole cannot complete successfully. Solving these problems requires careful use of error catchers and transitions.","title":"Recovery"},{"location":"getting_started/events/","text":"Events Direktiv has built-in support for CloudEvents, which can be a great way to interact with workflows. In this article you'll learn about events. Demo id : notifier start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : notify type : action action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.\"com.github.pull.create\")' CloudEvents CloudEvents are specification for describing event data in a common way. They're JSON objects with a number of required fields, some optional fields, and a payload. Here's an example CloudEvent: { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } CloudEvents can be sent via the API to a namespace, to be handled by any number of interested receivers on that namespace. Start Types The most common use for events in Direktiv is to have external services generate CloudEvents and send them to Direktiv to trigger your workflows. But to make your workflows trigger on an event you need to register the workflow's interest in the event by adding the appropriate start type to your workflow definition: start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" In this example a new instance will be created from our workflow whenever a cloudevent is received that has the matching type and source values. Two other event-based start types exist in Direktiv: the eventsXor , and the eventsAnd . The eventsXor registers an interest in multiple events and will trigger a new instance as soon as any one of them is received. The eventsAnd also registers an interest in multiple events, but will only trigger once all have been received. Event Payloads Whenever an event is received its payload will be added to the instance data under a field with the same name as the event \"type\". This allows for a uniform approach to accepting events that supports single events, eventsXor, and eventsAnd. The payload itself consists of the full cloudevent including attributes, extension context attributes and data. Instances Waiting for Events Triggering workflows is not the only thing you can do with events. Workflows can be constructed to run some logic and then wait for an event before proceeding. Like the event-based start types, there are three event consuming states: consumeEvent , eventsXor , and eventsAnd . Here's an example of what a ConsumeEvent State could look like: - id : wait-event type : consumeEvent event : type : com.github.pull.create context : source : \"https://github.com/cloudevents/spec/pull\" repository : 'jq(.repo)' timeout : PT5M transform : 'jq(.\"com.github.pull.create\")' transition : next-state Timeouts It's rarely a good idea to leave a workflow waiting indefinitely. Direktiv allows you to define timeouts in ISO8601 format when waiting on an event. If the state is not ready to proceed before the timeout has elapsed an error will be thrown. It's possible to catch this error, but that's for a later article. The timeout field is not required, but Direktiv caps the maximum timeout whether specified or not to prevent workflows from living forever. Context Similar to how the event-based start types have a filters field, event-consuming states have a context field. Like filters, the context field can restrict which events are considered matches by requiring an exact match on a CloudEvent context field. Unlike filters, context values can be determined dynamically based on instance data. GenerateEvent State Workflows can generate events for their namespace without relying on an Isolate using the GenerateEvent State. The fields for this state are fairly self-explanatory. Here's an example: - id : gen-event type : generateEvent event : type : \"my.custom.event\" source : \"direktiv\" data : 'jq(.)' datacontenttype : \"application/json\" If the jq command that populates the data field outputs a plain base64 encoded string and the datacontenttype field is set to anything other than application/json Direktiv will decode the string before sending the event.","title":"Events"},{"location":"getting_started/events/#events","text":"Direktiv has built-in support for CloudEvents, which can be a great way to interact with workflows. In this article you'll learn about events.","title":"Events"},{"location":"getting_started/events/#demo","text":"id : notifier start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : notify type : action action : function : httprequest input : method : \"POST\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.\"com.github.pull.create\")'","title":"Demo"},{"location":"getting_started/events/#cloudevents","text":"CloudEvents are specification for describing event data in a common way. They're JSON objects with a number of required fields, some optional fields, and a payload. Here's an example CloudEvent: { \"specversion\" : \"1.0\" , \"type\" : \"com.github.pull.create\" , \"source\" : \"https://github.com/cloudevents/spec/pull\" , \"subject\" : \"123\" , \"id\" : \"A234-1234-1234\" , \"time\" : \"2018-04-05T17:31:00Z\" , \"comexampleextension1\" : \"value\" , \"comexampleothervalue\" : 5 , \"datacontenttype\" : \"text/xml\" , \"data\" : \"<much wow=\\\"xml\\\"/>\" } CloudEvents can be sent via the API to a namespace, to be handled by any number of interested receivers on that namespace.","title":"CloudEvents"},{"location":"getting_started/events/#start-types","text":"The most common use for events in Direktiv is to have external services generate CloudEvents and send them to Direktiv to trigger your workflows. But to make your workflows trigger on an event you need to register the workflow's interest in the event by adding the appropriate start type to your workflow definition: start : type : event event : type : com.github.pull.create filters : source : \"https://github.com/cloudevents/spec/pull\" In this example a new instance will be created from our workflow whenever a cloudevent is received that has the matching type and source values. Two other event-based start types exist in Direktiv: the eventsXor , and the eventsAnd . The eventsXor registers an interest in multiple events and will trigger a new instance as soon as any one of them is received. The eventsAnd also registers an interest in multiple events, but will only trigger once all have been received.","title":"Start Types"},{"location":"getting_started/events/#event-payloads","text":"Whenever an event is received its payload will be added to the instance data under a field with the same name as the event \"type\". This allows for a uniform approach to accepting events that supports single events, eventsXor, and eventsAnd. The payload itself consists of the full cloudevent including attributes, extension context attributes and data.","title":"Event Payloads"},{"location":"getting_started/events/#instances-waiting-for-events","text":"Triggering workflows is not the only thing you can do with events. Workflows can be constructed to run some logic and then wait for an event before proceeding. Like the event-based start types, there are three event consuming states: consumeEvent , eventsXor , and eventsAnd . Here's an example of what a ConsumeEvent State could look like: - id : wait-event type : consumeEvent event : type : com.github.pull.create context : source : \"https://github.com/cloudevents/spec/pull\" repository : 'jq(.repo)' timeout : PT5M transform : 'jq(.\"com.github.pull.create\")' transition : next-state","title":"Instances Waiting for Events"},{"location":"getting_started/events/#timeouts","text":"It's rarely a good idea to leave a workflow waiting indefinitely. Direktiv allows you to define timeouts in ISO8601 format when waiting on an event. If the state is not ready to proceed before the timeout has elapsed an error will be thrown. It's possible to catch this error, but that's for a later article. The timeout field is not required, but Direktiv caps the maximum timeout whether specified or not to prevent workflows from living forever.","title":"Timeouts"},{"location":"getting_started/events/#context","text":"Similar to how the event-based start types have a filters field, event-consuming states have a context field. Like filters, the context field can restrict which events are considered matches by requiring an exact match on a CloudEvent context field. Unlike filters, context values can be determined dynamically based on instance data.","title":"Context"},{"location":"getting_started/events/#generateevent-state","text":"Workflows can generate events for their namespace without relying on an Isolate using the GenerateEvent State. The fields for this state are fairly self-explanatory. Here's an example: - id : gen-event type : generateEvent event : type : \"my.custom.event\" source : \"direktiv\" data : 'jq(.)' datacontenttype : \"application/json\" If the jq command that populates the data field outputs a plain base64 encoded string and the datacontenttype field is set to anything other than application/json Direktiv will decode the string before sending the event.","title":"GenerateEvent State"},{"location":"getting_started/functions-intro/","text":"Introduction to Functions Workflows wouldn't be very powerful if they were limited to just the predefined states. That's why Direktiv can run \"functions\" which are basically serverless containers (or even a separate workflow, referred to as a subflow ). In this article you'll learn about the Action State and get an introduction to functions. Demo id : httpget functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" This workflow will use the Docker container at https://hub.docker.com/r/direktiv/request to perform a GET request and return the results to the instance data. Not just any Docker container will work as a function, but it isn't difficult to make one compatible. We'll discuss that later. Run this workflow. Leave the Workflow Input empty for now. You should see something like the following: Input {} Output { \"return\" : { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"completed\" : false } } The JSON structure under \"return\" is the object returned by the GET request. What is a function? 'Function' is just a term we use when we run a serverless container. Direktiv grabs a Docker container from an available Docker Registry: hub.docker.com unless custom registries are defined (more on that later). It then runs this container as a \"function\". If the container handles input and output according to our Function requirements it can do just about anything (more on our Function requirements later as well). Function Definitions functions : - id : httprequest image : direktiv/request:v1 type : reusable To use a Function it must first be defined at the top of the workflow definition. Each function definition needs an identifier that must be unique within the workflow definition. For functions of type: reusable , the image field must always be provided, pointing to the desired container image. Action State - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Like all other states, the Action State requires an id and type field identifying it as such. But the great thing about the Action State is its ability to run user-made logic in the form of \"Functions\". The function field must reference one of the functions defined in the workflow definition. In this example we're using direktiv/request , which is a simple container that performs a HTTP request and returns the results. We use a jq command specified in the input field to generate the input for the Function. Once the Function has completed its task in the Action State the results are stored in the instance data under the \"return\" field.","title":"Introduction to Functions"},{"location":"getting_started/functions-intro/#introduction-to-functions","text":"Workflows wouldn't be very powerful if they were limited to just the predefined states. That's why Direktiv can run \"functions\" which are basically serverless containers (or even a separate workflow, referred to as a subflow ). In this article you'll learn about the Action State and get an introduction to functions.","title":"Introduction to Functions"},{"location":"getting_started/functions-intro/#demo","text":"id : httpget functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" This workflow will use the Docker container at https://hub.docker.com/r/direktiv/request to perform a GET request and return the results to the instance data. Not just any Docker container will work as a function, but it isn't difficult to make one compatible. We'll discuss that later. Run this workflow. Leave the Workflow Input empty for now. You should see something like the following:","title":"Demo"},{"location":"getting_started/functions-intro/#input","text":"{}","title":"Input"},{"location":"getting_started/functions-intro/#output","text":"{ \"return\" : { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"completed\" : false } } The JSON structure under \"return\" is the object returned by the GET request.","title":"Output"},{"location":"getting_started/functions-intro/#what-is-a-function","text":"'Function' is just a term we use when we run a serverless container. Direktiv grabs a Docker container from an available Docker Registry: hub.docker.com unless custom registries are defined (more on that later). It then runs this container as a \"function\". If the container handles input and output according to our Function requirements it can do just about anything (more on our Function requirements later as well).","title":"What is a function?"},{"location":"getting_started/functions-intro/#function-definitions","text":"functions : - id : httprequest image : direktiv/request:v1 type : reusable To use a Function it must first be defined at the top of the workflow definition. Each function definition needs an identifier that must be unique within the workflow definition. For functions of type: reusable , the image field must always be provided, pointing to the desired container image.","title":"Function Definitions"},{"location":"getting_started/functions-intro/#action-state","text":"- id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Like all other states, the Action State requires an id and type field identifying it as such. But the great thing about the Action State is its ability to run user-made logic in the form of \"Functions\". The function field must reference one of the functions defined in the workflow definition. In this example we're using direktiv/request , which is a simple container that performs a HTTP request and returns the results. We use a jq command specified in the input field to generate the input for the Function. Once the Function has completed its task in the Action State the results are stored in the instance data under the \"return\" field.","title":"Action State"},{"location":"getting_started/helloworld/","text":"Hello, World! Every workflow begins with a YAML-based \"Workflow Definition\". In this article you'll learn the first basics of the workflow definition. Demo id : helloworld states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' Run this workflow. Leave the Workflow Input empty for now. You should see something like the following: Output { \"msg\" : \"Hello, world!\" } Workflow Definition Now that we've seen it in action, let's go through each of the lines in the Workflow Definition and understand what they mean. Workflow ID id : helloworld Every workflow definition needs to specify a workflow identifier at the top of the document. The identifier is used by UIs and other workflows as a reference to this workflow. This identifier must be unique within the namespace (more on namespaces later). States states : A workflow just wouldn't be a workflow without states to actually do something. Every workflow must have at least one state. State ID - id : hello Like the workflow itself, every state has to have its own identifier. The state identifier is used in logging and to define transitions, which will come up in a later example when we define more than one state. A state identifier must be unique within the workflow definition. State Type type : noop There are many types of state that do all sorts of different things. We'll go over the possible states later, but for this example we're using the noop state. The noop state (short for \"no-operation\") does nothing other than log Instance Data. Transform Command transform : 'jq({ msg: \"Hello, world!\" })' Any state may optionally define a \"transform\", and it's used here to generate the classic \"Hello, World!\" message. Transform applies a jq command to the instance data and replaces the instance data with the results. We'll go into more detail about transforms later.","title":"Helloworld"},{"location":"getting_started/helloworld/#hello-world","text":"Every workflow begins with a YAML-based \"Workflow Definition\". In this article you'll learn the first basics of the workflow definition.","title":"Hello, World!"},{"location":"getting_started/helloworld/#demo","text":"id : helloworld states : - id : hello type : noop transform : 'jq({ msg: \"Hello, world!\" })' Run this workflow. Leave the Workflow Input empty for now. You should see something like the following:","title":"Demo"},{"location":"getting_started/helloworld/#output","text":"{ \"msg\" : \"Hello, world!\" }","title":"Output"},{"location":"getting_started/helloworld/#workflow-definition","text":"Now that we've seen it in action, let's go through each of the lines in the Workflow Definition and understand what they mean.","title":"Workflow Definition"},{"location":"getting_started/helloworld/#workflow-id","text":"id : helloworld Every workflow definition needs to specify a workflow identifier at the top of the document. The identifier is used by UIs and other workflows as a reference to this workflow. This identifier must be unique within the namespace (more on namespaces later).","title":"Workflow ID"},{"location":"getting_started/helloworld/#states","text":"states : A workflow just wouldn't be a workflow without states to actually do something. Every workflow must have at least one state.","title":"States"},{"location":"getting_started/helloworld/#state-id","text":"- id : hello Like the workflow itself, every state has to have its own identifier. The state identifier is used in logging and to define transitions, which will come up in a later example when we define more than one state. A state identifier must be unique within the workflow definition.","title":"State ID"},{"location":"getting_started/helloworld/#state-type","text":"type : noop There are many types of state that do all sorts of different things. We'll go over the possible states later, but for this example we're using the noop state. The noop state (short for \"no-operation\") does nothing other than log Instance Data.","title":"State Type"},{"location":"getting_started/helloworld/#transform-command","text":"transform : 'jq({ msg: \"Hello, world!\" })' Any state may optionally define a \"transform\", and it's used here to generate the classic \"Hello, World!\" message. Transform applies a jq command to the instance data and replaces the instance data with the results. We'll go into more detail about transforms later.","title":"Transform Command"},{"location":"getting_started/isolated-functions/","text":"Isolated Functions An isolated function is written for a specific purpose. It requires an input, presents an output, and is capable of returning error information in the event that anything goes worong - not unlike regular functions. An isolated function is booted up, performs its task, and then immediately terminates. Compared to other function types this is a simple and straight-forward design, but it comes at a cost to performance and latency. Because an isolated function is not a server, it does not receive or return input data via web requests. Instead, specific filepaths are used for input, output, errors, and to tell Direktiv that the function has completed. Input Your function's input data can be found at /direktiv-data/input.json . Logging Write logs to /direktiv-data/out.log . You should probably create this file on startup, even if you don't log anything. Logs should be written to this file. They will be read in line-by-line by Direktiv. Responding If the function executes correctly you must write the results to /direktiv-data/output.json . Standard rules apply. This will usually be JSON, but can be something else, which will be base64 encoded by direktiv. Errors If you want to report that something has gone wrong, write a JSON object to /direktiv-data/error.json with the code and message keys. Same rules as other functions: any provided code makes the error catchable, otherwise it's uncatchable. Example: { \"code\": \"badInput\", \"message\": \"Should be a bool, not an int.\" } Finishing It is critical that you create a file called /direktiv-data/done when you're finished. You don't need to write anything into it, but from the moment it's created you're in a race to the finish. The sidecar will kick in and report the results and terminate the container. You should either report an Error or Respond with results before creating this file. After creating this file, you should exit with exit status zero.","title":"Isolated Functions"},{"location":"getting_started/isolated-functions/#isolated-functions","text":"An isolated function is written for a specific purpose. It requires an input, presents an output, and is capable of returning error information in the event that anything goes worong - not unlike regular functions. An isolated function is booted up, performs its task, and then immediately terminates. Compared to other function types this is a simple and straight-forward design, but it comes at a cost to performance and latency. Because an isolated function is not a server, it does not receive or return input data via web requests. Instead, specific filepaths are used for input, output, errors, and to tell Direktiv that the function has completed.","title":"Isolated Functions"},{"location":"getting_started/isolated-functions/#input","text":"Your function's input data can be found at /direktiv-data/input.json .","title":"Input"},{"location":"getting_started/isolated-functions/#logging","text":"Write logs to /direktiv-data/out.log . You should probably create this file on startup, even if you don't log anything. Logs should be written to this file. They will be read in line-by-line by Direktiv.","title":"Logging"},{"location":"getting_started/isolated-functions/#responding","text":"If the function executes correctly you must write the results to /direktiv-data/output.json . Standard rules apply. This will usually be JSON, but can be something else, which will be base64 encoded by direktiv.","title":"Responding"},{"location":"getting_started/isolated-functions/#errors","text":"If you want to report that something has gone wrong, write a JSON object to /direktiv-data/error.json with the code and message keys. Same rules as other functions: any provided code makes the error catchable, otherwise it's uncatchable. Example: { \"code\": \"badInput\", \"message\": \"Should be a bool, not an int.\" }","title":"Errors"},{"location":"getting_started/isolated-functions/#finishing","text":"It is critical that you create a file called /direktiv-data/done when you're finished. You don't need to write anything into it, but from the moment it's created you're in a race to the finish. The sidecar will kick in and report the results and terminate the container. You should either report an Error or Respond with results before creating this file. After creating this file, you should exit with exit status zero.","title":"Finishing"},{"location":"getting_started/making-functions/","text":"Making Custom Functions Need to do something more than what's supported by Direktiv? You can do just about anything by making your own custom functions to run as Functions. To make things easier Direktiv makes its Functions automatically from Docker images, but you can't just run any Docker image. In this article you'll learn how to make a Direktiv-compatible Docker image. Functions Direktiv functions are only meant to be run for short lengths to time. They're meant to take some input, do something, optionally generate some output, and then return. To enforce this Direktiv will pull the plug on any instance that runs too long, and this timeout is separate from any timeouts defined in a workflow definition. In general each function is one http request from start to end. Input Data The input data comes as post request data for each call to this function. The format is usually JSON but depending on your workflow it can be any data. It is the function-author's responsibility to load this data and extract useful information from it. Input validation is optional, but encouraged. Output Data Data generated by the function should be returned as a response to that request, also usually as JSON. Reporting Errors If something goes wrong a function can report an error to the calling workflow instance by adding HTTP headers to the response. If these headers are populated the execution of the function will be considered a failure regardless of what's stored in response data. The headers to report errors are: Direktiv-ErrorCode and Direktiv-ErrorMessage . If an error message is defined without defining an error code the calling workflow instance will be marked as \"crashed\" without exposing any helpful information, so it's important to always define both. Errors raised by functions are always 'catchable' by their error codes. Here are sample error headers: \"Direktiv-ErrorCode\": \"myapp.input\", \"Direktiv-ErrorMessage\": \"Missing 'customerId' property in JSON input.\" Example Have a look at the source code for one of the functions we've used a lot in these articles: https://github.com/direktiv/direktiv-apps/blob/master/request/main.go FAQs & Other Information Function Logs Logging for functions is a simple HTTP POST or GET request to the address: http://localhost:8889/log?aid=$ACTIONID. If POST is used the body of the request is getting logged for GET requests add a log request parameter. The important parameter is $ACTIONID. Each requests gets an action id which identifies the workflow instance. This parameter has to be passed back to attach the log to the instance. This information is passed in as in the initial request ( Direktiv-ActionID ). Networking Direktiv configures the network so that functions can reach out to the internet and receive responses. In general functions have no externally routable IP address. Internal networking policies will be implemented in the next releases.","title":"Making Custom Functions"},{"location":"getting_started/making-functions/#making-custom-functions","text":"Need to do something more than what's supported by Direktiv? You can do just about anything by making your own custom functions to run as Functions. To make things easier Direktiv makes its Functions automatically from Docker images, but you can't just run any Docker image. In this article you'll learn how to make a Direktiv-compatible Docker image.","title":"Making Custom Functions"},{"location":"getting_started/making-functions/#functions","text":"Direktiv functions are only meant to be run for short lengths to time. They're meant to take some input, do something, optionally generate some output, and then return. To enforce this Direktiv will pull the plug on any instance that runs too long, and this timeout is separate from any timeouts defined in a workflow definition. In general each function is one http request from start to end.","title":"Functions"},{"location":"getting_started/making-functions/#input-data","text":"The input data comes as post request data for each call to this function. The format is usually JSON but depending on your workflow it can be any data. It is the function-author's responsibility to load this data and extract useful information from it. Input validation is optional, but encouraged.","title":"Input Data"},{"location":"getting_started/making-functions/#output-data","text":"Data generated by the function should be returned as a response to that request, also usually as JSON.","title":"Output Data"},{"location":"getting_started/making-functions/#reporting-errors","text":"If something goes wrong a function can report an error to the calling workflow instance by adding HTTP headers to the response. If these headers are populated the execution of the function will be considered a failure regardless of what's stored in response data. The headers to report errors are: Direktiv-ErrorCode and Direktiv-ErrorMessage . If an error message is defined without defining an error code the calling workflow instance will be marked as \"crashed\" without exposing any helpful information, so it's important to always define both. Errors raised by functions are always 'catchable' by their error codes. Here are sample error headers: \"Direktiv-ErrorCode\": \"myapp.input\", \"Direktiv-ErrorMessage\": \"Missing 'customerId' property in JSON input.\"","title":"Reporting Errors"},{"location":"getting_started/making-functions/#example","text":"Have a look at the source code for one of the functions we've used a lot in these articles: https://github.com/direktiv/direktiv-apps/blob/master/request/main.go","title":"Example"},{"location":"getting_started/making-functions/#faqs-other-information","text":"","title":"FAQs &amp; Other Information"},{"location":"getting_started/making-functions/#function-logs","text":"Logging for functions is a simple HTTP POST or GET request to the address: http://localhost:8889/log?aid=$ACTIONID. If POST is used the body of the request is getting logged for GET requests add a log request parameter. The important parameter is $ACTIONID. Each requests gets an action id which identifies the workflow instance. This parameter has to be passed back to attach the log to the instance. This information is passed in as in the initial request ( Direktiv-ActionID ).","title":"Function Logs"},{"location":"getting_started/making-functions/#networking","text":"Direktiv configures the network so that functions can reach out to the internet and receive responses. In general functions have no externally routable IP address. Internal networking policies will be implemented in the next releases.","title":"Networking"},{"location":"getting_started/persistent-data/","text":"Persistent Data Direktiv supports storing and retrieving data that is persisted beyond the scope of a single state or workflow instance. In this article you'll learn about the different scopes and the states that can interact with them. Demo id : counter states : - id : a type : getter variables : - key : x scope : workflow transform : 'jq(.var.x += 1)' transition : b - id : b type : setter variables : - key : x scope : workflow value : 'jq(.var.x)' This demo increments a counter each time the workflow is executed. Scopes There are three scopes for storing persistent data: instance , workflow , and namespace . Data stored in the instance scope only exists for the duration of the running workflow instance. Data stored in the workflow scope exists until the workflow definition is deleted, and is accessible to all instances of that workflow. Data stored in the namespace scope exists until the namespace itself is deleted, and is accessible to all instances of all workflows originating on that namespace. Setter State The Setter State can be used to store any number of variables. Each variable must be explicitly scoped, and the value stored for a variable is generated by the output of a jq query. - id : a type : setter variables : - key : MyVar scope : namespace value : 'jq(.x)' The only way to delete a stored value is to set it to null . Getter State The Getter State is used to retrieve any number of variables in persistent storage. Each variable must be explicitly scoped, and the value retrieved will be stored under .var.KEY where KEY is the variable's name. - id : a type : getter variables : - key : x scope : namespace A key doesn't need to exist in storage to return successfully, but the value returned will be null if it doesn't exist. Concurrency Direktiv makes no effort to guarantee any thread-safety on persistent data. Multiple instances that interact with the same variable may have inconsistent results. Getting & Setting from Functions Getting Accessing persistent data from within a function is a fairly straightforward process. The request that the custom function receives from Direktiv contains a header 'Direktiv-TempDir', which contains all of the variables specified in the function definition. The as , key , scope , and type fields can all play a role in the placement and naming of files within this directory: key The key used to select a variable from within the workflow definition. If no as field is provided, the file on a custom function will correspond to the value of key . scope Which scope to get the variable from: instance , workflow , or namespace . Defaults to instance if omitted. as An optional field used to set the name of the file as it appears on the isolate. type plain The variable data inside of the file will be written 'as-is'. base64 If the variable is stored as base64-encoded data, it will be decoded before being written to the file system. tar If the variable is a valid tar archive, a directory will be created instead of a file, with the contents of the tar archive populating it. tar.gz Similar to tar , this will result in a populated directory being created from a valid .tar.gz file. For example, given the following state definition, a directory named 'myFiles' should exist within the directory specified by the Direktiv-TempDir header. Assuming that this header has a value of /mnt/shared/example , the following structure would be expected: - id : get image : localhost:5000/iv-getter:v1 files : - key : \"myFiles\" scope : instance type : tar /mnt/shared/example/ \u2514\u2500\u2500 myFiles \u2514\u2500\u2500 file-1 \u2514\u2500\u2500 file-2 \u2514\u2500\u2500 file-3 Setting From within an isolate running on Direktiv, variables can be set by sending a POST request: POST http://localhost:8889/var?aid=<EXAMPLE>&scope=instance&key=myFiles Body: <VARIABLE DATA> query parameters aid The action ID, found from the Direktiv-ActionID header of the request being served by the isolate. scope The scope for which the variable is set ( namespace , workflow , or instance ) key The key used by subsequent actions to access the variable. An alternative approach is to write files into certain directories. The direktiv sidecar will store those files as variables. There are three different folders for the three different scopes. For the above example they would be: /mnt/shared/example/out/instance /mnt/shared/example/out/workflow /mnt/shared/example/out/namespace Files under these folders will be stored with their names under the scope of the folder. Diretories will be stored as tar.gz files.","title":"Persistent Data"},{"location":"getting_started/persistent-data/#persistent-data","text":"Direktiv supports storing and retrieving data that is persisted beyond the scope of a single state or workflow instance. In this article you'll learn about the different scopes and the states that can interact with them.","title":"Persistent Data"},{"location":"getting_started/persistent-data/#demo","text":"id : counter states : - id : a type : getter variables : - key : x scope : workflow transform : 'jq(.var.x += 1)' transition : b - id : b type : setter variables : - key : x scope : workflow value : 'jq(.var.x)' This demo increments a counter each time the workflow is executed.","title":"Demo"},{"location":"getting_started/persistent-data/#scopes","text":"There are three scopes for storing persistent data: instance , workflow , and namespace . Data stored in the instance scope only exists for the duration of the running workflow instance. Data stored in the workflow scope exists until the workflow definition is deleted, and is accessible to all instances of that workflow. Data stored in the namespace scope exists until the namespace itself is deleted, and is accessible to all instances of all workflows originating on that namespace.","title":"Scopes"},{"location":"getting_started/persistent-data/#setter-state","text":"The Setter State can be used to store any number of variables. Each variable must be explicitly scoped, and the value stored for a variable is generated by the output of a jq query. - id : a type : setter variables : - key : MyVar scope : namespace value : 'jq(.x)' The only way to delete a stored value is to set it to null .","title":"Setter State"},{"location":"getting_started/persistent-data/#getter-state","text":"The Getter State is used to retrieve any number of variables in persistent storage. Each variable must be explicitly scoped, and the value retrieved will be stored under .var.KEY where KEY is the variable's name. - id : a type : getter variables : - key : x scope : namespace A key doesn't need to exist in storage to return successfully, but the value returned will be null if it doesn't exist.","title":"Getter State"},{"location":"getting_started/persistent-data/#concurrency","text":"Direktiv makes no effort to guarantee any thread-safety on persistent data. Multiple instances that interact with the same variable may have inconsistent results.","title":"Concurrency"},{"location":"getting_started/persistent-data/#getting-setting-from-functions","text":"","title":"Getting &amp; Setting from Functions"},{"location":"getting_started/persistent-data/#getting","text":"Accessing persistent data from within a function is a fairly straightforward process. The request that the custom function receives from Direktiv contains a header 'Direktiv-TempDir', which contains all of the variables specified in the function definition. The as , key , scope , and type fields can all play a role in the placement and naming of files within this directory: key The key used to select a variable from within the workflow definition. If no as field is provided, the file on a custom function will correspond to the value of key . scope Which scope to get the variable from: instance , workflow , or namespace . Defaults to instance if omitted. as An optional field used to set the name of the file as it appears on the isolate. type plain The variable data inside of the file will be written 'as-is'. base64 If the variable is stored as base64-encoded data, it will be decoded before being written to the file system. tar If the variable is a valid tar archive, a directory will be created instead of a file, with the contents of the tar archive populating it. tar.gz Similar to tar , this will result in a populated directory being created from a valid .tar.gz file. For example, given the following state definition, a directory named 'myFiles' should exist within the directory specified by the Direktiv-TempDir header. Assuming that this header has a value of /mnt/shared/example , the following structure would be expected: - id : get image : localhost:5000/iv-getter:v1 files : - key : \"myFiles\" scope : instance type : tar /mnt/shared/example/ \u2514\u2500\u2500 myFiles \u2514\u2500\u2500 file-1 \u2514\u2500\u2500 file-2 \u2514\u2500\u2500 file-3","title":"Getting"},{"location":"getting_started/persistent-data/#setting","text":"From within an isolate running on Direktiv, variables can be set by sending a POST request: POST http://localhost:8889/var?aid=<EXAMPLE>&scope=instance&key=myFiles Body: <VARIABLE DATA> query parameters aid The action ID, found from the Direktiv-ActionID header of the request being served by the isolate. scope The scope for which the variable is set ( namespace , workflow , or instance ) key The key used by subsequent actions to access the variable. An alternative approach is to write files into certain directories. The direktiv sidecar will store those files as variables. There are three different folders for the three different scopes. For the above example they would be: /mnt/shared/example/out/instance /mnt/shared/example/out/workflow /mnt/shared/example/out/namespace Files under these folders will be stored with their names under the scope of the folder. Diretories will be stored as tar.gz files.","title":"Setting"},{"location":"getting_started/scheduling/","text":"Scheduling Sometimes you want a workflow to run periodically. Direktiv supports scheduling based on \"cron\", and in this article you'll see how that's done. Demo id : scraper start : type : scheduled cron : \"0 */2 * * *\" functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" transform : 'jq(.return)' transition : logger - id : logger type : noop log : 'jg(.)' Start Types Workflow definitions can have one of many different start types. Up until now you've left the start section out entirely, which causes it to default , which is appropriate for a direct-invoke/subflow workflow. Now we can have a look at scheduled workflows. start : type : scheduled cron : \"0 */2 * * *\" There's not much to see here. Add the start section, set type to scheduled , and define a valid cron string and away you go! Direktiv prevents scheduled workflows from being directly invoked or used as a subflow, which is why this demo doesn't specify any input data. Just configure the workflow and check the logs over time to see the scheduled workflow in action. Active/Inactive Workflows Every workflow definition can be considered \"active\" or \"inactive\". Being \"active\" doesn't mean that there's an instance running right now, it means that Direktiv will allow instances to be created from it. This setting is part of the API, not a part of the workflow definition. With scheduled workflows we can finally see why this setting could be useful: you can toggle the schedule on and off without modifying the workflow definition itself. Cron Cron is a time-based job scheduler in Unix-like operating systems. Direktiv doesn't run cron, but it does borrow their syntax and expressions for scheduling. In the example above our cron expression is \" 0 */2 * * * \". This tells Direktiv to run the workflow once every two hours. There are many great resources online to help you create your own custom cron expressions.","title":"Scheduling"},{"location":"getting_started/scheduling/#scheduling","text":"Sometimes you want a workflow to run periodically. Direktiv supports scheduling based on \"cron\", and in this article you'll see how that's done.","title":"Scheduling"},{"location":"getting_started/scheduling/#demo","text":"id : scraper start : type : scheduled cron : \"0 */2 * * *\" functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" transform : 'jq(.return)' transition : logger - id : logger type : noop log : 'jg(.)'","title":"Demo"},{"location":"getting_started/scheduling/#start-types","text":"Workflow definitions can have one of many different start types. Up until now you've left the start section out entirely, which causes it to default , which is appropriate for a direct-invoke/subflow workflow. Now we can have a look at scheduled workflows. start : type : scheduled cron : \"0 */2 * * *\" There's not much to see here. Add the start section, set type to scheduled , and define a valid cron string and away you go! Direktiv prevents scheduled workflows from being directly invoked or used as a subflow, which is why this demo doesn't specify any input data. Just configure the workflow and check the logs over time to see the scheduled workflow in action.","title":"Start Types"},{"location":"getting_started/scheduling/#activeinactive-workflows","text":"Every workflow definition can be considered \"active\" or \"inactive\". Being \"active\" doesn't mean that there's an instance running right now, it means that Direktiv will allow instances to be created from it. This setting is part of the API, not a part of the workflow definition. With scheduled workflows we can finally see why this setting could be useful: you can toggle the schedule on and off without modifying the workflow definition itself.","title":"Active/Inactive Workflows"},{"location":"getting_started/scheduling/#cron","text":"Cron is a time-based job scheduler in Unix-like operating systems. Direktiv doesn't run cron, but it does borrow their syntax and expressions for scheduling. In the example above our cron expression is \" 0 */2 * * * \". This tells Direktiv to run the workflow once every two hours. There are many great resources online to help you create your own custom cron expressions.","title":"Cron"},{"location":"getting_started/secrets-registries/","text":"Secrets & Registries Many workflows require sensitive information such as passwords or authentication tokens to access third-party APIs. In this article you'll learn the best way to handle sensitive data such as this so that you don't need to store them as plaintext in workflow definitions. You'll also learn how to source Docker containers from private repositories. Demo id : httpget functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : secrets : [ \"secretToken\" ] function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" headers : \"Content-type\" : \"application/json; charset=UTF-8\" \"Authorization\" : \"bearer jq(.secrets.secretToken)\" This workflow will use a private Docker container marketplace.gcr.io to perform a GET request and return the results to the instance data. Registries Direktiv can store authentication information for a Docker repository on a namespace-by-namespace basis. Creating secrets can be done via the Direktiv API or web interface in the settings page. With the relevant registry defined, functions referencing containers on that registry become accessible. For example, if a registry was created via the api with the following curl command: curl -X 'POST' \\ 'URL/api/functions/registries/namespaces/NAMESPACE' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data\": \"admin:8QwFLg%D$qg*\", \"reg\": \"https://index.docker.io\" }' This registry would be used automatically by Direktiv when running the workflow in the demo. Registry Types There are 3 types of registries that encompass different function scopes: Namespace Registries Namespace registries are applied to all services and functions created in the same namespace. Global Registries Global registries are applied to all services and functions irrelevant to the namespace. These registries are also used by global services. Global Private Registries Global private registries are only used by global services. Note: If a registry is created after a service, the service will need to be recreated to use the latest registry. Secrets Similar to how registry tokens are stored, arbitrary secrets can also be stored. That includes passwords, API tokens, certificates, or anything else. Secrets are stored on a namespace-by-namespace basis as key-value pairs. Secreats can be defined with the Direktiv API or web interface. Wherever actions appear in workflow definitions there's always an optional secrets field. For every secret named in this field, Direktiv will find and decrypt the relevant secret from your namespace and add it to the data from which the action input is generated just before running the jq command that generates that logic. This means your jq commands can reference your secret and place it wherever it needs to be. Direktiv discards the secret-enriched data after generating the action input, so the secrets won't naturally appear in your instance output or logs. But once Direktiv passes that data to your action it has no control over how it's used. It's up to you to ensure your action doesn't log sensitive information and doesn't send sensitive information where it shouldn't go. IMPORTANT: Be especially wary of subflows. Try to avoid passing secrets to subflows if you can, subflows can reference secrets the same way as their parents after all. Remember, your secret-enriched data will become the input for a subflow, which means it will be logged. It's also stored in that subflow's instance data and could be passed around automatically if you're not careful. If your subflow doesn't strip secrets out before it terminates those secrets could also end up in the caller's return object. Security Registry tokens and secrets are stored individually encrypted within Direktiv's database. Each namespace gets its own unique encryption keys, and the decryption key is stored in a different database. For the online Direktiv, these two databases are on different machines and are firewalled apart from one another, and all internal traffic is encrypted. These measures minimize the risk of damaging data breaches, but we still recommend using tokens rather than passwords wherever possible.","title":"Secrets & Registries"},{"location":"getting_started/secrets-registries/#secrets-registries","text":"Many workflows require sensitive information such as passwords or authentication tokens to access third-party APIs. In this article you'll learn the best way to handle sensitive data such as this so that you don't need to store them as plaintext in workflow definitions. You'll also learn how to source Docker containers from private repositories.","title":"Secrets &amp; Registries"},{"location":"getting_started/secrets-registries/#demo","text":"id : httpget functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : getter type : action action : secrets : [ \"secretToken\" ] function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" headers : \"Content-type\" : \"application/json; charset=UTF-8\" \"Authorization\" : \"bearer jq(.secrets.secretToken)\" This workflow will use a private Docker container marketplace.gcr.io to perform a GET request and return the results to the instance data.","title":"Demo"},{"location":"getting_started/secrets-registries/#registries","text":"Direktiv can store authentication information for a Docker repository on a namespace-by-namespace basis. Creating secrets can be done via the Direktiv API or web interface in the settings page. With the relevant registry defined, functions referencing containers on that registry become accessible. For example, if a registry was created via the api with the following curl command: curl -X 'POST' \\ 'URL/api/functions/registries/namespaces/NAMESPACE' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data\": \"admin:8QwFLg%D$qg*\", \"reg\": \"https://index.docker.io\" }' This registry would be used automatically by Direktiv when running the workflow in the demo.","title":"Registries"},{"location":"getting_started/secrets-registries/#registry-types","text":"There are 3 types of registries that encompass different function scopes:","title":"Registry Types"},{"location":"getting_started/secrets-registries/#namespace-registries","text":"Namespace registries are applied to all services and functions created in the same namespace.","title":"Namespace Registries"},{"location":"getting_started/secrets-registries/#global-registries","text":"Global registries are applied to all services and functions irrelevant to the namespace. These registries are also used by global services.","title":"Global Registries"},{"location":"getting_started/secrets-registries/#global-private-registries","text":"Global private registries are only used by global services. Note: If a registry is created after a service, the service will need to be recreated to use the latest registry.","title":"Global Private Registries"},{"location":"getting_started/secrets-registries/#secrets","text":"Similar to how registry tokens are stored, arbitrary secrets can also be stored. That includes passwords, API tokens, certificates, or anything else. Secrets are stored on a namespace-by-namespace basis as key-value pairs. Secreats can be defined with the Direktiv API or web interface. Wherever actions appear in workflow definitions there's always an optional secrets field. For every secret named in this field, Direktiv will find and decrypt the relevant secret from your namespace and add it to the data from which the action input is generated just before running the jq command that generates that logic. This means your jq commands can reference your secret and place it wherever it needs to be. Direktiv discards the secret-enriched data after generating the action input, so the secrets won't naturally appear in your instance output or logs. But once Direktiv passes that data to your action it has no control over how it's used. It's up to you to ensure your action doesn't log sensitive information and doesn't send sensitive information where it shouldn't go. IMPORTANT: Be especially wary of subflows. Try to avoid passing secrets to subflows if you can, subflows can reference secrets the same way as their parents after all. Remember, your secret-enriched data will become the input for a subflow, which means it will be logged. It's also stored in that subflow's instance data and could be passed around automatically if you're not careful. If your subflow doesn't strip secrets out before it terminates those secrets could also end up in the caller's return object.","title":"Secrets"},{"location":"getting_started/secrets-registries/#security","text":"Registry tokens and secrets are stored individually encrypted within Direktiv's database. Each namespace gets its own unique encryption keys, and the decryption key is stored in a different database. For the online Direktiv, these two databases are on different machines and are firewalled apart from one another, and all internal traffic is encrypted. These measures minimize the risk of damaging data breaches, but we still recommend using tokens rather than passwords wherever possible.","title":"Security"},{"location":"getting_started/subflows/","text":"Subflows Just like scripting or programming, with Direktiv it's possible to organize your logic into reusable modules. Anytime a workflow is invoked by another we call it a subflow. In this article you'll learn about namespaces, subflows, and instance data validation. Demo For a subflow demonstration we need to define multiple workflows. 1st Workflow Definition id : notifier functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify - id : notify type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.input)' transition : check-results - id : check-results type : switch conditions : - condition : 'jq(.warnings)' transition : throw - id : throw type : error error : notification.lint message : 'lint errors: %s' args : - 'jq(.warnings)' 2nd Workflow Definition id : worker functions : - id : httprequest image : direktiv/request:v1 type : reusable - id : notifier type : subflow workflow : notifier states : - id : do type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.input)' transition : notify transform : 'jq(del(.return) | .contact = \"Alan\")' - id : notify type : action action : function : notifier input : 'jq({ contact: .contact, payload: .payload })' Input input : contact : \"1\" payload : \"2\" Output { \"contact\" : \"Alan\" , \"payload\" : \"2\" , \"return\" : { \"contact\" : \"Alan\" , \"payload\" : \"2\" , \"return\" : { \"completed\" : false , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"userId\" : 1 } } } Namespaces Before learning about subflows you'll need to know what a \"Namespace\" is. Direktiv organizes everything into namespaces. Think of them a bit like a folder. All of your workflow definitions exist within a namespace, and any instances spawned from those definitions exist within that namespace as well. Any secrets or registries you've set up apply only within the namespace (more on these in a later article). Some limitations are applied on a namespace-level. And frontends may piggyback on the namespaces to handle permissions and multi-tennancy. Workflow identifiers are unique within a namespace, which allows them to be referenced as subflows. Subflows Anywhere an \"Action\" appears in a workflow definition either an Isolate or a Subflow can be run. Here's an example of what a subflow call could look like: id : httpget functions : - id : myworkflow type : subflow workflow : myworkflow states : - id : getter type : action action : function : myworkflow input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Nothing special needs to be done when writing a workflow definition that's intended for use as a subflow. Input is treated exactly the same way as if the workflow was directly invoked with the API, and output is merged into the caller's instance data exactly the same way as if the subflow was an Isolate. Validate State So far we've never demonstrated any way to validate external inputs. Validation is optional, but it can be important for preventing bugs in your workflows caused by unexpected data. Unexpected data could occur in many ways: bad workflow input, bad events, bad Isolate results, or mistakes in your own transforms. Using the Validate State Direktiv can detect these issues and throw an error instead of proceeding. From the example in the demo, here's what a Validate State definition might look like: - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify Unlike any other state, the fields for a Validate State are not fixed. Direktiv takes everything under the schema field and converts it into an equivalent JSON Schema , which it then uses to validate the instance data. Converting JSON Schema from JSON to YAML is straight-forward. Here's the equivalent JSON for the YAML schema in the example above: { \"type\" : \"object\" , \"required\" : [ \"contact\" , \"payload\" ], \"additionalProperties\" : false , \"properties\" : { \"contact\" : { \"type\" : \"string\" }, \"payload\" : { \"type\" : \"string\" } } } There are also many tools online that can convert JSON to YAML for you. If the instance data fails its validation Direktiv will throw a direktiv.schema.failed error, which will terminate the workflow unless an appropriate error catcher is defined (more on error handling in a later article). Error State Speaking of errors, there's another new state in this demo example: the Error State. The Error State is only really useful in the context of subflows. - id : throw type : error error : notification.lint message : 'lint errors: %s' args : - jq(.warnings) If an instance executes an Error State it will store the custom-defined error and mark the instance as failed after the instance terminates. The Error State has an optional transition field just like every other state, which might surprise you. That's because the Error State won't actually cause the workflow to terminate like you might expect. This is to allow the workflow to perform any cleanup, rollback, or recovery logic. The error will still be reported when the instance does finally finish.","title":"Subflows"},{"location":"getting_started/subflows/#subflows","text":"Just like scripting or programming, with Direktiv it's possible to organize your logic into reusable modules. Anytime a workflow is invoked by another we call it a subflow. In this article you'll learn about namespaces, subflows, and instance data validation.","title":"Subflows"},{"location":"getting_started/subflows/#demo","text":"For a subflow demonstration we need to define multiple workflows.","title":"Demo"},{"location":"getting_started/subflows/#1st-workflow-definition","text":"id : notifier functions : - id : httprequest image : direktiv/request:v1 type : reusable states : - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify - id : notify type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.input)' transition : check-results - id : check-results type : switch conditions : - condition : 'jq(.warnings)' transition : throw - id : throw type : error error : notification.lint message : 'lint errors: %s' args : - 'jq(.warnings)'","title":"1st Workflow Definition"},{"location":"getting_started/subflows/#2nd-workflow-definition","text":"id : worker functions : - id : httprequest image : direktiv/request:v1 type : reusable - id : notifier type : subflow workflow : notifier states : - id : do type : action action : function : httprequest input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" body : 'jq(.input)' transition : notify transform : 'jq(del(.return) | .contact = \"Alan\")' - id : notify type : action action : function : notifier input : 'jq({ contact: .contact, payload: .payload })'","title":"2nd Workflow Definition"},{"location":"getting_started/subflows/#input","text":"input : contact : \"1\" payload : \"2\"","title":"Input"},{"location":"getting_started/subflows/#output","text":"{ \"contact\" : \"Alan\" , \"payload\" : \"2\" , \"return\" : { \"contact\" : \"Alan\" , \"payload\" : \"2\" , \"return\" : { \"completed\" : false , \"id\" : 1 , \"title\" : \"delectus aut autem\" , \"userId\" : 1 } } }","title":"Output"},{"location":"getting_started/subflows/#namespaces","text":"Before learning about subflows you'll need to know what a \"Namespace\" is. Direktiv organizes everything into namespaces. Think of them a bit like a folder. All of your workflow definitions exist within a namespace, and any instances spawned from those definitions exist within that namespace as well. Any secrets or registries you've set up apply only within the namespace (more on these in a later article). Some limitations are applied on a namespace-level. And frontends may piggyback on the namespaces to handle permissions and multi-tennancy. Workflow identifiers are unique within a namespace, which allows them to be referenced as subflows.","title":"Namespaces"},{"location":"getting_started/subflows/#subflows_1","text":"Anywhere an \"Action\" appears in a workflow definition either an Isolate or a Subflow can be run. Here's an example of what a subflow call could look like: id : httpget functions : - id : myworkflow type : subflow workflow : myworkflow states : - id : getter type : action action : function : myworkflow input : method : \"GET\" url : \"https://jsonplaceholder.typicode.com/todos/1\" Nothing special needs to be done when writing a workflow definition that's intended for use as a subflow. Input is treated exactly the same way as if the workflow was directly invoked with the API, and output is merged into the caller's instance data exactly the same way as if the subflow was an Isolate.","title":"Subflows"},{"location":"getting_started/subflows/#validate-state","text":"So far we've never demonstrated any way to validate external inputs. Validation is optional, but it can be important for preventing bugs in your workflows caused by unexpected data. Unexpected data could occur in many ways: bad workflow input, bad events, bad Isolate results, or mistakes in your own transforms. Using the Validate State Direktiv can detect these issues and throw an error instead of proceeding. From the example in the demo, here's what a Validate State definition might look like: - id : validate-input type : validate schema : type : object required : - contact - payload additionalProperties : false properties : contact : type : string payload : type : string transition : notify Unlike any other state, the fields for a Validate State are not fixed. Direktiv takes everything under the schema field and converts it into an equivalent JSON Schema , which it then uses to validate the instance data. Converting JSON Schema from JSON to YAML is straight-forward. Here's the equivalent JSON for the YAML schema in the example above: { \"type\" : \"object\" , \"required\" : [ \"contact\" , \"payload\" ], \"additionalProperties\" : false , \"properties\" : { \"contact\" : { \"type\" : \"string\" }, \"payload\" : { \"type\" : \"string\" } } } There are also many tools online that can convert JSON to YAML for you. If the instance data fails its validation Direktiv will throw a direktiv.schema.failed error, which will terminate the workflow unless an appropriate error catcher is defined (more on error handling in a later article).","title":"Validate State"},{"location":"getting_started/subflows/#error-state","text":"Speaking of errors, there's another new state in this demo example: the Error State. The Error State is only really useful in the context of subflows. - id : throw type : error error : notification.lint message : 'lint errors: %s' args : - jq(.warnings) If an instance executes an Error State it will store the custom-defined error and mark the instance as failed after the instance terminates. The Error State has an optional transition field just like every other state, which might surprise you. That's because the Error State won't actually cause the workflow to terminate like you might expect. This is to allow the workflow to perform any cleanup, rollback, or recovery logic. The error will still be reported when the instance does finally finish.","title":"Error State"},{"location":"getting_started/transforms/","text":"Transforms & JQ Every workflow instance always has something called the \"Instance Data\", which is a JSON object that is used to pass data around. Almost everywhere a transition can happen in a workflow definition a transform can also happen allowing the author to filter, enrich, or otherwise modify the instance data. The transform field can contain a valid jq command, which will be applied to the existing instance data to generate a new JSON object that will entirely replace it. Note that only a JSON object will be considered a valid output from this jq command: jq is capable of outputting primitives and arrays, but these are not acceptable output for a transform . Because the Noop State logs its instance data before applying its transform & transition we can follow the results of these transforms throughout the demo. Transforms can be wrapped in 'jq()' or jq() . The difference between the two is that one instructs YAML more explicitly what's in the string. This can be important if you use jq commands containing braces, for example: jq({a: 1}) . Because if this is not explicitly quoted, YAML interprets it incorrectly and throws errors. The quoted form is always valid and generally safer. First Transform The first transform defines a completely new JSON object. Command transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' Resulting Instance Data { \"number\" : 2 , \"objects\" : [ { \"k1\" : \"v1\" } ] } Second Transform The second transform enriches the existing instance data by adding a new field to it. Command transform : 'jq(.multiplier = 10)' Resulting Instance Data { \"multiplier\" : 10 , \"number\" : 2 , \"objects\" : [ { \"k1\" : \"v1\" } ] } Third Transform The third transform multiplies two fields to produce a new field, then pipes the results into another command that deletes two fields. Command transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' Resulting Instance Data { \"objects\" : [ { \"k1\" : \"v1\" } ], \"result\" : 20 } Fourth Transform The fourth transform selects a child object nested within the instance data and makes that into the new instance data. Command transform : 'jq(.objects[0])' Resulting Instance Data { \"k1\" : \"v1\" }","title":"Transforms & JQ"},{"location":"getting_started/transforms/#transforms-jq","text":"Every workflow instance always has something called the \"Instance Data\", which is a JSON object that is used to pass data around. Almost everywhere a transition can happen in a workflow definition a transform can also happen allowing the author to filter, enrich, or otherwise modify the instance data. The transform field can contain a valid jq command, which will be applied to the existing instance data to generate a new JSON object that will entirely replace it. Note that only a JSON object will be considered a valid output from this jq command: jq is capable of outputting primitives and arrays, but these are not acceptable output for a transform . Because the Noop State logs its instance data before applying its transform & transition we can follow the results of these transforms throughout the demo. Transforms can be wrapped in 'jq()' or jq() . The difference between the two is that one instructs YAML more explicitly what's in the string. This can be important if you use jq commands containing braces, for example: jq({a: 1}) . Because if this is not explicitly quoted, YAML interprets it incorrectly and throws errors. The quoted form is always valid and generally safer.","title":"Transforms &amp; JQ"},{"location":"getting_started/transforms/#first-transform","text":"The first transform defines a completely new JSON object. Command transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' Resulting Instance Data { \"number\" : 2 , \"objects\" : [ { \"k1\" : \"v1\" } ] }","title":"First Transform"},{"location":"getting_started/transforms/#second-transform","text":"The second transform enriches the existing instance data by adding a new field to it. Command transform : 'jq(.multiplier = 10)' Resulting Instance Data { \"multiplier\" : 10 , \"number\" : 2 , \"objects\" : [ { \"k1\" : \"v1\" } ] }","title":"Second Transform"},{"location":"getting_started/transforms/#third-transform","text":"The third transform multiplies two fields to produce a new field, then pipes the results into another command that deletes two fields. Command transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' Resulting Instance Data { \"objects\" : [ { \"k1\" : \"v1\" } ], \"result\" : 20 }","title":"Third Transform"},{"location":"getting_started/transforms/#fourth-transform","text":"The fourth transform selects a child object nested within the instance data and makes that into the new instance data. Command transform : 'jq(.objects[0])' Resulting Instance Data { \"k1\" : \"v1\" }","title":"Fourth Transform"},{"location":"getting_started/transitions/","text":"Transitions The ability to string a number of different operations together is a fundamental part of workflows. In this article you'll learn about transitions, transforms, and jq . Demo id : transitioner states : - id : a type : noop transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' transition : b - id : b type : noop transform : 'jq(.multiplier = 10)' transition : c - id : c type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : d - id : d type : noop transform : 'jq(.objects[0])' Output { \"k1\" : \"v1\" } Logs [10:10:30] Beginning workflow triggered by API. [10:10:30] Running state logic -- a:1 (noop) [10:10:30] State data: {} [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: b (1). [10:10:30] Running state logic -- b:2 (noop) [10:10:30] State data: { \"number\": 2, \"objects\": [ { \"k1\": \"v1\" } ] } [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: c (2). [10:10:30] Running state logic -- c:3 (noop) [10:10:30] State data: { \"multiplier\": 10, \"number\": 2, \"objects\": [ { \"k1\": \"v1\" } ] } [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: d (3). [10:10:30] Running state logic -- d:4 (noop) [10:10:30] State data: { \"objects\": [ { \"k1\": \"v1\" } ], \"result\": 20 } [10:10:30] Transforming state data. [10:10:30] Workflow completed. Transitions More than one state can be defined in a workflow definition. Each begins under the states field and multiple states can be differentiated by looking for the dash symbol that denotes a new object in the list of states. In the demo there are four separate states: State 'a' - id : a type : noop transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' transition : b State 'b' - id : b type : noop transform : 'jq(.multiplier = 10)' transition : c State 'c' - id : c type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : d State 'd' - id : d type : noop transform : 'jq(.objects[0])' We've only got Noop States here, but most state types may optionally have a transition field, with a reference to the identifier for a state in the workflow definition. After a state finishes running Direktiv uses this field to figure out whether the instance has reached its end or not. If a transition to another state is defined the instance will continue on to that state. In this demo four Noop States are defined in a simple sequence that goes a \u2192 b \u2192 c \u2192 d . The instance data for each state is inherited from its predecessor, which is why it can be helpful to use Transforms.","title":"Transitions"},{"location":"getting_started/transitions/#transitions","text":"The ability to string a number of different operations together is a fundamental part of workflows. In this article you'll learn about transitions, transforms, and jq .","title":"Transitions"},{"location":"getting_started/transitions/#demo","text":"id : transitioner states : - id : a type : noop transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' transition : b - id : b type : noop transform : 'jq(.multiplier = 10)' transition : c - id : c type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : d - id : d type : noop transform : 'jq(.objects[0])'","title":"Demo"},{"location":"getting_started/transitions/#output","text":"{ \"k1\" : \"v1\" }","title":"Output"},{"location":"getting_started/transitions/#logs","text":"[10:10:30] Beginning workflow triggered by API. [10:10:30] Running state logic -- a:1 (noop) [10:10:30] State data: {} [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: b (1). [10:10:30] Running state logic -- b:2 (noop) [10:10:30] State data: { \"number\": 2, \"objects\": [ { \"k1\": \"v1\" } ] } [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: c (2). [10:10:30] Running state logic -- c:3 (noop) [10:10:30] State data: { \"multiplier\": 10, \"number\": 2, \"objects\": [ { \"k1\": \"v1\" } ] } [10:10:30] Transforming state data. [10:10:30] Transitioning to next state: d (3). [10:10:30] Running state logic -- d:4 (noop) [10:10:30] State data: { \"objects\": [ { \"k1\": \"v1\" } ], \"result\": 20 } [10:10:30] Transforming state data. [10:10:30] Workflow completed.","title":"Logs"},{"location":"getting_started/transitions/#transitions_1","text":"More than one state can be defined in a workflow definition. Each begins under the states field and multiple states can be differentiated by looking for the dash symbol that denotes a new object in the list of states. In the demo there are four separate states:","title":"Transitions"},{"location":"getting_started/transitions/#state-a","text":"- id : a type : noop transform : 'jq({ \"number\": 2, \"objects\": [{ \"k1\": \"v1\" }] })' transition : b","title":"State 'a'"},{"location":"getting_started/transitions/#state-b","text":"- id : b type : noop transform : 'jq(.multiplier = 10)' transition : c","title":"State 'b'"},{"location":"getting_started/transitions/#state-c","text":"- id : c type : noop transform : 'jq(.result = .multiplier * .number | del(.multiplier, .number))' transition : d","title":"State 'c'"},{"location":"getting_started/transitions/#state-d","text":"- id : d type : noop transform : 'jq(.objects[0])' We've only got Noop States here, but most state types may optionally have a transition field, with a reference to the identifier for a state in the workflow definition. After a state finishes running Direktiv uses this field to figure out whether the instance has reached its end or not. If a transition to another state is defined the instance will continue on to that state. In this demo four Noop States are defined in a simple sequence that goes a \u2192 b \u2192 c \u2192 d . The instance data for each state is inherited from its predecessor, which is why it can be helpful to use Transforms.","title":"State 'd'"},{"location":"getting_started/using-functions/","text":"Using Functions Functions exist in the following forms: subflow isolated reusable knative-namespace knative-global This article demonstrates how each of the available function types is used within a workflow. subflow A subflow function allows a workflow to execute another workflow. Assuming that 2 workflows exist, parent and child , the parent workflow could use the child workflow as a'subflow' function. id: parent functions: - id: my-subflow type: subflow workflow: child states: - id: invoke-subflow type: action action: function: my-subflow View this article for more detailed information. isolated Isolated functions are less performant than other types, but can be useful if you need greater isolation or just want to write your own function and prefer the simpler design pattern of interacting with the file-system instead of implementing a server that adheres to our reusable function spec. For more information about isolated functions, click here . id: my-workflow functions: - id: my-isolated-function type: isolated image: example/isolated-function states: - id: invoke-isolated-function type: action action: function: my-isolated-function reusable reusable functions exist only within the scope of the workflow in which they are defined. They can not be shared between workflows. The following code block shows a workflow with a single reusable function defined. This function did not exist before this code block was written. The image field dictates which container image is used to build the function. id: my-workflow functions: - id: my-function type: reusable image: direktiv/request:v1 knative-namespace knative-namespace functions exist within the scope of a single namespace. Multiple workflows within a common namespace can use an existing knative-namespace function, regardless of whether or not other workflows also require it. The following code block shows a workflow that uses a single knative-namespace function. This function must exist, and have a name specified by the service field shown here: id: my-workflow functions: - id: ns-function type: knative-namespace service: ns-function knative-global knative-global functions exist outside of the scope of namespaces, and can be included in any workflow within the Direktiv environment. The following code block shows a workflow that uses a single knative-global function. This function must exist, and have a name specified by the service field shown here: id: my-workflow functions: - id: global-function type: knative-global service: global-function","title":"Using Functions"},{"location":"getting_started/using-functions/#using-functions","text":"Functions exist in the following forms: subflow isolated reusable knative-namespace knative-global This article demonstrates how each of the available function types is used within a workflow.","title":"Using Functions"},{"location":"getting_started/using-functions/#subflow","text":"A subflow function allows a workflow to execute another workflow. Assuming that 2 workflows exist, parent and child , the parent workflow could use the child workflow as a'subflow' function. id: parent functions: - id: my-subflow type: subflow workflow: child states: - id: invoke-subflow type: action action: function: my-subflow View this article for more detailed information.","title":"subflow"},{"location":"getting_started/using-functions/#isolated","text":"Isolated functions are less performant than other types, but can be useful if you need greater isolation or just want to write your own function and prefer the simpler design pattern of interacting with the file-system instead of implementing a server that adheres to our reusable function spec. For more information about isolated functions, click here . id: my-workflow functions: - id: my-isolated-function type: isolated image: example/isolated-function states: - id: invoke-isolated-function type: action action: function: my-isolated-function","title":"isolated"},{"location":"getting_started/using-functions/#reusable","text":"reusable functions exist only within the scope of the workflow in which they are defined. They can not be shared between workflows. The following code block shows a workflow with a single reusable function defined. This function did not exist before this code block was written. The image field dictates which container image is used to build the function. id: my-workflow functions: - id: my-function type: reusable image: direktiv/request:v1","title":"reusable"},{"location":"getting_started/using-functions/#knative-namespace","text":"knative-namespace functions exist within the scope of a single namespace. Multiple workflows within a common namespace can use an existing knative-namespace function, regardless of whether or not other workflows also require it. The following code block shows a workflow that uses a single knative-namespace function. This function must exist, and have a name specified by the service field shown here: id: my-workflow functions: - id: ns-function type: knative-namespace service: ns-function","title":"knative-namespace"},{"location":"getting_started/using-functions/#knative-global","text":"knative-global functions exist outside of the scope of namespaces, and can be included in any workflow within the Direktiv environment. The following code block shows a workflow that uses a single knative-global function. This function must exist, and have a name specified by the service field shown here: id: my-workflow functions: - id: global-function type: knative-global service: global-function","title":"knative-global"},{"location":"installation/","text":"Installation Installing Direktiv can be done with a simple helm install command. The only requirements for a basic installation is a PostgreSQL database and a kubernetes cluster. Direktiv has been tested with Kubernetes and PostgreSQL offerings of all major cloud providers. kubetcl create ns direktiv-services-direktiv helm repo add direktiv https://charts.direktiv.io helm install knative direktiv/knative helm install -n direktiv --create-namespace direktiv direktiv/direktiv The following diagram shows a high-level architecture of Direktiv and the required and optional components. Although a few simple helm command will install a working Direktiv instance there can be other requirements. The following list will explain how to install and configure the individual components. It is possible to deploy them in an order of choice but it is recommended to follow the suggested order listed below. There is also a quick installation guide and a docker image for testing: Run docker image docker run --privileged -p 8080:80 -ti direktiv/direktiv-kube","title":"Install"},{"location":"installation/#installation","text":"Installing Direktiv can be done with a simple helm install command. The only requirements for a basic installation is a PostgreSQL database and a kubernetes cluster. Direktiv has been tested with Kubernetes and PostgreSQL offerings of all major cloud providers. kubetcl create ns direktiv-services-direktiv helm repo add direktiv https://charts.direktiv.io helm install knative direktiv/knative helm install -n direktiv --create-namespace direktiv direktiv/direktiv The following diagram shows a high-level architecture of Direktiv and the required and optional components. Although a few simple helm command will install a working Direktiv instance there can be other requirements. The following list will explain how to install and configure the individual components. It is possible to deploy them in an order of choice but it is recommended to follow the suggested order listed below. There is also a quick installation guide and a docker image for testing:","title":"Installation"},{"location":"installation/#run-docker-image","text":"docker run --privileged -p 8080:80 -ti direktiv/direktiv-kube","title":"Run docker image"},{"location":"installation/database/","text":"Database Direktiv requires a PostgreSQL 13+ database. It acts as datastore as well as pub/sub system between Direktiv's components. It has been tested with Postgres offerings from cloud providers as well as on-premise installations. The following documentation explains how to install a HA PostgreSQL in a Kubernetes cluster. This is just an example and it is important to assess inidividual project requirements. PostgreSQL Installation In this example CrunchyData's postgres operator is used. To install the operator Direktiv provides a helm chart based on this source helm chart. helm repo add direktiv https://charts.direktiv.io helm install -n postgres --create-namespace --set singleNamespace=true postgres direktiv/pgo After installing the operator the PostgreSQL cluster can be created with Kubernetes YAML as custom resource definition: apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : users : - name : direktiv databases : - direktiv image : >- registry.developers.crunchydata.com/crunchydata/crunchy-postgres-ha:centos8-13.4-0 postgresVersion : 13 instances : - name : instance1 dataVolumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 4Gi backups : pgbackrest : image : >- registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:centos8-2.33-2 repoHost : dedicated : {} repos : - name : repo1 volume : volumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 4Gi The above example can be installed with the following command: kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/pg.yaml The details of this PostgreSQL cluster are stored as secrets in the 'postgres' namespace called direktiv-pguser-direktiv . uri port user verifier dbname host password Retrieve the database password secret {% raw %} kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode {% endraw %} \u2757 Please be aware that persistent volume claims are not deleted and must be manually deleted when uninstalling. Backup Direktiv stores all relevant data in the database so that it can be recreated on a new Kubernetes environment without any additional backup or restore of Kubernetes components. CrunchyData's postgres operator comes with 'pgBackRest' as an automated backup solution. Alternatively, a simple cron job can export Direktiv's data as a plain SQL text file. A simple backup can be created with the following Kubernetes cron job: apiVersion : batch/v1beta1 kind : CronJob metadata : name : backupdb spec : schedule : '* 0 * * *' # change to different interval if needed jobTemplate : spec : template : metadata : annotations : linkerd.io/inject : disabled spec : containers : - name : dbbackup image : 'postgres:13.4' imagePullPolicy : IfNotPresent env : - name : PGPASSWORD # change password value : direktivdirektiv # here and add additional commands. the example is bad btw but works :) command : - /bin/sh - '-c' - >- date; echo \"backup start\"; /usr/bin/pg_dump -d direktiv -h postgres-postgresql-ha-pgpool.postgres -f /tmp/data.sql -U direktiv; ls -la /tmp; apt-get update; apt install -y sshpass; sshpass -p \"pwd\" scp -o StrictHostKeyChecking=no /tmp/data.sql username@192.168.1.1:/tmp/data.sql; echo \"copied\" restartPolicy : Never","title":"Database"},{"location":"installation/database/#database","text":"Direktiv requires a PostgreSQL 13+ database. It acts as datastore as well as pub/sub system between Direktiv's components. It has been tested with Postgres offerings from cloud providers as well as on-premise installations. The following documentation explains how to install a HA PostgreSQL in a Kubernetes cluster. This is just an example and it is important to assess inidividual project requirements.","title":"Database"},{"location":"installation/database/#postgresql-installation","text":"In this example CrunchyData's postgres operator is used. To install the operator Direktiv provides a helm chart based on this source helm chart. helm repo add direktiv https://charts.direktiv.io helm install -n postgres --create-namespace --set singleNamespace=true postgres direktiv/pgo After installing the operator the PostgreSQL cluster can be created with Kubernetes YAML as custom resource definition: apiVersion : postgres-operator.crunchydata.com/v1beta1 kind : PostgresCluster metadata : name : direktiv namespace : postgres spec : users : - name : direktiv databases : - direktiv image : >- registry.developers.crunchydata.com/crunchydata/crunchy-postgres-ha:centos8-13.4-0 postgresVersion : 13 instances : - name : instance1 dataVolumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 4Gi backups : pgbackrest : image : >- registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:centos8-2.33-2 repoHost : dedicated : {} repos : - name : repo1 volume : volumeClaimSpec : accessModes : - ReadWriteOnce resources : requests : storage : 4Gi The above example can be installed with the following command: kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/pg.yaml The details of this PostgreSQL cluster are stored as secrets in the 'postgres' namespace called direktiv-pguser-direktiv . uri port user verifier dbname host password Retrieve the database password secret {% raw %} kubectl get secrets -n postgres direktiv-pguser-direktiv -o 'go-template={{index .data \"password\"}}' | base64 --decode {% endraw %} \u2757 Please be aware that persistent volume claims are not deleted and must be manually deleted when uninstalling.","title":"PostgreSQL Installation"},{"location":"installation/database/#backup","text":"Direktiv stores all relevant data in the database so that it can be recreated on a new Kubernetes environment without any additional backup or restore of Kubernetes components. CrunchyData's postgres operator comes with 'pgBackRest' as an automated backup solution. Alternatively, a simple cron job can export Direktiv's data as a plain SQL text file. A simple backup can be created with the following Kubernetes cron job: apiVersion : batch/v1beta1 kind : CronJob metadata : name : backupdb spec : schedule : '* 0 * * *' # change to different interval if needed jobTemplate : spec : template : metadata : annotations : linkerd.io/inject : disabled spec : containers : - name : dbbackup image : 'postgres:13.4' imagePullPolicy : IfNotPresent env : - name : PGPASSWORD # change password value : direktivdirektiv # here and add additional commands. the example is bad btw but works :) command : - /bin/sh - '-c' - >- date; echo \"backup start\"; /usr/bin/pg_dump -d direktiv -h postgres-postgresql-ha-pgpool.postgres -f /tmp/data.sql -U direktiv; ls -la /tmp; apt-get update; apt install -y sshpass; sshpass -p \"pwd\" scp -o StrictHostKeyChecking=no /tmp/data.sql username@192.168.1.1:/tmp/data.sql; echo \"copied\" restartPolicy : Never","title":"Backup"},{"location":"installation/direktiv/","text":"Direktiv Installing direktiv is a two-step process. The first part is to install Knative , the platform to execute Direktiv's serverless functions. The second part is installing and configuring Direktiv itself. If Linkerd is required it needs to be installed first before installing Knative and Direktiv. Knative Knative is an essential part of Direktiv. Although Knative provides YAML files for installation it is recommended to use the helm installation with Direktiv's Helm charts. It uses the correct Knative (> 0.25.0) version and comes pre-configured to work seamlessly with Direktiv. helm repo add direktiv https://charts.direktiv.io helm install -n knative-serving --create-namespace knative direktiv/knative For more configuration options click here . For high availability both Kong ingress controllers, for internal and external services, need to be scaled up. The Helm chart values would be: kong-external : replicaCount : 2 kong-internal : replicaCount : 2 Direktiv kubectl create namespace direktiv-services-direktiv helm install -f direktiv.yaml direktiv direktiv/direktiv For more configuration options click here but the most important configuration values are the database settings which need to be identical to settings used during database setup. database : # -- database host host : \"direktiv-ha.postgres.svc\" # -- database port port : 5432 # -- database user user : \"direktiv\" # -- database password password : \"direktivdirektiv\" # -- database name, auto created if it does not exist name : \"direktiv\" # -- sslmode for database sslmode : require","title":"Direktiv"},{"location":"installation/direktiv/#direktiv","text":"Installing direktiv is a two-step process. The first part is to install Knative , the platform to execute Direktiv's serverless functions. The second part is installing and configuring Direktiv itself. If Linkerd is required it needs to be installed first before installing Knative and Direktiv.","title":"Direktiv"},{"location":"installation/direktiv/#knative","text":"Knative is an essential part of Direktiv. Although Knative provides YAML files for installation it is recommended to use the helm installation with Direktiv's Helm charts. It uses the correct Knative (> 0.25.0) version and comes pre-configured to work seamlessly with Direktiv. helm repo add direktiv https://charts.direktiv.io helm install -n knative-serving --create-namespace knative direktiv/knative For more configuration options click here . For high availability both Kong ingress controllers, for internal and external services, need to be scaled up. The Helm chart values would be: kong-external : replicaCount : 2 kong-internal : replicaCount : 2","title":"Knative"},{"location":"installation/direktiv/#direktiv_1","text":"kubectl create namespace direktiv-services-direktiv helm install -f direktiv.yaml direktiv direktiv/direktiv For more configuration options click here but the most important configuration values are the database settings which need to be identical to settings used during database setup. database : # -- database host host : \"direktiv-ha.postgres.svc\" # -- database port port : 5432 # -- database user user : \"direktiv\" # -- database password password : \"direktivdirektiv\" # -- database name, auto created if it does not exist name : \"direktiv\" # -- sslmode for database sslmode : require","title":"Direktiv"},{"location":"installation/kubernetes/","text":"Kubernetes Direktiv works with Kubernetes offerings from all major cloud providers and the requirements for on-premise or local installations is Kubernetes 1.19+. The following documentation describes a small installation with k3s . K3s Direktiv is using k3s as one of the recommended certified Kubernetes distribution because of its low resource requirements. Although all version starting from 1.20 will most likely work, the installation has been tested with the following versions. Versions v1.20.5+k3s1 v1.21.4+k3s1 Server configuration Direktiv supports Kubernetes setups with seperate server and agents nodes as well as small setups with nodes acting as server and agent in one node. The small setup can use the internal etcd instance whereas seperated installation might use external database. Both installation types achieve high availability. The nodes communicate with each other on different ports and protocols. The following table shows the ports required to be accessible (incoming) for the nodes to enable this. On some Linux distributions firewall changes have to be applied. Please see k3s installation guide for detailed installation instructions. Protocol Port Source Description TCP 6443 K3s agent nodes Kubernetes API Server UDP 8472 K3s server and agent nodes VXLAN TCP 10250 K3s server and agent nodes Kubelet metrics TCP 2379-2380 K3s server nodes Required for HA with embedded etcd only Firewall changes (Centos/RedHat): sudo firewall-cmd --permanent --add-port = 6443 /tcp sudo firewall-cmd --permanent --add-port = 10250 /tcp sudo firewall-cmd --permanent --add-port = 8472 /udp sudo firewall-cmd --permanent --add-port = 2379 -2380/tcp sudo firewall-cmd --reload Info: For additional Centos/RedHat instructions: https://rancher.com/docs/k3s/latest/en/advanced/#additional-preparation-for-red-hat-centos-enterprise-linux One of Kubernetes' requirements is to disable swap on the nodes. This change need to be applied permanently to survive reboots. Disable swap sudo swapoff -a sudo sed -e '/swap/s/^/#/g' -i /etc/fstab Node Installation K3s provides a script to install K3s. It is recommended to use this for installation. The configuration can be done via environment variables during installation. For Direktiv the default ingress controller (Traefik) needs to be disabled because Kong will be used. For installations using the embedded etcd the first server node requires the '--cluster-init' flag. First Server node curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644 --cluster-init\" sh - To add nodes to the cluster the node token is required, which is saved under /var/lib/rancher/k3s/server/node-token . With this token additional nodes can be added. Additional server nodes curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 sh - K3s will download container images during installation. For the downloads of those internet connectivity is required. If the nodes are behind a proxy server the Linux environment variables need to provided to the service, e.g.: curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 HTTP_PROXY = \"http://192.168.1.10:3128\" HTTPS_PROXY = \"http://192.168.1.10:3128\" NO_PROXY = \"localhost,127.0.0.1,svc,.cluster.local,192.168.1.100,192.168.1.101,192.168.1.102,10.0.0.0/8\" sh -","title":"Kubernetes"},{"location":"installation/kubernetes/#kubernetes","text":"Direktiv works with Kubernetes offerings from all major cloud providers and the requirements for on-premise or local installations is Kubernetes 1.19+. The following documentation describes a small installation with k3s .","title":"Kubernetes"},{"location":"installation/kubernetes/#k3s","text":"Direktiv is using k3s as one of the recommended certified Kubernetes distribution because of its low resource requirements. Although all version starting from 1.20 will most likely work, the installation has been tested with the following versions. Versions v1.20.5+k3s1 v1.21.4+k3s1","title":"K3s"},{"location":"installation/kubernetes/#server-configuration","text":"Direktiv supports Kubernetes setups with seperate server and agents nodes as well as small setups with nodes acting as server and agent in one node. The small setup can use the internal etcd instance whereas seperated installation might use external database. Both installation types achieve high availability. The nodes communicate with each other on different ports and protocols. The following table shows the ports required to be accessible (incoming) for the nodes to enable this. On some Linux distributions firewall changes have to be applied. Please see k3s installation guide for detailed installation instructions. Protocol Port Source Description TCP 6443 K3s agent nodes Kubernetes API Server UDP 8472 K3s server and agent nodes VXLAN TCP 10250 K3s server and agent nodes Kubelet metrics TCP 2379-2380 K3s server nodes Required for HA with embedded etcd only Firewall changes (Centos/RedHat): sudo firewall-cmd --permanent --add-port = 6443 /tcp sudo firewall-cmd --permanent --add-port = 10250 /tcp sudo firewall-cmd --permanent --add-port = 8472 /udp sudo firewall-cmd --permanent --add-port = 2379 -2380/tcp sudo firewall-cmd --reload Info: For additional Centos/RedHat instructions: https://rancher.com/docs/k3s/latest/en/advanced/#additional-preparation-for-red-hat-centos-enterprise-linux One of Kubernetes' requirements is to disable swap on the nodes. This change need to be applied permanently to survive reboots. Disable swap sudo swapoff -a sudo sed -e '/swap/s/^/#/g' -i /etc/fstab","title":"Server configuration"},{"location":"installation/kubernetes/#node-installation","text":"K3s provides a script to install K3s. It is recommended to use this for installation. The configuration can be done via environment variables during installation. For Direktiv the default ingress controller (Traefik) needs to be disabled because Kong will be used. For installations using the embedded etcd the first server node requires the '--cluster-init' flag. First Server node curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644 --cluster-init\" sh - To add nodes to the cluster the node token is required, which is saved under /var/lib/rancher/k3s/server/node-token . With this token additional nodes can be added. Additional server nodes curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable servicelb --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 sh - K3s will download container images during installation. For the downloads of those internet connectivity is required. If the nodes are behind a proxy server the Linux environment variables need to provided to the service, e.g.: curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"server --disable traefik --write-kubeconfig-mode=644\" K3S_TOKEN = \"<TOKEN FROM NODE-TOKEN FILE>\" K3S_URL = https://<cluster ip>:6443 HTTP_PROXY = \"http://192.168.1.10:3128\" HTTPS_PROXY = \"http://192.168.1.10:3128\" NO_PROXY = \"localhost,127.0.0.1,svc,.cluster.local,192.168.1.100,192.168.1.101,192.168.1.102,10.0.0.0/8\" sh -","title":"Node Installation"},{"location":"installation/linkerd/","text":"Linkerd (Optional) Linkerd is a lightweight service mesh for Kubernetes and can be used in Direktiv as a mechanism to secure communication between the components. Linkerd can enable mTLS between the core Direktiv pods as well as the containers running in a flow. Installation of Linkerd is optional. The easiest way to install Linkerd is via Helm . The following describes how Linkerd is installed in the Direktiv test docker container . Creating Certificates The identity component of Linkerd requires setting up a trust anchor certificate, and an issuer certificate with its key. The following script starts a container and generates the certificates needed: certDir=$(tmpDir=$(mktemp -d); \\ exe='cd /certs && step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 8760h --no-password --insecure \\ --ca ca.crt --ca-key ca.key'; \\ docker run -v $tmpDir:/certs -i smallstep/step-cli /bin/bash -c \"$exe\"; \\ echo $tmpDir); \u2757 The directory where the certificates are located is stored in $certDir. Install with Helm After creating the certificates the certificate folder should be located at $certDir. The expiry date provided during installation has to be the same as the value for the certificates (in this case: one year). The following script installs Linkerd with the previously generated certificates: helm repo add linkerd https://helm.linkerd.io/stable; \\ exp=$(date -d '+8760 hour' +\"%Y-%m-%dT%H:%M:%SZ\"); \\ helm install linkerd2 \\ --set-file identityTrustAnchorsPEM=$certDir/ca.crt \\ --set-file identity.issuer.tls.crtPEM=$certDir/issuer.crt \\ --set-file identity.issuer.tls.keyPEM=$certDir/issuer.key \\ --set identity.issuer.crtExpiry=$exp \\ linkerd/linkerd2 --wait Annotate Namespaces To use the service mesh (and, in particular, the mTLS communication) between pods within a Direktiv cluster the namespaces need to be annotated for Linkerd to inject its proxy. The default namespaces to annotate are: direktiv postgres knative-serving direktiv-services-direktiv This script will create and annotate the namespaces for ns in \"default\" \"postgres\" \"knative-serving\" \"direktiv-services-direktiv\" do kubectl create namespace $ns || true kubectl annotate ns --overwrite=true $ns linkerd.io/inject=enabled done;","title":"Linkerd"},{"location":"installation/linkerd/#linkerd-optional","text":"Linkerd is a lightweight service mesh for Kubernetes and can be used in Direktiv as a mechanism to secure communication between the components. Linkerd can enable mTLS between the core Direktiv pods as well as the containers running in a flow. Installation of Linkerd is optional. The easiest way to install Linkerd is via Helm . The following describes how Linkerd is installed in the Direktiv test docker container .","title":"Linkerd (Optional)"},{"location":"installation/linkerd/#creating-certificates","text":"The identity component of Linkerd requires setting up a trust anchor certificate, and an issuer certificate with its key. The following script starts a container and generates the certificates needed: certDir=$(tmpDir=$(mktemp -d); \\ exe='cd /certs && step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 8760h --no-password --insecure \\ --ca ca.crt --ca-key ca.key'; \\ docker run -v $tmpDir:/certs -i smallstep/step-cli /bin/bash -c \"$exe\"; \\ echo $tmpDir); \u2757 The directory where the certificates are located is stored in $certDir.","title":"Creating Certificates"},{"location":"installation/linkerd/#install-with-helm","text":"After creating the certificates the certificate folder should be located at $certDir. The expiry date provided during installation has to be the same as the value for the certificates (in this case: one year). The following script installs Linkerd with the previously generated certificates: helm repo add linkerd https://helm.linkerd.io/stable; \\ exp=$(date -d '+8760 hour' +\"%Y-%m-%dT%H:%M:%SZ\"); \\ helm install linkerd2 \\ --set-file identityTrustAnchorsPEM=$certDir/ca.crt \\ --set-file identity.issuer.tls.crtPEM=$certDir/issuer.crt \\ --set-file identity.issuer.tls.keyPEM=$certDir/issuer.key \\ --set identity.issuer.crtExpiry=$exp \\ linkerd/linkerd2 --wait","title":"Install with Helm"},{"location":"installation/linkerd/#annotate-namespaces","text":"To use the service mesh (and, in particular, the mTLS communication) between pods within a Direktiv cluster the namespaces need to be annotated for Linkerd to inject its proxy. The default namespaces to annotate are: direktiv postgres knative-serving direktiv-services-direktiv This script will create and annotate the namespaces for ns in \"default\" \"postgres\" \"knative-serving\" \"direktiv-services-direktiv\" do kubectl create namespace $ns || true kubectl annotate ns --overwrite=true $ns linkerd.io/inject=enabled done;","title":"Annotate Namespaces"},{"location":"installation/summary/","text":"Quick Install Linkerd Create certificates certDir=$(tmpDir=$(mktemp -d); \\ exe='cd /certs && step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 8760h --no-password --insecure \\ --ca ca.crt --ca-key ca.key'; \\ docker run -v $tmpDir:/certs -i smallstep/step-cli /bin/bash -c \"$exe\"; \\ echo $tmpDir); Install Linkerd helm repo add linkerd https://helm.linkerd.io/stable; \\ exp=$(date -d '+8760 hour' +\"%Y-%m-%dT%H:%M:%SZ\"); \\ helm install linkerd2 \\ --set-file identityTrustAnchorsPEM=$certDir/ca.crt \\ --set-file identity.issuer.tls.crtPEM=$certDir/issuer.crt \\ --set-file identity.issuer.tls.keyPEM=$certDir/issuer.key \\ --set identity.issuer.crtExpiry=$exp \\ linkerd/linkerd2 --wait Annotate the Namespaces for ns in \"default\" \"knative-serving\" \"direktiv-services-direktiv\" do kubectl create namespace $ns || true kubectl annotate ns --overwrite=true $ns linkerd.io/inject=enabled done; Database helm repo add direktiv https://charts.direktiv.io helm install -n postgres --create-namespace --set singleNamespace=true postgres direktiv/pgo kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/pg.yaml Knative helm repo add direktiv https://charts.direktiv.io helm install -n knative-serving --create-namespace knative direktiv/knative Direktiv kubectl create namespace direktiv helm install -n direktiv direktiv direktiv/direktiv","title":"Quick Install"},{"location":"installation/summary/#quick-install","text":"","title":"Quick Install"},{"location":"installation/summary/#linkerd","text":"","title":"Linkerd"},{"location":"installation/summary/#create-certificates","text":"certDir=$(tmpDir=$(mktemp -d); \\ exe='cd /certs && step certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \\ && step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 8760h --no-password --insecure \\ --ca ca.crt --ca-key ca.key'; \\ docker run -v $tmpDir:/certs -i smallstep/step-cli /bin/bash -c \"$exe\"; \\ echo $tmpDir);","title":"Create certificates"},{"location":"installation/summary/#install-linkerd","text":"helm repo add linkerd https://helm.linkerd.io/stable; \\ exp=$(date -d '+8760 hour' +\"%Y-%m-%dT%H:%M:%SZ\"); \\ helm install linkerd2 \\ --set-file identityTrustAnchorsPEM=$certDir/ca.crt \\ --set-file identity.issuer.tls.crtPEM=$certDir/issuer.crt \\ --set-file identity.issuer.tls.keyPEM=$certDir/issuer.key \\ --set identity.issuer.crtExpiry=$exp \\ linkerd/linkerd2 --wait","title":"Install Linkerd"},{"location":"installation/summary/#annotate-the-namespaces","text":"for ns in \"default\" \"knative-serving\" \"direktiv-services-direktiv\" do kubectl create namespace $ns || true kubectl annotate ns --overwrite=true $ns linkerd.io/inject=enabled done;","title":"Annotate the Namespaces"},{"location":"installation/summary/#database","text":"helm repo add direktiv https://charts.direktiv.io helm install -n postgres --create-namespace --set singleNamespace=true postgres direktiv/pgo kubectl apply -f https://raw.githubusercontent.com/direktiv/direktiv/main/kubernetes/install/db/pg.yaml","title":"Database"},{"location":"installation/summary/#knative","text":"helm repo add direktiv https://charts.direktiv.io helm install -n knative-serving --create-namespace knative direktiv/knative","title":"Knative"},{"location":"installation/summary/#direktiv","text":"kubectl create namespace direktiv helm install -n direktiv direktiv direktiv/direktiv","title":"Direktiv"}]}